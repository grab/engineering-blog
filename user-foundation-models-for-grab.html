<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>User foundation models for Grab</title>
    <meta name="description" content="Grab has developed a groundbreaking foundation model specifically designed to understand user behavior. Grab's custom solution addresses the unique challenges of a multi-service platform spanning food delivery, ride-hailing, grocery shopping, financial services, and more. The blog delves into the architecture and technical achievements that this innovation is built on.">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Open Graph -->
    <meta property="og:url" content="https://engineering.grab.com/user-foundation-models-for-grab">
    <meta property="og:title" content="User foundation models for Grab">
    <meta property="og:description" content="Grab has developed a groundbreaking foundation model specifically designed to understand user behavior. Grab's custom solution addresses the unique challenges of a multi-service platform spanning food delivery, ride-hailing, grocery shopping, financial services, and more. The blog delves into the architecture and technical achievements that this innovation is built on.">
    <meta property="og:site_name" content="Grab Tech">
    <meta property="og:type" content="article">
    <meta property="og:image" content="https://engineering.grab.com/img/user-found-model-img/banner-4.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">

    <!-- Favicons -->
    <link rel="icon" href="/favicon.ico">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">

    <!-- CSS -->
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,400i,700,700i" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap-theme.min.css">
    <script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="https://engineering.grab.com/user-foundation-models-for-grab">

    <!-- RSS -->
    <link rel="alternate" type="application/rss+xml" title="RSS for Official Grab Tech Blog" href="/feed.xml">
    <!-- OneTrust Cookies Consent Notice start for grab.com -->
    <script type="text/javascript" src="https://cdn-apac.onetrust.com/consent/a3be3527-7455-48e0-ace6-557ddbd506d5/OtAutoBlock.js" ></script>
    <script src="https://cdn-apac.onetrust.com/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="a3be3527-7455-48e0-ace6-557ddbd506d5" ></script>
    <script type="text/javascript">
    function OptanonWrapper() { }
    </script>
    <!-- OneTrust Cookies Consent Notice end for grab.com -->
  </head>

  <body>
    <header class="site-header">
  <div class="wrapper-navbar">
    <div class="site-title-wrapper">
      <div class="row site-title-wrapper-inner">
        <div class="col-xs-1 visible-xs hamburger-nav" id="mobile-menu-btn">
          <div class="menu-btn"></div>
        </div>
        <div class="col-sm-3 col-xs-3">
          <div class="site-title-container">
            <a class="site-title" href="/"></a>
            <span class="site-subtitle">&nbsp;Tech Blog</span>
          </div>
        </div>
        <div class="col-sm-9 col-xs-8 text-right">
          <ul class="nav-category hidden-xs">
            
              
              <li>
                <a href="/categories/engineering/">Engineering</a>
              </li>
            
              
              <li>
                <a href="/categories/data-science/">Data Science</a>
              </li>
            
              
              <li>
                <a href="/categories/design/">Design</a>
              </li>
            
              
              <li>
                <a href="/categories/product/">Product</a>
              </li>
            
              
              <li>
                <a href="/categories/security/">Security</a>
              </li>
            
          </ul>
          <div class="site-search text-right">
            <form action="/search.html" role="search" class="search-form">
  <div class="search-icon"> </div>
  <input type="search" name="q" class="search-text" placeholder="Search...">
  <button class="search-submit"><i class="fa fa-chevron-right"></i></button>
</form>
    

          </div>
        </div>
      </div>
      <!--  Only visible on mobile view -->
      <div class="mobile-menu-container">
        <ul class="mobile-menu">
          
            
            <li>
              <a href="/categories/engineering/">Engineering</a>
            </li>
          
            
            <li>
              <a href="/categories/data-science/">Data Science</a>
            </li>
          
            
            <li>
              <a href="/categories/design/">Design</a>
            </li>
          
            
            <li>
              <a href="/categories/product/">Product</a>
            </li>
          
            
            <li>
              <a href="/categories/security/">Security</a>
            </li>
          
        </ul>
      </div>
    </div>
  </div>
</header>
<script src="/js/main.js"></script>

    <div class="page-content">
      
<div class="wrapper">
  <div class="post">
    <header class="post-header">
      <img src="/img/user-found-model-img/banner-4.png" class="post-cover-photo" alt="User foundation models for Grab cover photo">
      
        
          <a class="post-category" href="/categories/engineering/">Engineering </a>
      

      <h1 class="post-title">User foundation models for Grab</h1>
      
      <div class="post-meta">
        <div class="row">
          <div class="col-xs-12">
            <div class="post-author-thumbnail-container">
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/abhinav-rai.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/boonping-chong.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/chongyu-zhou.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/jenny-kang.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/nick-buhrer.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/sneha-krishnaswamy.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/son-phat-tran.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/zulfikar-layuardi.png">
                
              
            </div>

            <div class="post-meta-text-container">
              <span class="post-author-large">
                
                  
                  
                    
                    <a href="/authors#abhinav-rai">Abhinav Rai</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#boonping-chong">Boon Ping Chong</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#chongyu-zhou">Chongyu Zhou</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#jenny-kang">Jenny Kang</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#nick-buhrer">Nick Buhrer</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#sneha-krishnaswamy">Sneha Krishnaswamy</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#tran-son-phat">Tran Son Phat (Trần Sơn Phát)</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#zulfikar-layuardi">Zulfikar Lazuardi Maulana</a>
                  
                
              </span>
              <span class="post-date-large">26 Sep 2025 | 19 min read</span>
            </div>
          </div>
        </div>
      </div>

    </header>
    <div class="wrapper-content">
      <article class="post-content">
        <h2 id="introduction">Introduction</h2>

<p>Artificial intelligence (AI) is central to Grab’s mission of delivering valuable, personalised experiences to millions of users across Southeast Asia. Achieving this requires a deep understanding of individual preferences, such as their favorite foods, relevant advertisements, spending habits, and more. This personalisation is driven by recommender models, which depend heavily on high-quality representations of the user.</p>

<p>Traditionally, these models have relied on hundreds to thousands of manually engineered features. Examples include the types of food ordered in the past week, the frequency of rides taken, or the average spending per transaction. However, these features were often highly specific to individual tasks, siloed within teams, and required substantial manual effort to create. Furthermore, they struggled to effectively capture time-series data, such as the sequence of user interactions with the app.</p>

<p>With advancements in learning from tabular and sequential data, Grab has developed a foundation model that addresses these limitations. By simultaneously learning from user interactions (clickstream data) and tabular data (e.g. transaction data), the model generates user embeddings that capture app behavior in a more holistic and generalised manner. These embeddings, represented as numerical values, serve as input features for downstream recommender models, enabling higher levels of personalisation and improved performance. Unlike manually engineered features, they generalise effectively across a wide range of tasks, including advertisement optimisation, dual app prediction, fraud detection, and churn probability, among others.</p>

<div class="post-image-section"><figure>
  <img src="/img/user-found-model-img/img-1.png" alt="" style="width:80%" /><figcaption align="middle">Figure 1. The process of building a foundation model involves three steps.</figcaption>
  </figure>
</div>

<p>We build foundation models by first constructing a diverse training corpus encompassing user, merchant, and driver interactions. The pre-trained model can then be used in two ways. Based on <strong>Figure 1</strong>, in 2a we extract user embeddings from the model to serve downstream tasks to improve user understanding. The other path is 2b, where we fine-tune the model to make predictions directly.</p>

<h2 id="crafting-a-foundation-model-for-grabs-users">Crafting a foundation model for Grab’s users</h2>

<p>Grab’s journey towards building its own foundation model began with a clear recognition: existing models are not well-suited to our data. A general-purpose Large Language Model (LLM), for example, lacks the contextual understanding required to interpret why a specific <code class="language-plaintext highlighter-rouge">geohash</code> represents a bustling mall rather than a quiet residential area. Yet, this level of insight is precisely what we need for effective personalisation. This challenge extends beyond IDs, encompassing our entire ecosystem of text, numerical values, locations, and transactions.</p>

<p>Moreover, this rich data exists in two distinct forms: tabular data that captures a user’s long-term profile, and sequential time-series data that reflects their immediate intent. To truly understand our users, we needed a model capable of mastering both forms simultaneously. It became evident that off-the-shelf solutions would not suffice, prompting us to develop a custom foundation model tailored specifically to our users and their unique data.</p>

<h2 id="the-importance-of-data">The importance of data</h2>

<div class="post-image-section"><figure>
  <img src="/img/user-found-model-img/img-2.png" alt="" style="width:80%" /><figcaption align="middle">Figure 2. We use tabular and time-series data to build user embeddings.</figcaption>
  </figure>
</div>

<p>The success of foundation models hinges on the quality and diversity of the datasets used for training. Grab identified two essential sources of data for building user embeddings as shown in <strong>Figure 2</strong>. Tabular data provides general attributes and long-term behavior. Time-series data reflects how the user uses the app and captures the evolution of user preferences.</p>

<ul>
  <li>
    <p><strong>Tabular data</strong>: This classic data source provides general user attributes and insights into long-term behavior. For example, this includes attributes like a user’s age and saved locations, along with aggregated behavioral data such as their average monthly spending or most frequently used service.</p>
  </li>
  <li>
    <p><strong>Time-series clickstream data</strong>: Sequential data captures the dynamic nature of user decision-making and trends. Grab tracks every interaction on its app, including what users view, click, consider, and ultimately transact. Additionally, metrics like the duration between events reveal insights into user decisiveness. Time-series data provides a valuable perspective on evolving user preferences.</p>
  </li>
</ul>

<p>A successful user foundation model must be capable of integrating both tabular and time-series data. Adding to the complexity is the diversity of data modalities, including categorical/text, numerical, user IDs, images, and location data. Each modality carries unique information, often specific to Grab’s business, underscoring the need for a bespoke architecture.</p>

<p>This inherent diversity in data modalities distinguishes Grab from many other platforms. For example, a video recommendation platform primarily deals with a single modality: videos, supplemented by user interaction data such as watch history and ratings. Similarly, social media platforms are largely centred around posts, images, and videos. In contrast, Grab’s identity as a “superapp” generates a far broader spectrum of user actions and data types. As users navigate between ordering food, booking taxis, utilising courier services, and more, their interactions produce a rich and varied data trail that a successful model must be able to comprehend. Moreover, an effective foundation model for Grab must not only create embeddings for our users but also for our merchant-partners and driver-partners, each of whom brings their own distinctive sets of data modalities.</p>

<h3 id="examples-of-data-modalities-at-grab">Examples of data modalities at Grab</h3>

<p>To illustrate the breadth of data, consider these examples across different modalities:</p>

<ul>
  <li>
    <p><strong>Text:</strong> This includes user-provided information such as search queries within GrabFood or GrabMart (“chicken rice,” “fresh milk”) and reviews or ratings for drivers and restaurants. For merchants, this could encompass the restaurant’s name, menu descriptions, and promotional texts.</p>
  </li>
  <li>
    <p><strong>Numerical:</strong> This modality is rich with data points such as the price of a food order, the fare for a ride, the distance of a delivery, the waiting time for a driver, and the commission earned by a driver-partner. User behavior can also be quantified through numerical data, such as the frequency of app usage or average spending over a month.</p>
  </li>
  <li>
    <p><strong>Merchant/User/Driver ID:</strong> These categorical identifiers are central to the platform. A <code class="language-plaintext highlighter-rouge">user_id</code> tracks an individual’s activity across all of Grab’s services. A <code class="language-plaintext highlighter-rouge">merchant_id</code> represents a specific restaurant or store, linking to its menu, location, and order history. A <code class="language-plaintext highlighter-rouge">driver_id</code> corresponds to a driver-partner, associated with their vehicle type, service area, and performance metrics.</p>
  </li>
  <li>
    <p><strong>Location data:</strong> Geographic information is fundamental to Grab’s operations. This includes airport locations, malls, pickup and drop-off points for a ride (<code class="language-plaintext highlighter-rouge">(lat_A, lon_A)</code> to <code class="language-plaintext highlighter-rouge">(lat_B, lon_B)</code>), the delivery address for a food order, and the real-time location of drivers. This data helps in understanding user routines (e.g., commuting patterns) and logistical flows.</p>
  </li>
</ul>

<h3 id="the-challenges-and-opportunities-of-diverse-modalities">The challenges and opportunities of diverse modalities</h3>

<p>The sheer variety of these data modalities presents several significant challenges and opportunities for building a unified user foundation model:</p>

<ul>
  <li>
    <p><strong>Data heterogeneity:</strong> The different data types—text, numbers, geographical coordinates, and categorical IDs do not naturally lend themselves to being combined. Each modality has its own unique structure and requires specialised processing techniques before it can be integrated into a single model.</p>
  </li>
  <li>
    <p><strong>Complex interactions as an opportunity:</strong> The relationships between different modalities are often intricate, revealing a user’s context and intent. A model that only sees one data type at a time will miss the full picture.</p>
  </li>
</ul>

<p>For example, consider a single user’s evening out. The journey begins when they book a ride (involving their <code class="language-plaintext highlighter-rouge">user_id</code> and a <code class="language-plaintext highlighter-rouge">driver_id</code>) to a specific drop-off point, such as a popular shopping mall (<code class="language-plaintext highlighter-rouge">location data</code>). Two hours later, from that same mall location, they open the app again and perform a search for “Japanese food” (<code class="language-plaintext highlighter-rouge">text data</code>). They then browse several restaurant profiles (<code class="language-plaintext highlighter-rouge">merchant_ids</code>) before placing an order, which includes a price (<code class="language-plaintext highlighter-rouge">numerical data</code>).</p>

<p>A traditional, siloed model would treat the ride and the food search as two independent events. However, the real opportunity lies in capturing the interactions within a single user’s journey. This is precisely what our unified foundation model is designed to achieve: to identify the connections and recognise that the <code class="language-plaintext highlighter-rouge">drop-off location</code> of a ride provides valuable context for a subsequent <code class="language-plaintext highlighter-rouge">text search</code>. A model that understands a location is not merely a coordinate, but a place that influences a user’s next action, can develop a far deeper understanding of user context. Unlocking this capability is the key to achieving superior performance in downstream tasks, such as personalisation.</p>

<h2 id="model-architecture">Model architecture</h2>

<div class="post-image-section"><figure>
  <img src="/img/user-found-model-img/img-3.png" alt="" style="width:80%" /><figcaption align="middle">Figure 3. Transformer architecture</figcaption>
  </figure>
</div>

<p><strong>Figure 3</strong> displays Grab’s transformer architecture, enabling joint pre-training on tabular and time-series data with different modalities. Grab’s foundation model is built on a transformer architecture specifically designed to tackle four fundamental challenges inherent to Grab’s superapp ecosystem:</p>

<ol>
  <li>
    <p><strong>Jointly training on tabular and time-series data:</strong> A core requirement is to unify column order invariant tabular data (e.g. user attributes) with order-dependent time-series data (e.g. a sequence of user actions) within a single, coherent model.</p>
  </li>
  <li>
    <p><strong>Handling a wide variety of data modalities:</strong> The model must process and integrate diverse data types, including text, numerical values, categorical IDs, and geographic locations, each requiring its own specialised encoding techniques.</p>
  </li>
  <li>
    <p><strong>Generalising beyond a single task:</strong> The model must learn a universal representation from the entire ecosystem to power a wide array of downstream applications (e.g., recommendations, churn prediction, logistics) across all of Grab’s verticals.</p>
  </li>
  <li>
    <p><strong>Scaling to massive entity vocabularies:</strong> The architecture must efficiently handle predictions across vocabularies containing hundreds of millions of unique entities (users, merchants, drivers), a scale that makes standard classification techniques computationally prohibitive.</p>
  </li>
</ol>

<p>In the following section, we highlight how we tackled each challenge.</p>

<h2 id="1-unifying-tabular-and-time-series-data">1. Unifying tabular and time-series data</h2>

<div class="post-image-section"><figure>
  <img src="/img/user-found-model-img/img-4.png" alt="" style="width:80%" /><figcaption align="middle">Figure 4. Differences between tabular data and time-series data</figcaption>
  </figure>
</div>

<p>A key architectural challenge lies in jointly training on both tabular and time-series data. Tabular data, which contains user attributes, is inherently order-agnostic — the sequence of columns does not matter. In contrast, time-series data is order-dependent, as the sequence of user actions is critical for understanding intent and behavior.</p>

<p>Traditional approaches often process these data types separately or attempt to force tabular data into a sequential format. However, this can result in suboptimal representations, as the model may incorrectly infer meaning from the arbitrary order of columns.</p>

<p>Our solution begins with a novel tokenisation strategy. We define a universal token structure as a <strong><code class="language-plaintext highlighter-rouge">key:value</code></strong> pair.</p>

<ul>
  <li>
    <p>For <strong>tabular data</strong>, the <code class="language-plaintext highlighter-rouge">key</code> is the column name (e.g. <code class="language-plaintext highlighter-rouge">online_hours</code>) and the <code class="language-plaintext highlighter-rouge">value</code> is the user’s attribute (e.g. <code class="language-plaintext highlighter-rouge">4</code>).</p>
  </li>
  <li>
    <p>For <strong>time-series data</strong>, the <code class="language-plaintext highlighter-rouge">key</code> is the event type (e.g. <code class="language-plaintext highlighter-rouge">view_merchant</code>) and the <code class="language-plaintext highlighter-rouge">value</code> is the specific entity involved (e.g. <code class="language-plaintext highlighter-rouge">merchant_id_114</code>).</p>
  </li>
</ul>

<p>This <code class="language-plaintext highlighter-rouge">key:value</code> format creates a common language for all input data. To preserve the distinct nature of each data source, we employ custom positional embeddings and attention masks. These components instruct the model to treat <code class="language-plaintext highlighter-rouge">key:value</code> pairs from tabular data as an unordered set while treating tokens from time-series data as an ordered sequence. This allows the model to benefit from both data structures simultaneously within a single, coherent framework.</p>

<h2 id="2-handling-diverse-modalities-with-an-adapter-based-design">2. Handling diverse modalities with an adapter-based design</h2>

<p>The second major challenge is the sheer variety of data modalities: user IDs, text, numerical values, locations, and more. To manage this diversity, our model uses a flexible <strong>adapter-based design</strong>. Each adapter acts as a specialised “expert” encoder for a specific modality, transforming its unique data format into a unified, high-dimensional vector space.</p>

<ul>
  <li>
    <p>For modalities like <strong>text</strong>, adapters can be initialised with powerful pre-trained language models to leverage their existing knowledge.</p>
  </li>
  <li>
    <p>For ID data like <strong>user/merchant/driver IDs</strong>, we initialise dedicated embedding layers.</p>
  </li>
  <li>
    <p>For complex and specialised data like <strong>location coordinates</strong> or not-so-well-modeled modalities like numbers in existing LLMs, we design custom adapters.</p>
  </li>
</ul>

<p>After each token passes through its corresponding modality adapter, an additional <strong>alignment layer</strong> ensures that all the resulting vectors are projected into the same representation space. This step is critical for allowing the model to compare and combine insights from different data types, for example, to understand the relationship between a text search query (“chicken rice”) and a location pin (a specific hawker center). Finally, we feed the aligned vectors into the main transformer model.</p>

<p>This modular adapter approach is highly scalable and future-proof, enabling us to easily incorporate new modalities like images or audio and upgrade individual components as more advanced architectures become available.</p>

<h2 id="3-unsupervised-pre-training-for-a-complex-ecosystem">3. Unsupervised pre-training for a complex ecosystem</h2>

<p>A powerful model architecture is only half the story; the learning strategy determines the quality and generality of the knowledge captured in the final embeddings.</p>

<p>In the industry, recommender models are often trained using a semi-supervised approach. A model is trained on a specific, supervised objective, such as predicting the next movie a user will watch or whether they will click on an ad. After this training, the internal embeddings, which now carry information fine-tuned for that one task, can be extracted and used for related applications. This method is highly effective for platforms with a relatively homogeneous primary task, like video recommendation or social media platforms.</p>

<p>However, this single-task approach is fundamentally misaligned with the needs of a superapp. At Grab, we need to power a vast and diverse set of downstream use cases, including food recommendations, ad targeting, transport optimisation, fraud detection, and churn prediction. Training a model solely on one of these objectives would create biased embeddings, limiting their utility for all other tasks. Furthermore, focusing on a single vertical like <code class="language-plaintext highlighter-rouge">Food</code> would mean ignoring the rich signals from a user’s activity in <code class="language-plaintext highlighter-rouge">Transport</code>, <code class="language-plaintext highlighter-rouge">GrabMart</code>, and <code class="language-plaintext highlighter-rouge">Financial Services</code>, preventing the model from forming a truly holistic understanding.</p>

<p>Our goal is to capture the complex and diverse interactions between our users, merchants, and drivers across all verticals. To achieve this, we concluded that <strong>unsupervised pre-training</strong> is the most effective path forward. This approach allows us to leverage the full breadth of data available, learning a universal representation of the entire Grab ecosystem without being constrained to a single predictive task.</p>

<p>To pre-train our model on tabular and time-series data, we combine masked language modeling (reconstructing randomly masked tokens) with next action prediction. On a superapp like Grab, a user’s journey is inherently unpredictable. A user might finish a ride and immediately search for a place to eat, or transition from browsing groceries on GrabMart to sending a package with GrabExpress. The next action could belong to any of our diverse services like mobility, deliveries, or financial services.</p>

<p>This ambiguity means the model faces a complex challenge: it’s not enough to predict <em>which</em> item a user might choose; it must first predict the <em>type</em> of interaction they will even initiate. Therefore, to capture the full complexity of user intent, our model performs a dual prediction that directly mirrors our <code class="language-plaintext highlighter-rouge">key:value</code> token structure:</p>

<ul>
  <li>
    <p>It predicts the <strong>type of the next action</strong>, such as <code class="language-plaintext highlighter-rouge">click_restaurant</code>, <code class="language-plaintext highlighter-rouge">book_ride</code>, or <code class="language-plaintext highlighter-rouge">search_mart</code>.</p>
  </li>
  <li>
    <p>It predicts the <strong>value associated with that action</strong>, like the specific restaurant ID, the destination coordinates, or the text of the search query.</p>
  </li>
</ul>

<p>This dual-prediction task forces the model to learn the intricate patterns of user behavior, creating a powerful foundation that can be extended across our entire platform. To handle these predictions, where the output could be of any modality (an ID, a location, text, etc.), we employ modality-specific reconstruction heads. Each head is designed for a particular data type and uses a tailored loss function (e.g. cross-entropy for categorical IDs, mean squared error for numerical values) to accurately evaluate the model’s predictions.</p>

<h2 id="4-the-id-reconstruction-challenge">4. The ID reconstruction challenge</h2>

<p>A significant challenge is the sheer scale of our categorical ID vocabularies. The total number of unique merchants, users, and drivers on the Grab platform runs into the hundreds of millions. A standard cross-entropy loss function would require a final prediction layer with a massive output dimension. For instance, a vocabulary of 100 million IDs with a 768-dimension embedding would result in a prediction head of nearly 80 billion parameters, blowing up model parameter count.</p>

<p>To overcome this, we employ <strong>hierarchical classification</strong>. Instead of predicting from a single flat list of millions of IDs, we first classify IDs into smaller, meaningful groups based on their attributes (e.g. by city, cuisine type, etc). This is followed by a second-stage prediction within that much smaller subgroup. This technique dramatically reduces the computational complexity, making it feasible to learn meaningful representations for an enormous vocabulary of entities.</p>

<h2 id="extracting-value-from-our-foundation-model">Extracting value from our foundation model</h2>

<div class="post-image-section"><figure>
  <img src="/img/user-found-model-img/img-5.png" alt="" style="width:80%" /><figcaption align="middle">Figure 5. Our foundation model is pre-trained with tabular and time-series data.</figcaption>
  </figure>
</div>

<p>Once our foundation model is pre-trained on the vast and diverse data within the Grab ecosystem, it becomes a powerful engine for driving business value. There are two primary pathways to harness its capabilities: fine-tuning and embedding extraction.</p>

<p>The first pathway involves fine-tuning the entire model on a labeled dataset for a specific downstream task, such as churn probability or fraud detection, to create a highly specialised and performant predictor.</p>

<p>The second, more flexible pathway is to use the model to generate powerful pre-trained embeddings. These embeddings serve as rich, general-purpose features that can support a wide range of separate downstream models. The remainder of this section will focus on this second pathway, exploring the types of embeddings we extract and how they empower our applications.</p>

<h2 id="the-dual-embedding-strategy-long-term-and-short-term-memory">The dual-embedding strategy: Long-term and short-term memory</h2>

<p>Our architecture is deliberately designed to produce two distinct but complementary types of user embeddings, providing a holistic view by capturing both the user’s stable, long-term identity and their dynamic, short-term intent.</p>

<h4 id="the-long-term-representation-a-stable-identity-profile">The long-term representation: A stable identity profile</h4>

<p>The long-term embedding captures a user’s persistent habits, established preferences, and overall persona. This representation is the learned vector for a given <code class="language-plaintext highlighter-rouge">user_id,</code> which is stored within the specialised User ID adapter. As the model trains on countless sequences from a user’s history, the adapter learns to distill their consistent behaviors into this single, stable vector. After training, we can directly extract this embedding, which effectively serves as the user’s “long-term memory” on the platform.</p>

<h4 id="the-short-term-representation-a-snapshot-of-recent-intent">The short-term representation: A snapshot of recent intent</h4>

<p>The short-term embedding is designed to capture a user’s immediate context and current mission. To generate this, a sequence of the user’s most recent interactions is processed through the model’s adapters and main transformer block. A <strong>Sequence Aggregation Module</strong> then condenses the transformer’s output into a single vector. This creates a snapshot of recent user intent, reflecting their most up-to-date activities and providing a fresh understanding of what they are trying to accomplish.</p>

<h2 id="scaling-the-foundation-from-terabytes-of-data-to-millions-of-daily-embeddings">Scaling the foundation: From terabytes of data to millions of daily embeddings</h2>

<div class="post-image-section"><figure>
  <img src="/img/user-found-model-img/img-6.png" alt="" style="width:80%" /><figcaption align="middle">Figure 6. Ray framework</figcaption>
  </figure>
</div>

<p>Building a foundation model of this magnitude introduces monumental engineering challenges that extend beyond the model architecture itself. The practical success of our system hinges on our ability to solve two distinct scalability problems:</p>

<ul>
  <li>
    <p><strong>Massive-scale training:</strong> Pre-training our model involves processing terabytes of diverse, multimodal data. This requires a distributed computing framework that is not only powerful but also flexible enough to handle our unique data processing needs efficiently.</p>
  </li>
  <li>
    <p><strong>High-throughput inference:</strong> To keep our user understanding current, we must regenerate embeddings for millions of active users daily. This demands a highly efficient, scalable, and reliable batch processing system.</p>
  </li>
</ul>

<p>To meet these challenges, we built upon the <strong>Ray framework</strong>, an open-source standard for scalable computing. This choice allows us to manage both training and inference within a unified ecosystem, tailored to our specific needs.</p>

<h2 id="core-principle-a-unified-architecture-for-heterogeneous-workloads">Core principle: A unified architecture for heterogeneous workloads</h2>

<p>As illustrated by the Ray framework, both our training and inference pipelines share a fundamental workflow: they begin with a complex Central Processing Unit (CPU) intensive data preprocessing stage (tokenisation), which is followed by a Graphics Processing Unit (GPU) intensive neural network computation.</p>

<p>A naive approach would bundle these tasks together, forcing expensive GPU resources to sit idle while the CPU handles data preparation. Our core architectural principle is to decouple these workloads. Using Ray’s native ability to manage heterogeneous hardware, we create distinct, independently scalable pools of CPU and GPU workers.</p>

<p>This allows for a highly efficient, assembly-line-style process. Data is first ingested by the CPU workers for parallelised tokenisation. The resulting tensors are then streamed directly to the GPU workers for model computation. This separation is the key to achieving near-optimal GPU utilisation, which dramatically reduces costs and accelerates processing times for both training and inference.</p>

<h4 id="distributed-training">Distributed training</h4>

<p>Applying this core principle, our training pipeline efficiently processes terabytes of raw data. The CPU workers handle the complex <code class="language-plaintext highlighter-rouge">key:value</code> tokenisation at scale, ensuring the GPU workers are consistently fed with training batches. This robust setup significantly reduces the end-to-end training time, enabling faster experimentation and iteration. We will go into more detail on our training framework in a future blog post.</p>

<h4 id="efficient-and-scalable-daily-inference">Efficient and scalable daily inference</h4>

<p>This same efficient architecture is mirrored for our daily inference task. To generate fresh embeddings for millions of users, we leverage Ray Data—an open-source library used for data processing in AI and Machine Learning (ML) workload, to execute a distributed batch inference pipeline. The process seamlessly orchestrates our CPU workers for tokenisation and our GPU workers for model application.</p>

<p>This batch-oriented approach is the key to our efficiency, allowing us to process thousands of users’ data simultaneously and maximise throughput. This robust and scalable inference setup ensures that our dozens of downstream systems are always equipped with fresh, high-quality embeddings, enabling the timely and personalised experiences our users expect.</p>

<h2 id="conclusion-a-general-foundation-for-intelligence-across-grab">Conclusion: A general foundation for intelligence across Grab</h2>

<p>The development of our user foundation model marks a pivotal shift in how Grab leverages AI. It moves us beyond incremental improvements on task-specific models toward a general, unified intelligence layer designed to understand our entire ecosystem. While previous efforts at Grab have combined different data modalities, this model is the first to do so at a foundational level, creating a truly holistic and reusable understanding of our users, merchants, and drivers.</p>

<p>The generality of this model is its core strength. By pre-training on diverse and distinct data sources from across our platform—ranging from deep, vertical-specific interactions to broader behavioral signals—it is designed to capture rich, interconnected signals that task-specific models invariably miss. The potential of this approach is immense: a user’s choice of transport can become a powerful signal to inform food recommendations, and a merchant’s location can help predict ride demand.</p>

<p>This foundational approach fundamentally accelerates AI development across the organisation. Instead of starting from scratch, teams can now build new models on top of our high-quality, pre-trained embeddings, significantly reducing development time and improving performance. Existing models can be enhanced by incorporating these rich features, leading to better predictions and more personalised user experiences. Key areas such as ad optimisation, dual app prediction, fraud detection, and churn probability already heavily benefit from our foundation model, but this is just the beginning.</p>

<h2 id="our-vision-for-the-future">Our vision for the future</h2>

<p>Our work on this foundation model is just the beginning. The ultimate goal is to deliver “embeddings as a product”. A stable, reliable, and powerful basis for any AI-driven application at Grab. While our initial embeddings for users, driver-partners, and merchant-partners have already proven their value, our vision extends to becoming the central provider for all fundamental entities within our ecosystem, including Locations, Bookings, Marketplace items, and more.</p>

<p>To realise this vision, we are focused on a path of continuous improvement across several key areas:</p>

<ul>
  <li>
    <p><strong>Unifying and enriching our datasets:</strong> Our current success comes from leveraging distinct, powerful data sources that capture different facets of the user journey. The next frontier is to unify these streams into a single, cohesive training corpus that holistically represents user activity across all of Grab’s services. This effort will create a comprehensive, low-noise view of user behavior, unlocking an even deeper level of insight.</p>
  </li>
  <li>
    <p><strong>Evolving the model architecture:</strong> We will continue to evolve the model itself, focusing on research to enhance its learning capabilities and predictive power to make the most of our increasingly rich data.</p>
  </li>
  <li>
    <p><strong>Improving scale and efficiency:</strong> As Grab grows, so must our systems. We are dedicated to further scaling our training and inference infrastructure to handle more data and complexity at an even greater efficiency.</p>
  </li>
</ul>

<p>By providing a continuously improving, general-purpose understanding of these core components, we are not just building a better model; we are building a more intelligent future for Grab. This enables us to innovate faster and deliver exceptional value to the millions who rely on our platform every day.</p>

<h2 id="join-us">Join us</h2>

<p>Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.</p>

<p>Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, <a href="https://www.grab.careers/en/">join our team</a> today!</p>

      </article>
      <div>
        
          <div class="post-tags">
  
  
    <a href="/tags#ai" class="label tags-label">ai</a>
  
    <a href="/tags#artificial-intelligence" class="label tags-label">artificial-intelligence</a>
  
    <a href="/tags#llm" class="label tags-label">llm</a>
  
    <a href="/tags#machine-learning" class="label tags-label">machine-learning</a>
  
</div>

        
        <br>
      </div>
      <div class="sharing-links text-right">
  Share on &nbsp;
  <a href="https://twitter.com/intent/tweet?text=User foundation models for Grab&url=https://engineering.grab.com/user-foundation-models-for-grab&via=grabengineering&related=grabengineering" class="btn btn-sm btn-share btn-share-twitter" rel="nofollow" target="_new" title="Share on Twitter" onclick="onShareButtonClick(this); return false;"><i class="fa fa-lg fa-twitter"></i>&nbsp; Twitter</a>
  <a href="https://facebook.com/sharer.php?u=https://engineering.grab.com/user-foundation-models-for-grab" class="btn btn-sm btn-share btn-share-facebook" rel="nofollow" target="_new" title="Share on Facebook" onclick="onShareButtonClick(this); return false;"><i class="fa fa-lg fa-facebook"></i>&nbsp; Facebook</a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://engineering.grab.com/user-foundation-models-for-grab&title=User foundation models for Grab
&summary=Grab has developed a groundbreaking foundation model specifically designed to understand user behavior. Grab's custom solution addresses the unique challenges of a multi-service platform spanning food delivery, ride-hailing, grocery shopping, financial services, and more. The blog delves into the architecture and technical achievements that this innovation is built on.&source=Grab Tech" class="btn btn-sm btn-share btn-share-linkedin" rel="nofollow" target="_new" title="Share on LinkedIn" onclick="onShareButtonClick(this); return false;"><i class="fa fa-lg fa-linkedin"></i>&nbsp; LinkedIn</a>
</div>
<script>
  function onShareButtonClick(button) {
    var width = 600;
    var height = 600;
    var left = (window.screen.width / 2) - (width / 2);
    var top = (window.screen.height / 2) - (height / 2);
    window.open(button.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=' + height + ',width=' + width + ',top=' + top + ',left=' + left);
    return false;
  }
</script>

      <hr class="section-divider">

      <br/>
      <!-- 
        <div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    this.page.url = 'https://engineering.grab.com/user-foundation-models-for-grab';
    this.page.identifier = '/user-foundation-models-for-grab';
  };
  (function() {
    var d = document, s = d.createElement('script');
    s.src = '//grabengineering.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

       -->

    </div>
  </div>
</div>

    </div>
    <div class="progress-wrap">
    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98" />
    </svg>
    <i class="fa fa-chevron-up btt-btn"></i>
</div>
    <footer class="site-footer">
  <div class="wrapper">
    <div class="row">
      <div class="col-sm-6 col-xs-12">
        <h2 class="footer-heading">Grab Tech</h2>
        <ul class="social-media-list">
  
    <li>
      <a href="https://github.com/grab" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-github fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://www.linkedin.com/company/grabapp" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-linkedin fa-lg"></i>
      </a>
    </li>
  
  <li>
    <a href="https://engineering.grab.com/feed.xml" target="_blank">
      <i class="fa fa-rss fa-lg"></i>
    </a>
  </li>
</ul>

        <div>
          <script src="//platform.linkedin.com/in.js" type="text/javascript"> lang: en_US</script>
          <script type="IN/FollowCompany" data-id="5382086" data-counter="right"></script>
        </div>        
        <br>
      </div>
      <div class="col-sm-6 col-xs-12 hiring-section">
        <h2 class="footer-heading">Join Us</h2>
        <p class="text">
          Want to join us in our mission to revolutionize transportation?
        </p>
        <a class="btn" href="https://grab.careers" target="_blank">View open positions</a>

      </div>
    </div>
    
  <!-- Google Tag Manager -->
  <script>
    (function (w, d, s, l, i) {
      w[l] = w[l] || [];
      w[l].push({
        'gtm.start': new Date().getTime(),
        event: 'gtm.js'
      });
      var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s),
        dl = l != 'dataLayer' ? '&l=' + l : '';
      j.async = true;
      j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
      f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-T3CT72T');
  </script>
  <!-- End Google Tag Manager -->

  <!-- Old script 
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'GTM-T3CT72T', 'auto');
    ga('send', 'pageview');
  </script> -->
<!-- End of olf script -->


  </body>
</html>
