I"’9<h1 id="introduction">Introduction</h1>

<p>As Grab grew from a small startup to an organisation serving millions of consumers and driver-partners, making day-to-day data-driven decisions became paramount. We needed a system to efficiently ingest data from mobile apps and backend systems and then make it available for analytics and engineering teams.</p>

<p>Thanks to modern data processing frameworks, ingesting data isnâ€™t a big issue. However, at Grabâ€™s scale, it is a non-trivial task. We had to prepare for two key scenarios:</p>

<ul>
  <li>Business growth, including organic growth over time and expected <a href="https://en.wikipedia.org/wiki/Seasonality">seasonality</a>Â effects.</li>
  <li>Any unexpected peaks due to unforeseen circumstances. Our systems have to be <a href="https://en.wikipedia.org/wiki/Scalability">horizontally scalable</a>.</li>
</ul>

<p>We could ingest data in batches, in real time, or a combination of the two. When you ingest data in batches, you can import it at regularly scheduled intervals or when it reaches a certain size. This is very useful when processes run on a schedule, such as reports that run daily at a specific time. Typically, batched data is useful for offline analytics and data science.</p>

<p>On the other hand, real-time ingestion has significant <a href="https://www.forbes.com/sites/forbestechcouncil/2017/08/08/the-value-of-real-time-data-analytics/#459fc6d61220">business value</a>, such as with <a href="https://www.reactivemanifesto.org/">reactive systems</a>. For example, when a consumer provides feedback for a Grab superappÂ widget, we re-rank widgets based on that consumerâ€™s likes or dislikes.Â Note that when information is very time-sensitive, you must continuously monitor its data.</p>

<p>This blog post describes how Grab built a scalable data ingestion system and how we went from prototyping with Spark Streaming to running a production-grade data processing cluster written in Golang.</p>

<h1 id="building-the-system-without-reinventing-the-wheel">Building the System Without Reinventing the Wheel</h1>

<p>The data ingestion system:</p>

<ol>
  <li>Collects raw data as app events.</li>
  <li>Transforms the data into a structured format.</li>
  <li>Stores the data for analysis and monitoring.</li>
</ol>

<p>In a <a href="https://engineering.grab.com/experimentation-platform-data-pipeline">previous blog post</a>, we discussed dealing with batched data ETL with Spark. This post focusesÂ on real-time ingestion.</p>

<p>We separated the data ingestion system into 3 layers: collection, transformation, and storage. This table and diagram highlights the tools used in each layer in our systemâ€™s first design.</p>

<table class="table">
  <thead>
    <tr><th>Layer</th><th>Tools</th></tr>
  </thead>
  <tbody>
    <tr>
      <td>Collection</td>
      <td>Gateway, <a href="https://kafka.apache.org/">Kafka</a></td>
    </tr>
    <tr>
      <td>Transformation</td>
      <td>Go processing service, <a href="https://spark.apache.org/streaming/">Spark Streaming</a></td>
    </tr>
    <tr>
      <td>Storage</td>
      <td><a href="https://engineering.grab.com/big-data-real-time-presto-talariadb">TalariaDB</a></td>
    </tr>
  </tbody>
</table>

<p><img src="img/data-ingestion-transformation-product-insights/image3.png" alt="" /></p>

<p>Our first design might seem complex, but we used battle-tested and common tools such as Apache <a href="https://kafka.apache.org/uses">Kafka</a> and <a href="https://spark.apache.org/streaming/">Spark Streaming</a>. This let us get an end-to-end solution up and running quickly.</p>

<h3 id="collection-layer">Collection Layer</h3>

<p>Our collection layer had two sub-layers:</p>

<ol>
  <li>Our custom built API Gateway received HTTP requests from the mobile app. It simplyÂ decoded and authenticated HTTP requests, streaming the data to the Kafka queue.</li>
  <li>The Kafka queue decoupled the transformation layer (shown in the above figure as the processing service and Spark streaming) from the collection layer (shown above as the Gateway service). We needed to retain raw data in the Kafka queue for <a href="https://en.wikipedia.org/wiki/Fault_tolerance">fault tolerance</a>Â of the entire system. Imagine an error where a data pipeline pollutes the data with flawed transformation code or just simply crashes. The Kafka queue saves us from data loss by data backfilling.</li>
</ol>

<p>Since itâ€™s robust and battle-tested, we chose Kafka as our queueing solution. It perfectly met our requirements, such as high throughput and low latency. Although Kafka takes some operational effort such as self-hosting and monitoring, Grab has a proficient and dedicated team managing our Kafka cluster.</p>

<h3 id="transformation-layer">Transformation Layer</h3>

<p>There are many options for real-time data processing, including <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark</a><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Â Streaming</a>, <a href="https://flink.apache.org/">Flink</a>, and <a href="http://storm.apache.org/">Storm</a>. Since we use Spark for all our batch processing, we decided to use Spark Streaming.</p>

<p>We deployed a Golang processing service between Kafka and Spark Streaming. This service converts the data from <a href="https://developers.google.com/protocol-buffers/">Protobuf</a> to <a href="https://avro.apache.org/docs/current/">Avro</a>. Instead of pointing Spark Streaming directly to Kafka, we used this processing service as an intermediary. This was because our Spark Streaming job was written in Python and Spark doesnâ€™t natively support <a href="https://developers.google.com/protocol-buffers/">protobuf</a>Â decoding.Â We used Avro format, since Grab historically used it for archiving streaming data. Each raw event was enriched and batched together with other events. Batches were then uploaded to S3.</p>

<h3 id="storage-layer">Storage Layer</h3>

<p><a href="https://engineering.grab.com/big-data-real-time-presto-talariadb">TalariaDB</a> is a Grab-built time-series database. It ingests events as columnar ORC files, indexing them by event name and time. We use the same ORCÂ format files for batch processing. TalariaDB also implements the Presto Thrift connector interface, so our users could query certain event types by time range. They did this by connecting a Presto to a TalariaDB hosting distributed cluster.</p>

<h1 id="problems">Problems</h1>

<p>Building and deploying our data pipelineâ€™s <a href="https://en.wikipedia.org/wiki/Minimum_viable_product">MVP</a> provided great value to our data analysts, engineers, and QA team. For example, our mobile app team could monitor any abnormal change in the real-time metrics, such as the screen load time for the latest released app version. The QA team could perform app side actions (book a ride, make payment, etc.) and check which events were triggered and received by the backend. The latency between the ingestion and the serving layer was only 4 minutes instead of the batch processing systemâ€™s 60 minutes. The streaming processingâ€™s data showed good business value.</p>

<p>This prompted us to develop more features on top of our platform-collected real-time data. Very soon our QA engineers and the product analytics team used more and more of the real-time data processing system. They started <a href="https://en.wikipedia.org/wiki/Instrumentation_(computer_programming)">instrumenting</a>Â various mobile applications so more data started flowing in. However,Â asÂ our ingestedÂ data increased, so did our problems. These were mostly related to operational complexity and the increased latency.</p>

<h3 id="operational-complexity">Operational Complexity</h3>

<p>Only a few team members could operate Spark Streaming and EMR. With more dataÂ and variable rates, our streaming jobs had scaling issues and failed occasionally. This was due to checkpoint issues when the cluster was under heavy load. Increasing the cluster size helped, but adding more nodes also increased the likelihood of losing more cluster nodes. When we lost nodes, our latency went up and added more work for our already busy on-call engineers.</p>

<h3 id="supporting-native-protobuf">Supporting Native Protobuf</h3>

<p>To simplify the architecture, we initially planned to bypass our Golang-written processing service forÂ theÂ real-time data pipeline. Our plan was to let Spark directly talk to the Kafka queueÂ and send the output to S3. This required packaging the decoders for our protobuf messages for Python Spark jobs, which was cumbersome. We thought about rewriting our job in Scala, but we didnâ€™t have enough experience with it.</p>

<p>Also, weâ€™d soon hit some streaming limits from S3. Our Spark streaming job was consuming objects from S3, but the process wasÂ notÂ continuous due toÂ S3â€™s <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">Â eventual consistency</a>. To avoid long pagination queries in the S3 API, we had to prefix the data with the hour in which itÂ was ingested. This resulted in some data lossÂ after processing by the Spark streaming. The loss happened because the new data would appear in S3 while Spark Streaming hadÂ already moved on to the next hour.Â We tried various tweaks, but it was just a bad design. As our data grew to over one terabyte per hour, our dataÂ loss grew with it.</p>

<p><img src="img/data-ingestion-transformation-product-insights/image1.png" alt="" /></p>

<h3 id="processing-lag">Processing Lag</h3>

<p>On average, the time from our system ingesting an event to when it was available on the Presto was 4 to 6 minutes. We call that processing lag, as it happened due to our data processing. It was substantially worse under heavy loads, increasing to 8 to 13 minutes. While that wasnâ€™t bad at this scale (a few TBs of data), it made some use cases impossible, such as monitoring. We needed to do better.</p>

<h2 id="simplifying-the-architecture-and-rewriting-in-golang">Simplifying the Architecture and Rewriting in Golang</h2>

<p>After completing the MVP phase development, we noticed the Spark Streaming functionality we actually usedÂ was relatively trivial. In the Spark Streaming job, we only:</p>

<ul>
  <li>PartitionedÂ the batch of events by event name.</li>
  <li>EncodedÂ the data in ORC format.</li>
  <li>And uploaded to an S3 bucket.</li>
</ul>

<p>To mitigate the problems mentioned above, we tried re-implementing the features in our existing Golang processing service. Besides consuming the data and publishing to an S3 bucket, the transformation service also needed to deal with event partitioningÂ and ORC encoding.</p>

<p><img src="img/data-ingestion-transformation-product-insights/image4.png" alt="" /></p>

<p>One key problem we addressed was implementing a robust event partitioner with a large write throughput and low read latency. Fortunately, Golang has a nice <a href="https://golang.org/pkg/sync/#Map">concurrent map</a> package. To further reduce the lock contention, we added <a href="https://www.openmymind.net/Shard-Your-Hash-table-to-reduce-write-locks/">sharding</a>.</p>

<p>We made theÂ changes, deployed the service to production, and discovered our service was now <a href="https://en.wikipedia.org/wiki/Memory_bound_function">memory-bound</a>Â as we buffered data for 1 minute. We did thorough benchmarking and profiling on heap allocation to improve memory utilisation. By iteratively reducing inefficiencies and contributing to a lower CPU consumption, we made our data transformation more efficient.</p>

<p><img src="img/data-ingestion-transformation-product-insights/image2.png" alt="" /></p>

<h3 id="performance">Performance</h3>

<p>After revamping the system, the elapsed time for a single event to travel from the gateway to our dashboard is about 1 minute. We also fixed the data loss issue. Finally, we significantly reduced our on-call workload by removing Spark Streaming.</p>

<h3 id="validation">Validation</h3>

<p>At this point, we had both our old and new pipelines running in parallel. After drastically improving our performance, we needed to confirm we still got the same end results. This was done by running a query against each of theÂ pipelines and comparing the results. Both systems were registered to the same Presto cluster.</p>

<p>We ran two SQL â€œexcerptsâ€ between the two pipelines in different order. Both queries returned the same events, validating our new pipelineâ€™s correctness.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>select count(1) from ((
 select uuid, time from grab_x.realtime_new
 where event = 'app.metric1' and time between 1541734140 and 1541734200
) except (
 select uuid, time from grab_x.realtime_old
 where event = 'app.metric1' and time between 1541734140 and 1541734200
))

/* output: 0 */
</code></pre></div></div>

<h1 id="conclusions">Conclusions</h1>

<p>Scaling a data ingestion system to handle hundreds of thousands of events per second was a non-trivial task. However, by iterating and constantly simplifying our overall architecture, we were able to efficiently ingest the data and drive down its lag to around one minute.</p>

<p>Spark Streaming was a great tool and gave us time to understand the problem. But, understanding what we actually needed to build andÂ iteratively optimise the entire data pipeline led us to:</p>

<ul>
  <li>Replacing Spark Streaming with our new Golang-implemented pipeline.</li>
  <li>Removing Avro encoding.</li>
  <li>Removing an intermediary S3 step.</li>
</ul>

<p>Differences between the old and new pipelinesÂ are:</p>

<table class="table">
  <tr>
    <th></th>
    <th>Old Pipeline</th>
    <th>New Pipeline</th>
  </tr>
  <tr>
    <th>Languages</th>
    <td>Python, Go</td>
    <td>Go</td>
  </tr>
  <tr>
    <th>Stages</th>
    <td>4 services</td>
    <td>3 services</td>
  </tr>
  <tr>
    <th>Conversions</th>
    <td>Protobuf â†’ Avro â†’ ORC</td>
    <td>Protobuf â†’ ORC</td>
  </tr>
  <tr>
    <th>Lag</th>
    <td>4-13 min</td>
    <td>1 min</td>
  </tr>
</table>

<p>Systems usually become more and more complex over time, leading to tech debt and decreased performance. In our case, starting with more steps in the data pipeline was actually the simple solution,Â since we could re-use existing tools. But as we reduced processing stages, weâ€™ve also seen fewer failures. By simplifying the problem,Â we improvedÂ performance and decreased operational complexity. At the end of the day, our data pipeline solves exactly our problem and does nothing else, keeping things fast.</p>
:ET