<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 19 May 2022 06:28:18 +0000</pubDate>
    <lastBuildDate>Thu, 19 May 2022 06:28:18 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Search architecture revamp</title>
        <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Prior to 2021, Grab’s search architecture was designed to only support textual matching, which takes in a user query and looks for exact matches within the ecosystem through an &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverted_index&quot;&gt;inverted index&lt;/a&gt;. This legacy system meant that only textual matching results could be fetched.&lt;/p&gt;

&lt;p&gt;In the second half of 2021, the Deliveries search team worked on improving this architecture to make it smarter, more scalable and also unlock future growth for different search use cases at Grab. The figure below shows a simplified overview of the legacy architecture.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-architecture-revamp/image2.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Legacy architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;With the legacy system, we noticed several problems.&lt;/p&gt;

&lt;h4 id=&quot;search-results-were-textually-matched-without-considering-intention-and-context&quot;&gt;Search results were textually matched without considering intention and context&lt;/h4&gt;

&lt;p&gt;If a user types in a query “Roti Prata” (flatbread), he is likely looking for Roti Prata dishes and those matches with the dish name should be prioritised compared with matches with the merchant-partner’s name or matches with other entities.&lt;/p&gt;

&lt;p&gt;In the legacy system, all entities whose names partially matched “Roti Prata” were displayed and ranked according to hard coded weights, and matches with merchant-partner names were always prioritised, even if the user intention was clearly to search for the “Roti Prata” dish itself.  &lt;/p&gt;

&lt;p&gt;This problem was more common in Mart, as users often intended to search for items instead of shops. Besides the lack of intention recognition, the search system was also unable to take context into consideration; users searching the same keyword query at different times and locations could have different objectives. E.g. if users search for “Bread” in the day, they may be likely to look for cafes while searches at night could be for breakfast the next day.&lt;/p&gt;

&lt;h4 id=&quot;search-results-from-multiple-business-verticals-were-not-blended-effectively&quot;&gt;Search results from multiple business verticals were not blended effectively&lt;/h4&gt;

&lt;p&gt;In Grab’s context, results from multiple verticals were often merged. For example, in Mart searches, Ads and Mart organic search results were displayed together; in Food searches, Ads, Food and Mart organic results were blended together.&lt;/p&gt;

&lt;p&gt;In the legacy architecture, multiple business verticals were merged on the Deliveries API layer, which resulted in the leak of abstraction and loss of useful data as data from the search recall stage was also not taken into account during the merge stage.&lt;/p&gt;

&lt;h4 id=&quot;inability-to-quickly-scale-to-new-search-use-cases-and-difficulty-in-reusing-existing-components&quot;&gt;Inability to quickly scale to new search use cases and difficulty in reusing existing components&lt;/h4&gt;

&lt;p&gt;The legacy code base was not written in a structured way that could scale to new use cases easily. If new search use cases cannot be built on top of an existing system, it can be rather tedious to keep rebuilding the function every time there is a new search use case.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;In this section, solutions from both architecture and implementation perspectives are presented to address the above problem statements.&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;In the new architecture, the flow is extended from lexical recall only to multi-layer including boosting, multi-recall, and ranking. The addition of boosting enables capabilities like intent recognition and query expansion, while the change from single lexical recall to multi-recall opens up the potential for other recall methods, e.g. embedding based and graph based.&lt;/p&gt;

&lt;p&gt;These help address the &lt;a href=&quot;#search-results-were-textually-matched-without-considering-intention-and-context&quot;&gt;first problem statement&lt;/a&gt;. Furthermore, the multi-recall framework enables fetching results from multiple business verticals, addressing the &lt;a href=&quot;#search-results-from-multiple-business-verticals-were-not-blended-effectively&quot;&gt;second problem statement&lt;/a&gt;. In the new framework, results from different verticals and different recall methods were grouped and ranked together without any leak of abstraction or loss of useful data from search recall stage in ranking.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-architecture-revamp/image1.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Upgraded architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;We believe that the key to a platform’s success is modularisation and flexible assembling of plugins to enable quick product iteration. That is why we implemented a combination of a framework defined by the platform and plugins provided by service teams. In this implementation, plugins are assembled through configurations, which addresses the &lt;a href=&quot;#inability-to-quickly-scale-to-new-search-use-cases-and-difficulty-in-reusing-existing-components&quot;&gt;third problem statement&lt;/a&gt; and has two advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Separation of concern. With the main flow abstracted and maintained by the platform, service team developers could focus on the application logic by writing plugins and fitting them into the main flow. In this case, developers without search experience could quickly enable new search flows.&lt;/li&gt;
  &lt;li&gt;Reusing plugins and economies of scale. With more use cases onboarded, more plugins are written by service teams and these plugins are reusable assets, resulting in scale effect. For example, an Ads recall plugin could be reused in Food keyword or non-keyword searches, Mart keyword or non-keyword searches and universal search flows as all these searches contain non-organic Ads. Similarly, a Mart recall plugin could be reused in Mart keyword or non-keyword searches, universal search and Food keyword search flows, as all these flows contain Mart results. With more plugins accumulated on our platform, developers might be able to ship a new search flow by just reusing and assembling the existing plugins.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Our platform now has a smart search with intent recognition and semantic (embedding-based) search. The process of adding new modules is also more straightforward and adds intention recognition to the boosting step as well as embedding as an additional recall to the multi-recall step. These modules can be easily reused by other use cases.&lt;/p&gt;

&lt;p&gt;On top of that, we also have a mixed Ads and an organic framework. This means that data in the recall stage is taken into consideration and Ads can now be ranked together with organic results, e.g. text relevance.&lt;/p&gt;

&lt;p&gt;With a modularised design and plugins provided by the platform, it is easier for clients to use our platform with a simple onboarding process. Furthermore, plugins can be reused to cater to new use cases and achieve a scale effect.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 17 May 2022 03:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/search-architecture-revamp</link>
        <guid isPermaLink="true">https://engineering.grab.com/search-architecture-revamp</guid>
        
        <category>Architecture</category>
        
        <category>Optimisation</category>
        
        <category>Search</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Embracing a Docs-as-Code approach</title>
        <description>&lt;p&gt;The Docs-as-Code concept has been gaining traction in the past few years as more tech companies start implementing this approach. One of the most widely-known examples is &lt;a href=&quot;https://backstage.io/blog/2020/09/08/announcing-tech-docs&quot;&gt;Spotify&lt;/a&gt;, that ​​uses Docs-as-Code to publish documentation in an internal developer portal.&lt;/p&gt;

&lt;p&gt;Since the start of 2021, Grab has also adopted a Docs-as-Code approach to improve our technical documentation. Before we talk about how this is done at Grab, let’s explain what this concept really means.&lt;/p&gt;

&lt;h2 id=&quot;what-is-docs-as-code&quot;&gt;What is Docs-as-Code?&lt;/h2&gt;

&lt;p&gt;Docs-as-Code is a mindset of creating and maintaining technical documentation. The goal is to empower engineers to write technical documentation frequently and keep it up to date by integrating with their tools and processes.&lt;/p&gt;

&lt;p&gt;This means that technical documentation is placed in the same repository as the code, making it easier for engineers to write and update. Next, we’ll go through the motivations behind this initiative.&lt;/p&gt;

&lt;h2 id=&quot;why-embark-on-this-journey&quot;&gt;Why embark on this journey?&lt;/h2&gt;

&lt;p&gt;After speaking to Grab engineers, we found that some of their biggest challenges are around finding and writing documentation. Like many other companies on the same journey, Grab is rather big and our engineers are split into many different teams. Within each team, technical documentation can be stored on different platforms and in different formats, e.g. Google drive documents, text files, etc. This makes it hard to find relevant information, especially if you are trying to find another team’s documentation.&lt;/p&gt;

&lt;p&gt;On top of that, we realised that the documentation process is disconnected from an engineer’s everyday activities, making technical documentation an awkward afterthought. This means that even if people could find the information, there was a good chance that it would not be up to date.&lt;/p&gt;

&lt;p&gt;To address these issues, we need a centralised platform, a single source of truth, so that people can find and discover technical documentation easily. But first, we need to change how we write technical documentation. This is where Docs-as-Code comes in.  &lt;/p&gt;

&lt;h3 id=&quot;how-does-docs-as-code-solve-the-problem&quot;&gt;How does Docs-as-Code solve the problem?&lt;/h3&gt;

&lt;p&gt;With Docs-as-Code, technical documentation is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Written in plaintext.&lt;/li&gt;
  &lt;li&gt;Editable in a code editor.&lt;/li&gt;
  &lt;li&gt;Stored in the same repository as the source code so it’s easier to update docs whenever a code change is committed.&lt;/li&gt;
  &lt;li&gt;Published on a central platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea is to consolidate all technical documentation on a central platform, making it easier to discover and find content by using an easy-to-navigate information architecture and targeted search.&lt;/p&gt;

&lt;h2 id=&quot;how-is-grabembracing-docs-as-code&quot;&gt;How is Grab embracing Docs-as-Code?&lt;/h2&gt;

&lt;p&gt;We’ve developed an internal developer portal that simplifies the process of writing, reviewing and publishing technical documentation.&lt;/p&gt;

&lt;p&gt;Here’s a brief overview of the process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a dedicated docs folder in a Git repository.&lt;/li&gt;
  &lt;li&gt;Push Markdown files into the docs folder.&lt;/li&gt;
  &lt;li&gt;Configure the developer portal to publish docs from the respective code repository.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The latest version of the documentation will automatically be built and published in the developer portal.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/doc-as-code/image1.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Simplified documentation process&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This way, technical documentation is closer to the source code and integrated into the code development process. Writing and updating technical documentation becomes part of writing code, and this encourages engineers to keep documentation updated.&lt;/p&gt;

&lt;h2 id=&quot;measuring-success&quot;&gt;Measuring success&lt;/h2&gt;

&lt;p&gt;Whenever there’s a change throughout big organisations like Grab, it can be tough to implement. But thankfully, our engineers recognised the importance of improving documentation and making it easier to maintain or update.&lt;/p&gt;

&lt;p&gt;We surveyed our users and here’s what some have said about our Docs-as-Code initiative:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“[W]ith the doc and source code in one place, test backend engineers can now make doc changes via standard code review process and re-use the same content for CLI helper message and documentation.” - Kang Yaw Ong, Test Automation - Engineering Manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;“[Docs-as-Code] is a great initiative, as it keeps documentation in line and up-to-date with the development of a project. Managing documentation using a version control system and the same tools to handle merges and conflicts reduces overhead and friction in an engineer’s workflow.” - Eugene Chiang, Foundations - Engineering Manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;progress-and-future-optimisations&quot;&gt;Progress and future optimisations&lt;/h2&gt;

&lt;p&gt;Since we first started the Docs-as-Code initiative in Grab, we’ve made a lot of progress in terms of adoption - approximately 80% of Grab services will have their technical documentation on the internal portal by April 2022.&lt;/p&gt;

&lt;p&gt;We’ve also improved overall user experience by enhancing stability and performance, improving navigation and content formatting, and enabling feedback. But it doesn’t stop there; we are continuously improving the internal portal and providing more features for our engineers.&lt;/p&gt;

&lt;p&gt;Apart from technical documentation, we are also applying the Docs-as-Code approach to our technical training content. This means moving both self-paced and workshop training content to a centralised repository and providing engineers a single platform for all their learning needs.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to the Tech Learning - Documentation team for their contributions to this blog post.
&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;we-are-hiring&quot;&gt;We are hiring!&lt;/h2&gt;

&lt;p&gt;We are looking for more technical content developers to join the team. If you’re keen on joining our Docs-as-Code journey and improving developer experience, check out our open listings in &lt;a href=&quot;https://grab.careers/jobs/job-details/?id=be841c804fee010177fceb2a4a740001&quot;&gt;Singapore&lt;/a&gt; and &lt;a href=&quot;https://grab.careers/jobs/job-details/?id=2b029fdf01590101835c6a2e830c0002&quot;&gt;Malaysia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Join us in driving this initiative forward and making documentation more approachable for everyone!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 04 May 2022 03:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/doc-as-code</link>
        <guid isPermaLink="true">https://engineering.grab.com/doc-as-code</guid>
        
        <category>Docs-as-Code</category>
        
        <category>Documentation</category>
        
        <category>Technical documentation</category>
        
        <category>Engineering practices</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Graph Networks - Striking fraud syndicates in the dark</title>
        <description>&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-intro/image2.png&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As a leading superapp in Southeast Asia, Grab serves millions of consumers daily. This naturally makes us a target for fraudsters and to enhance our defences, the Integrity team at Grab has launched several hyper-scaled services, such as the &lt;a href=&quot;https://engineering.grab.com/griffin&quot;&gt;Griffin real-time rule engine&lt;/a&gt; and &lt;a href=&quot;https://engineering.grab.com/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;Advanced Feature Engineering&lt;/a&gt;. These systems enable data scientists and risk analysts to develop real-time scoring, and take fraudsters out of our ecosystems.&lt;/p&gt;

&lt;p&gt;Apart from individual fraudsters, we have also observed the fast evolution of the dark side over time. We have had to evolve our defences to deal with professional syndicates that use advanced equipment such as device farms and GPS spoofing apps to perform fraud at scale. These professional fraudsters are able to camouflage themselves as normal users, making it significantly harder to identify them with rule-based detection.&lt;/p&gt;

&lt;p&gt;Since 2020, Grab’s Integrity team has been advancing fraud detection with more sophisticated techniques and experimenting with a range of graph network technologies such as graph visualisations, graph neural networks and graph analytics. We’ve seen a lot of progress in this journey and will be sharing some key learnings that might help other teams who are facing similar issues.&lt;/p&gt;

&lt;h3 id=&quot;what-are-graph-based-prediction-platforms&quot;&gt;What are Graph-based Prediction Platforms?&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“You can fool some of the people all of the time, and all of the people some of the time, but you cannot fool all of the people all of the time.” - Abraham Lincoln&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A Graph-based Prediction Platform connects multiple entities through one or more common features. When such entities are viewed as a macro graph network, we uncover new patterns that are otherwise unseen to the naked eye. For example, when investigating if two users are sharing IP addresses or devices, we might not be able to tell if they are fraudulent or just family members sharing a device.&lt;/p&gt;

&lt;p&gt;However, if we use a graph system and look at all users sharing this device or IP address, it could show us if these two users are part of a much larger syndicate network in a device farming operation. In operations like these, we may see up to hundreds of other fake accounts that were specifically created for promo and payment fraud. With graphs, we can identify fraudulent activity more easily.&lt;/p&gt;

&lt;h3 id=&quot;grabs-graph-based-prediction-platform&quot;&gt;Grab’s Graph-based Prediction Platform&lt;/h3&gt;

&lt;p&gt;Leveraging the power of graphs, the team has primarily built two types of systems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Graph Database Platform&lt;/strong&gt;: An ultra-scalable storage system with over one billion nodes that powers:
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Graph Visualisation&lt;/strong&gt;: Risk specialists and data analysts can review user connections real-time and are able to quickly capture new fraud patterns with over 10 dimensions of features (see Fig 1).&lt;/p&gt;

        &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-intro/image1.gif&quot; alt=&quot;Change Data Capture flow&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 1: Graph visualisation&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Network-based feature system&lt;/strong&gt;: A configurable system for engineers to adjust machine learning features based on network connectivity, e.g. number of hops between two users, numbers of shared devices between two IP addresses.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Graph-based Machine Learning&lt;/strong&gt;: Unlike traditional fraud detection models, Graph Neural Networks (GNN) are able to utilise the structural correlations on the graph and act as a sustainable foundation to combat many different kinds of fraud. The data science team has built large-scale GNN models for scenarios like anti-money laundering and fraud detection.&lt;/p&gt;

    &lt;p&gt;Fig 2 shows a Money Laundering Network where hundreds of accounts coordinate the placement of funds, layering the illicit monies through a complex web of transactions making funds hard to trace, and consolidate funds into spending accounts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-intro/image3.gif&quot; alt=&quot;Change Data Capture flow&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 2: Money Laundering Network&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h3&gt;

&lt;p&gt;In the next article of our Graph Network blog series, we will dive deeper into how we develop the graph infrastructure and database using AWS Neptune. Stay tuned for the next part.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Apr 2022 10:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/graph-networks</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-networks</guid>
        
        <category>Graph Networks</category>
        
        <category>Graphs</category>
        
        <category>Fraud detection</category>
        
        <category>Security</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>How we reduced our CI YAML files from 1800 lines to 50 lines</title>
        <description>&lt;p&gt;This article illustrates how the Cauldron Machine Learning (ML) Platform team uses &lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html&quot;&gt;GitLab parent-child pipelines&lt;/a&gt; to dynamically generate GitLab CI files to solve several limitations of GitLab for large repositories, namely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Limitations to the number of includes (&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab/-/issues/207270#:~:text=GitLab%20Next&amp;amp;text=Please%20increase%20the%20maximum%20number,methods%20for%20managing%20CICD%20configurations.&quot;&gt;100&lt;/a&gt; by default).&lt;/li&gt;
  &lt;li&gt;Simplifying the GitLab CI file from 1800 lines to 50 lines.&lt;/li&gt;
  &lt;li&gt;Reducing the need for nested &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci&lt;/code&gt; yml files.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Cauldron is the Machine Learning (ML) Platform team at Grab. The Cauldron team provides tools for ML practitioners to manage the end to end lifecycle of ML models, from training to deployment. GitLab and its tooling are an integral part of our stack, for continuous delivery of machine learning.&lt;/p&gt;

&lt;p&gt;One of our core products is MerLin Pipelines. Each team has a dedicated repo to maintain the code for their ML pipelines. Each pipeline has its own subfolder. We rely heavily on GitLab rules to detect specific changes to trigger deployments for the different stages of different pipelines (for example, model serving with Catwalk, and so on).&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;approach-1-nested-child-files&quot;&gt;Approach 1: Nested child files&lt;/h3&gt;

&lt;p&gt;Our initial approach was to rely heavily on static code generation to generate the child &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; files in individual stages. See Figure 1 for an example directory structure. These nested yml files are pre-generated by our cli and committed to the repository.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1: Example directory structure with nested gitlab-ci.yml files. &quot; src=&quot;img/how-we-reduced-our-ci-yaml/image6.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1: Example directory structure with nested gitlab-ci.yml files. Child `gitlab-ci.yml` files are added by using the &lt;a href=&quot;https://docs.gitlab.com/ee/ci/yaml/#include&quot;&gt;include&lt;/a&gt; keyword.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2: Example root .gitlab-ci.yml file, and include clauses.&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image4.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2: Example root .gitlab-ci.yml file, and include clauses.&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3: Example child .gitlab-ci.yml file for a given stage (Deploy Model) in a pipeline (pipeline 1).
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image1.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3: Example child `.gitlab-ci.yml` file for a given stage (Deploy Model) in a pipeline (pipeline 1).
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;As teams add more pipelines and stages, we soon hit a limitation in this approach:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There was a &lt;a href=&quot;https://docs.gitlab.com/ee/ci/yaml/#include&quot;&gt;soft limit&lt;/a&gt; in the number of includes that could be in the base &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.gitlab-ci.yml&lt;/code&gt; file.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It became evident that this approach would not scale to our use-cases.&lt;/p&gt;

&lt;h3 id=&quot;approach-2-dynamically-generating-a-big-ci-file&quot;&gt;Approach 2: Dynamically generating a big CI file&lt;/h3&gt;

&lt;p&gt;Our next attempt to solve this problem was to try to inject and inline the nested child &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; contents into the root &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; file, so that we no longer needed to rely on the in-built GitLab “include” clause.&lt;/p&gt;

&lt;p&gt;To achieve it, we wrote a utility that parsed a raw &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci&lt;/code&gt; file, walked the tree to retrieve all “included” child &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci&lt;/code&gt; files, and to replace the includes to generate a final big &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Figure 4 illustrates the resulting file is generated from Figure 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4: “Fat” YAML file generated through this approach, assumes the original raw file of Figure 3.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image2.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4: “Fat” YAML file generated through this approach, assumes the original raw file of Figure 3.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This approach solved our issues temporarily. Unfortunately, we ended up with GitLab files that were up to 1800 lines long. There is also a soft limit to the size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; files. It became evident that we would eventually hit the limits of this approach.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Our initial attempt at using static code generation put us partially there. We were able to pre-generate and infer the stage and pipeline names from the information available to us. Code generation was definitely needed, but upfront generation of code had some key limitations, as shown above. We needed a way to improve on this, to somehow generate GitLab stages on the fly. After some research, we stumbled upon &lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html#dynamic-child-pipelines&quot;&gt;Dynamic Child Pipelines&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Quoting the official website:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Instead of running a child pipeline from a static YAML file, you can define a job that runs your own script to generate a YAML file, which is then used to trigger a child pipeline.&lt;/p&gt;

  &lt;p&gt;This technique can be very powerful in generating pipelines targeting content that changed or to build a matrix of targets and architectures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We were already on the right track. We just needed to combine code generation with child pipelines, to dynamically generate the necessary stages on the fly.&lt;/p&gt;

&lt;h2 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 5: Flow diagram of how we use dynamic yaml generation. The user raises a merge request in a branch, and subsequently merges the branch to master.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image8.png&quot; width=&quot;75%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 5: Flow diagram of how we use dynamic yaml generation. The user raises a merge request in a branch, and subsequently merges the branch to master.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;The user Git flow can be seen in Figure 5, where the user modifies or adds some files in their respective Git team repo. As a refresher, a typical repo structure consists of pipelines and stages (see Figure 1). We would need to extract the information necessary from the branch environment in Figure 5, and have a stage to programmatically generate the proper stages (for example, Figure 3).&lt;/p&gt;

&lt;p&gt;In short, our requirements can be summarized as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Detecting the files being changed in the Git branch.&lt;/li&gt;
  &lt;li&gt;Extracting the information needed from the files that have changed.&lt;/li&gt;
  &lt;li&gt;Passing this to be templated into the necessary stages.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s take a very simple example, where a user is modifying a file in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_1&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_1&lt;/code&gt; in Figure 1. Our desired output would be:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 6: Desired output that should be dynamically generated.&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image7.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 6: Desired output that should be dynamically generated.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Our template would be in the form of:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 7: Example template, and information needed. Let’s call it template\_file.yml.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image5.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 7: Example template, and information needed. Let’s call it template_file.yml.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;First, we need to detect the files being modified in the branch. We achieve this with native &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git diff&lt;/code&gt; commands, checking against the base of the branch to track what files are being modified in the merge request. The output (let’s call it &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diff.txt&lt;/code&gt;) would be in the form of:&lt;/p&gt;
&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;M        pipelines/pipeline_1/stage_1/modelserving.yaml
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
 &lt;small class=&quot;post-image-caption&quot;&gt;Figure 8: Example diff.txt generated from git diff.
 &lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;We must extract the yellow and green information from the line, corresponding to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_name&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_name&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 9: Information that needs to be extracted from the file.&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image9.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 9: Information that needs to be extracted from the file.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;We take a very simple approach here, by introducing a concept called stop patterns.&lt;/p&gt;

&lt;p&gt;Stop patterns are defined as a comma separated list of variable names, and the words to stop at. The colon (:) denotes how many levels before the stop word to stop.&lt;/p&gt;

&lt;p&gt;For example, the stop pattern:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;pipeline_name:pipelines&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;tells the parser to look for the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipelines&lt;/code&gt; and stop before that, extracting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_1&lt;/code&gt; from the example above tagged to the variable name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The stop pattern with two colons (::):&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;stage_name::pipelines&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;tells the parser to stop two levels before the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipelines&lt;/code&gt;, and extract &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_1&lt;/code&gt; as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Our cli tool allows the stop patterns to be comma separated, so the final command would be:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;cauldron_repo_util diff.txt template_file.yml
&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;pipeline_name:pipelines,stage_name::pipelines &amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;generated.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We elected to write the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;util&lt;/code&gt; in Rust due to its high performance, and its rich templating libraries (for example, &lt;a href=&quot;https://github.com/Keats/tera&quot;&gt;Tera&lt;/a&gt;) and decent cli libraries (&lt;a href=&quot;https://github.com/clap-rs/clap&quot;&gt;clap&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Combining all these together, we are able to extract the information needed from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git diff&lt;/code&gt;, and use stop patterns to extract the necessary information to be passed into the template. Stop patterns are flexible enough to support different types of folder structures.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 10: Example Rust code snippet for parsing the Git diff file.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image3.png&quot; width=&quot;75%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 10: Example Rust code snippet for parsing the Git diff file.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;When triggering pipelines in the master branch (see right side of Figure 5), the flow is the same, with a small caveat that we must retrieve the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diff.txt&lt;/code&gt; file from the source branch. We achieve this by using the rich GitLab API, retrieving the pipeline artifacts and using the same util above to generate the necessary GitLab steps dynamically.&lt;/p&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;After implementing this change, our biggest success was reducing one of the biggest ML pipeline Git repositories from 1800 lines to 50 lines. This approach keeps the size of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.gitlab-ci.yaml&lt;/code&gt; file constant at 50 lines, and ensures that it scales with however many pipelines are added.&lt;/p&gt;

&lt;p&gt;Our users, the machine learning practitioners, also find it more productive as they no longer need to worry about GitLab yaml files.&lt;/p&gt;

&lt;h2 id=&quot;learnings-and-conclusion&quot;&gt;Learnings and conclusion&lt;/h2&gt;

&lt;p&gt;With some creativity, and the flexibility of GitLab Child Pipelines, we were able to invest some engineering effort into making the configuration re-usable, adhering to &lt;a href=&quot;https://en.wikipedia.org/wiki/Don%27t_repeat_yourself&quot;&gt;DRY&lt;/a&gt; principles.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to the Cauldron ML Platform team.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;We might open source our solution.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html&quot;&gt;Parent-child pipelines&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab/-/issues/207270#:~:text=GitLab%20Next&amp;amp;text=Please%20increase%20the%20maximum%20number,methods%20for%20managing%20CICD%20configurations&quot;&gt;Backend: The gitlab-ci.yml is limited to 100 includes&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Apr 2022 15:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/how-we-reduced-our-ci-yaml</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-we-reduced-our-ci-yaml</guid>
        
        <category>CI</category>
        
        <category>Machine Learning</category>
        
        <category>Pipelines</category>
        
        <category>Continuous Integration</category>
        
        <category>Continuous Delivery</category>
        
        <category>Optimisation</category>
        
        <category>Rust</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How Kafka Connect helps move data seamlessly</title>
        <description>&lt;p&gt;Grab’s real-time data platform team a.k.a. Coban has written about &lt;a href=&quot;https://engineering.grab.com/plumbing-at-scale&quot;&gt;Plumbing at scale&lt;/a&gt;, &lt;a href=&quot;https://engineering.grab.com/optimally-scaling-kafka-consumer-applications&quot;&gt;Optimally scaling Kakfa consumer applications&lt;/a&gt;, and &lt;a href=&quot;https://engineering.grab.com/exposing-kafka-cluster&quot;&gt;Exposing Kafka via VPCE&lt;/a&gt;. In this article, we will cover the importance of being able to easily move data in and out of Kafka in a low-code way and how we achieved this with Kafka Connect.&lt;/p&gt;

&lt;p&gt;To build a &lt;a href=&quot;https://www.cio.com/article/220351/what-is-noops-the-quest-for-fully-automated-it-operations.html&quot;&gt;NoOps&lt;/a&gt; managed streaming platform in Grab, the Coban team has:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Engineered an ecosystem on top of Apache Kafka.&lt;/li&gt;
  &lt;li&gt;Successfully adopted it to production for both transactional and analytical use cases.&lt;/li&gt;
  &lt;li&gt;Made it a battle-tested industrial-standard platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In 2021, the Coban team embarked on a new journey (Kafka Connect) that enables and empowers Grabbers to move data in and out of Apache Kafka seamlessly and conveniently.&lt;/p&gt;

&lt;h2 id=&quot;kafka-connect-stack-in-grab&quot;&gt;Kafka Connect stack in Grab&lt;/h2&gt;

&lt;p&gt;This is what Coban’s Kafka Connect stack looks like today. Multiple data sources and data sinks, such as MySQL, S3 and Azure Data Explorer, have already been supported and productionised.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image4.png&quot; alt=&quot;Kafka Connect stack in Grab&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The Coban team has been using Protobuf as the serialisation-deserialisation (SerDes) format in Kafka. Therefore, the role of Confluent schema registry (shown at the top of the figure) is crucial to the Kafka Connect ecosystem, as it serves as the building block for conversions such as Protobuf-to-Avro, Protobuf-to-JSON and Protobuf-to-Parquet.&lt;/p&gt;

&lt;h2 id=&quot;what-problems-are-we-trying-to-solve&quot;&gt;What problems are we trying to solve?&lt;/h2&gt;

&lt;h3 id=&quot;problem-1-change-data-capture-cdc&quot;&gt;Problem 1: Change Data Capture (CDC)&lt;/h3&gt;

&lt;p&gt;In a big organisation like Grab, we handle large volumes of data and changes across many services on a daily basis, so it is important for these changes to be reflected in real time.&lt;/p&gt;

&lt;p&gt;In addition, there are other technical challenges to be addressed:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As shown in the figure below, data is written twice in the code base - once into the database (DB) and once as a message into Kafka. In order for the data in the DB and Kafka to be consistent, the two writes have to be atomic in a two-phase commit protocol (or other atomic commitment protocols), which is non-trivial and impacts availability.&lt;/li&gt;
  &lt;li&gt;Some use cases require data both before and after a change.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image5.png&quot; alt=&quot;Change Data Capture flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;problem-2-message-mirroring-for-disaster-recovery&quot;&gt;Problem 2: Message mirroring for disaster recovery&lt;/h3&gt;

&lt;p&gt;The Coban team has done some research on Kafka MirrorMaker, an open-source solution. While it can ensure better data consistency, it takes significant effort to adopt it onto existing Kubernetes infrastructure hosted by the Coban team and achieve high availability.&lt;/p&gt;

&lt;p&gt;Another major challenge that the Coban team faces is offset mirroring and translation, which is a known challenge in Kafka communities. In order for Kafka consumers to seamlessly resume their work with a backup Kafka after a disaster, we need to cater for &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-545%3A+support+automated+consumer+offset+sync+across+clusters+in+MM+2.0#:~:text=The%20offset%20translation%20is%20great,off%20at%20the%20primary%20cluster%2C&quot;&gt;offset translation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;data-ingestion-into-azure-event-hubs&quot;&gt;Data ingestion into Azure Event Hubs&lt;/h3&gt;

&lt;p&gt;Azure Event Hubs has a Kafka-compatible interface and natively supports JSON and Avro schema. The Coban team uses Protobuf as the SerDes framework, which is not supported by Azure Event Hubs. It means that conversions have to be done for message ingestion into Azure Event Hubs.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;To tackle these problems, the Coban team has picked Kafka Connect because:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It is an open-source framework with a relatively big community that we can consult if we run into issues.&lt;/li&gt;
  &lt;li&gt;It has the ability to plug in transformations and custom conversion logic.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let us see how Kafka Connect can be used to resolve the previously mentioned problems.&lt;/p&gt;

&lt;h3 id=&quot;kafka-connect-with-debezium-connectors&quot;&gt;Kafka Connect with Debezium connectors&lt;/h3&gt;

&lt;p&gt;Debezium is a framework built for capturing data changes on top of Apache Kafka and the Kafka Connect framework. It provides a series of connectors for various databases, such as MySQL, MongoDB and Cassandra.&lt;/p&gt;

&lt;p&gt;Here are the benefits of MySQL &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; streams:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They not only provide changes on data, but also give snapshots of data before and after a specific change.&lt;/li&gt;
  &lt;li&gt;Some producers no longer have to push a message to Kafka after writing a row to a MySQL database. With Debezium connectors, services can choose not to deal with Kafka and only handle MySQL data stores.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;architecture&quot;&gt;Architecture&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image6.png&quot; alt=&quot;Kafka Connect architecture&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;in-case-of-db-upgrades-and-outages&quot;&gt;In case of DB upgrades and outages&lt;/h4&gt;

&lt;p&gt;DB Data Definition Language (DDL) changes, migrations, splits and outages are common in database operations, and each operation type has a systematic resolution.&lt;/p&gt;

&lt;p&gt;The Debezium connector has built-in features to handle DDL changes made by DB migration tools, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pt-online-schema-change&lt;/code&gt;, which is used by the Grab DB Ops team.&lt;/p&gt;

&lt;p&gt;To deal with MySQL instance changes and database splits, the Coban team leverages on the Kafka Connect framework’s ability to change the offsets of connectors. By changing the offsets, Debezium connectors can properly function after DB migrations and resume binlog synchronisation from any position in any binlog file on a MySQL instance.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image1.png&quot; alt=&quot;Database upgrades and outages&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Refer to the &lt;a href=&quot;https://debezium.io/documentation/faq/%23how_to_change_the_offsets_of_the_source_database&quot;&gt;Debezium documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h4 id=&quot;success-stories&quot;&gt;Success stories&lt;/h4&gt;

&lt;p&gt;The CDC project on MySQL via Debezium connectors has been greatly successful in Grab. One of the biggest examples is its adoption in the Elasticsearch optimisation carried out by GrabFood, which has been published in &lt;a href=&quot;https://engineering.grab.com/search-indexing-optimisation&quot;&gt;another blog&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mirrormaker2-with-offset-translation&quot;&gt;MirrorMaker2 with offset translation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/tree/trunk/connect/mirror&quot;&gt;Kafka MirrorMaker2&lt;/a&gt; (MM2), developed in and shipped together with the Apache Kafka project, is a utility to mirror messages and consumer offsets. However, in the Coban team, the MM2 stack is deployed on the Kafka Connect framework per connector because:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A few Kafka Connect clusters have already been provisioned.&lt;/li&gt;
  &lt;li&gt;Compared to launching three connectors bundled in MM2, Coban can have finer controls on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MirrorSourceConnector&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MirrorCheckpointConnector&lt;/code&gt;, and manage both of them in an infrastructure-as-code way via Hashicorp Terraform.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image7.png&quot; alt=&quot;MirrorMaker2 flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;success-stories-1&quot;&gt;Success stories&lt;/h4&gt;

&lt;p&gt;Ensuring business continuity is a key priority for Grab and this includes the ability to recover from incidents quickly. In 2021H2, there was a campaign that ran across many teams to examine the readiness and robustness of various services and middlewares. Coban’s Kafka is one of these services that proved to be robust after rounds of chaos engineering. With MM2 on Kafka Connect to mirror both messages and consumer offsets, critical services and pipelines could safely be replicated and launched across AWS regions if outages occur.&lt;/p&gt;

&lt;p&gt;Because the Coban team has proven itself as the battle-tested Kafka service provider in Grab, other teams have also requested to migrate streams from self-managed Kafka clusters to ones managed by Coban. MM2 has been used in such migrations and brought zero downtime to the streams’ producers and consumers.&lt;/p&gt;

&lt;h3 id=&quot;mirror-to-azure-event-hubs-with-an-in-house-converter&quot;&gt;Mirror to Azure Event Hubs with an in-house converter&lt;/h3&gt;

&lt;p&gt;The Analytics team runs some real time ingestion and analytics projects on Azure. To support this cross-cloud use case, the Coban team has adopted MM2 for message mirroring to Azure Event Hubs.&lt;/p&gt;

&lt;p&gt;Typically, Event Hubs only accept JSON and Avro bytes, which is incompatible with the existing SerDes framework. The Coban team has developed a custom converter that converts bytes serialised in Protobuf to JSON bytes at runtime.&lt;/p&gt;

&lt;p&gt;These steps explain how the converter works:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Deserialise bytes in Kafka to a Protobuf &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DynamicMessage&lt;/code&gt; according to a schema retrieved from the Confluent™ schema registry.&lt;/li&gt;
  &lt;li&gt;Perform a recursive post-order depth-first-search on each field descriptor in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DynamicMessage&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Convert every Protobuf field descriptor to a JSON node.&lt;/li&gt;
  &lt;li&gt;Serialise the root JSON node to bytes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The converter has not been open sourced yet.&lt;/p&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image2.png&quot; alt=&quot;Deployment&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Docker containers are the Coban team’s preferred infrastructure, especially since some production Kafka clusters are already deployed on Kubernetes. The long-term goal is to provide Kafka in a software-as-a-service (SaaS) model, which is why Kubernetes was picked. The diagram below illustrates how Kafka Connect clusters are built and deployed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image3.png&quot; alt=&quot;Terraform for connectors&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The Coban team is iterating on a unified control plane to manage resources like Kafka topics, clusters and Kafka Connect. In the foreseeable future, internal users should be able to provision Kafka Connect connectors via RESTful APIs and a graphical user interface (GUI).&lt;/p&gt;

&lt;p&gt;At the same time, the Coban team is closely working with the Data Engineering team to make Kafka Connect the preferred tool in Grab for moving data in and out of external storages (S3 and Apache Hudi).&lt;/p&gt;

&lt;h2 id=&quot;coban-is-hiring&quot;&gt;Coban is hiring!&lt;/h2&gt;
&lt;p&gt;The Coban (Real-time Data Platform) team at Grab in Singapore is hiring software and site reliability engineers at all levels as we double down on growing our platform capabilities.&lt;/p&gt;

&lt;p&gt;Join us in building state-of-the-art, mission critical, TB/hour scale data platforms that enable thousands of engineers, data scientists, and analysts to serve millions of consumers, businesses, and partners across Southeast Asia!&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.
Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Apr 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/kafka-connect</link>
        <guid isPermaLink="true">https://engineering.grab.com/kafka-connect</guid>
        
        <category>Kafka</category>
        
        <category>Data processing</category>
        
        <category>Real-Time</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Supporting large campaigns at scale</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we run large marketing campaigns every day. A typical campaign may require executing multiple actions for millions of users all at once. The actions may include sending rewards, awarding points, and sending messages. Here is what a campaign may look like: On 1st Jan 2022, send two ride rewards to all the users in the “heavy users” segment. Then, send them a congratulatory message informing them about the reward.&lt;/p&gt;

&lt;p&gt;Years ago, Grab’s marketing team used to stay awake at midnight to manually trigger such campaigns. They would upload a file at 12 am and then wait for a long time for the campaign execution to complete. To solve this pain point and support more capabilities down this line, we developed a “batch job” service, which is part of our in-house real-time automation engine, &lt;a href=&quot;https://engineering.grab.com/trident-real-time-event-processing-at-scale&quot;&gt;Trident&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following are some services we use to support Grab’s marketing teams:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rewards: responsible for managing rewards.&lt;/li&gt;
  &lt;li&gt;Messaging: responsible for sending messages to users. For example, push notifications.&lt;/li&gt;
  &lt;li&gt;Segmentation: responsible for storing and retrieving segments of users based on certain criteria.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For simplicity, only the services above will be referenced for this article. The “batch job” service we built uses rewards and messaging services for executing actions, and uses the segmentation service for fetching users in a segment.&lt;/p&gt;

&lt;h2 id=&quot;system-requirements&quot;&gt;System requirements&lt;/h2&gt;

&lt;h3 id=&quot;functional-requirements&quot;&gt;Functional requirements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Apply a sequence of actions targeting a large segment of users at a scheduled time, display progress to the campaign manager and provide a final report.
    &lt;ul&gt;
      &lt;li&gt;For each user, the actions must be executed in sequence; the latter action can only be executed if the preceding action is successful.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;non-functional-requirements&quot;&gt;Non-functional requirements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Quick execution and high turnover rate.
    &lt;ul&gt;
      &lt;li&gt;Definition of turnover rate: the number of scheduled jobs completed per unit time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Maximise resource utilisation and balance server load.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the sake of brevity, we will not cover the scheduling logic, nor the generation of the report. We will focus specifically on executing actions.&lt;/p&gt;

&lt;h2 id=&quot;naive-approach&quot;&gt;Naive approach&lt;/h2&gt;
&lt;p&gt;Let’s start thinking from the most naive solution, and improve from there to reach an optimised solution.&lt;/p&gt;

&lt;p&gt;Here is the pseudocode of a naive action executor.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def executeActionOnSegment(segment, actions):
   for user in fetchUsersInSegment(segment):
       for action in actions:
           success := doAction(user, action)
           if not success:
               break
           recordActionResult(user, action)

def doAction(user, action):
   if action.type == &quot;awardReward&quot;:
       rewardService.awardReward(user, action.meta)
   elif action.type == &quot;sendMessage&quot;:
       messagingService.sendMessage(user, action.meta)
   else:
       # other action types ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One may be able to quickly tell that the naive solution does not satisfy our non-functional requirements for the following reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Execution is slow:
    &lt;ul&gt;
      &lt;li&gt;The programme is single-threaded.&lt;/li&gt;
      &lt;li&gt;Actions are executed for users one by one in sequence.&lt;/li&gt;
      &lt;li&gt;Each call to the rewards and messaging services will incur network trip time, which impacts time cost.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Resource utilisation is low: The actions will only be executed on one server. When we have a cluster of servers, the other servers will sit idle.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are our alternatives for fixing the above issues:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Actions for different users should be executed in parallel.&lt;/li&gt;
  &lt;li&gt;API calls to other services should be minimised.&lt;/li&gt;
  &lt;li&gt;Distribute the work of executing actions evenly among different servers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Actions for the same user have to be executed in sequence. For example, if a sequence of required actions are (1) award a reward, (2) send a message informing the user to use the reward, then we can only execute action (2) after action (1) is successfully done for logical reasons and to avoid user confusion.&lt;/p&gt;

&lt;h2 id=&quot;our-approach&quot;&gt;Our approach&lt;/h2&gt;
&lt;p&gt;A message queue is a well-suited solution to distribute work among multiple servers. We selected Kafka, among numerous message services, due to its following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High throughput: Kafka can accept reads and writes at a very high speed.&lt;/li&gt;
  &lt;li&gt;Robustness: Events in Kafka are distributedly stored with redundancy, without a need to worry about data loss.&lt;/li&gt;
  &lt;li&gt;Pull-based consumption: Consumers can consume events at their own speed. This helps to avoid overloading our servers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a scheduled campaign is triggered, we retrieve the users from the segment in batches; each batch comprises around 100 users. We write the batches into a Kafka stream, and all our servers consume from the stream to execute the actions for the batches. The following diagram illustrates the overall flow.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/supporting-large-campaigns-at-scale/flow.png&quot; alt=&quot;Flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Data in Kafka is stored in partitions. The partition configuration is important to ensure that the batches are evenly distributed among servers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Number of partitions: Ensure that the number of stream partitions is greater than or equal to the max number of servers we will have in our cluster. This is because one Kafka partition can only be consumed by one consumer. If we have more consumers than partitions, some consumers will not receive any data.&lt;/li&gt;
  &lt;li&gt;Partition key: For each batch, assign a hash value as the partition key to randomly allocate batches into different partitions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that work is distributed among servers in batches, we can consider how to process each batch faster. If we follow the naive logic, for each user in the batch, we need to call the rewards or messaging service to execute the actions. This will create very high QPS (queries per second) to those services, and incur significant network round trip time.&lt;/p&gt;

&lt;p&gt;To solve this issue, we decided to build batch endpoints in rewards and messaging services. Each batch endpoint takes in a list of user IDs and action metadata as input parameters, and returns the action result for each user, regardless of success or failure. With that, our batch processing logic looks like the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def processBatch(userBatch, actions):
   users = userBatch
   for action in actions:
       successUsers, failedUsers = doAction(users, action)
       recordFailures(failedUsers, action)
       users = successUsers

def doAction(users, action):
   resp = {}
   if action.type == &quot;awardReward&quot;:
       resp = rewardService.batchAwardReward(users, action.meta)
   elif action.type == &quot;sendMessage&quot;:
       resp = messagingService.batchSendMessage(users, action.meta)
   else:
   # other action types ...

   return getSuccessUsers(resp), getFailedUsers(resp)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the implementation of batch endpoints, we also made optimisations to reduce latency. For example, when awarding rewards, we need to write the records of a reward being given to a user in multiple database tables. If we make separate DB queries for each user in the batch, it will cause high QPS to DB and incur high network time cost. Therefore, we grouped all the users in the batch into one DB query for each table update instead.&lt;/p&gt;

&lt;p&gt;Benchmark tests show that using the batch DB query reduced API latency by up to 85%.&lt;/p&gt;

&lt;h2 id=&quot;further-optimisations&quot;&gt;Further optimisations&lt;/h2&gt;
&lt;p&gt;As more campaigns started running in the system, we came across various bottlenecks. Here are the optimisations we implemented for some major examples.&lt;/p&gt;

&lt;h3 id=&quot;shard-stream-by-action-type&quot;&gt;Shard stream by action type&lt;/h3&gt;
&lt;p&gt;Two widely used actions are awarding rewards and sending messages to users. We came across situations where the sending of messages was blocked because a different campaign of awarding rewards had already started. If millions of users were targeted for rewards, this could result in significant waiting time before messages are sent, ultimately leading them to become irrelevant.&lt;/p&gt;

&lt;p&gt;We found out the API latency of awarding rewards is significantly higher than sending messages. Hence, to make sure messages are not blocked by long-running awarding jobs, we created a dedicated Kafka topic for messages. By having different Kafka topics based on the action type, we were able to run different types of campaigns in parallel.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/supporting-large-campaigns-at-scale/shard-stream.png&quot; alt=&quot;Flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;shard-stream-by-country&quot;&gt;Shard stream by country&lt;/h3&gt;
&lt;p&gt;Grab operates in multiple countries. We came across situations where a campaign of awarding rewards to a small segment of users in one country was delayed by another campaign that targeted a huge segment of users in another country. The campaigns targeting a small set of users are usually more time-sensitive.&lt;/p&gt;

&lt;p&gt;Similar to the above solution, we added different Kafka topics for each country to enable the processing of campaigns in different countries in parallel.&lt;/p&gt;

&lt;h3 id=&quot;remove-unnecessary-waiting&quot;&gt;Remove unnecessary waiting&lt;/h3&gt;
&lt;p&gt;We observed that in the case of chained actions, messaging actions are generally the last action in the action list. For example, after awarding a reward, a congratulatory message would be sent to the user.&lt;/p&gt;

&lt;p&gt;We realised that it was not necessary to wait for a sending message action to complete before processing the next batch of users. Moreover, the latency of the sending messages API is lower than awarding rewards. Hence, we adjusted the sending messages API to be asynchronous, so that the task of awarding rewards to the next batch of users can start while messages are being sent to the previous batch.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We have architected our batch jobs system in such a way so that it can be enhanced and optimised without redoing its work. For example, although we currently obtain the list of targeted users from a segmentation service, in the future, we may obtain this list from a different source, for example, all Grab Platinum tier members.&lt;/p&gt;

&lt;h3 id=&quot;join-us&quot;&gt;Join us&lt;/h3&gt;
&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.
Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Apr 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/supporting-large-campaigns-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/supporting-large-campaigns-at-scale</guid>
        
        <category>Kafka</category>
        
        <category>Scheduling</category>
        
        <category>Stream processing</category>
        
        <category>Batch processing</category>
        
        <category>Scheduled job</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How telematics helps Grab to improve safety</title>
        <description>&lt;p&gt;Telematics is a collection of sensor data such as accelerometer data, gyroscope data, and GPS data that a driver’s mobile phone provides, and we collect, during the ride. With this information, we apply data science logic to detect traffic events such as harsh braking, acceleration, cornering, and unsafe lane changes, in order to help improve our consumers’ ride experience.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As Grab grows to meet our consumers’ needs, the number of driver-partners has also grown. This requires us to ensure that our consumers’ safety continues to remain the highest priority as we scale. We developed an in-house telematics engine which uses mobile phone sensors to determine, evaluate, and quantify the driving behaviour of our driver-partners. This telemetry data is then evaluated and gives us better insights into our driver-partners’ driving patterns.&lt;/p&gt;

&lt;p&gt;Through our data, we hope to improve our driver-partners’ driving habits and reduce the likelihood of driving-related incidents on our platform. This telemetry data also helps us determine optimal insurance premiums for driver-partners with risky driving patterns and reward driver-partners who have better driving habits.&lt;/p&gt;

&lt;p&gt;In addition, we also merge telematics data with spatial data to further identify areas where dangerous driving manoeuvres happen frequently. This data is used to inform our driver-partners to be alert and drive more safely in such areas.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;With more consumers using the Grab app, we realised that purely relying on passenger feedback is not enough; we had no definitive way to tell which driver-partners were actually driving safely, when they deviated from their routes or even if they had been involved in an accident.&lt;/p&gt;

&lt;p&gt;To help address these issues, we developed an in-house telematics engine that analyses telemetry data, identifies driver-partners’ driving behaviour and habits, and provides safety reports for them.&lt;/p&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/telematics/image2.png&quot; alt=&quot;Real time ingestion architecture&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As shown in the diagram, our telematics SDK receives raw sensor data from our driver-partners’ devices and processes it in two ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;On-device processing for crash detection: Used to determine situations such as if the driver-partner has been in an accident.&lt;/li&gt;
  &lt;li&gt;Raising traffic events and generating safety reports after each job: Useful for detecting events like speeding and harsh braking.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: Safety reports are generated by our backend service using sensor data that is only uploaded as a text file after each ride.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;Our telematics framework relies on accelerometer, gyroscope and GPS sensors within the mobile device to infer the vehicle’s driving parameters. Both accelerometer and gyroscope are triaxial sensors, and their respective measurements are in the mobile device’s frame of reference.&lt;/p&gt;

&lt;p&gt;That being said, the data collected from these sensors have no fixed sample rate, so we need to implement sensor data time synchronisation. For example, there will be temporal misalignment between gyroscope and accelerometer data if they do not share the same timestamp. The sample rate that comes from the accelerometer and gyroscope also varies independently. Therefore, we need to uniformly sample the sensor data to be at the same frequency rate.&lt;/p&gt;

&lt;p&gt;This synchronisation process is done in two steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Interpolation to uniform time grid at a reasonably higher frequency.&lt;/li&gt;
  &lt;li&gt;Decimation from the higher frequency to the output data rate for accelerometer and gyroscope data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We then use the &lt;a href=&quot;https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/&quot;&gt;Fourier Transform&lt;/a&gt; to transform a signal from time domain to frequency domain for compression. These components are then written to a text file on the mobile device, compressed, and uploaded after the end of each ride.&lt;/p&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;There are a few takeaways that we learned from this project:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sensor data frequency: There are many device manufacturers out there for Android and each one of them has a different sensor chipset. The frequency of the sensor data may vary from device to device.&lt;/li&gt;
  &lt;li&gt;Four-wheel (4W) vs two-wheel (2W): The behaviour is different for a driver-partner on 2W vs 4W, so we need different rules for each.&lt;/li&gt;
  &lt;li&gt;Hardware axis-bias: The device may not be aligned with the vehicle during the ride. It cannot be assumed that the phone will remain in a fixed orientation throughout the trip, so the mobile device sensors might not accurately measure the acceleration/braking or sharp turning of the vehicle.&lt;/li&gt;
  &lt;li&gt;Sensor noise: There are artifacts in sensor readings, which are basically a single outlier event that represents an error and is not a valid sensor reading.&lt;/li&gt;
  &lt;li&gt;Time-synchronisation: GPS, accelerometer, and gyroscope events are captured independently by three different sensors and have different time formats. These events will need to be transformed into the same time grid in order to work together. For example, the GPS location from 30 seconds prior to the gyroscope event will not work as they are out of sync.&lt;/li&gt;
  &lt;li&gt;Data compression and network consumption: Longer rides will contain more telematics data.  It will result in a bigger upload size and increase in time for file compression.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;There are a few milestones that we want to accomplish with our telematics framework in the future. However, our number one goal is to extend telematics to all bookings across Grab verticals. We are also planning to add more on-device rules and data processing for event detections to further eliminate future delays from backend communication for crash detection.&lt;/p&gt;

&lt;p&gt;With the data from our telematics framework, we can improve our passengers’ experience and improve safety for both passengers and driver-partners.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 24 Mar 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/telematics-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/telematics-at-grab</guid>
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Driving patterns</category>
        
        <category>Safety</category>
        
        <category>Analytics</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Real-time data ingestion in Grab</title>
        <description>&lt;p&gt;Typically, modern applications use various database engines for their service needs; within Grab, these would be MySQL, Aurora and DynamoDB. Lately, the Caspian team has observed an increasing need to consume real-time data for many service teams. These real-time changes in database records help to support online and offline business decisions for hundreds of teams.&lt;/p&gt;

&lt;p&gt;Because of that, we have invested time into synchronising data from MySQL, Aurora and Dynamodb to the message queue, i.e. Kafka. In this blog, we share how real-time data ingestion has helped since it was launched.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Over the last few years, service teams had to write all transactional data twice: once into Kafka and once into the database. This helped to solve the inter-service communication challenges and obtain audit trail logs. However, if the transactions fail, data integrity becomes a prominent issue. Moreover, it is a daunting task for developers to maintain the schema of data written into Kafka.&lt;/p&gt;

&lt;p&gt;With real-time ingestion, there is a notably better schema evolution and guaranteed data consistency; service teams no longer need to write data twice.&lt;/p&gt;

&lt;p&gt;You might be wondering, why don’t we have a single transaction that spans the services’ databases and Kafka, to make data consistent? This would not work as Kafka does not support being enlisted in distributed transactions. In some situations, we might end up having new data persisting into the services’ databases, but not having the corresponding message sent to Kafka topics.&lt;/p&gt;

&lt;p&gt;Instead of registering or modifying the mapped table schema in Golang writer into Kafka beforehand, service teams tend to avoid such schema maintenance tasks entirely. In such cases, real-time ingestion can be adopted where data exchange among the heterogeneous databases or replication between source and replica nodes is required.&lt;/p&gt;

&lt;p&gt;While reviewing the key challenges around real-time data ingestion, we realised that there were many potential user requirements to include. To build a standardised solution, we identified several points that we felt were high priority:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make transactional data readily available in real time to drive business decisions at scale.&lt;/li&gt;
  &lt;li&gt;Capture audit trails of any given database.&lt;/li&gt;
  &lt;li&gt;Get rid of the &lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html&quot;&gt;burst read&lt;/a&gt; on databases caused by SQL-based query ingestion.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To empower Grabbers with real-time data to drive their business decisions, we decided to take a scalable event-driven approach, which is being facilitated with a bunch of internal products, and designed a solution for real-time ingestion.  &lt;/p&gt;

&lt;h2 id=&quot;anatomy-of-architecture&quot;&gt;Anatomy of architecture&lt;/h2&gt;
&lt;p&gt;The solution for real-time ingestion has several key components:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Stream data storage&lt;/li&gt;
  &lt;li&gt;Event producer&lt;/li&gt;
  &lt;li&gt;Message queue&lt;/li&gt;
  &lt;li&gt;Stream processor&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image2.png&quot; alt=&quot;Real time ingestion architecture&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1. Real time ingestion architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;stream-storage&quot;&gt;Stream storage&lt;/h3&gt;
&lt;p&gt;Stream storage acts as a repository that stores the data transactions in order with &lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html&quot;&gt;exactly-once guarantee&lt;/a&gt;. However, the level of order in stream storage differs with regards to different databases.&lt;/p&gt;

&lt;p&gt;For MySQL or Aurora, transaction data is stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; files in sequence and rotated, thus ensuring global order. Data with global order assures that all MySQL records are ordered and reflects the real life situation. For example, when transaction logs are replayed or consumed by downstream consumers, consumer A’s Grab food order at 12:01:44 pm will &lt;strong&gt;always appear before&lt;/strong&gt; consumer B’s order at 12:01:45 pm.&lt;/p&gt;

&lt;p&gt;However, this does not necessarily hold true for DynamoDB stream storage as DynamoDB streams are partitioned. Audit trails of a given record show that they go into the same partition in the same order, ensuring consistent partitioned order. Thus when replay happens, consumer B’s order might appear before consumer A’s.&lt;/p&gt;

&lt;p&gt;Moreover, there are multiple formats to choose from for both MySQL &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; and DynamoDB stream records. We eventually set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ROW&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; formats and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEW_AND_OLD_IMAGES&lt;/code&gt; for DynamoDB stream records. This depicts the detailed information before and after modifying any given table record. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; and DynamoDB stream main fields are tabulated in Figures 2 and 3 respectively.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image8.png&quot; alt=&quot;Binlog record schema&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2. Binlog record schema&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image6.png&quot; alt=&quot;DynamoDB stream record schema&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3. DynamoDB stream record schema&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;event-producer&quot;&gt;Event producer&lt;/h3&gt;
&lt;p&gt;Event producers take in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; messages or stream records and output to the message queue. We evaluated several technologies for the different database engines.&lt;/p&gt;

&lt;p&gt;For MySQL or Aurora, three solutions were evaluated: Debezium, Maxwell, and Canal. We chose to onboard Debezium as it is deeply integrated with the Kafka Connect framework. Also, we see the potential of extending solutions among other external systems whenever moving large collections of data in and out of the Kafka cluster.&lt;/p&gt;

&lt;p&gt;One such example is the &lt;a href=&quot;https://github.com/trustpilot/kafka-connect-dynamodb&quot;&gt;open source project&lt;/a&gt; that attempts to build a custom DynamoDB connector extending the Kafka Connect (KC) framework. It self manages checkpointing via an additional DynamoDB table and can be deployed on KC smoothly.&lt;/p&gt;

&lt;p&gt;However, the DynamoDB connector fails to exploit the fundamental nature of storage DynamoDB streams: dynamic partitioning and auto-scaling based on the traffic. Instead, it spawns only a single thread task to process all shards of a given DynamoDB table. As a result, downstream services suffer from data latency the most when write traffic surges.&lt;/p&gt;

&lt;p&gt;In light of this, the lambda function becomes the most suitable candidate as the event producer. Not only does the concurrency of lambda functions scale in and out based on actual traffic, but the trigger frequency is also adjustable at your discretion.&lt;/p&gt;

&lt;h3 id=&quot;kafka&quot;&gt;Kafka&lt;/h3&gt;
&lt;p&gt;This is the distributed data store optimised for ingesting and processing data in real time. It is widely adopted due to its high scalability, fault-tolerance, and parallelism. The messages in Kafka are abstracted and encoded into Protobuf. &lt;/p&gt;

&lt;h3 id=&quot;stream-processor&quot;&gt;Stream processor&lt;/h3&gt;
&lt;p&gt;The stream processor consumes messages in Kafka and writes into S3 every minute. There are a number of options readily available in the market; Spark and Flink are the most common choices. Within Grab, we deploy a Golang library to deal with the traffic.&lt;/p&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use cases&lt;/h2&gt;
&lt;p&gt;Now that we’ve covered how real-time data ingestion is done in Grab, let’s look at some of the situations that could benefit from real-time data ingestion.&lt;/p&gt;

&lt;h3 id=&quot;1-data-pipelines&quot;&gt;1. Data pipelines&lt;/h3&gt;
&lt;p&gt;We have thousands of pipelines running hourly in Grab. Some tables have significant growth and generate workload beyond what a SQL-based query can handle. An hourly data pipeline would incur a read spike on the production database shared among various services, draining CPU and memory resources. This deteriorates other services’ performance and could even block them from reading. With real-time ingestion, the query from data pipelines would be incremental and span over a period of time.&lt;/p&gt;

&lt;p&gt;Another scenario where we switch to real-time ingestion is when a missing index is detected on the table. To speed up the query, SQL-based query ingestion requires indexing on columns such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;created_at&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;updated_at&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id&lt;/code&gt;. Without indexing, SQL based query ingestion would either result in high CPU and memory usage, or fail entirely.&lt;/p&gt;

&lt;p&gt;Although adding indexes for these columns would resolve this issue, it comes with a cost, i.e. a copy of the indexed column and primary key is created on disk and the index is kept in memory. Creating and maintaining an index on a huge table is much costlier than for small tables. With performance consideration in mind, it is &lt;em&gt;not&lt;/em&gt; recommended to add indexes to an existing huge table.&lt;/p&gt;

&lt;p&gt;Instead, real-time ingestion overshadows SQL-based ingestion. We can spawn a new connector, archiver (Coban team’s Golang library that dumps data from Kafka at minutes-level frequency) and compaction job to bubble up the table record from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; to the destination table in the Grab data lake.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image5.png&quot; alt=&quot;Using real-time ingestion for data pipelines&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4. Using real-time ingestion for data pipelines&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-drive-business-decisions&quot;&gt;2. Drive business decisions&lt;/h3&gt;

&lt;p&gt;A key use case of enabling real-time ingestion is driving business decisions at scale without even touching the source services. &lt;a href=&quot;https://microservices.io/patterns/data/saga.html&quot;&gt;Saga&lt;/a&gt; pattern is commonly adopted in the microservice world. Each service has its own database, splitting an overarching database transaction into a series of multiple database transactions. Communication is established among services via message queue i.e. Kafka.&lt;/p&gt;

&lt;p&gt;In an earlier &lt;a href=&quot;https://engineering.grab.com/search-indexing-optimisation&quot;&gt;tech blog&lt;/a&gt; published by the Grab Search team, we talked about how real-time ingestion with Debezium optimised and boosted search capabilities. Each MySQL table is mapped to a Kafka topic and one or multiple topics build up a search index within Elasticsearch.&lt;/p&gt;

&lt;p&gt;With this new approach, there is no data loss, i.e. changes via MySQL command line tool or other DB management tools can be captured. Schema evolution is also naturally supported; the new schema defined within a MySQL table is inherited and stored in Kafka. No producer code change is required to make the schema consistent with that in MySQL. Moreover, the database read has been reduced by 90 percent including the efforts of the Data Synchronisation Platform.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image4.png&quot; alt=&quot;Grab Search team use case&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5. Grab Search team use case&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The GrabFood team exemplifies mostly similar advantages in the DynamoDB area. The only differences compared to MySQL are that the frequency of the lambda functions is adjustable and parallelism is auto-scaled based on the traffic. By auto-scaling, we mean that more lambda functions will be auto-deployed to cater to a sudden spike in traffic, or destroyed as the traffic falls.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image1.png&quot; alt=&quot;Grab Food team use case&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6. Grab Food team use case&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-database-replication&quot;&gt;3. Database replication&lt;/h3&gt;

&lt;p&gt;Another use case we did not originally have in mind is incremental data replication for disaster recovery. Within Grab, we enable DynamoDB streams for tier 0 and critical DynamoDB tables. Any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;insert&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delete&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modify&lt;/code&gt; operations would be propagated to the disaster recovery table in another availability zone.&lt;/p&gt;

&lt;p&gt;When migrating or replicating databases, we use the &lt;a href=&quot;https://martinfowler.com/bliki/StranglerFigApplication.html&quot;&gt;strangler fig pattern&lt;/a&gt;, which offers an incremental, reliable process for migrating databases. This is a method whereby a new system slowly grows on top of an old system and is gradually adopted until the old system is “strangled” and can simply be removed. Figure 7 depicts how DynamoDB streams drive real-time synchronisation between tables in different regions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image3.png&quot; alt=&quot;Data replication among DynamoDB tables across different regions in DBOps team&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7. Data replication among DynamoDB tables across different regions in DBOps team&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;4-deliver-audit-trails&quot;&gt;4. Deliver audit trails&lt;/h3&gt;

&lt;p&gt;Reasons for maintaining data audit trails are manifold in Grab: regulatory requirements might mandate businesses to keep complete historical information of a consumer or to apply machine learning techniques to detect fraudulent transactions made by consumers. Figure 8 demonstrates how we deliver audit trails in Grab.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-ingestion/image9.png&quot; alt=&quot;Data replication among DynamoDB tables across different regions in DBOps team&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 8. Deliver audit trails in Grab&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Real time ingestion is playing a pivotal role in Grab’s ecosystem. It:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;boosts data pipelines with less read pressure imposed on databases shared among various services;&lt;/li&gt;
  &lt;li&gt;empowers real-time business decisions with assured resource efficiency;&lt;/li&gt;
  &lt;li&gt;provides data replication among tables residing in various regions; and&lt;/li&gt;
  &lt;li&gt;delivers audit trails that either keep complete history or help unearth fraudulent operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since this project launched, we have made crucial enhancements to facilitate daily operations with several in-house products that are used for data onboarding, quality checking, maintaining freshness, etc.&lt;/p&gt;

&lt;p&gt;We will continuously improve our platform to provide users with a seamless experience in data ingestion, starting with unifying our internal tools. Apart from providing a unified platform, we will also contribute more ideas to the ingestion, extending it to Azure and GCP, supporting multi-catalogue and offering multi-tenancy.&lt;/p&gt;

&lt;p&gt;In our next blog, we will drill down to other interesting features of real-time ingestion, such as how ordering is achieved in different cases and custom partitioning in real-time ingestion. Stay tuned!&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Mar 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/real-time-data-ingestion</link>
        <guid isPermaLink="true">https://engineering.grab.com/real-time-data-ingestion</guid>
        
        <category>Engineering</category>
        
        <category>Data ingestion</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Abacus - Issuing points for multiple sources</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Earlier in 2021 we published an article on &lt;a href=&quot;https://engineering.grab.com/trident-real-time-event-processing-at-scale&quot;&gt;Trident&lt;/a&gt;, Grab’s in-house real-time if this, then that (IFTTT) engine which manages campaigns for the Grab Loyalty Programme. The Grab Loyalty Programme encourages consumers to make Grab transactions by rewarding points when transactions are made. Grab rewards two types of points namely OVOPoints and GrabRewards Points (GRP). OVOPoints are issued for transactions made in Indonesia and GRP are for the transactions that are made in all other markets. In this article, the term GRP will be used to refer to both OVOPoints and GrabRewards Points.&lt;/p&gt;

&lt;p&gt;Rewarding GRP is one of the main components of the Grab Loyalty Programme. By rewarding GRP, our consumers are incentivised to transact within the Grab ecosystem. Consumers can then redeem their GRP for a range of exciting items on the GrabRewards catalogue or to offset the cost of their spendings.&lt;/p&gt;

&lt;p&gt;As we continue to grow our consumer base and our product offerings, a more robust platform is needed to ensure successful points transactions. In this post, we will share the challenges in rewarding GRP and how Abacus, our Point Issuance platform helps to overcome these challenges while managing various use cases.&lt;/p&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;h3 id=&quot;growing-number-of-products&quot;&gt;Growing number of products&lt;/h3&gt;
&lt;p&gt;The number of Grab’s product offerings has grown as part of Grab’s goal in becoming a superapp. The demand for rewarding GRP increased as each product team looked for ways to retain consumer loyalty. For this, we needed a platform which could support the different requirements from each product team.&lt;/p&gt;

&lt;h3 id=&quot;external-partnerships&quot;&gt;External partnerships&lt;/h3&gt;
&lt;p&gt;Grab’s external partnerships consist of both one- and two-way point exchanges. With selected partners, Grab users are able to convert their GRP for the partner’s loyalty programme points, and the other way around.&lt;/p&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use cases&lt;/h2&gt;
&lt;p&gt;Besides the need to cater for the growing number of products and external partnerships, Grab needed a centralised points management system which could cater to various use cases of points rewarding. Let’s take a look at the use cases.&lt;/p&gt;

&lt;h3 id=&quot;any-product-any-points&quot;&gt;Any product, any points&lt;/h3&gt;
&lt;p&gt;There are many products in Grab and each product should be able to reward different GRP for different scenarios. Each product rewards GRP based on the goal they are trying to achieve.&lt;/p&gt;

&lt;p&gt;The following examples illustrate the different scenarios:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GrabCar&lt;/strong&gt;: Reward 100 GRP for when a driver cancels a booking as a form of compensation or to reward GRP for every ride a consumer makes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GrabFood&lt;/strong&gt;: Reward consumers for each meal order.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GrabPay&lt;/strong&gt;: Reward consumers three times the number of GRP for using GrabPay instead of cash as the mode of payment.&lt;/p&gt;

&lt;h3 id=&quot;more-points-for-loyal-consumers&quot;&gt;More points for loyal consumers&lt;/h3&gt;
&lt;p&gt;Another use case is to reward loyal consumers with more points. This incentivises consumers to transact within the Grab ecosystem. One example are membership tiers granted based on the number of GRP a consumer has accumulated. There are four membership tiers: Member, Silver, Gold and Platinum.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/point-multiplier.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Point multiplier&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There are different points multipliers for different membership tiers. For example, a Gold member would earn 2.25 GRP for every dollar spent while a Silver member earns only 1.5 GRP for the same amount spent. A consumer can view their membership tier and GRP information from the account page on the Grab app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/grp-and-membership-tier.png&quot; alt=&quot;GrabRewards Points and membership tier information&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;GrabRewards Points and membership tier information&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;growing-number-of-transactions&quot;&gt;Growing number of transactions&lt;/h3&gt;
&lt;p&gt;Teams within Grab and external partners use GRP in their business. There is a need for a platform that can process millions of transactions every day with high availability rates. Errors can easily impact the issuance of points which may affect our consumers’ trust.&lt;/p&gt;

&lt;h2 id=&quot;our-solution---abacus&quot;&gt;Our solution - Abacus&lt;/h2&gt;
&lt;p&gt;To overcome the challenges and cater for various use cases, we developed a Points Management System known as Abacus. It offers an interface for external partners with the capability to handle millions of daily transactions without significant downtime.&lt;/p&gt;

&lt;h2 id=&quot;points-rewarding&quot;&gt;Points rewarding&lt;/h2&gt;
&lt;p&gt;There are seven main components of Abacus as shown in the following architectural diagram. Details of each component are explained in this section.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/abacus-architecture.png&quot; alt=&quot;Abacus architecture&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Abacus architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;transaction-input-source&quot;&gt;Transaction input source&lt;/h3&gt;
&lt;p&gt;The points rewarding process begins when a transaction is complete. Abacus listens to streams for completed transactions on the Grab platform. Each transaction that abacus receives in the stream carries the data required to calculate the GRP to be rewarded such as country ID, product ID, and payment ID etc.&lt;/p&gt;

&lt;p&gt;Apart from computing the number of GRP to be rewarded for a transaction and then rewarding the points, Abacus also allows clients from within the Grab platform and outside of the Grab platform to make an API call to reward GRP to consumers. The client who wants to reward their consumers with GRP will call Abacus with either a specific point value (for example 100 points) or will provide the necessary details like transaction amount and the relevant multipliers for Abacus to compute the points and then reward them.&lt;/p&gt;

&lt;h3 id=&quot;point-calculation-module&quot;&gt;Point Calculation module&lt;/h3&gt;
&lt;p&gt;The Point Calculation module calculates the GRP using the data and multipliers that are unique to each transaction.&lt;/p&gt;

&lt;h4 id=&quot;point-calculation-dependencies-for-internal-services&quot;&gt;Point Calculation dependencies for internal services&lt;/h4&gt;
&lt;p&gt;Point Calculation dependencies are the multipliers needed to calculate the number of points. The Point Calculation module fetches the correct point multipliers for each transaction. The multipliers are configured by specific country teams when the product is launched. They may vary by country to allow country teams the flexibility to achieve their growth and retention targets. There are different types of multipliers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vertical multiplier&lt;/strong&gt;: The multiplier for each vertical. A vertical is a service or product offered by Grab. Examples of verticals are GrabCar and GrabFood. The multiplier can be different for each vertical.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EPPF multiplier&lt;/strong&gt;: The effective price per fare multiplier. EPPF is the reference conversion rate per point. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;EPPF = 1.0; if you are issuing X points per SGD1&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EPPF = 0.1; if you are issuing X points per THB10&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EPPF = 0.0001; if you are issuing X points per IDR10,000&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Payment Type multiplier&lt;/strong&gt;: The multiplier for different modes of payments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tier multiplier&lt;/strong&gt;: The multiplier for each tier.&lt;/p&gt;

&lt;h4 id=&quot;point-calculation-formula-for-internal-clients&quot;&gt;Point Calculation formula for internal clients&lt;/h4&gt;
&lt;p&gt;The Point Calculation module uses a formula to calculate GRP. The formula is the product of all the multipliers and the transaction amount.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GRP = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following are examples for calculating GRP:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example 1:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bob is a platinum member of Grab. He orders lunch in Singapore for SGD15 using GrabPay as the payment method. Let’s assume the following:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Vertical multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;EPPF multiplier&lt;/em&gt; = 1&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cashless multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tier multiplier&lt;/em&gt; = 3&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GRP&lt;/strong&gt; = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier&lt;/p&gt;

&lt;p&gt;= 15 * 2 * 1 * 2 * 3&lt;/p&gt;

&lt;p&gt;= 180&lt;/p&gt;

&lt;p&gt;From this transaction, Bob earns 180 GRP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example 2:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Jane is a Gold member of Grab. She orders lunch in Indonesia for Rp150000 using GrabPay as the payment method. Let’s assume the following:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Vertical multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;EPPF multiplier&lt;/em&gt; = 0.00005&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cashless multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tier multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GRP&lt;/strong&gt; = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier&lt;/p&gt;

&lt;p&gt;= 150000 * 2 * 0.00005 * 2 * 2&lt;/p&gt;

&lt;p&gt;= 60&lt;/p&gt;

&lt;p&gt;From this transaction, Jane earns 60 GRP.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/multipliers-for-payment-options-and-tiers.jpg&quot; alt=&quot;Example of multipliers for payment options and tiers&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Example of multipliers for payment options and tiers&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;point-calculation-dependencies-for-external-clients&quot;&gt;Point Calculation dependencies for external clients&lt;/h4&gt;
&lt;p&gt;External partners supply the Point Calculation dependencies which are then configured in our backend at the time of integration. These external partners can set their own multipliers instead of using the above mentioned multipliers which are specific to Grab. This &lt;a href=&quot;https://developer.grab.com/assets/docs/grab-rewards/Rewards_Events_API.pdf&quot;&gt;document&lt;/a&gt; details the APIs which are used to award points for external clients.&lt;/p&gt;

&lt;h3 id=&quot;simple-queue-service&quot;&gt;Simple Queue Service&lt;/h3&gt;
&lt;p&gt;Abacus uses Amazon Simple Queue Service (SQS) to ensure that the points system process is robust and fault tolerant.&lt;/p&gt;

&lt;h4 id=&quot;point-awarding-sqs&quot;&gt;Point Awarding SQS&lt;/h4&gt;
&lt;p&gt;If there are no errors during the Point Calculation process, the Point Calculation module will send a message containing the points to be awarded to the Point Awarding SQS.&lt;/p&gt;

&lt;h4 id=&quot;retry-sqs&quot;&gt;Retry SQS&lt;/h4&gt;
&lt;p&gt;The Point Calculation module may not receive the required data when there is a downtime in the Point Calculation dependencies. If this occurs,  an error is triggered and the Point Calculation module will send a message to Retry SQS. Messages sent to the Retry SQS will be re-processed by the Point Calculation module. This ensures that the points are properly calculated despite having outages on dependencies. Every message that we push to either the Point Awarding SQS or Retry SQS will have a field called Idempotency key which is used to ensure that we reward the points only once to a particular transaction.&lt;/p&gt;

&lt;h3 id=&quot;point-awarding-module&quot;&gt;Point Awarding module&lt;/h3&gt;
&lt;p&gt;The successful calculation of GRP triggers a message to the Point Awarding module via the Point SQS. The Point Awarding module tries to reward GRP to the consumer’s account. Upon successful completion, an ACK is sent back to the Point SQS signalling that the message was successfully processed and triggers deletion of the message. If Point SQS does not receive an ACK, the message is redelivered after an interval. This process ensures that the points system is robust and fault tolerant.&lt;/p&gt;

&lt;h3 id=&quot;ledger&quot;&gt;Ledger&lt;/h3&gt;
&lt;p&gt;GRP is rewarded to the consumer once it is updated in the Ledger. The Ledger tracks how many GRP a consumer has accumulated, what they were earned for, and the running total number of GRP.&lt;/p&gt;

&lt;h3 id=&quot;notification-service&quot;&gt;Notification service&lt;/h3&gt;
&lt;p&gt;Once the Ledger is updated, the Notification service sends the consumer a message about the GRP they receive.&lt;/p&gt;

&lt;h3 id=&quot;point-kafka-stream&quot;&gt;Point Kafka stream&lt;/h3&gt;
&lt;p&gt;For all successful GRP transactions, Abacus sends a message to the Point Kafka stream. Downstream services listen to this stream to identify the consumer’s behaviour and take the appropriate actions. Services of this stream can listen to events they are interested in and execute their business logic accordingly. For example, a service can use the information from the Point Kafka stream to determine a consumer’s membership tier.&lt;/p&gt;

&lt;h2 id=&quot;points-expiry&quot;&gt;Points expiry&lt;/h2&gt;
&lt;p&gt;Further addition to Abacus is the handling of points expiry. The Expiry Extension module enables activity-based points expiry. This enables GRP to not expire as long as the consumer makes one Grab transaction within the next three or six months from their last transaction.&lt;/p&gt;

&lt;p&gt;The Expiry Extension module updates the point expiry date to the database after successfully rewarding GRP to the consumer. At the end of each month, a process loads all consumers whose points will expire in that particular month and sends it to the Point Expiry SQS. The Point Expiry Consumer will then expire all the points for the consumers and this data is updated in the Ledger. This process repeats on a monthly basis.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/expiry-extension-module.png&quot; alt=&quot;Expiry Extension module&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Expiry Extension module&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Points expiry date is always the last day of the third or sixth month. For example, Adam makes a transaction on 10 January. His points expiry date is 31 July which is six months from the month of his last transaction. Adam then makes a transaction on 28 February. His points expiry period is shifted by one month to 31 August.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/points-expiry.gif&quot; alt=&quot;Points expiry&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Points expiry&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The Abacus platform enables us to perform millions of GRP transactions on a daily basis. Being able to curate rewards for consumers increases the value proposition of our products and consumer retention. If you have any comments or questions about Abacus, feel free to leave a comment below.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Arianto Wibowo and Vaughn Friesen.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Mar 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/abacus-issuing-points-for-multiple-sources</link>
        <guid isPermaLink="true">https://engineering.grab.com/abacus-issuing-points-for-multiple-sources</guid>
        
        <category>Engineering</category>
        
        <category>Event processing</category>
        
        <category>Optimisation</category>
        
        <category>Stream Processing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Exposing a Kafka Cluster via a VPC Endpoint Service</title>
        <description>&lt;p&gt;In large organisations, it is a common practice to isolate the cloud resources of different verticals. Amazon Web Services (AWS) Virtual Private Cloud (VPC) is a convenient way of doing so. At Grab, while our core AWS services reside in a main VPC, a number of Grab Tech Families (TFs) have their own dedicated VPC. One such example is &lt;a href=&quot;https://www.grab.com/id/kios/&quot;&gt;GrabKios&lt;/a&gt;. Previously known as “Kudo”, GrabKios was acquired by Grab in 2017 and has always been residing in its own AWS account and dedicated VPC.&lt;/p&gt;

&lt;p&gt;In this article, we explore how we exposed an Apache Kafka cluster across multiple Availability Zones (AZs) in Grab’s main VPC, to producers and consumers residing in the GrabKios VPC, via a &lt;a href=&quot;https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html&quot;&gt;VPC Endpoint Service&lt;/a&gt;. This design is part of Coban unified stream processing platform at Grab.&lt;/p&gt;

&lt;p&gt;There are several ways of enabling communication between applications across distinct VPCs; VPC peering is the most straightforward and affordable option. However, it potentially exposes the entire VPC networks to each other, needlessly increasing the attack surface.&lt;/p&gt;

&lt;p&gt;Security has always been one of Grab’s top concerns and with Grab’s increasing growth, there is a need to deprecate VPC peering and shift to a method of only exposing services that require remote access. The AWS VPC Endpoint Service allows us to do exactly that for TCP/IPv4 communications within a single &lt;a href=&quot;https://aws.amazon.com/about-aws/global-infrastructure/regions_az/&quot;&gt;AWS region&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Setting up a VPC Endpoint Service compared to VPC peering is already relatively complex. On top of that, we need to expose an Apache Kafka cluster via such an endpoint, which comes with an extra challenge. Apache Kafka requires clients, called producers and consumers, to be able to deterministically establish a TCP connection to all &lt;a href=&quot;https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-brokers.html&quot;&gt;brokers&lt;/a&gt; forming the cluster, not just any one of them.&lt;/p&gt;

&lt;p&gt;Last but not least, we need a design that optimises performance and cost by limiting data transfer across AZs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: &lt;strong&gt;All&lt;/strong&gt; variable names, port numbers and other details used in this article are only used as examples.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;architecture-overview&quot;&gt;Architecture overview&lt;/h2&gt;

&lt;p&gt;As shown in this diagram, the Kafka cluster resides in the service provider VPC (Grab’s main VPC) while local Kafka producers and consumers reside in the service consumer VPC (GrabKios VPC).&lt;/p&gt;

&lt;p&gt;In Grab’s main VPC, we created a Network Load Balancer (NLB) and set it up across all three AZs, enabling cross-zone load balancing. We then created a VPC Endpoint Service associated with that NLB.&lt;/p&gt;

&lt;p&gt;Next, we created a VPC Endpoint Network Interface in the GrabKios VPC, also set up across all three AZs, and attached it to the remote VPC endpoint service in Grab’s main VPC. Apart from this, we also created a Route 53 Private Hosted Zone &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.grab&lt;/code&gt; and a CNAME record &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt; that points to the VPC Endpoint Network Interface hostname.&lt;/p&gt;

&lt;p&gt;Lastly, we configured producers and consumers to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab:10000&lt;/code&gt; as their Kafka bootstrap server endpoint, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10000/tcp&lt;/code&gt; being an arbitrary port of our choosing. We will explain the significance of these in later sections.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/exposing-kafka-cluster/image2.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;network-load-balancer-setup&quot;&gt;Network Load Balancer setup&lt;/h2&gt;

&lt;p&gt;On the NLB in Grab’s main VPC, we set up the corresponding bootstrap listener on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10000/tcp&lt;/code&gt;, associated with a target group containing all of the Kafka brokers forming the cluster. But this listener alone is not enough.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, Apache Kafka requires producers and consumers to be able to deterministically establish a TCP connection to all brokers. That’s why we created one listener for every broker in the cluster, incrementing the TCP port number for each new listener, so each broker endpoint would have the same name but with different port numbers, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab:10001&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab:10002&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We then associated each listener with a dedicated target group containing only the targeted Kafka broker, so that remote producers and consumers could differentiate between the brokers by their TCP port number.&lt;/p&gt;

&lt;p&gt;The following listeners and associated target groups were set up on the NLB:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10000/tcp&lt;/code&gt; (bootstrap) -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 101, broker 201, broker 301]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10001/tcp&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 101]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10002/tcp&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 201]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10003/tcp&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 301]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;security-group-rules&quot;&gt;Security Group rules&lt;/h2&gt;

&lt;p&gt;In the Kafka brokers’ Security Group (SG), we added an ingress SG rule allowing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; traffic from each of the three private IP addresses of the NLB. As mentioned earlier, the NLB was set up across all three AZs, with each having its own private IP address.&lt;/p&gt;

&lt;p&gt;On the GrabKios VPC (consumer side), we created a new SG and attached it to the VPC Endpoint Network Interface. We also added ingress rules to allow all producers and consumers to connect to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tcp/10000-10003&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;kafka-setup&quot;&gt;Kafka setup&lt;/h2&gt;

&lt;p&gt;Kafka brokers typically come with a listener on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9092/tcp&lt;/code&gt;, advertising the brokers by their private IP addresses. We kept that default listener so that local producers and consumers in Grab’s main VPC could still connect directly.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kcat -L -b 10.0.0.1:9092
 3 brokers:
 broker 101 at 10.0.0.1:9092 (controller)  
 broker 201 at 10.0.0.2:9092
 broker 301 at 10.0.0.3:9092
... truncated output ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also configured all brokers with an additional listener on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; that advertises the brokers by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Their shared private name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Their distinct TCP ports previously set up on the NLB’s dedicated listeners.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kcat -L -b 10.0.0.1:9094
 3 brokers:
 broker 101 at kafka.grab:10001 (controller)  
 broker 201 at kafka.grab:10002
 broker 301 at kafka.grab:10003
... truncated output ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that there is a difference in how the broker’s endpoints are advertised in the two outputs above. The latter enables connection to any particular broker from the GrabKios VPC via the VPC Endpoint Service.&lt;/p&gt;

&lt;p&gt;It would definitely be possible to advertise the brokers directly with the remote VPC Endpoint Interface hostname instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt;, but relying on such a private name presents at least two advantages.&lt;/p&gt;

&lt;p&gt;First, it decouples the Kafka deployment in the service provider VPC from the infrastructure deployment in the service consumer VPC. Second, it makes the Kafka cluster easier to expose to other remote VPCs, should we need it in the future.&lt;/p&gt;

&lt;h2 id=&quot;limiting-data-transfer-across-availability-zones&quot;&gt;Limiting data transfer across Availability Zones&lt;/h2&gt;

&lt;p&gt;At this stage of the setup, our Kafka cluster is &lt;strong&gt;fully reachable&lt;/strong&gt; from producers and consumers in the GrabKios VPC. Yet, the design is not optimal.&lt;/p&gt;

&lt;p&gt;When a producer or a consumer in the GrabKios VPC needs to connect to a particular broker, it uses its individual endpoint made up of the shared name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt; and the broker’s dedicated TCP port.&lt;/p&gt;

&lt;p&gt;The shared name arbitrarily resolves into one of the three IP addresses of the VPC Endpoint Network Interface, one for each AZ.&lt;/p&gt;

&lt;p&gt;Hence, there is a fair chance that the obtained IP address is neither in the client’s AZ nor in that of the target Kafka broker. The probability of this happening can be as high as 2/3 when both client and broker reside in the same AZ and 1/3 when they do not.&lt;/p&gt;

&lt;p&gt;While that is of little concern for the initial bootstrap connection, it becomes a serious drawback for actual data transfer, impacting the performance and incurring unnecessary data transfer cost.&lt;/p&gt;

&lt;p&gt;For this reason, we created &lt;strong&gt;three additional&lt;/strong&gt; CNAME records in the Private Hosted Zone in the GrabKios VPC, one for each AZ, with each pointing to the VPC Endpoint Network Interface zonal hostname in the corresponding AZ:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-az1.grab&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-az2.grab&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-az3.grab&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that we used az1, az2, az3 instead of the typical AWS 1a, 1b, 1c suffixes, because the latter’s mapping is not consistent across AWS accounts.&lt;/p&gt;

&lt;p&gt;We also reconfigured each Kafka broker in Grab’s main VPC by setting their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; listener to advertise brokers by their new zonal private names.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kcat -L -b 10.0.0.1:9094
 3 brokers:
 broker 101 at kafka-az1.grab:10001 (controller)  
 broker 201 at kafka-az2.grab:10002
 broker 301 at kafka-az3.grab:10003
... truncated output ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our private zonal names are shared by all brokers in the same AZ while TCP ports remain distinct for each broker. However, this is not clearly shown in the output above because our cluster only counts three brokers, one in each AZ.&lt;/p&gt;

&lt;p&gt;The previous common name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt; remains in the GrabKios VPC’s Private Hosted Zone and allows connections to any broker via an arbitrary, likely non-optimal route. GrabKios VPC producers and consumers still use that highly-available endpoint to initiate bootstrap connections to the cluster.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/exposing-kafka-cluster/image1.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;future-improvements&quot;&gt;Future improvements&lt;/h2&gt;

&lt;p&gt;For this setup, scalability is our main challenge. If we add a new broker to this Kafka cluster, we would need to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Assign a new TCP port number to it.&lt;/li&gt;
  &lt;li&gt;Set up a new dedicated listener on that TCP port on the NLB.&lt;/li&gt;
  &lt;li&gt;Configure the newly spun up Kafka broker to advertise its service with the same TCP port number and the private zonal name corresponding to its AZ.&lt;/li&gt;
  &lt;li&gt;Add the new broker to the target group of the bootstrap listener on the NLB.&lt;/li&gt;
  &lt;li&gt;Update the network SG rules on the service consumer side to allow connections to the newly allocated TCP port.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We rely on Terraform to dynamically deploy all AWS infrastructure and on Jenkins and Ansible to deploy and configure Apache Kafka. There is limited overhead but there are still a few manual actions due to a lack of integration. These include transferring newly allocated TCP ports and their corresponding EC2 instances’ IP addresses to our Ansible inventory, commit them to our codebase and trigger a Jenkins job deploying the new Kafka broker.&lt;/p&gt;

&lt;p&gt;Another concern of this setup is that it is only applicable for AWS. As we are aiming to be multi-cloud, we may need to port it to Microsoft Azure and leverage the &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/private-link/private-link-service-overview&quot;&gt;Azure Private Link service&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In both cases, running Kafka on Kubernetes with the Strimzi operator would be helpful in addressing the scalability challenge and reducing our adherence to one particular cloud provider. We will explain how this solution has helped us address these challenges in a future article.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to David Virgil Naranjo whose &lt;a href=&quot;https://dvirgiln.github.io/exposing-kafka-throw-different-aws-vpcs/&quot;&gt;blog post&lt;/a&gt; inspired this work.
&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 18 Feb 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/exposing-kafka-cluster</link>
        <guid isPermaLink="true">https://engineering.grab.com/exposing-kafka-cluster</guid>
        
        <category>Engineering</category>
        
        <category>Cloud</category>
        
        <category>Kafka</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
