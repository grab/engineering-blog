<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 26 Sep 2025 01:17:29 +0000</pubDate>
    <lastBuildDate>Fri, 26 Sep 2025 01:17:29 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>User foundation models for Grab</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Artificial intelligence (AI) is central to Grab’s mission of delivering valuable, personalised experiences to millions of users across Southeast Asia. Achieving this requires a deep understanding of individual preferences, such as their favorite foods, relevant advertisements, spending habits, and more. This personalisation is driven by recommender models, which depend heavily on high-quality representations of the user.&lt;/p&gt;

&lt;p&gt;Traditionally, these models have relied on hundreds to thousands of manually engineered features. Examples include the types of food ordered in the past week, the frequency of rides taken, or the average spending per transaction. However, these features were often highly specific to individual tasks, siloed within teams, and required substantial manual effort to create. Furthermore, they struggled to effectively capture time-series data, such as the sequence of user interactions with the app.&lt;/p&gt;

&lt;p&gt;With advancements in learning from tabular and sequential data, Grab has developed a foundation model that addresses these limitations. By simultaneously learning from user interactions (clickstream data) and tabular data (e.g. transaction data), the model generates user embeddings that capture app behavior in a more holistic and generalised manner. These embeddings, represented as numerical values, serve as input features for downstream recommender models, enabling higher levels of personalisation and improved performance. Unlike manually engineered features, they generalise effectively across a wide range of tasks, including advertisement optimisation, dual app prediction, fraud detection, and churn probability, among others.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. The process of building a foundation model involves three steps.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We build foundation models by first constructing a diverse training corpus encompassing user, merchant, and driver interactions. The pre-trained model can then be used in two ways. Based on &lt;strong&gt;Figure 1&lt;/strong&gt;, in 2a we extract user embeddings from the model to serve downstream tasks to improve user understanding. The other path is 2b, where we fine-tune the model to make predictions directly.&lt;/p&gt;

&lt;h2 id=&quot;crafting-a-foundation-model-for-grabs-users&quot;&gt;Crafting a foundation model for Grab’s users&lt;/h2&gt;

&lt;p&gt;Grab’s journey towards building its own foundation model began with a clear recognition: existing models are not well-suited to our data. A general-purpose Large Language Model (LLM), for example, lacks the contextual understanding required to interpret why a specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;geohash&lt;/code&gt; represents a bustling mall rather than a quiet residential area. Yet, this level of insight is precisely what we need for effective personalisation. This challenge extends beyond IDs, encompassing our entire ecosystem of text, numerical values, locations, and transactions.&lt;/p&gt;

&lt;p&gt;Moreover, this rich data exists in two distinct forms: tabular data that captures a user’s long-term profile, and sequential time-series data that reflects their immediate intent. To truly understand our users, we needed a model capable of mastering both forms simultaneously. It became evident that off-the-shelf solutions would not suffice, prompting us to develop a custom foundation model tailored specifically to our users and their unique data.&lt;/p&gt;

&lt;h2 id=&quot;the-importance-of-data&quot;&gt;The importance of data&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. We use tabular and time-series data to build user embeddings.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The success of foundation models hinges on the quality and diversity of the datasets used for training. Grab identified two essential sources of data for building user embeddings as shown in &lt;strong&gt;Figure 2&lt;/strong&gt;. Tabular data provides general attributes and long-term behavior. Time-series data reflects how the user uses the app and captures the evolution of user preferences.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tabular data&lt;/strong&gt;: This classic data source provides general user attributes and insights into long-term behavior. For example, this includes attributes like a user’s age and saved locations, along with aggregated behavioral data such as their average monthly spending or most frequently used service.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Time-series clickstream data&lt;/strong&gt;: Sequential data captures the dynamic nature of user decision-making and trends. Grab tracks every interaction on its app, including what users view, click, consider, and ultimately transact. Additionally, metrics like the duration between events reveal insights into user decisiveness. Time-series data provides a valuable perspective on evolving user preferences.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A successful user foundation model must be capable of integrating both tabular and time-series data. Adding to the complexity is the diversity of data modalities, including categorical/text, numerical, user IDs, images, and location data. Each modality carries unique information, often specific to Grab’s business, underscoring the need for a bespoke architecture.&lt;/p&gt;

&lt;p&gt;This inherent diversity in data modalities distinguishes Grab from many other platforms. For example, a video recommendation platform primarily deals with a single modality: videos, supplemented by user interaction data such as watch history and ratings. Similarly, social media platforms are largely centred around posts, images, and videos. In contrast, Grab’s identity as a “superapp” generates a far broader spectrum of user actions and data types. As users navigate between ordering food, booking taxis, utilising courier services, and more, their interactions produce a rich and varied data trail that a successful model must be able to comprehend. Moreover, an effective foundation model for Grab must not only create embeddings for our users but also for our merchant-partners and driver-partners, each of whom brings their own distinctive sets of data modalities.&lt;/p&gt;

&lt;h3 id=&quot;examples-of-data-modalities-at-grab&quot;&gt;Examples of data modalities at Grab&lt;/h3&gt;

&lt;p&gt;To illustrate the breadth of data, consider these examples across different modalities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Text:&lt;/strong&gt; This includes user-provided information such as search queries within GrabFood or GrabMart (“chicken rice,” “fresh milk”) and reviews or ratings for drivers and restaurants. For merchants, this could encompass the restaurant’s name, menu descriptions, and promotional texts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Numerical:&lt;/strong&gt; This modality is rich with data points such as the price of a food order, the fare for a ride, the distance of a delivery, the waiting time for a driver, and the commission earned by a driver-partner. User behavior can also be quantified through numerical data, such as the frequency of app usage or average spending over a month.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Merchant/User/Driver ID:&lt;/strong&gt; These categorical identifiers are central to the platform. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt; tracks an individual’s activity across all of Grab’s services. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merchant_id&lt;/code&gt; represents a specific restaurant or store, linking to its menu, location, and order history. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;driver_id&lt;/code&gt; corresponds to a driver-partner, associated with their vehicle type, service area, and performance metrics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Location data:&lt;/strong&gt; Geographic information is fundamental to Grab’s operations. This includes airport locations, malls, pickup and drop-off points for a ride (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(lat_A, lon_A)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(lat_B, lon_B)&lt;/code&gt;), the delivery address for a food order, and the real-time location of drivers. This data helps in understanding user routines (e.g., commuting patterns) and logistical flows.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-challenges-and-opportunities-of-diverse-modalities&quot;&gt;The challenges and opportunities of diverse modalities&lt;/h3&gt;

&lt;p&gt;The sheer variety of these data modalities presents several significant challenges and opportunities for building a unified user foundation model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data heterogeneity:&lt;/strong&gt; The different data types—text, numbers, geographical coordinates, and categorical IDs do not naturally lend themselves to being combined. Each modality has its own unique structure and requires specialised processing techniques before it can be integrated into a single model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Complex interactions as an opportunity:&lt;/strong&gt; The relationships between different modalities are often intricate, revealing a user’s context and intent. A model that only sees one data type at a time will miss the full picture.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, consider a single user’s evening out. The journey begins when they book a ride (involving their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt; and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;driver_id&lt;/code&gt;) to a specific drop-off point, such as a popular shopping mall (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location data&lt;/code&gt;). Two hours later, from that same mall location, they open the app again and perform a search for “Japanese food” (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text data&lt;/code&gt;). They then browse several restaurant profiles (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merchant_ids&lt;/code&gt;) before placing an order, which includes a price (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numerical data&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;A traditional, siloed model would treat the ride and the food search as two independent events. However, the real opportunity lies in capturing the interactions within a single user’s journey. This is precisely what our unified foundation model is designed to achieve: to identify the connections and recognise that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;drop-off location&lt;/code&gt; of a ride provides valuable context for a subsequent &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text search&lt;/code&gt;. A model that understands a location is not merely a coordinate, but a place that influences a user’s next action, can develop a far deeper understanding of user context. Unlocking this capability is the key to achieving superior performance in downstream tasks, such as personalisation.&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model architecture&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Transformer architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt; displays Grab’s transformer architecture, enabling joint pre-training on tabular and time-series data with different modalities. Grab’s foundation model is built on a transformer architecture specifically designed to tackle four fundamental challenges inherent to Grab’s superapp ecosystem:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Jointly training on tabular and time-series data:&lt;/strong&gt; A core requirement is to unify column order invariant tabular data (e.g. user attributes) with order-dependent time-series data (e.g. a sequence of user actions) within a single, coherent model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Handling a wide variety of data modalities:&lt;/strong&gt; The model must process and integrate diverse data types, including text, numerical values, categorical IDs, and geographic locations, each requiring its own specialised encoding techniques.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generalising beyond a single task:&lt;/strong&gt; The model must learn a universal representation from the entire ecosystem to power a wide array of downstream applications (e.g., recommendations, churn prediction, logistics) across all of Grab’s verticals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scaling to massive entity vocabularies:&lt;/strong&gt; The architecture must efficiently handle predictions across vocabularies containing hundreds of millions of unique entities (users, merchants, drivers), a scale that makes standard classification techniques computationally prohibitive.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the following section, we highlight how we tackled each challenge.&lt;/p&gt;

&lt;h2 id=&quot;1-unifying-tabular-and-time-series-data&quot;&gt;1. Unifying tabular and time-series data&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Differences between tabular data and time-series data&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;A key architectural challenge lies in jointly training on both tabular and time-series data. Tabular data, which contains user attributes, is inherently order-agnostic — the sequence of columns does not matter. In contrast, time-series data is order-dependent, as the sequence of user actions is critical for understanding intent and behavior.&lt;/p&gt;

&lt;p&gt;Traditional approaches often process these data types separately or attempt to force tabular data into a sequential format. However, this can result in suboptimal representations, as the model may incorrectly infer meaning from the arbitrary order of columns.&lt;/p&gt;

&lt;p&gt;Our solution begins with a novel tokenisation strategy. We define a universal token structure as a &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt;&lt;/strong&gt; pair.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For &lt;strong&gt;tabular data&lt;/strong&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt; is the column name (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;online_hours&lt;/code&gt;) and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;value&lt;/code&gt; is the user’s attribute (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4&lt;/code&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For &lt;strong&gt;time-series data&lt;/strong&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt; is the event type (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;view_merchant&lt;/code&gt;) and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;value&lt;/code&gt; is the specific entity involved (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merchant_id_114&lt;/code&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; format creates a common language for all input data. To preserve the distinct nature of each data source, we employ custom positional embeddings and attention masks. These components instruct the model to treat &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; pairs from tabular data as an unordered set while treating tokens from time-series data as an ordered sequence. This allows the model to benefit from both data structures simultaneously within a single, coherent framework.&lt;/p&gt;

&lt;h2 id=&quot;2-handling-diverse-modalities-with-an-adapter-based-design&quot;&gt;2. Handling diverse modalities with an adapter-based design&lt;/h2&gt;

&lt;p&gt;The second major challenge is the sheer variety of data modalities: user IDs, text, numerical values, locations, and more. To manage this diversity, our model uses a flexible &lt;strong&gt;adapter-based design&lt;/strong&gt;. Each adapter acts as a specialised “expert” encoder for a specific modality, transforming its unique data format into a unified, high-dimensional vector space.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For modalities like &lt;strong&gt;text&lt;/strong&gt;, adapters can be initialised with powerful pre-trained language models to leverage their existing knowledge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For ID data like &lt;strong&gt;user/merchant/driver IDs&lt;/strong&gt;, we initialise dedicated embedding layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For complex and specialised data like &lt;strong&gt;location coordinates&lt;/strong&gt; or not-so-well-modeled modalities like numbers in existing LLMs, we design custom adapters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After each token passes through its corresponding modality adapter, an additional &lt;strong&gt;alignment layer&lt;/strong&gt; ensures that all the resulting vectors are projected into the same representation space. This step is critical for allowing the model to compare and combine insights from different data types, for example, to understand the relationship between a text search query (“chicken rice”) and a location pin (a specific hawker center). Finally, we feed the aligned vectors into the main transformer model.&lt;/p&gt;

&lt;p&gt;This modular adapter approach is highly scalable and future-proof, enabling us to easily incorporate new modalities like images or audio and upgrade individual components as more advanced architectures become available.&lt;/p&gt;

&lt;h2 id=&quot;3-unsupervised-pre-training-for-a-complex-ecosystem&quot;&gt;3. Unsupervised pre-training for a complex ecosystem&lt;/h2&gt;

&lt;p&gt;A powerful model architecture is only half the story; the learning strategy determines the quality and generality of the knowledge captured in the final embeddings.&lt;/p&gt;

&lt;p&gt;In the industry, recommender models are often trained using a semi-supervised approach. A model is trained on a specific, supervised objective, such as predicting the next movie a user will watch or whether they will click on an ad. After this training, the internal embeddings, which now carry information fine-tuned for that one task, can be extracted and used for related applications. This method is highly effective for platforms with a relatively homogeneous primary task, like video recommendation or social media platforms.&lt;/p&gt;

&lt;p&gt;However, this single-task approach is fundamentally misaligned with the needs of a superapp. At Grab, we need to power a vast and diverse set of downstream use cases, including food recommendations, ad targeting, transport optimisation, fraud detection, and churn prediction. Training a model solely on one of these objectives would create biased embeddings, limiting their utility for all other tasks. Furthermore, focusing on a single vertical like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Food&lt;/code&gt; would mean ignoring the rich signals from a user’s activity in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Transport&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GrabMart&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Financial Services&lt;/code&gt;, preventing the model from forming a truly holistic understanding.&lt;/p&gt;

&lt;p&gt;Our goal is to capture the complex and diverse interactions between our users, merchants, and drivers across all verticals. To achieve this, we concluded that &lt;strong&gt;unsupervised pre-training&lt;/strong&gt; is the most effective path forward. This approach allows us to leverage the full breadth of data available, learning a universal representation of the entire Grab ecosystem without being constrained to a single predictive task.&lt;/p&gt;

&lt;p&gt;To pre-train our model on tabular and time-series data, we combine masked language modeling (reconstructing randomly masked tokens) with next action prediction. On a superapp like Grab, a user’s journey is inherently unpredictable. A user might finish a ride and immediately search for a place to eat, or transition from browsing groceries on GrabMart to sending a package with GrabExpress. The next action could belong to any of our diverse services like mobility, deliveries, or financial services.&lt;/p&gt;

&lt;p&gt;This ambiguity means the model faces a complex challenge: it’s not enough to predict &lt;em&gt;which&lt;/em&gt; item a user might choose; it must first predict the &lt;em&gt;type&lt;/em&gt; of interaction they will even initiate. Therefore, to capture the full complexity of user intent, our model performs a dual prediction that directly mirrors our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; token structure:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It predicts the &lt;strong&gt;type of the next action&lt;/strong&gt;, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;click_restaurant&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;book_ride&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search_mart&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It predicts the &lt;strong&gt;value associated with that action&lt;/strong&gt;, like the specific restaurant ID, the destination coordinates, or the text of the search query.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This dual-prediction task forces the model to learn the intricate patterns of user behavior, creating a powerful foundation that can be extended across our entire platform. To handle these predictions, where the output could be of any modality (an ID, a location, text, etc.), we employ modality-specific reconstruction heads. Each head is designed for a particular data type and uses a tailored loss function (e.g. cross-entropy for categorical IDs, mean squared error for numerical values) to accurately evaluate the model’s predictions.&lt;/p&gt;

&lt;h2 id=&quot;4-the-id-reconstruction-challenge&quot;&gt;4. The ID reconstruction challenge&lt;/h2&gt;

&lt;p&gt;A significant challenge is the sheer scale of our categorical ID vocabularies. The total number of unique merchants, users, and drivers on the Grab platform runs into the hundreds of millions. A standard cross-entropy loss function would require a final prediction layer with a massive output dimension. For instance, a vocabulary of 100 million IDs with a 768-dimension embedding would result in a prediction head of nearly 80 billion parameters, blowing up model parameter count.&lt;/p&gt;

&lt;p&gt;To overcome this, we employ &lt;strong&gt;hierarchical classification&lt;/strong&gt;. Instead of predicting from a single flat list of millions of IDs, we first classify IDs into smaller, meaningful groups based on their attributes (e.g. by city, cuisine type, etc). This is followed by a second-stage prediction within that much smaller subgroup. This technique dramatically reduces the computational complexity, making it feasible to learn meaningful representations for an enormous vocabulary of entities.&lt;/p&gt;

&lt;h2 id=&quot;extracting-value-from-our-foundation-model&quot;&gt;Extracting value from our foundation model&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Our foundation model is pre-trained with tabular and time-series data.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Once our foundation model is pre-trained on the vast and diverse data within the Grab ecosystem, it becomes a powerful engine for driving business value. There are two primary pathways to harness its capabilities: fine-tuning and embedding extraction.&lt;/p&gt;

&lt;p&gt;The first pathway involves fine-tuning the entire model on a labeled dataset for a specific downstream task, such as churn probability or fraud detection, to create a highly specialised and performant predictor.&lt;/p&gt;

&lt;p&gt;The second, more flexible pathway is to use the model to generate powerful pre-trained embeddings. These embeddings serve as rich, general-purpose features that can support a wide range of separate downstream models. The remainder of this section will focus on this second pathway, exploring the types of embeddings we extract and how they empower our applications.&lt;/p&gt;

&lt;h2 id=&quot;the-dual-embedding-strategy-long-term-and-short-term-memory&quot;&gt;The dual-embedding strategy: Long-term and short-term memory&lt;/h2&gt;

&lt;p&gt;Our architecture is deliberately designed to produce two distinct but complementary types of user embeddings, providing a holistic view by capturing both the user’s stable, long-term identity and their dynamic, short-term intent.&lt;/p&gt;

&lt;h4 id=&quot;the-long-term-representation-a-stable-identity-profile&quot;&gt;The long-term representation: A stable identity profile&lt;/h4&gt;

&lt;p&gt;The long-term embedding captures a user’s persistent habits, established preferences, and overall persona. This representation is the learned vector for a given &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id,&lt;/code&gt; which is stored within the specialised User ID adapter. As the model trains on countless sequences from a user’s history, the adapter learns to distill their consistent behaviors into this single, stable vector. After training, we can directly extract this embedding, which effectively serves as the user’s “long-term memory” on the platform.&lt;/p&gt;

&lt;h4 id=&quot;the-short-term-representation-a-snapshot-of-recent-intent&quot;&gt;The short-term representation: A snapshot of recent intent&lt;/h4&gt;

&lt;p&gt;The short-term embedding is designed to capture a user’s immediate context and current mission. To generate this, a sequence of the user’s most recent interactions is processed through the model’s adapters and main transformer block. A &lt;strong&gt;Sequence Aggregation Module&lt;/strong&gt; then condenses the transformer’s output into a single vector. This creates a snapshot of recent user intent, reflecting their most up-to-date activities and providing a fresh understanding of what they are trying to accomplish.&lt;/p&gt;

&lt;h2 id=&quot;scaling-the-foundation-from-terabytes-of-data-to-millions-of-daily-embeddings&quot;&gt;Scaling the foundation: From terabytes of data to millions of daily embeddings&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Ray framework&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Building a foundation model of this magnitude introduces monumental engineering challenges that extend beyond the model architecture itself. The practical success of our system hinges on our ability to solve two distinct scalability problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Massive-scale training:&lt;/strong&gt; Pre-training our model involves processing terabytes of diverse, multimodal data. This requires a distributed computing framework that is not only powerful but also flexible enough to handle our unique data processing needs efficiently.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High-throughput inference:&lt;/strong&gt; To keep our user understanding current, we must regenerate embeddings for millions of active users daily. This demands a highly efficient, scalable, and reliable batch processing system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To meet these challenges, we built upon the &lt;strong&gt;Ray framework&lt;/strong&gt;, an open-source standard for scalable computing. This choice allows us to manage both training and inference within a unified ecosystem, tailored to our specific needs.&lt;/p&gt;

&lt;h2 id=&quot;core-principle-a-unified-architecture-for-heterogeneous-workloads&quot;&gt;Core principle: A unified architecture for heterogeneous workloads&lt;/h2&gt;

&lt;p&gt;As illustrated by the Ray framework, both our training and inference pipelines share a fundamental workflow: they begin with a complex Central Processing Unit (CPU) intensive data preprocessing stage (tokenisation), which is followed by a Graphics Processing Unit (GPU) intensive neural network computation.&lt;/p&gt;

&lt;p&gt;A naive approach would bundle these tasks together, forcing expensive GPU resources to sit idle while the CPU handles data preparation. Our core architectural principle is to decouple these workloads. Using Ray’s native ability to manage heterogeneous hardware, we create distinct, independently scalable pools of CPU and GPU workers.&lt;/p&gt;

&lt;p&gt;This allows for a highly efficient, assembly-line-style process. Data is first ingested by the CPU workers for parallelised tokenisation. The resulting tensors are then streamed directly to the GPU workers for model computation. This separation is the key to achieving near-optimal GPU utilisation, which dramatically reduces costs and accelerates processing times for both training and inference.&lt;/p&gt;

&lt;h4 id=&quot;distributed-training&quot;&gt;Distributed training&lt;/h4&gt;

&lt;p&gt;Applying this core principle, our training pipeline efficiently processes terabytes of raw data. The CPU workers handle the complex &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; tokenisation at scale, ensuring the GPU workers are consistently fed with training batches. This robust setup significantly reduces the end-to-end training time, enabling faster experimentation and iteration. We will go into more detail on our training framework in a future blog post.&lt;/p&gt;

&lt;h4 id=&quot;efficient-and-scalable-daily-inference&quot;&gt;Efficient and scalable daily inference&lt;/h4&gt;

&lt;p&gt;This same efficient architecture is mirrored for our daily inference task. To generate fresh embeddings for millions of users, we leverage Ray Data—an open-source library used for data processing in AI and Machine Learning (ML) workload, to execute a distributed batch inference pipeline. The process seamlessly orchestrates our CPU workers for tokenisation and our GPU workers for model application.&lt;/p&gt;

&lt;p&gt;This batch-oriented approach is the key to our efficiency, allowing us to process thousands of users’ data simultaneously and maximise throughput. This robust and scalable inference setup ensures that our dozens of downstream systems are always equipped with fresh, high-quality embeddings, enabling the timely and personalised experiences our users expect.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-a-general-foundation-for-intelligence-across-grab&quot;&gt;Conclusion: A general foundation for intelligence across Grab&lt;/h2&gt;

&lt;p&gt;The development of our user foundation model marks a pivotal shift in how Grab leverages AI. It moves us beyond incremental improvements on task-specific models toward a general, unified intelligence layer designed to understand our entire ecosystem. While previous efforts at Grab have combined different data modalities, this model is the first to do so at a foundational level, creating a truly holistic and reusable understanding of our users, merchants, and drivers.&lt;/p&gt;

&lt;p&gt;The generality of this model is its core strength. By pre-training on diverse and distinct data sources from across our platform—ranging from deep, vertical-specific interactions to broader behavioral signals—it is designed to capture rich, interconnected signals that task-specific models invariably miss. The potential of this approach is immense: a user’s choice of transport can become a powerful signal to inform food recommendations, and a merchant’s location can help predict ride demand.&lt;/p&gt;

&lt;p&gt;This foundational approach fundamentally accelerates AI development across the organisation. Instead of starting from scratch, teams can now build new models on top of our high-quality, pre-trained embeddings, significantly reducing development time and improving performance. Existing models can be enhanced by incorporating these rich features, leading to better predictions and more personalised user experiences. Key areas such as ad optimisation, dual app prediction, fraud detection, and churn probability already heavily benefit from our foundation model, but this is just the beginning.&lt;/p&gt;

&lt;h2 id=&quot;our-vision-for-the-future&quot;&gt;Our vision for the future&lt;/h2&gt;

&lt;p&gt;Our work on this foundation model is just the beginning. The ultimate goal is to deliver “embeddings as a product”. A stable, reliable, and powerful basis for any AI-driven application at Grab. While our initial embeddings for users, driver-partners, and merchant-partners have already proven their value, our vision extends to becoming the central provider for all fundamental entities within our ecosystem, including Locations, Bookings, Marketplace items, and more.&lt;/p&gt;

&lt;p&gt;To realise this vision, we are focused on a path of continuous improvement across several key areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unifying and enriching our datasets:&lt;/strong&gt; Our current success comes from leveraging distinct, powerful data sources that capture different facets of the user journey. The next frontier is to unify these streams into a single, cohesive training corpus that holistically represents user activity across all of Grab’s services. This effort will create a comprehensive, low-noise view of user behavior, unlocking an even deeper level of insight.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Evolving the model architecture:&lt;/strong&gt; We will continue to evolve the model itself, focusing on research to enhance its learning capabilities and predictive power to make the most of our increasingly rich data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Improving scale and efficiency:&lt;/strong&gt; As Grab grows, so must our systems. We are dedicated to further scaling our training and inference infrastructure to handle more data and complexity at an even greater efficiency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By providing a continuously improving, general-purpose understanding of these core components, we are not just building a better model; we are building a more intelligent future for Grab. This enables us to innovate faster and deliver exceptional value to the millions who rely on our platform every day.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://www.grab.careers/en/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Sep 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/user-foundation-models-for-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/user-foundation-models-for-grab</guid>
        
        <category>ai</category>
        
        <category>artificial-intelligence</category>
        
        <category>machine-learning</category>
        
        <category>llm</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Powering Partner Gateway metrics with Apache Pinot</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Grab operates as a dynamic ecosystem involving partners and various service providers, necessitating real-time intelligence and decision-making for seamless integration and service delivery. To facilitate this, &lt;a href=&quot;http://developer.grab.com/&quot;&gt;&lt;strong&gt;GrabDeveloper&lt;/strong&gt;&lt;/a&gt; serves as Grab’s centralized platform for developers and partners. It supports API integration, partner onboarding, and product management. It also provides tech support through staging and production portals with detailed documentation.&lt;/p&gt;

&lt;p&gt;Working alongside Developer Home, &lt;strong&gt;Partner Gateway&lt;/strong&gt; acts as Grab’s secure interface for exposing APIs to third-party entities. It enables seamless interactions between Grab’s hosted services and external consumers, such as mobile apps, web browsers, and partners. Partner Gateway enhances the experience by offering advanced metrics tracking through time-series charts and dashboards. Partner Gateway delivers actionable insights that ensure high performance, reliability, and user satisfaction in application integrations with Grab services.&lt;/p&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use cases&lt;/h2&gt;

&lt;p&gt;Let’s explore GrabDeveloper integration use cases with one of our partners, whom we’ll refer to as “Alpha.” Alpha is a company that specializes in producing and distributing a diverse range of perishable goods. To optimize their operations, time-series charts tracking API traffic request status codes and average API response times play a crucial role.&lt;/p&gt;

&lt;h3 id=&quot;api-traffic-request-service-status-codes-chart&quot;&gt;API traffic request service status codes chart&lt;/h3&gt;

&lt;p&gt;Time-series charts tracking API traffic request status codes offer valuable insights into the performance and reliability of APIs used for managing supply chain logistics, customer orders, and distribution networks. By monitoring these status codes, Alpha can promptly detect and resolve disruptions or failures in their digital systems, ensuring seamless operations and minimizing downtime.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: API traffic chart from 5th Jan 2025 to 4th Mar 2025.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;api-average-response-times-chart&quot;&gt;API average response times chart&lt;/h3&gt;

&lt;p&gt;Analyzing average response times helps the company maintain efficient communication between various systems, enhancing the speed and reliability of transactions and data exchanges. This proactive monitoring supports Alpha in delivering consistent, high-quality service to customers and partners, ultimately contributing to improved operational efficiency and customer satisfaction.&lt;/p&gt;

&lt;p&gt;Analyzing average response times enables a company to ensure efficient communication across various systems, enhancing transaction speed and data exchange reliability. Proactive monitoring helps Alpha deliver consistent, high-quality service to customers and partners, boosting operational efficiency and customer satisfaction.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Average response time chart from 12 Mar 2025 3am to 12 Mar 2025 3pm (Endpoints are mocked for security purposes).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;endpoint-status-dashboard&quot;&gt;Endpoint status dashboard&lt;/h3&gt;

&lt;p&gt;For Alpha, the endpoint status dashboard delivers real-time insights into API performance, enabling swift issue resolution and seamless integration with the company’s systems. The dashboard enhances service reliability, supports business operations, and ensures uninterrupted data exchange, all of which are critical for Alpha’s business processes and customer satisfaction. Furthermore, the transparency and reliability provided by the dashboard strengthens trust in the partnership, ensuring Alpha to confidently rely on the integration to drive their digital initiatives and operational goals.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Endpoint status dashboard of express API for company Alpha. *Endpoints are mocked for security purposes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;why-choose-apache-pinot-and-what-is-it&quot;&gt;Why choose Apache Pinot and what is it?&lt;/h2&gt;

&lt;p&gt;To accommodate these use cases, we need a backend storage system engineered for low-latency queries across a wide range of temporal intervals, spanning from one-hour snapshots to 30-day retrospective analyses, whereby it could contain up to ~6.8 billion rows of data in a 30 day period for a particular dataset. This led us to choose Apache Pinot for these use cases, a distributed Online Analytical Processing (OLAP) system designed for low-latency analytical queries on large-scale data with millisecond query latencies.&lt;/p&gt;

&lt;p&gt;Apache Pinot is a real-time distributed OLAP datastore designed to deliver low-latency analytics on large-scale data. It is optimized for high-throughput ingestion and real-time query processing making it ideal for scenarios such as user-facing analytics, dashboards, and anomaly detection. Apache Pinot supports complex queries, including aggregations and filtering. It delivers sub-second response times by leveraging techniques like columnar storage, indexing, and data partitioning to achieve efficient query execution.&lt;/p&gt;

&lt;h3 id=&quot;data-ingestion-process&quot;&gt;Data ingestion process&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Data ingestion process.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;API call initiation&lt;/strong&gt;: An API call is made on the partner application and routed through the Partner Gateway.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metric tracking&lt;/strong&gt;: Dimensions such as client ID, partner ID, status code, endpoint, metric name, timestamp, and value (which is the metric) are tracked and uploaded to Datadog, a cloud-based monitoring platform.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka message transformation&lt;/strong&gt;: Within the partner gateway code, an Apache Kafka Producer converts these metrics into Kafka messages and stores them in a Kafka Topic. Grab utilizes Protobuf for serialization and deserialization of Kafka messages. Since Grab’s Golang Kafka ecosystem does not use the Confluent Schema Registry, Kafka messages must be serialized with a magic byte which indicates that they are using Confluent’s Schema Registry, followed by the Schema ID.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Serialization via Apache Flink&lt;/strong&gt;: Serialization is managed using Apache Flink, an open-source stream processing framework. This ensures compatibility with the Confluent Schema Registry Protobuf Decoder plugin on Apache Pinot. The messages are then written to a separate Kafka Topic.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ingestion to Apache Pinot&lt;/strong&gt;: Messages from the Kafka Topic containing the magic byte are ingested directly into Pinot, which references the Confluent Schema Registry to accurately deserialize the messages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Query execution&lt;/strong&gt;: Queries on the Pinot table can be executed via the Pinot Rest Proxy API.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data visualization&lt;/strong&gt;: Users can view their project charts and dashboards on the GrabDeveloper Home UI, where data points are retrieved from queries executed in step 6.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;challenges-faced&quot;&gt;Challenges faced&lt;/h2&gt;

&lt;p&gt;During the initial setup, we encountered significant performance challenges when executing aggregation queries on large datasets exceeding 150GB. Specifically, attempts to retrieve and process data for periods ranging from 20 to 30 days resulted in frequent timeout issues as the queries took longer than 10 seconds. This was particularly concerning as it compromised our ability to meet our Service Level Agreement (SLA) of delivering query results within 300 milliseconds. The existing query infrastructure struggled to efficiently manage the volume and complexity of data within the required timeframe, necessitating optimization efforts to improve performance and reliability.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Drawing from the insights gained on the limitations of our initial solutions, we implemented these strategic optimizations to significantly enhance our table’s performance.&lt;/p&gt;

&lt;h3 id=&quot;partitioning-by-metric-name&quot;&gt;Partitioning by metric name&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Improved data locality&lt;/strong&gt;: Partitioning the Kafka Topic by metric name ensures that related data is grouped together. When a query filters on a specific metric, Pinot can directly access the relevant partitions, minimizing the need to scan unrelated data. This significantly reduces I/O overhead and processing time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficient query pruning&lt;/strong&gt;: By physically partitioning data, only the servers holding the relevant partitions are queried. This leads to more efficient query pruning, as irrelevant data is excluded early in the process, further optimizing performance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhanced parallel processing&lt;/strong&gt;: Partitioning enables Pinot to distribute queries across multiple nodes, allowing different metrics to be processed in parallel. This leverages distributed computing resources, accelerating query execution and improving scalability for large datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;column-based-on-aggregation-intervals&quot;&gt;Column based on aggregation intervals&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/table-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 1&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Facilitates time-based aggregations&lt;/strong&gt;: Rounded time columns (e.g., Timestamp_1h for hourly intervals) group data into coarser time buckets, enabling efficient aggregations such as hourly or daily metrics. This simplifies indexing and optimizes storage by precomputing aggregates for specific time intervals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficient data filtering&lt;/strong&gt;: Rounded time columns allow for precise filtering of data within specific aggregation intervals. For example, the query &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT SUM(Value) FROM Table WHERE Timestamp_1h = &apos;2025-01-20 01:00:00&apos;&lt;/code&gt; can exclude irrelevant columns (e.g., column 2) and focus only on rows within the specified time interval, further enhancing query efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;utilizing-the-star-tree-index-in-apache-pinot&quot;&gt;Utilizing the Star-tree index in Apache Pinot&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://docs.pinot.apache.org/basics/indexing/star-tree-index&quot;&gt;Star-tree Index&lt;/a&gt; in Apache Pinot is an advanced indexing structure that enhances query performance by pre-aggregating data across multiple dimensions (e.g., D1, D2). It features a hierarchical tree with a root node, leaf nodes (holding up to T records), and non-leaf nodes that split into child nodes when exceeding T records. Special star nodes store pre-aggregated records by omitting the splitting dimension. The tree is constructed based on a dimensionSplitOrder, dictating node splitting at each level.&lt;/p&gt;

&lt;h4 id=&quot;sample-table-configuration-for-star-tree-index&quot;&gt;Sample table configuration for Star-tree index:&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;tableIndexConfig&quot;: {
  &quot;starTreeIndexConfigs&quot;: [{
    &quot;dimensionsSplitOrder&quot;: [
      &quot;Metric&quot;,
      &quot;Endpoint&quot;,
      &quot;Timestamp_1h&quot;
    ],
    &quot;skipStarNodeCreationForDimensions&quot;: [
    ],
    &quot;functionColumnPairs&quot;: [
      &quot;AVG__Value&quot;
    ],
    &quot;maxLeafRecords&quot;: 1
  }],
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;configuration-explanation&quot;&gt;Configuration explanation:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;dimensionsSplitOrder&lt;/strong&gt;: This specifies the order in which dimensions are split at each level of the tree. The order is “Metric”, “Endpoint”, “Timestamp_1h”. This means the tree will first split by Metric, then by Endpoint, and finally by Timestamp_1h.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;skipStarNodeCreationForDimensions&lt;/strong&gt;: This array is empty, indicating that star nodes will be created for all dimensions specified in the split order. No dimensions are omitted from star node creation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;functionColumnPairs&lt;/strong&gt;: This specifies the aggregation functions to be applied to columns when creating star nodes. The configuration includes “AVG__Value”, meaning the average of the “Value” column will be calculated and stored in star nodes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;maxLeafRecords&lt;/strong&gt;: This is set to 1, indicating that each leaf node will contain only one record. If a node exceeds this number, it will split into child nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;star-tree-diagram&quot;&gt;Star-tree diagram&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: Star-tree Index Structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Components:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Root node (orange)&lt;/strong&gt;: This is the starting point for traversing the tree structure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Leaf node (blue)&lt;/strong&gt;: These nodes contain up to a configurable number of records, denoted by T. In this configuration, maxLeafRecords is set to 1, meaning each leaf node will contain a maximum of one record.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-leaf node (green)&lt;/strong&gt;: These nodes will split into child nodes if they exceed the maxLeafRecords threshold. Since maxLeafRecords is set to 1, any node with more than one record will split.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Star-node (yellow)&lt;/strong&gt;: These nodes store pre-aggregated records by omitting the dimension used for splitting at that level. This helps in reducing the data size and improving query performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;A practical explanation of the start-tree diagram would be to display the star-tree documents in a table format along with the sample queries used to retrieve the data.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/table-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 2: Star-tree documents table&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Sample queries&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;With&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;no&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimensions&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quickly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obtain&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregated&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;processing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;just&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;XYZ_Req_Count&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XYZ_Req_Count&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;This&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduces&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;processing&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;returning&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregated&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;130&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;instead&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filtering&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregating&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;three&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2025-01-20 00:00:00&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2025-01-20 00:00:00&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;This&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allows&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregation&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yielding&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;With&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Process&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obtain&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;efficiently&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;comparing-performance-after-the-optimization&quot;&gt;Comparing performance after the optimization&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6: Chart of query latency with and without optimization.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The graph above in Figure 6, provides a comparison analysis of query performance, showcasing the significant improvements achieved through the implemented optimization solutions. The query execution times are significantly reduced, as evidenced by the logarithmic scale values.&lt;/p&gt;

&lt;p&gt;For the first query which calculates the latency for a particular aggregation interval, the log scale indicates a reduction from 4.64 to 2.32, translating to a decrease in query latency from 43,713 to 209 milliseconds.&lt;/p&gt;

&lt;p&gt;Similarly, the second query, which aggregates the sum of the latency based on the tags for a particular metric, shows a log scale reduction from 3.71 to 1.54, with query latency improving from 5,072 to 35 milliseconds. These results underscore the efficacy of optimization in enhancing query performance, enabling faster data retrieval and processing&lt;/p&gt;

&lt;h2 id=&quot;tradeoffs&quot;&gt;Tradeoffs&lt;/h2&gt;

&lt;p&gt;Star-tree indexes in Apache Pinot are designed to significantly enhance query performance by pre-computing aggregations. This approach allows for rapid query execution by utilizing pre-calculated results, rather than computing aggregations on-the-fly. However, this performance boost comes with a tradeoff in terms of storage space.&lt;/p&gt;

&lt;p&gt;Before implementing the Star-tree index, the total storage size for 30 days of data was approximately 192GB. With the Star-tree index, this increased to 373GB, nearly doubling the storage requirements. Despite the increase in storage, the performance benefits substantially outweigh the costs associated with additional storage.&lt;/p&gt;

&lt;p&gt;The cost impact is relatively minor. We utilize &lt;a href=&quot;https://aws.amazon.com/ebs/pricing/&quot;&gt;AWS gp3 EBS&lt;/a&gt; volumes, which roughly cost $14.48 USD monthly for the extra table (calculated as 0.08 USD x 181 GB). This cost is considered insignificant when compared to the substantial gains in query performance. Alternatively, precomputing the metrics via an ETL job is also feasible; however, it is less cost-effective due to the additional expenses required to maintain the pipeline.&lt;/p&gt;

&lt;p&gt;The decision to use Star-tree indexes is justified by the dramatic improvement in query speed, which enhances user experience and efficiency. The modest increase in storage costs is a worthwhile investment for achieving optimal performance.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In conclusion, Grab’s integration of Apache Pinot as a backend solution within the Partner Gateway represents a forward-thinking strategy to meet the evolving demands of real-time analytics. Apache Pinot’s ability to deliver low-latency queries empowers our partners with immediate, actionable insights into API performance that enhances their integration experience and operational efficiency. This is crucial for partners who require rapid data access to make informed decisions and optimize their services.&lt;/p&gt;

&lt;p&gt;The adoption of Star-tree indexing within Pinot further refines our analytics infrastructure by strategically balancing the trade-offs between query latency and storage costs. This optimization ensures Partner Gateway can support a diverse range of use cases with subsecond query latencies while maintaining high performance and reliability in service delivery reinforcing Grab’s commitment to delivering superior performance across its ecosystem.&lt;/p&gt;

&lt;p&gt;Ultimately, the integration of Apache Pinot enhances Grab’s real-time analytics capabilities while empowering the company to drive innovation and consistently deliver exceptional service to both partners and users.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Credits to Manh Nguyen from the Coban Infrastructure Team, Michael Wengle from the Midas Team and Yuqi Wang from the DevHome team.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebpinot&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/pinot-partnergateway-tech-blog</link>
        <guid isPermaLink="true">https://engineering.grab.com/pinot-partnergateway-tech-blog</guid>
        
        <category>Database</category>
        
        <category>Data</category>
        
        <category>Apache</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Taming the monorepo beast: Our journey to a leaner, faster GitLab repo</title>
        <description>&lt;p&gt;At Grab, our engineering teams rely on a massive Go monorepo that serves as the backbone for a large portion of our backend services. This repository has been our development foundation for over a decade, but age brought complexity, and size brought sluggishness. What was once a source of unified code became a bottleneck that was slowing down our developers and straining our infrastructure.&lt;/p&gt;

&lt;h2 id=&quot;a-primer-on-gitlab-gitaly-and-replication&quot;&gt;A primer on GitLab, Gitaly, and replication&lt;/h2&gt;

&lt;p&gt;To understand our core problem, it’s helpful to know how GitLab handles repositories at scale. GitLab uses &lt;strong&gt;Gitaly&lt;/strong&gt;, its Git RPC service, to manage all Git operations. In a high-availability setup like ours, we use a &lt;strong&gt;Gitaly Cluster&lt;/strong&gt; with multiple nodes.&lt;/p&gt;

&lt;p&gt;Here’s how it works:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Write operations&lt;/strong&gt;: A &lt;strong&gt;primary&lt;/strong&gt; Gitaly node handles all write operations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Replication&lt;/strong&gt;: Data is replicated to &lt;strong&gt;secondary&lt;/strong&gt; nodes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Read operations&lt;/strong&gt;: Secondary nodes handle read operations, such as clones and fetches, effectively distributing the load across the cluster.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Failover&lt;/strong&gt;: If the primary node fails, a secondary node can take over.
For the system to function effectively, replication must be nearly instantaneous. When secondary nodes experience significant delays syncing with the primary—a condition called &lt;strong&gt;replication lag&lt;/strong&gt;—GitLab stops routing read requests to the secondary nodes to ensure data consistency. This forces all traffic back to the primary node, eliminating the benefits of our distributed setup. Figure 1 illustrates the replication architecture of Gitaly nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/taming-monorepo-beast/replication-architecture-gitaly.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: The replication architecture of Gitaly nodes in a high-availability setup.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;the-scale-of-our-problem&quot;&gt;The scale of our problem&lt;/h2&gt;
&lt;p&gt;Our Go monorepo started as a simple repository 11 years ago but ballooned as Grab grew. A Git analysis using the &lt;a href=&quot;https://github.com/github/git-sizer&quot;&gt;git-sizer&lt;/a&gt; utility in early 2025 revealed the shocking scale:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;12.7 million commits&lt;/strong&gt; accumulated over a decade.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;22.1 million Git trees&lt;/strong&gt; consuming 73GB of metadata.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5.16 million blob objects&lt;/strong&gt; totaling 176GB.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;12 million references&lt;/strong&gt;, mostly leftovers from automated processes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;429,000 commits deep&lt;/strong&gt; on some branches.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;444,000 files&lt;/strong&gt; in the latest checkout.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This massive size wasn’t just a number—it was crippling our daily operations.&lt;/p&gt;

&lt;h3 id=&quot;infrastructure-problems&quot;&gt;Infrastructure problems&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/taming-monorepo-beast/replication-delays.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Replication delays of up to four minutes during peak working hours.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In high-availability setups, replication is critical for distributing workloads and ensuring system reliability. However, when replication delays occur, they can severely impact infrastructure performance and create bottlenecks. Figure 2 illustrates replication delays of up to four minutes which caused both secondary nodes, &lt;strong&gt;Gitaly S1 (orange)&lt;/strong&gt; and &lt;strong&gt;Gitaly S2 (blue)&lt;/strong&gt;, to lag behind the primary node, &lt;strong&gt;Gitaly P (green)&lt;/strong&gt;. As a result, all requests were routed exclusively to the primary node, creating significant performance challenges.&lt;/p&gt;

&lt;p&gt;The key issues here are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Single point of failure&lt;/strong&gt;: Only one of our three Gitaly nodes could handle the load, creating a bottleneck.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Throttled throughput&lt;/strong&gt;: The system limits the read capacity to just one-third of the cluster’s potential.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;developer-experience-issues&quot;&gt;Developer experience issues&lt;/h3&gt;

&lt;p&gt;The growing size of the monorepo directly impacted developer workflows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Slow clones&lt;/strong&gt;: 8+ minutes even on fast networks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Painful Git operations&lt;/strong&gt;: Every commit, diff, and blame had to process millions of objects.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CI pipeline overhead&lt;/strong&gt;: Repository cloning added up 5-8 minutes to every CI job.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Frustrated developers&lt;/strong&gt;: “Why is this repo so slow?” became a common question.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;operational-challenges&quot;&gt;Operational challenges&lt;/h3&gt;

&lt;p&gt;The repository’s scale introduced significant operational hurdles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Storage issues&lt;/strong&gt;: 250GB of Git data made backups and maintenance cumbersome.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GitLab UI timeouts&lt;/strong&gt;: The web interface struggled to handle millions of commits and refs, frequently timing out.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited CI scalability&lt;/strong&gt;: Adding more CI runners overloaded the single working node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these factors were dragging down developer productivity. It was clear that &lt;strong&gt;continuing to let the monorepo grow unchecked wasn’t sustainable&lt;/strong&gt;. We needed to make the repository leaner and faster, without losing the important history that teams relied on.&lt;/p&gt;

&lt;h2 id=&quot;our-solution-journey&quot;&gt;Our solution journey&lt;/h2&gt;

&lt;h3 id=&quot;proof-of-concept-validating-the-theory&quot;&gt;Proof of concept: Validating the theory&lt;/h3&gt;

&lt;p&gt;Before making any changes, we needed to answer a critical question: “Would trimming repository history solve our replication issues?” Without proof, committing to such a major change felt risky. So we set out to test the idea.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The test setup&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;We designed a simple experiment. In our staging environment, we created two repositories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full history repository: This repository mirrored the original repository with full history.&lt;/li&gt;
  &lt;li&gt;Shallow history repository: This repository contained only a single commit history.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both repositories contained the same number of files and directories. We then simulated production-like load on both of the repositories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full history repository: &lt;strong&gt;160-240 seconds&lt;/strong&gt; replication delay.&lt;/li&gt;
  &lt;li&gt;Shallow history repository: &lt;strong&gt;1-2.5 seconds&lt;/strong&gt; replication delay.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was nearly a &lt;strong&gt;100x improvement&lt;/strong&gt; in replication performance.&lt;/p&gt;

&lt;p&gt;This proof of concept gave us confidence that history trimming was the right approach and provided baseline performance expectations.&lt;/p&gt;

&lt;h2 id=&quot;content-preservation-strategies-what-to-keep&quot;&gt;Content preservation strategies: What to keep&lt;/h2&gt;

&lt;h3 id=&quot;initial-strategy-time-based-approach-1-2-years&quot;&gt;Initial strategy: Time-based approach (1-2 years)&lt;/h3&gt;

&lt;p&gt;Initially, we wanted to keep commits from the last 1-2 years and archive everything else, as this seemed like a reasonable balance between recent history and size reduction. However, when we developed our custom migration script, we discovered it could only process &lt;strong&gt;100 commits per hour&lt;/strong&gt;, approximately 2,400 commits per day. With millions of commits in the original repository, even keeping 1-2 years of history would take months.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We can only process ~100 commits per hour in batches of 20 to avoid memory limits on GitLab runners.&lt;/li&gt;
  &lt;li&gt;Each batch takes 2 minutes to process, but requires 10 minutes of cleanup (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git gc&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git reflog expire&lt;/code&gt;) to prevent local disk and memory exhaustion.&lt;/li&gt;
  &lt;li&gt;This means each batch takes 12 minutes, allowing only 5 batches per hour (60 ÷ 12 = 5), totaling to 100 commits per hour (5 × 20 = 100).&lt;/li&gt;
  &lt;li&gt;Larger batches increased cleanup time and skipping cleanup caused jobs to crash after 200-300 commits.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The bottleneck wasn’t just the number of commits, it was the 10-minute cleanup process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additional constraints discovered&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;As we dug deeper, we discovered more obstacles.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Critical dependencies extended beyond two years. Some Go module tags from six years ago were still actively used.&lt;/li&gt;
  &lt;li&gt;A pure time-based cut would break existing build pipelines.&lt;/li&gt;
  &lt;li&gt;Development teams needed some recent history for troubleshooting and daily operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;revised-strategy-tag-based--recent-history&quot;&gt;Revised strategy: Tag-based + recent history&lt;/h3&gt;

&lt;p&gt;Given the processing speed constraint of 100 commits per hour, we needed to drastically reduce the number of commits while preserving essential functionality. After careful evaluation, we settled on a tag-based approach combined with recent history.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What we decided to keep&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Critical tags&lt;/strong&gt;: All commits reachable by 2,000+ identified tags, ensuring semantic importance for releases and dependencies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recent history&lt;/strong&gt;: Complete commit history for the last month only addressing stakeholder needs within processing constraints.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Simplified merge commits&lt;/strong&gt;: Converted complex merge commits into single commits to further reduce processing time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Why this approach worked&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Time-feasible&lt;/strong&gt;: Reduced processing time from months to weeks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Functionally complete&lt;/strong&gt;: Preserved all tagged releases and recent development context.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stakeholder satisfaction&lt;/strong&gt;: Met development teams’ need for recent history.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Massive size reduction&lt;/strong&gt;: Achieved 99.9% fewer commits while keeping what matters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The trade-off&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;We sacrificed deep historical browsing of 1 to 2 years for practical migration feasibility, while ensuring no critical functionality was lost.&lt;/p&gt;

&lt;h2 id=&quot;technical-implementation-methods-how-to-execute&quot;&gt;Technical implementation methods: How to execute&lt;/h2&gt;

&lt;h3 id=&quot;method-1-git-filter-repo-failed&quot;&gt;Method 1: &lt;a href=&quot;https://github.com/newren/git-filter-repo/blob/main/Documentation/examples-from-user-filed-issues.md#Remove-commits-older-than-N-days&quot;&gt;git filter-repo&lt;/a&gt; (Failed)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;The approach&lt;/strong&gt;: Use Git’s filter-repo tool with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git replace --graft&lt;/code&gt; to remove commits older than a specified criteria.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it failed&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Complex history&lt;/strong&gt;: Our repository’s highly non-linear history, with multiple branches and merges, made this approach impractical.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Workflow complexity&lt;/strong&gt;: The process required numerous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git replace --graft&lt;/code&gt; commands to account for various branches and dependencies, significantly complicating the workflow.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Risk of inconsistencies&lt;/strong&gt;: The complexity introduced a high risk of errors and inconsistencies, making this method unsuitable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;method-2-git-rebase-onto-failed&quot;&gt;Method 2: &lt;a href=&quot;https://git-scm.com/book/en/v2/Git-Tools-Replace&quot;&gt;git rebase –onto&lt;/a&gt; (Failed)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;The approach&lt;/strong&gt;: Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git rebase --onto&lt;/code&gt; to preserve selected commits while pruning unwanted history.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it failed&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Scale issues&lt;/strong&gt;: The repository size overwhelmed the rebase process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conflict resolution&lt;/strong&gt;: High number of unexpected conflicts that couldn’t be resolved automatically.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Technical limitations&lt;/strong&gt;: Batch processing couldn’t solve the performance issues; Git’s internal mechanisms struggled with the scale.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;method-3-patch-based-implementation-failed&quot;&gt;Method 3: &lt;a href=&quot;https://www.gitkraken.com/learn/git/git-patch&quot;&gt;Patch-based&lt;/a&gt; implementation (Failed)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;The approach&lt;/strong&gt;: Create and apply patches for each commit individually to preserve repository history.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it failed&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Merge commit complexity&lt;/strong&gt;: Couldn’t maintain correct parent-child relationships for merge commits.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;History integrity&lt;/strong&gt;: Resulted in linear sequence instead of preserving original merge structure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Missing commits&lt;/strong&gt;: Important merge commits were lost or incorrectly applied.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;method-4-custom-migration-script-success&quot;&gt;Method 4: Custom migration script (Success!)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;The breakthrough&lt;/strong&gt;: A sophisticated custom script that could handle our specific requirements and processing constraints. Unlike traditional Git history rewriting tools, our script implements a two-phase chronological processing approach that efficiently handles large-scale repositories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 1: Bulk migration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this phase, the script focuses on reconstructing history based on critical tags.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Fetch tags chronologically&lt;/strong&gt;: Retrieve all tags in the order they were created.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pre-fetch Large File Storage (LFS) objects&lt;/strong&gt;: Collect LFS objects for tag-related commits before processing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt;: Process tags in batches of 20 to optimize memory and network usage. For each tag:
    &lt;ul&gt;
      &lt;li&gt;Check for associated LFS objects.&lt;/li&gt;
      &lt;li&gt;Perform selective LFS fetch if required.&lt;/li&gt;
      &lt;li&gt;Create a new commit using the original tree hash and metadata.&lt;/li&gt;
      &lt;li&gt;Embed the original commit hash in the commit message for traceability.&lt;/li&gt;
      &lt;li&gt;Gracefully handle LFS checkout failures.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then, push the processed batch of 20 commits to the destination repository, with LFS tolerance.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Cleanup and continue&lt;/strong&gt;: Perform cleanup operations after each batch and proceed to the next.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Phase 2: Delta migration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This phase integrates recent commits after the cutoff date.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Fetch recent commits&lt;/strong&gt;: Retrieve all commits created after the cutoff date in chronological order.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt;: Process commits in batches of 20 for efficiency. For each commit:
    &lt;ul&gt;
      &lt;li&gt;Check for associated LFS objects.&lt;/li&gt;
      &lt;li&gt;Perform selective LFS fetch if required.&lt;/li&gt;
      &lt;li&gt;Recreate the commit with its original metadata.&lt;/li&gt;
      &lt;li&gt;Embed the original commit hash for resumption tracking in case of interruptions.&lt;/li&gt;
      &lt;li&gt;Gracefully handle LFS checkout failures.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then, push the processed batch of commits to the destination repository, with LFS tolerance.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Tag mapping&lt;/strong&gt;: Map tags to their corresponding new commit hashes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Push tags&lt;/strong&gt;: Push related tags pointing to the correct new commits.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Final validation&lt;/strong&gt;: Validate all LFS objects to ensure completeness.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;LFS handling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The script incorporates robust mechanisms to handle Git LFS efficiently.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Configure LFS for incomplete pushes.&lt;/li&gt;
  &lt;li&gt;Skip LFS download errors when possible.&lt;/li&gt;
  &lt;li&gt;Retry checkout with LFS smudge skip.&lt;/li&gt;
  &lt;li&gt;Perform selective LFS object fetching.&lt;/li&gt;
  &lt;li&gt;Gracefully degrade processing for missing LFS objects.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sequential processing of tags and commits in chronological order.&lt;/li&gt;
  &lt;li&gt;Resumable operations that could restart from the last processed item if interrupted.&lt;/li&gt;
  &lt;li&gt;Batch processing to manage memory and network resources efficiently.&lt;/li&gt;
  &lt;li&gt;Robust error handling for network issues and Git complications.&lt;/li&gt;
  &lt;li&gt;Maintains repository integrity while simplifying complex merge structures.&lt;/li&gt;
  &lt;li&gt;Optimized for our specific preservation strategy (tags + recent history).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation-executing-the-migration&quot;&gt;Implementation: Executing the migration&lt;/h2&gt;

&lt;p&gt;With our strategy defined (tags + last month), we executed the migration using our custom script. This process involved careful planning, smart processing techniques, and overcoming technical challenges.&lt;/p&gt;

&lt;h3 id=&quot;smart-processing-approach&quot;&gt;Smart processing approach&lt;/h3&gt;

&lt;p&gt;Our custom script employed several key strategies to ensure efficient and reliable migration:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sequential tag processing&lt;/strong&gt;: Replay tags chronologically to maintain logical history.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resumable operations&lt;/strong&gt;: The migration could restart from the last processed item if interrupted.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt;: Handle items in manageable groups to prevent resource exhaustion.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Progress tracking&lt;/strong&gt;: Monitor processing rate and estimated completion time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;technical-challenges-solved&quot;&gt;Technical challenges solved&lt;/h3&gt;
&lt;p&gt;The migration addressed several critical technical hurdles.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Large file support&lt;/strong&gt;: Handled Git LFS objects with incomplete push allowances.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Error handling&lt;/strong&gt;: Robust retry logic for network issues and Git errors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Merge commit simplification&lt;/strong&gt;: Converted complex merge structures to linear commits.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;two-phase-migration-strategy&quot;&gt;Two-phase migration strategy&lt;/h3&gt;
&lt;p&gt;The migration was executed in two carefully planned phases.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Phase 1&lt;/strong&gt; - Bulk migration: Migrated 95% of tags while keeping the old repo live.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Phase 2&lt;/strong&gt; - Delta migration: Performed final synchronization during a maintenance window to migrate recent changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-and-impact&quot;&gt;Results and impact&lt;/h2&gt;

&lt;h3 id=&quot;infrastructure-transformation&quot;&gt;Infrastructure transformation&lt;/h3&gt;

&lt;p&gt;Replication delay, or the time required to sync across all Gitaly nodes, improved by 99.4% following the pruning process. As illustrated in Figures 3 and 4, the new pruned monorepo achieves replication in under ~1.5 seconds on average, compared to ~240 seconds for the old repository. This transformation eliminated the previous single-node bottleneck, enabling read requests to be distributed evenly across all three storage nodes, significantly enhancing system reliability and performance.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/taming-monorepo-beast/new-replication-delays.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: In the new pruned monorepo, replication delay ranges from 200 - 2,000 ms.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/taming-monorepo-beast/old-replication-delays.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: In the old monorepo, replication delay ranged from 16,000 - 28,000 ms.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The migration significantly improved load distribution across Gitaly nodes. As shown in Figure 5, the new monorepo leverages all three Gitaly nodes to serve requests, effectively tripling read capacity. Additionally, the migration eliminated the single point of failure that existed in the old monorepo, ensuring greater reliability and scalability.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/taming-monorepo-beast/new-request-distributed.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: In the new monorepo, requests are evenly distributed across all three servers, demonstrating improved performance and replication across nodes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/taming-monorepo-beast/old-request-distributed.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6: In the old monorepo, requests were served only by a single server during working hours, creating a single point of failure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;performance-improvements&quot;&gt;Performance improvements&lt;/h3&gt;
&lt;p&gt;The migration resulted in significant improvements across multiple areas.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Clone time&lt;/strong&gt;: Reduced from 7.9 minutes to 5.1 minutes, achieving a 36% improvement, making repository cloning faster and more efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Commit count&lt;/strong&gt;: Achieved a 99.9% reduction, trimming the repository from 13 million commits to just 15.8 thousand commits, drastically simplifying its structure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;References&lt;/strong&gt;: Reduced by 99.9%, going from 12 million to 9.8 thousand refs, streamlining repository metadata.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Reduced by 59%, shrinking storage requirements from 214GB to 87GB, optimizing resource usage.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;developer-experience&quot;&gt;Developer experience&lt;/h3&gt;
&lt;p&gt;The migration also transformed the developer experience.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Faster Git operations&lt;/strong&gt;: Commits, diffs, and history commands are noticeably snappier.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Responsive GitLab UI&lt;/strong&gt;: Web interface no longer times out.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalable CI&lt;/strong&gt;: The system can now safely run 3x more concurrent jobs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following table summarizes the key repository metrics, comparing the state of the repository before and after the migration:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Metric&lt;/th&gt;
      &lt;th&gt;Old Monorepo&lt;/th&gt;
      &lt;th&gt;New Monorepo&lt;/th&gt;
      &lt;th&gt;Reduction&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Commits&lt;/td&gt;
      &lt;td&gt;~13,000,000&lt;/td&gt;
      &lt;td&gt;~15,800&lt;/td&gt;
      &lt;td&gt;−99.9% (histories squashed)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Git trees&lt;/td&gt;
      &lt;td&gt;~23,600,000&lt;/td&gt;
      &lt;td&gt;~2,080,000&lt;/td&gt;
      &lt;td&gt;−91% (pruned)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Git references&lt;/td&gt;
      &lt;td&gt;~12,200,000&lt;/td&gt;
      &lt;td&gt;9,860&lt;/td&gt;
      &lt;td&gt;−99.9% (cleaned)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Blob storage&lt;/td&gt;
      &lt;td&gt;214 GiB&lt;/td&gt;
      &lt;td&gt;86.8 GiB&lt;/td&gt;
      &lt;td&gt;−59% (smaller packs)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Files in checkout&lt;/td&gt;
      &lt;td&gt;~444,000&lt;/td&gt;
      &lt;td&gt;~444,000&lt;/td&gt;
      &lt;td&gt;~0% (no change)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Latest code size&lt;/td&gt;
      &lt;td&gt;~9.9 GiB&lt;/td&gt;
      &lt;td&gt;~8.4 GiB&lt;/td&gt;
      &lt;td&gt;~−15% (slightly leaner)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;key-challenges-and-lessons-learned&quot;&gt;Key challenges and lessons learned&lt;/h2&gt;

&lt;p&gt;Such a large-scale migration wasn’t without its hiccups and lessons. Here are some challenges we faced and what we learned:&lt;/p&gt;

&lt;h3 id=&quot;git-lfs-woes&quot;&gt;Git LFS woes&lt;/h3&gt;

&lt;p&gt;Initially, GitLab rejected some commits due to missing LFS objects, even old commits that we weren’t keeping. This happened because GitLab’s push hook expected the content of LFS pointers, even if the files weren’t required. To fix this, we had to allow incomplete pushes and skip LFS download errors. We also wrote logic to &lt;strong&gt;selectively fetch LFS objects&lt;/strong&gt; for commits we were keeping. This ensured that any binary assets needed by tagged commits were present in the new repo. The takeaway is that &lt;strong&gt;LFS adds complexity to history rewrites&lt;/strong&gt; – plan for it by adjusting Git LFS settings (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfs.allowincompletepush&lt;/code&gt;) and verifying important large files are carried over.&lt;/p&gt;

&lt;h3 id=&quot;pipeline-token-scoping&quot;&gt;Pipeline token scoping&lt;/h3&gt;

&lt;p&gt;Right after the cutover, some CI pipelines failed to access resources. We discovered a GitLab CI/CD pipeline token issue – our new repo’s ID wasn’t in the &lt;a href=&quot;https://docs.gitlab.com/ci/jobs/ci_job_token/#add-a-group-or-project-to-the-job-token-allowlist&quot;&gt;allowed list&lt;/a&gt; for certain secure token scopes. We quickly updated the settings to include the new project, resolving the authorization error. If your CI jobs interact with other projects or use project-scoped tokens, remember to update those references when you migrate repositories.&lt;/p&gt;

&lt;h3 id=&quot;commit-hash-references-broke&quot;&gt;Commit hash references broke&lt;/h3&gt;

&lt;p&gt;One of our internal tools was using commit SHA-1 hashes to track deployed versions. Since rewriting history means changing &lt;strong&gt;all commit hashes&lt;/strong&gt;, the tool couldn’t find the expected commits. The solution was to map old hashes to new ones for the tagged releases, or better, to modify the tool to use tag names instead of raw hashes going forward. We learned to &lt;strong&gt;communicate early with teams&lt;/strong&gt; that have any dependency on Git commit IDs or history assumptions. In our case, providing a mapping of old tag→new tag (which were mostly 1-to-1 except for the commit SHA) helped them adjust. In hindsight, using stable identifiers like semantic version tags, is much more robust than relying on commit hashes, which are ephemeral in a rewritten history.&lt;/p&gt;

&lt;h3 id=&quot;developer-concerns-wheres-my-history&quot;&gt;Developer concerns: &lt;strong&gt;“Where’s my history?”&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;A few engineers were concerned when they noticed that the git log in the new repo only showed two years of history. From their perspective, useful historical context seemed gone. We addressed this by pointing them to the &lt;strong&gt;archived full-history repo&lt;/strong&gt;. In fact, we kept the old repository read-only in our GitLab, so anyone can still search the old history if needed (just not in the main repo). Additionally, we received suggestions on making the archive easily accessible or even automate a way to query old commits on demand. From this we learned, if you prune history, ensure there’s a plan to &lt;strong&gt;access legacy information&lt;/strong&gt; for those rare times it’s needed – whether that’s an archive repo, a Git bundle, or a read-only mirror.&lt;/p&gt;

&lt;h3 id=&quot;office-network-bottleneck&quot;&gt;Office network bottleneck&lt;/h3&gt;

&lt;p&gt;Interestingly, after the migration, a few developers in certain offices didn’t feel a huge speed improvement in clones. It turned out their corporate network/VPN was the limiting factor – cloning 8 GiB vs 10 GiB over a slow link is not a night and day difference. This highlighted that we should continue to work with the IT team on improving network performance. The repo is faster, but the environment matters too. We’re using this as an opportunity to improve our office VPN throughput so that the 36% clone improvement is realized by everyone, not just CI machines.&lt;/p&gt;

&lt;h3 id=&quot;automation-and-hardcoded-ids&quot;&gt;Automation and hardcoded IDs&lt;/h3&gt;

&lt;p&gt;We had a lot of automation around the monorepo (scripts, webhooks, integrations). Most of these referenced the project by name, which remained the same, so they were fine. However, a few used the project’s numeric ID in the GitLab API, which changed when we created a new repo. Those broke. We had to scan and update some configs to use the new project ID. Our learning here is to &lt;strong&gt;audit all external references&lt;/strong&gt; such as CI configs, deploy scripts, and monitor jobs when migrating repositories. Ideally, use identifiable names instead of IDs, or ensure you’re prepared to update them during the cutover.&lt;/p&gt;

&lt;h3 id=&quot;adjusting-to-new-boundaries&quot;&gt;Adjusting to new boundaries&lt;/h3&gt;

&lt;p&gt;Some teams had to adjust their workflows after the prune. For instance, one team was in the habit of digging into 3 to 5 year old commit logs to debug issues. Post-migration, git log doesn’t go back that far in the main repo; they have to consult the archive for that. It’s a cultural shift to not have all history at your fingertips. We held a short information session to explain how to access the archived repo and emphasized the benefits (faster operations) that come with the lean history. After a while, teams embraced the new normal, appreciating the speed and rarely needing the older commits anyway.&lt;/p&gt;

&lt;p&gt;In the end, we had zero data loss – all actual code and tags were preserved – and only some minor inconveniences that were resolved within a day or two. The challenges reinforced the importance of thorough testing (our staging dry-runs caught many issues) and cross-team communication when making such a change.&lt;/p&gt;

&lt;h2 id=&quot;impact-and-next-steps&quot;&gt;Impact and next steps&lt;/h2&gt;

&lt;p&gt;This migration transformed our development infrastructure from a bottleneck into a performance enabler. We eliminated the single point of failure, restored confidence in our Git operations, and created a foundation that can support our growing engineering team.&lt;/p&gt;

&lt;p&gt;As the next step, we plan to generalize our pruning script to apply the same optimization techniques to other repositories, ensuring consistency and scalability across our infrastructure. Additionally, we will implement continuous performance monitoring to track repository health and proactively address any emerging issues. To prevent future repository bloat, we aim to establish clear best practices and guidelines, empowering teams to maintain efficiency while supporting the growth of our engineering operations.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;What started as a performance crisis became one of our most successful infrastructure projects. By focusing on the right problems—infrastructure reliability and performance rather than just size—we achieved dramatic improvements that benefit every developer daily.&lt;/p&gt;

&lt;p&gt;The key takeaway is that sometimes the biggest technical challenges require custom solutions, careful planning, and willingness to iterate until you find what works. Our 99% improvement in replication performance is just the beginning of what’s possible when you tackle infrastructure problems systematically.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;This migration was completed by Grab Tech Infra DevTools team, involving months of analysis, custom tooling development, and careful production migration of critical infrastructure serving thousands of developers across multiple time zones.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://bit.ly/gebtaming&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Sep 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/taming-monorepo-beast</link>
        <guid isPermaLink="true">https://engineering.grab.com/taming-monorepo-beast</guid>
        
        <category>Engineering</category>
        
        <category>Monorepo</category>
        
        <category>Go</category>
        
        <category>Infrastructure</category>
        
        <category>Performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Data mesh at Grab part I: Building trust through certification</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, our journey towards a more robust and scalable data ecosystem has been a continuous evolution.&lt;/p&gt;

&lt;p&gt;Considering the size of our data lake and complexity of our ecosystem, with businesses spanning across ride hailing, food delivery, and financial services, we have been long past the point where a single centrally managed data warehouse could serve all these data needs. Over its first decade, Grab experienced dramatic growth. Like most growing businesses, teams in Grab prioritised delivering new features to meet the demands of their users. This meant that the task of data maintenance had to take a back seat so that development and stabilisation works can be focused to keep up with the growth. However, to prepare Grab for the next 10 years, especially for a future where AI is likely to play an important role, our leadership understood the need for high quality data foundation and gave a mandate to our data teams to uplevel our entire enterprise data ecosystem.&lt;/p&gt;

&lt;p&gt;Acknowledging the rising need for data-driven insights and the continuous expansion of our data repository, we initiated our data mesh journey, named the Signals Marketplace, in 2024.&lt;/p&gt;

&lt;p&gt;However, this journey was far from simple. We encountered several critical challenges that required a significant transformation in our approach to data management. Some of the challenges encountered include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High volume and variety of data being generated&lt;/strong&gt;: Grab’s diverse operations created both opportunities and complexities. Effectively harnessing this wealth of information required a scalable, streamlined and accessible approach.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Gaps in data ownership&lt;/strong&gt;: As our data landscape expanded, maintaining data quality and reliability became increasingly difficult without clear lines of ownership and accountability. This often led to ad-hoc discussions and delays in resolving data related issues. Since it was difficult to trust the reliability of an existing pipeline, teams were likely to create duplicate pipelines just so they have something they can control.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unscalable reliance on central Data Engineering (DE) team&lt;/strong&gt;: Our traditional reliance on a central DE team to curate and serve all data needs was becoming a bottleneck. This centralised model struggled to keep pace with the distributed nature of data creation and consumption across various product and engineering teams.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Lack of communication between data consumers and producers&lt;/strong&gt;: Data producers are unaware of downstream dependencies of their data which led to several instances of critical pipelines breaking because of upstream changes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;No single source of truth&lt;/strong&gt;: While we did have a central data warehouse, it still left a lot of data gaps across Grab’s many business lines. Teams would struggle to identify the correct data definitions and reliable sources of truth.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Varied sophistication of data practitioners&lt;/strong&gt;: Different teams have different levels of expertise in regards to data. Some teams had dedicated data engineers, but many didn’t.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address these challenges, we made a strategic decision to adopt a data mesh architecture. Data mesh is a decentralised approach to data management that treats data as a product, owned and served by domain specific teams. This paradigm shift empowers teams closest to the data to take responsibility for its quality, reliability, and accessibility.&lt;/p&gt;

&lt;p&gt;Our primary goal in adopting a data mesh was to significantly increase the reusability and reliability of our data assets across the organisation. By fostering a culture of data ownership and providing the necessary tools and processes, we aimed to unlock the full potential of our data to drive innovation and better serve our users and partners.&lt;/p&gt;

&lt;h2 id=&quot;certification&quot;&gt;Certification&lt;/h2&gt;

&lt;p&gt;A cornerstone of our data mesh implementation is the concept of data certification. We believe that clearly identifying high quality, trustworthy datasets is crucial for both data producers and consumers.&lt;/p&gt;

&lt;h3 id=&quot;why-certification&quot;&gt;Why certification?&lt;/h3&gt;

&lt;p&gt;Certification offers significant benefits to both sides of the data ecosystem. Data producers can clearly define and communicate the expectations and guarantees associated with their certified data assets, like defining Service Level Agreements (SLAs) for engineering services. This includes aspects like schema, data quality, and freshness. For data consumers, certification provides the confidence to readily discover and utilise these assets. Knowing that they come with stronger reliability guarantees and clear documentation, data consumers can confidently “shop” for certified data products, reducing the need for extensive validation and ad-hoc inquiries.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/data-mesh-at-grab-part-1/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Concept of data certification&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To achieve widespread data certification, we focused on several key enablers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ownership&lt;/strong&gt;: Establishing decentralised ownership and accountability is fundamental and non-trivial. We clearly identified teams which we call Data Domains, individuals responsible as Business Data Owners (BDOs), and Technical Data Owners (TDOs) for the upkeep, usability, documentation, and associated Service Level Objective (SLOs) of each data product. This step was bootstrapped by leveraging the identification of the data asset creator’s team. However, if the creator had changed teams or left the company, the initial mapping of Domain &amp;lt;&amp;gt; Data Asset needs to be reviewed by the Domain Leads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data contract&lt;/strong&gt;: We introduced data contracts as formal agreements between data producers and consumers. These contracts define the schema, SLA guarantees (including freshness, completeness, and retention policies), notice period for changes, and communication channels for a data product. Data certification helps set clear expectations and ensures reliability across data pipelines.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-operational-excellence&quot;&gt;Data operational excellence&lt;/h3&gt;

&lt;p&gt;To further enhance accountability and ensure adherence to data contracts, we implemented &lt;strong&gt;automated Data Production Incidents&lt;/strong&gt; (DPIs) for breached contracts. When data quality tests are done on data availability, timeliness, consistency, completeness, accuracy, validity, or other reliability guarantees fail, a DPI ticket is automatically created and assigned to the TDO. This system aims to standardise and drive accountability in investigating and fixing issues related to reliability guarantees within Data Contracts. The goal is for teams to acknowledge and fix the root cause of the DPIs.&lt;/p&gt;

&lt;h2 id=&quot;operationalisation-and-outcomes&quot;&gt;Operationalisation and outcomes&lt;/h2&gt;

&lt;p&gt;To drive the adoption of data certification and the principles of data mesh across Grab, we focused on the following north star metric: &lt;strong&gt;percentage of queries hitting certified assets (%)&lt;/strong&gt;. This metric serves as a direct indicator of the reusability and trust in our certified data products. It also helps teams prioritise their certification efforts towards the most frequently used tables. It essentially pushes every data team in two synergistic directions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;To certify their most used datasets.&lt;/li&gt;
  &lt;li&gt;To query only certified datasets as much as possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;operationalisation&quot;&gt;Operationalisation&lt;/h3&gt;

&lt;p&gt;The successful operationalisation of our data mesh and certification efforts relied on several key factors listed below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Executive buy-in&lt;/strong&gt;: Strong leadership support was crucial in driving this organisational change and emphasising the importance of data as a product.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Organisation-wide push with clear measurable reporting&lt;/strong&gt;: We implemented an organisation-wide initiative with clearly defined goals and measurable targets for data certification. Progress is tracked and reported to ensure accountability and drive momentum.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dashboard to guide Grabbers target most used tables&lt;/strong&gt;: Dashboards and tooling likely within Hubble, provided visibility into data usage patterns, guiding teams to prioritise the certification of their most popular and impactful datasets.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;outcomes&quot;&gt;Outcomes&lt;/h3&gt;

&lt;p&gt;As a result of these efforts, we have observed significant positive outcomes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;75% of Grab queries hitting certified assets&lt;/strong&gt;: We achieved a significant milestone with 75% of Grab’s data queries now targeting certified assets. This indicates a strong adoption of certified data products and a growing trust in their reliability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Active deprecation of assets&lt;/strong&gt;: The focus on data ownership and the push for certification has also led to increased visibility into our data landscape, allowing us to identify and actively deprecate redundant and duplicated data assets. Deprecated tables increases 400% year over year (YoY). This not only improves efficiency but also reduces the complexity and cost of maintaining our data infrastructure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Accelerated innovation and cross-domain reusability&lt;/strong&gt;: Prior to data mesh, every team often resorted to building their own data sources which leads to lower quality outcomes and slower turn around time. Today, internet of things datasets (IoT) like weather data collected by one team can now be reused by another team to optimise marketplace decisions — a practical step toward a more connected data ecosystem.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Beyond these individual instances, we observe a convergence across Grab towards most used datasets, with the number of P80 datasets (the top 80% of Grab’s most used data) reducing by over 58% since the start of the campaign.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;While we have made significant strides in our data mesh journey, we recognise that this is an ongoing evolution. This progress wouldn’t be as smooth sailing without the platforms we build for data management and observability. In our next article, we will be delving into the enhancements for crucial tooling and platforms like Genchi (in-house data quality observational tool) and &lt;a href=&quot;https://engineering.grab.com/hubble-data-discovery&quot;&gt;Hubble&lt;/a&gt; (metadata management platform, built on DataHub and Grab proprietary technology), which underpin our data mesh vision and enable greater data reliability and reusability.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Massive credits to Grab’s leadership, Mohan Krishnan and Nikhil Dwarakanath, as well as Data owners on driving this Grab-wide effort to build strong data foundations in Grab. Grab’s data mesh would not have been possible without the commitment of all data owners to certify and curate their data products.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/geb_mlfs&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Aug 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/signals-market-place</link>
        <guid isPermaLink="true">https://engineering.grab.com/signals-market-place</guid>
        
        <category>Data</category>
        
        <category>Database</category>
        
        <category>Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>The evolution of Grab&apos;s machine learning feature store</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this post, we outline how we transformed the way we serve data for our machine learning (ML) models and why we chose Amazon Aurora Postgres as the storage layer for our new feature store. At Grab, we have always been at the forefront of leveraging technology to enhance our services and provide the best possible experience for our platform users. This journey has led us to transition from a traditional approach to a more sophisticated and efficient ML feature store.&lt;/p&gt;

&lt;p&gt;Over the years, ML at Grab has progressed from being used for specific, tactical purposes to being utilised to create long-term business value. As the complexity of our systems and ML models increased, requiring richer amounts of data over time, our platforms faced new challenges in managing more complex features such as a large number of feature keys (high-cardinality) and high-dimensional data or vectors. This evolution necessitated a shift in our data processing and management strategy. We needed a system to store and manage these complex features efficiently. In November 2023, this brought us back to the drawing board to evolve from Amphawa, our initial feature store.&lt;/p&gt;

&lt;p&gt;We landed on the concept of feature tables: a set of data lake tables for ML features that are automatically and atomically ingested into our serving layer. While this concept is not new to the industry, as other platforms like Databricks and &lt;a href=&quot;https://docs.tecton.ai/docs/defining-features/feature-tables&quot;&gt;Tecton&lt;/a&gt; have evolved towards it, our approach is focused on atomicity and reproducibility. The rationale is that ensuring consistency and reliability during the serving lifecycle has become more critical, which has made it more challenging to manage within the model serving layer itself.&lt;/p&gt;

&lt;h2 id=&quot;from-feature-store-to-data-serving&quot;&gt;From feature store to data serving&lt;/h2&gt;

&lt;p&gt;A feature store is a repository that stores, serves, and manages ML features. It provides a unified platform where data scientists can access and share features, reducing redundancy and ensuring consistency across different models.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grab-ml-store/figure1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: The high-level architecture of our centralised feature platform.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our feature data is a key-value dataset. The key identifies a specific entity, such as a consumer ID, which is a known value in the incoming request. A composite key is supported by concatenating two or more entity identifiers. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(key = consumer_id + restaurant_id)&lt;/code&gt;. The value is a binary that encodes the feature value and its type. Whenever a new value for a given entity needs to be updated, we write a new key-value pair for that entity.&lt;/p&gt;

&lt;h2 id=&quot;new-functional-requirements&quot;&gt;New functional requirements&lt;/h2&gt;

&lt;p&gt;As our users designed and deployed increasingly complex ML solutions, new essential functional requirements were requested by our users:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Ability to retrieve the features given in the composite keys (contextual retrieval):&lt;/strong&gt; The ML models in an upstream service might need to fetch all matching entities to form complete contextual information in order to make the recommendation. We build that context inside our ML orchestration layer before calling the model. This was previously not possible due to the design of composite keys in our initial approach.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ability to update not just one entity atomically, but all the entities in a collection (atomic updates):&lt;/strong&gt; This requirement concerns reducing the complexity of operations, such as rolling out new models and switching between versions of feature data. In Amphawa, newly ingested data is visible to consumer systems immediately after it’s written. As changes to the data may be ingested over a long period of time, users are responsible for ensuring the models or services don’t break while the new and old data coexist during ingestion, especially during potentially breaking changes to data. This complexity translates into quite complex code, which is hard to refactor over time. With the new approach, all feature changes will only become visible through atomic updates once the entire operation finishes successfully. This eases users’ pain of maintaining compatibility across versions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Isolation of reading and writing capacity:&lt;/strong&gt; The noisy-neighbor effect is one of the significant issues in our centralised feature store. Different readers could compete for read capacity. For some storage systems, write traffic could consume I/O capacity and affect reading latency. While reading capacity can be adjusted by scaling the storage, the competition between reading and writing capacity is highly dependent on the choice of storage. Hence, from the beginning, one of the top requirements of our second-generation feature store design was isolating reads from writes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;feature-table&quot;&gt;Feature table&lt;/h2&gt;

&lt;p&gt;To meet the functional requirements, we landed on the concept of a “feature table,” where users define and manage the schema and data on a per-table basis. Feature consumers can customise their tables based on their needs. Working with a table format directly makes it easier for data scientists to work with complex tabular data that needs to be retrieved in different ways depending on the context of the request. Moreover, it’s more manageable for us, on the platform side, because it’s closer to the actual format in the storage layer.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Amphawa (feature-centric)&lt;/th&gt;
      &lt;th&gt;New design (feature-table centric)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;A user defines individual features and their types. Grouping into the table is storage layer is implicit.&lt;/td&gt;
      &lt;td&gt;A user defines their tables with compatibility with data-lake tools such as Spark.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;The only index is on the data key.&lt;/td&gt;
      &lt;td&gt;A user defines their own indexes for their tables, based on their access pattern.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
&lt;figcaption align=&quot;middle&quot;&gt;Table 1: Comparison between Amphawa and the new feature table.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Our feature tables are not just a storage solution but a critical component of our ML infrastructure. They enable us to simplify our feature management, efficiently handle the model lifecycle, and enhance our ML models’ performance. This has allowed us to provide a better experience for our platform users and dramatically improve the quality of our ML models based on our A/B testing results.&lt;/p&gt;

&lt;h2 id=&quot;the-data-servings-ingestion-workflow&quot;&gt;The data serving’s ingestion workflow&lt;/h2&gt;

&lt;p&gt;We designed an ingestion framework to address the atomicity requirements from the ground up.&lt;/p&gt;

&lt;p&gt;The data ingestion process in Amphawa was meticulously crafted to ensure efficiency and reduce the pressure on the key-value store. Conversely, our priority has shifted to atomicity (all or nothing) to serve our feature tables and simplify version compatibility.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grab-ml-store/figure2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Ingestion workflow.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Landing feature table in the data lake&lt;/strong&gt;: Data scientists use SQL or Python on Spark to build ML pipelines that output data lake tables. These tables and metadata for version control are stored as Parquet objects in Amazon S3.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Register collection summary&lt;/strong&gt;: A “collection summary” consists of a group of feature tables to be served and other related metadata regarding customised individual tables. In this step, our registry will validate the table’s schema and ensure the customisations are defined correctly.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy collection summary&lt;/strong&gt;: Data scientists send another request to our registry to deploy a collection summary.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pre-ingestion validation&lt;/strong&gt;: The schema is validated to ensure compatibility with the target online ML models. This process ensures consistency and compatibility across feature updates.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ingestion&lt;/strong&gt;: The ingestion mechanism is a classic reverse ETL where the data from S3 is loaded into our Aurora Postgres tables.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Post-ingestion warm-up&lt;/strong&gt;: To avoid cold-start latency spikes, shadow reading duplicates a part of the ongoing reading queries to the new tables for a period before the final switch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the core propositions of feature tables is to offer a simplified interface for writing. Compared to writing directly to a database or providing SDKs for different processing frameworks, we provide a single, common interface for writing, independent of the actual choice of database. This allows us to evolve or even integrate feature tables with other data stores without requiring our users to modify their pipelines. We can decide how the data is represented in the database at a specific isolation level while guaranteeing total transparency for writes and reducing the complexity of read operations.&lt;/p&gt;

&lt;p&gt;However, if a producer has access to S3 and can write in a columnar format, they can always write feature tables. This also means they can access samples from the data lake and use other tools for data validation, as well as tools for data discovery.&lt;/p&gt;

&lt;p&gt;Do take note that a feature table can only be used for data that can be represented in tabular format and requires a minimum of one index to be present in the data. In this initial phase, we support the following data types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atomic types (int, long, boolean, string, float, double)&lt;/li&gt;
  &lt;li&gt;List of atomic types (List[atomic])&lt;/li&gt;
  &lt;li&gt;List of list of atomic types (2d array)&lt;/li&gt;
  &lt;li&gt;Dictionary with strict types of keys and values&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;leveraging-aws-aurora-for-postgres-to-meet-our-non-functional-requirements&quot;&gt;Leveraging AWS Aurora for Postgres to meet our non-functional requirements&lt;/h2&gt;

&lt;p&gt;In our quest to optimise our ML infrastructure, we strategically decided to use &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Amazon Web Services (AWS) Aurora for PostgreSQL&lt;/a&gt; to meet our non-functional requirements. Aurora’s unique features and capabilities, which aligned perfectly with our operational needs, drove this decision.&lt;/p&gt;

&lt;p&gt;AWS Aurora is a fully managed relational database service that combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. A key differentiator is Aurora’s distributed storage architecture.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grab-ml-store/figure3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: AWS Aurora storage architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;architecture-breakdown&quot;&gt;Architecture breakdown&lt;/h3&gt;

&lt;p&gt;The cluster volume consists of copies of the data across three “Availability Zones” in a single AWS Region. Since each database instance in the cluster reads from the distributed storage, this allows for minimal replication lag and ease of scaling out read replicas to meet performance requirements as traffic patterns change.&lt;/p&gt;

&lt;p&gt;The separation between readers and writers also allows us to scale each independently. This is a crucial feature as our traffic is predominantly read-heavy. Most of our data-writes occur once a day. Using a serverless instance class with the writer node being scaled down during idle time significantly reduces our overall operational costs.&lt;/p&gt;

&lt;p&gt;The independent scaling of reader and writer nodes allows us to maintain high performance and availability of our feature store. During peak read times, we can scale out the reader nodes to handle the increased load, ensuring that our ML models have uninterrupted access to the features they need. Conversely, during periods of heavy data ingestion, we can scale up the writer nodes to ensure efficient data storage.&lt;/p&gt;

&lt;p&gt;To facilitate the seamless scaling up and down of the writer node, Aurora also allows a cluster to have a mixture of &lt;a href=&quot;https://aws.amazon.com/rds/aurora/serverless/&quot;&gt;Serverless&lt;/a&gt; and Provisioned nodes. The key difference between the two is that with Serverless, the Aurora service manages the capacity of a single node and adjusts accordingly as the load increases and decreases. In our context, we’re using Serverless for our writer node to quickly scale up when heavy data ingestion starts and scale down automatically once the ingestion is done. We then use Provisioned nodes for the reader nodes since read traffic is more consistent.&lt;/p&gt;

&lt;p&gt;In addition to cost and performance benefits, AWS Aurora simplifies our database management tasks. As a fully managed service, Aurora takes care of time-consuming database administration tasks such as hardware provisioning, software patching, setup, configuration, or backups, allowing our team to focus on optimising our ML models.&lt;/p&gt;

&lt;h2 id=&quot;accessing-the-data-through-our-sdk&quot;&gt;Accessing the data through our SDK&lt;/h2&gt;

&lt;p&gt;With the goal of providing a high-performing and highly available data serving SDK design, we’ve moved on from the centralised API design of Amphawa to a decentralised access architecture in Data Serving. Each data serving deployment is a self-contained system with a cluster and feature catalogue stored within the cluster as additional metadata tables. This minimises dependency, which improves the availability of the system.&lt;/p&gt;

&lt;p&gt;The data serving SDK is designed to be a thin wrapper around the database driver to optimise performance. The SDK contains only a set of utility functions that load user configuration from the &lt;a href=&quot;https://engineering.grab.com/catwalk-evolution&quot;&gt;Catwalk platform&lt;/a&gt; and a query builder to translate user queries to SQL. No additional data validation is performed in the query code path, as all validation is done during feature table generation and ingestion. Therefore, the database handles most of the heavy lifting.&lt;/p&gt;

&lt;h2 id=&quot;decentralised-deployments-a-strategic-shift-in-our-infrastructure&quot;&gt;Decentralised deployments: A strategic shift in our infrastructure&lt;/h2&gt;

&lt;p&gt;We also investigated the difference between centralised and decentralised deployments. We have been exploring these options in the context of our ML feature store, specifically with our Amphawa service and Catwalk orchestrators.&lt;/p&gt;

&lt;p&gt;Our original feature store was deployed as a standalone service where different model-serving applications can connect to it. On the other hand, a decentralised deployment is integrated within a model-serving orchestrator, and a specific orchestrator is bound to a set of pods.&lt;/p&gt;

&lt;p&gt;After extensive discussions and evaluations, we concluded that a decentralised deployment for data serving would better align with our operational needs and objectives. Below is the list of factors we compared that led us to this decision:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Version control&lt;/strong&gt;: Centralised deployments simplify version control but risk accumulating technical debt due to backward compatibility requirements. Decentralised deployments, while needing robust tracking, offer more flexibility.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment strategies&lt;/strong&gt;: Decentralised deployments enabled seamless use of Blue-green and Rolling Deployment strategies. They allow multiple versions to coexist and easy rollbacks, reducing client mismatch issues.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noisy neighbour problem&lt;/strong&gt;: Centralised deployments struggle with the noisy neighbour issue, which requires complex rate limiting. Decentralised setups mitigate this problem by isolating services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Caching efficiency&lt;/strong&gt;: Centralised deployments often suffer low cache hits, whereas decentralised deployments improve caching efficiency by better fitting data into the cache.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In conclusion, leveraging AWS Aurora for Postgres has enabled us to create a robust, scalable, and cost-effective feature store that supports our complex ML infrastructure. This is a testament to our commitment to using cutting-edge technology to enhance our services and provide the best possible experience for our users. Our shift towards decentralised deployments represents our dedication to optimising our infrastructure to support our ML models effectively. By aligning our deployment strategy with our operational needs, we aim to enhance the performance of our services and provide the best possible experience for our users.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/geb_mlfs&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 24 Jul 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/evolution-of-grab-machine-learning-feature-store</link>
        <guid isPermaLink="true">https://engineering.grab.com/evolution-of-grab-machine-learning-feature-store</guid>
        
        <category>Database</category>
        
        <category>AWS</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Grab&apos;s service mesh evolution: From Consul to Istio</title>
        <description>&lt;h2 id=&quot;the-challenge-when-good-enough-isnt-good-enough&quot;&gt;The challenge: When good enough isn’t good enough&lt;/h2&gt;

&lt;p&gt;Picture this: It’s 2024, and Grab’s microservices ecosystem is thriving with over 1000 services running in different infrastructure. But behind the scenes, our service mesh setup is showing its age. We’re running Consul with a fallback mechanism called Catcher - a setup that has  served us well but is starting to feel like wearing a winter coat in Singapore’s heat.&lt;/p&gt;

&lt;p&gt;The challenges we faced were becoming increasingly apparent. A single Consul server issue could trigger a fleet-wide impact, affecting critical services like food delivery and ride-hailing. Our fallback solution, while necessary, added complexity and limited our ability to implement advanced features like circuit breaking and retry policies. As we expanded our presence across Southeast Asia, the need for robust multi-cluster support became more critical than ever. The existing setup struggled with modern requirements like advanced traffic management and fine-grained security controls, while the growing complexity of our microservices architecture demanded better traffic management capabilities.&lt;/p&gt;

&lt;h2 id=&quot;the-complexity-of-grabs-infrastructure&quot;&gt;The complexity of Grab’s infrastructure&lt;/h2&gt;

&lt;p&gt;Our infrastructure landscape is as diverse as the Southeast Asian markets we serve. We operate a complex hybrid environment encompassing services on traditional VMs and EKS clusters with diverse infrastructure provisioning and deployment approaches. This diversity isn’t merely about deployment models—it’s about meeting the unique needs of different business units and regulatory requirements across the region.&lt;/p&gt;

&lt;p&gt;The complexity doesn’t stop there. We handle dual traffic protocols (HTTP and &lt;a href=&quot;https://en.wikipedia.org/wiki/GRPC&quot;&gt;gRPC&lt;/a&gt;) across our entire service ecosystem. Our services communicate across cloud providers between AWS and GCP. Within AWS alone, we maintain multiple organizations to segregate different Grab entities, each operating in its own isolated network. This multi-cloud, multi-protocol, multi-organization setup presented unique challenges for our service mesh implementation.&lt;/p&gt;

&lt;h2 id=&quot;the-quest-for-a-better-solution&quot;&gt;The quest for a better solution&lt;/h2&gt;

&lt;p&gt;Like any good tech team, we didn’t just jump to conclusions. We embarked on a thorough evaluation of service mesh solutions, considering various options including Application Load Balancer (&lt;a href=&quot;https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html&quot;&gt;ALB&lt;/a&gt;), &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/vpc/lattice/&quot;&gt;AWS Lattice&lt;/a&gt;, and &lt;a href=&quot;https://linkerd.io/&quot;&gt;Linkerd&lt;/a&gt;. Our evaluation process was comprehensive and focused on real-world needs, examining everything from stability under high load to the performance impact on service-to-service communication.&lt;/p&gt;

&lt;p&gt;We needed a solution that could handle our distributed architecture while maintaining operational simplicity. The ideal service mesh would need to integrate seamlessly with our existing infrastructure landscape while offering the flexibility to scale with our growing needs. After careful consideration, Istio emerged as the clear winner, offering robust multi-cluster support with flexible deployment models and a comprehensive set of features for traffic management, security, and observability.&lt;/p&gt;

&lt;p&gt;What really sealed the deal was Istio’s strong Kubernetes integration and native support, combined with active community backing. The rich ecosystem of tools and integrations meant we wouldn’t be building everything from scratch, while the flexible deployment options could accommodate our unique requirements.&lt;/p&gt;

&lt;h2 id=&quot;designing-our-istio-architecture&quot;&gt;Designing our Istio architecture&lt;/h2&gt;

&lt;p&gt;When it came to designing our Istio implementation, we took a slightly unconventional approach. Instead of following the traditional “one control plane per cluster” pattern, we designed a more resilient architecture that would better suit our needs. We implemented multiple control planes running on dedicated Kubernetes clusters for better isolation and scalability, with active-active pairs ensuring high availability.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/service-mesh-evolution/external-control-planes.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. External control planes and Kubernetes API servers - Endpoints discovery&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/service-mesh-evolution/istio-proxy.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Istio proxy to Istio control plane - xDS flow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our architecture needed to support high-throughput service-to-service communication while enabling complex routing rules for A/B testing and canary deployments. We implemented custom resource definitions for service mesh configuration and integrated with our existing monitoring and alerting systems. The organization-based mesh boundaries we designed would support our multi-tenant architecture, while our solution for cross-cluster endpoint discovery would ensure reliable service communication across our distributed system.&lt;/p&gt;

&lt;p&gt;This design wasn’t just about following best practices - it was about learning from our past experiences with etcd and Consul. We wanted a setup that could handle Grab’s scale while maintaining simplicity and reliability. The architecture needed to support everything from high-throughput service-to-service communication to complex routing rules for A/B testing and canary deployments, all while maintaining fine-grained security policies and comprehensive observability.&lt;/p&gt;

&lt;h2 id=&quot;the-migration-journey-begins&quot;&gt;The migration journey begins&lt;/h2&gt;

&lt;p&gt;In Q4 2024, we kicked off our migration journey with a clear plan. While our initial strategy focused on gRPC traffic migration, real-world priorities led us down a different path. Our first major milestone was the GCP to AWS migration, a cross-cloud initiative that would test our service mesh capabilities in a complex, multi-cloud environment.&lt;/p&gt;

&lt;p&gt;This cross-cloud migration was a significant undertaking, requiring careful coordination between teams and careful consideration of network policies, security requirements, and service dependencies. We had to ensure seamless communication between services running in different cloud providers while maintaining security and performance standards.&lt;/p&gt;

&lt;p&gt;Alongside our ongoing cloud migration efforts, we launched parallel initiatives focused on gRPC and HTTP traffic migration with cross-mesh connectivity requirements. This phase introduced distinct challenges, as it involved migrating business-critical services while implementing gradual traffic shifting capabilities and quick rollback mechanisms to ensure zero-downtime migrations. We also maintained close monitoring of performance metrics throughout the process.&lt;/p&gt;

&lt;p&gt;Additionally, we needed to ensure seamless compatibility between different service mesh implementations and navigate the complexities of cross-mesh communication. The insights and experience gained from our cloud migration phase have proven invaluable in informing our approach and execution strategy for this critical migration effort.&lt;/p&gt;

&lt;p&gt;The journey hasn’t been without its challenges. We’ve had to balance migration speed with stability while coordinating across multiple teams and organizations. Handling both gRPC and HTTP traffic patterns required careful planning and execution. We’ve had to deal with legacy systems and technical debt while training and supporting teams through the transition. Maintaining service continuity during these transitions has been our top priority.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h2&gt;

&lt;p&gt;This journey has taught us several valuable lessons. We’ve learned that sometimes the standard approach isn’t the best fit, and innovation often comes from questioning assumptions. We’ve discovered the importance of balancing innovation with stability, taking calculated risks while building capability for a quick mitigation.&lt;/p&gt;

&lt;p&gt;Keeping the bigger picture in mind has been crucial, considering long-term implications and planning for scale and growth. We’ve learned to document challenges and solutions, sharing knowledge across teams to avoid repeating mistakes. Most importantly, we’ve learned to stay flexible and adapt to changing needs, being ready to pivot when necessary while keeping an eye on emerging technologies.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The service mesh landscape is constantly evolving, and we’re excited to be part of this journey. Our next steps include continuing our migration efforts with a focus on stability while exploring mesh features like advanced traffic management, and enhanced security policies.&lt;/p&gt;

&lt;p&gt;We’re also working on enhancing our operational capabilities through automated testing and validation, improved monitoring and alerting, and better debugging tools. As we progress, we’re committed to sharing our experiences with the community through open source contributions, conference participation, and technical blogs.&lt;/p&gt;

&lt;h2 id=&quot;shape-the-future-with-us&quot;&gt;Shape the future with us&lt;/h2&gt;

&lt;p&gt;We’re not just implementing a service mesh—we’re architecting the backbone of Grab’s microservices future. Every decision prioritizes reliability, scalability, and maintainability, ensuring we build something that will stand the test of time.&lt;/p&gt;

&lt;p&gt;The journey continues, and we’re excited about what lies ahead. Follow our progress for real-world insights that might shape your own service mesh evolution.&lt;/p&gt;

&lt;p&gt;Want to help us build the future? We have exciting &lt;a href=&quot;https://jobs.smartrecruiters.com/Grab/744000061741882-infra-engineer-manager-service-mesh&quot;&gt;opportunities&lt;/a&gt; waiting for you.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Credits to the Service Mesh team: Aashif Bari, Hilman Kurniawan, Hofid Mashudi, Jingshan Pang, Kaitong Guo, Mikko Turpeinen, Sok Ann Yap, Jesse Nguyen, and Xing Yii. &lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmesh&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Jul 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/service-mesh-evolution</link>
        <guid isPermaLink="true">https://engineering.grab.com/service-mesh-evolution</guid>
        
        <category>Microservice</category>
        
        <category>Service mesh</category>
        
        <category>Kubernetes</category>
        
        <category>AWS</category>
        
        <category>GCP</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>DispatchGym: Grab’s reinforcement learning research framework</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;DispatchGym is a research framework designed to facilitate Reinforcement Learning (RL) studies and applications for the dispatch system, which matches bookings with drivers. The primary goal is to empower data scientists with a tool that allows them to independently develop and test RL-related concepts for dispatching systems. It accelerates research by providing a suite of modules that include a reinforcement learning algorithm, a dispatching process simulation, and an interface connecting the two through the &lt;a href=&quot;https://gymnasium.farama.org/introduction/basic_usage/&quot;&gt;Gymnasium&lt;/a&gt; API.&lt;/p&gt;

&lt;p&gt;To ensure efficient and cost-effective RL research without compromising on quality, DispatchGym aims to be both comprehensive and accessible. Anyone with basic RL knowledge and Python programming skills can use it to explore new ideas in RL and dispatch system logic.&lt;/p&gt;

&lt;p&gt;This article walks you through the principles behind DispatchGym, how these principles effectively and efficiently empower impactful research, and how it can be applied to solve real world problems.&lt;/p&gt;

&lt;h2 id=&quot;the-challenge-with-rl&quot;&gt;The challenge with RL&lt;/h2&gt;

&lt;p&gt;Although RL methods can be applied to a wide variety of problems that can be formulated as a Markov Decision Process (MDP), designing an effective RL-based solution is not a trivial task. The primary challenges stem from two key components: the reward function and the lever.&lt;/p&gt;

&lt;p&gt;In RL, the reward function represents the objective we aim to maximize. At first glance, it might seem straightforward to plug in any metric, such as the company’s profit or the number of completed bookings per day. However, these metrics are not always sensitive to the lever that RL can manipulate, or the lever itself may not significantly influence the objective. For example, consider a setup where we aim to maximize the daily number of completed bookings by adjusting the maximum number of candidate drivers considered to each booking. Beyond a minimal threshold (e.g., one driver), further increasing this limit provides negligible benefits. As a result, RL struggles to determine whether setting this limit to 11 or 15 would result in higher rewards.&lt;/p&gt;

&lt;p&gt;In summary, when a lever exerts weak influence on a reward function, the RL setup becomes ineffective. Therefore, we should strive to select a lever that strongly influences the reward function and define a reward function that is both sensitive to manipulations of that lever and aligned with our overall goal. Note that the reward function does not have to be identical to our ultimate objective; it merely needs to be highly correlated with it.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dispatchgym/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Illustration of weak lever influence on a reward function.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;empowering-research-with-dispatchgym&quot;&gt;Empowering research with DispatchGym&lt;/h2&gt;

&lt;p&gt;The primary application of DispatchGym is to accelerate and broaden cost-effective research and impactful RL applications for Grab’s dispatching system. A system which is responsible for assigning a driver to each booking. To achieve this, DispatchGym must have the following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reliable&lt;/strong&gt;&lt;br /&gt;
The simulation component should be accurate enough to capture essential behaviors strongly linked to the metrics of interest, without necessarily modeling everything else. While it’s beneficial if the simulation can do more than the specific use case (e.g., simulating both batching and allocation when only allocation is needed), it is not strictly required.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost-effective&lt;/strong&gt;&lt;br /&gt;
Updating all of DispatchGym’s components should require minimal monetary and labor costs to enable rapid iteration. This includes keeping the simulation component aligned with real system behaviors, incorporating the latest technologies in the optimization component, and maintaining seamless integration between the simulation and optimization components.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Empowering&lt;/strong&gt;&lt;br /&gt;
It should be as easy as possible for data scientists and engineers to modify any DispatchGym component and then run experiments. This flexibility is crucial because new research typically requires adjustments to both the simulation and optimization components. By granting users the freedom to adapt DispatchGym, the framework fosters continuous innovation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-friendly-simulated-environment&quot;&gt;Research-friendly simulated environment&lt;/h2&gt;

&lt;p&gt;The simulation component of DispatchGym, or the “simulated environment,” is designed with reliability, cost-effectiveness, and user empowerment in mind. It models the full dispatching process, from booking creation and driver dispatch to driver movement and booking completion. While this environment may not be perfectly accurate in absolute terms (there can be differences between real and simulated metric values), it emphasizes directional accuracy. This means that the metric trends (up or down) in the simulation closely match real-world behavior. This focus on directional accuracy is crucial because most research involves sim-to-sim comparisons, where shifts in metrics are the most important. Verifying directional accuracy is also simpler and more practical for evaluating simulation performance. For instance, we can test various supply-demand imbalance scenarios and check whether a supply-rich situation indeed fulfills more bookings, and vice versa.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dispatchgym/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Simulated processes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The simulated environment’s cost-effectiveness and empowerment features come from a modular architecture and Python, a research-friendly programming language. The modular design offers a gentle learning curve, allowing users to easily navigate and make necessary changes in the codebase. Meanwhile, Python is selected to lower the entry barrier for adopting DispatchGym. To mitigate Python’s runtime overhead, DispatchGym leverages Numba to significantly speed up simulation execution.&lt;/p&gt;

&lt;h2 id=&quot;dispatchgym-in-action&quot;&gt;DispatchGym in action&lt;/h2&gt;

&lt;p&gt;Data scientists use DispatchGym by modifying a local copy of the codebase to implement their ideas. They then upload the updated codebase to an internal infrastructure using a single CLI command, which spawns a Spark job to run the DispatchGym program. This setup grants complete flexibility over the simulation and optimization components without requiring users to manage the underlying infrastructure.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dispatchgym/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Data scientist interactions with DispatchGym.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;applying-rl-approach-for-dispatch&quot;&gt;Applying RL approach for dispatch&lt;/h2&gt;

&lt;p&gt;Amongst its many uses, DispatchGym was applied in building an effective contextual bandit strategy for the auto-adaptive tuning of dispatch-related hyperparameters. Its flexibility allowed us to experiment with various contextual bandit model variants, including linear bandits, neural-linear bandits, and Gaussian-process bandits, as well as multiple action sampling strategies, such as epsilon-greedy, Thompson sampling, SquareCB, and FastCB. These capabilities accelerated our progress in determining the best combination of levers, reward functions, and contextual bandits for improved fulfilment efficiency and reliability.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;DispatchGym provides us a framework that equips data scientists with everything they need to develop and test RL solutions for dispatch systems. By integrating an RL optimization approach and a realistic dispatch simulation using a Gymnasium API, it enables rapid exploration and iteration of RL applications with just basic RL knowledge and Python programming language.&lt;/p&gt;

&lt;p&gt;A major hurdle in applying RL to dispatch problems modeled as MDP is ensuring that the reward function aligns with ultimate business goals and is sensitive to the lever under control. If the lever (e.g., tweaking driver count) does not meaningfully influence the reward, the RL approach falters. DispatchGym addresses this by making it easy for data scientists to determine the most effective combinations of levers, reward functions, and RL approaches, ultimately driving positive business impact.&lt;/p&gt;

&lt;p&gt;DispatchGym’s architecture focuses on reliability, cost-effectiveness, and user empowerment. Its simulation is designed to capture critical metrics and reflect real-world trends (directional accuracy), while its Python-based modular design enhanced by Numba enables easy prototyping. Researchers can adjust the environment locally before deploying changes seamlessly via a command-line interface, avoiding infrastructure overhead. These design decisions and capabilities empower data scientists to refine contextual bandit approaches for optimizing dispatch hyperparameters and explore innovative RL applications in the dispatch process.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to thank Chongyu Zhou, Guowei Wong, and Roman Kotelnikov for their collaboration in developing the RL-based optimizer. &lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdispgym&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Jul 2025 07:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/techblog_-dispatchgym</link>
        <guid isPermaLink="true">https://engineering.grab.com/techblog_-dispatchgym</guid>
        
        <category>Dispatch</category>
        
        <category>Python</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Counter Service: How we rewrote it in Rust</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;The Integrity Data Platform (IDP) team decided to rewrite one of our heavy Queries Per Second (QPS) Golang microservices in Rust. It resulted in 70% infrastructure savings at a similar performance, but was not without its pitfalls. This article will elaborate on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How we picked what to rewrite in Rust.&lt;/li&gt;
  &lt;li&gt;Approach taken to tackle the rewrite.&lt;/li&gt;
  &lt;li&gt;The pitfalls and speed bumps along the way.&lt;/li&gt;
  &lt;li&gt;Was it worthwhile?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Grab is predominantly based on a microservice architecture, with the vast majority of microservices being hosted in a monorepo and written in Golang. It has served the company well so far, as the “simplicity” of Golang allows developers to ramp up and iterate quickly.&lt;/p&gt;

&lt;p&gt;However, Rust has seen some gradual adoption across the company. Starting with a few minor &lt;a href=&quot;/how-we-reduced-our-ci-yaml&quot;&gt;CLIs&lt;/a&gt;, which then progressed to notable success with a Rust-based reverse proxy in &lt;a href=&quot;/catwalk-serving-machine-learning-models-at-scale&quot;&gt;Catwalk&lt;/a&gt; for model serving. Additionally, a growing community of Rust enthusiasts within the organisation has expressed interest in advocating for and expanding the adoption of Rust more proactively.&lt;/p&gt;

&lt;p&gt;After achieving success with several projects on the ML platform and addressing concerns about Rust’s ability to handle traffic at scale, the next logical step was to assess the Return on Investment (ROI) of rewriting a Golang microservice in Rust.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Rust has the reputation of being &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3136014.3136031&quot;&gt;highly efficient&lt;/a&gt; yet poses a steep learning curve. Rust is often touted to perform close to C, doing away with garbage collection while remaining memory safe through strict compile checks and the borrow checker. It is loved by developers for having rich features like being multi-paradigm (supporting both functional and OOP style), having a rich type system, and doing away with nil pointers and errors.&lt;/p&gt;

&lt;p&gt;However, regardless of how well regarded a certain language is in the industry, rewrites of any system should always be considered very carefully. When it comes to “legacy software”, there is a prevalent assumption that rewriting legacy software is a solution to eliminate technical debt and phase out legacy systems. The reality is often more nuanced.&lt;/p&gt;

&lt;p&gt;Legacy code occurs when the developers who originally wrote the code are no longer working on the project. There are often business logic and edge-cases baked into complex legacy codebases of which the context has been lost over time. In practice, rewrites frequently take longer than anticipated and tend to reintroduce bugs and edge cases that must be identified and resolved all over again.&lt;/p&gt;

&lt;p&gt;Rewriting vs refactoring has been written at length across the internet, you can read more about it &lt;a href=&quot;https://herbcaudill.com/words/20190219-rewrite-refactor-reinvent&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The trade-offs of rewriting need to be properly weighed and balanced. It must take into consideration:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How much engineering bandwidth goes into the rewrite?&lt;/li&gt;
  &lt;li&gt;What is the complexity of the rewrite?&lt;/li&gt;
  &lt;li&gt;What tangible benefits are brought about by the rewrite?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rewriting a system solely for the purpose of “rewriting it in Rust” is not a strong enough business justification.&lt;/p&gt;

&lt;p&gt;A legitimate concern was the steep learning curve of Rust, coupled with the risk of having only one team member proficient in the language, which would make its adoption unsustainable.&lt;/p&gt;

&lt;p&gt;Therefore, we established a set of guidelines to follow when identifying a suitable system for a potential rewrite:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The system must be “simple” enough in functionality. For example, it has one or two main functionalities that can be rewritten in a reasonable amount of time and have its complexity constrained.&lt;/li&gt;
  &lt;li&gt;The system targeted should have large enough traffic such that cost savings brought about by adopting Rust is something tangible when balanced against the effort.&lt;/li&gt;
  &lt;li&gt;The members of the team must be comfortable and willing to pick up the language and achieve a certain level of familiarity to make maintaining the service sustainable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;finding-the-right-service&quot;&gt;Finding the right service&lt;/h2&gt;

&lt;p&gt;The ideal service should have a sufficiently large infrastructure footprint to justify the potential cost savings, while also being straightforward in functionality to minimise time spent on handling edge cases and complex business logic.&lt;/p&gt;

&lt;p&gt;Looking across the stack of microservices in Integrity, &lt;a href=&quot;/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;Counter Service&lt;/a&gt; stands out. As its name implies, Counter Service is a service that “counts” and serves the counters for ML models and fraud rules. The original service has two primary functionalities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consuming from streams, counting events and &lt;strong&gt;writing&lt;/strong&gt; to &lt;a href=&quot;/seamless-migration&quot;&gt;Scylla&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Exposing Google Remote Procedure Call (GRPC) endpoints to query from Scylla (and Redis) and return counts of events based on query keys. For example, &lt;strong&gt;BatchRead&lt;/strong&gt;. BatchRead’s functionality of Counter Service serves up to tens of thousands of QPS at peak and is fairly constrained in functionality. Hence, it fulfilled our target criteria of being “simple” in functionality yet serving a large enough amount of traffic that justifies the ROI of a rewrite.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rust-blog/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: BatchRead flow of Counter Service, reading data from Scylla, DynamoDB, Redis, MySQL and serving the counters through GRPC.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;rewrite-approach&quot;&gt;Rewrite approach&lt;/h3&gt;

&lt;p&gt;There are a few ways to approach a rewrite in another language. One popular way is to convert your code line by line. If the languages are close enough, it might even be possible to programmatically convert your code like &lt;a href=&quot;https://github.com/immunant/c2rust&quot;&gt;C2Rust&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We decided not to use such an approach for our rewrite. The major reason is that idiomatic Golang was not necessarily idiomatic Rust. We wanted to approach this rewrite with a fresh perspective and treat this as a true rewrite.&lt;/p&gt;

&lt;p&gt;We treated the application like a black box, with the interfaces well defined, like GRPC endpoints and contracts. Similar to a function, you could call the API and get a deterministic result, and we had the data that was stored in Scylla.&lt;/p&gt;

&lt;p&gt;Based on how we understood the application to work based on its specs and contract, we chose to rewrite the application logic from scratch to meet the API contract and to get as close as identical outputs from the new black box.&lt;/p&gt;

&lt;h3 id=&quot;oss-library-support&quot;&gt;OSS library support&lt;/h3&gt;

&lt;p&gt;We started out by mapping out the key external dependencies and checking how well they were supported in the Rust ecosystem and in open source. 
&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;&quot; align=&quot;middle&quot;&gt;
&lt;b&gt;Table 1: List of libraries and their star ratings&lt;/b&gt;
&lt;/div&gt;
&lt;table class=&quot;table&quot; style=&quot;&quot; align=&quot;middle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Functionality&lt;/th&gt;
      &lt;th&gt;Library&lt;/th&gt;
      &lt;th&gt;Stars (as of Nov 24) &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Datadog (Statsd Client)&lt;/td&gt;
      &lt;td&gt; &lt;a href=&quot;https://github.com/56quarters/cadence&quot;&gt;https://github.com/56quarters/cadence&lt;/a&gt; &lt;/td&gt;
      &lt;td&gt;&amp;lt; 500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Lightstep (OpenTelemetry) &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-rust&quot;&gt;https://github.com/56quarters/cadence&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &amp;gt; 1000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; GRPC Server  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/hyperium/tonic&quot;&gt;https://github.com/hyperium/tonic&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;  &amp;gt; 500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Web Server   &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/actix/actix-web&quot;&gt;https://github.com/actix/actix-web&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &amp;gt; 20,000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Redis Client  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/aembke/fred.rs&quot;&gt;https://github.com/aembke/fred.rs&lt;/a&gt; (Async Redis Library + Client pool)&lt;/td&gt;
      &lt;td&gt; &amp;gt; 5000&lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt; 
    &lt;td&gt; Redis Client  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/redis-rs/redis-rs&quot;&gt;https://github.com/redis-rs/redis-rs&lt;/a&gt; (“Official” redis client, initially picked but discarded)&lt;/td&gt;
      &lt;td&gt; &amp;gt; 3000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Scylla Client  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/scylladb/scylla-rust-driver&quot;&gt;https://github.com/scylladb/scylla-rust-driver&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; ~500 &lt;/td&gt;
    &lt;/tr&gt;
   &lt;tr&gt;
    &lt;td&gt; Kafka Client &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kafka-rust/kafka-rust&quot;&gt;https://github.com/kafka-rust/kafka-rust&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &amp;gt;1000 &lt;/td&gt;
    &lt;/tr&gt;
 &lt;/tbody&gt;  
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;All the functionality we need is available through libraries in the Rust ecosystem. However, we found that some libraries are not particularly “popular,” as indicated by their relatively low number of GitHub stars.&lt;/p&gt;

&lt;p&gt;The practical concern with using less “popular” libraries is the risk of limited community support or potential abandonment over time. That said, if an “unpopular” library is officially maintained by the associated open-source project—for instance, the Scylla driver has only about 500 stars but is officially provided by the Scylla project—we would need to ensure confidence that it will continue to receive active support.&lt;/p&gt;

&lt;p&gt;Out of the list of libraries above, the “unpopular” and unofficial libraries can be narrowed down to two libraries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Datadog - Cadence&lt;/li&gt;
  &lt;li&gt;Redis - Fred&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For Datadog, there is no “official” Datadog Rust client. Yet, we picked Cadence as the API looked intuitive and the features we needed were already supported.&lt;/p&gt;

&lt;p&gt;In regards to Redis, after testing it, we discovered that the support was not up to par with our requirements. We then opted for a newer and less popular library, fred.rs that seemed to be actively being developed by the community.&lt;/p&gt;

&lt;h3 id=&quot;company-specific-internal-libraries&quot;&gt;Company specific internal libraries&lt;/h3&gt;

&lt;p&gt;With the vast majority of microservices being written in Golang, most internal libraries are also written in Golang. Opting to rewrite a service in Rust means we are not able to use these internal libraries.&lt;/p&gt;

&lt;p&gt;Examples include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An internal configuration library that utilises Go Templates to template configurations for different environments (staging and production).&lt;/li&gt;
  &lt;li&gt;The internal configuration library has its own wrappers and injectors to pull and render secrets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To overcome this gap and re-use Go Templates and configuration language, we decided to write a simple wrapper and parser using the &lt;a href=&quot;https://github.com/rust-bakery/nom&quot;&gt;nom parser combinator&lt;/a&gt; to parse the templates and render the config.&lt;/p&gt;

&lt;p&gt;Nom poses a steep learning curve. But once familiarised, it is flexible and performant enough to build an equivalent to the internal library. Parser combinators are an interesting subset of tooling that allows you to create some fairly elegant parsers.&lt;/p&gt;

&lt;h2 id=&quot;road-bumps&quot;&gt;Road bumps&lt;/h2&gt;

&lt;h3 id=&quot;the-borrow-checker&quot;&gt;The borrow checker&lt;/h3&gt;

&lt;p&gt;One of the most striking paradigm shifts for developers transitioning to Rust is adapting to the strict rules of the borrow checker, which enforces that variables cannot be reused multiple times unless explicitly cloned or borrowed.&lt;/p&gt;

&lt;p&gt;Interestingly, the borrow checker was not the biggest hurdle for new developers. The key is to avoid introducing lifetimes too early in the development process, as this can lead to premature code optimisation.&lt;/p&gt;

&lt;p&gt;In many cases, adding a few clones (and occasionally Arcs) can help new developers get up to speed and iterate more quickly during development. The resulting code is usually “fast” enough for initial purposes. After that, the code can be revisited to eliminate unnecessary clones for improved performance. An efficient approach to this can be taken by using Flamegraph to profile your code and identify memory allocation bottlenecks.&lt;/p&gt;

&lt;h3 id=&quot;async-gotchas&quot;&gt;Async gotchas&lt;/h3&gt;

&lt;p&gt;When rewriting Golang logic in Rust, there are fundamental differences in how they treat concurrency and parallelism.&lt;/p&gt;

&lt;p&gt;One of Golang’s most remarkable strengths is its ability to deliver high-performance concurrency while preserving simplicity.&lt;/p&gt;

&lt;p&gt;There are two fundamental approaches to concurrency in programming languages, namely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Preemptive scheduling (stackful coroutines).&lt;/li&gt;
  &lt;li&gt;Cooperative scheduling (stackless coroutines).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Preemptive vs cooperative scheduling is an in-depth topic with the gist of it being, Golang uses preemptive scheduling and each “Goroutine” has a stack that needs a runtime. The Golang scheduler has the power to “preempt” and “freeze” functions and switch to another stack like stackful coroutine. This is a gross oversimplification of the nuances. For more details, this is a good &lt;a href=&quot;https://medium.com/a-journey-with-go/go-asynchronous-preemption-b5194227371c&quot;&gt;introduction&lt;/a&gt; to the topic.&lt;/p&gt;

&lt;p&gt;Rust opts for cooperative scheduling whereby it has no runtime and each coroutine does not maintain a stack. Hence, it has no ability to “freeze” a function and swap context. This allows Rust to be more efficient in terms of memory and resources, as it maintains a state machine. However, the consequence is that this moves the complexity up the stack to the programming language itself. Similar to Javascript, functions are “coloured”, and the developer has to explicitly annotate their functions to be async or sync. Await points need to be explicitly called and control needs to be “yielded” (i.e. cooperative and stackless) so the Rust program knows when it is allowed to stop and swap between coroutines. To read more on this, refer to &lt;a href=&quot;https://tokio.rs/blog/2019-10-scheduler&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;https://without.boats/blog/why-async-rust/&quot;&gt;this&lt;/a&gt; article for the history of async Rust.&lt;/p&gt;

&lt;p&gt;Needing to annotate a function is a classic complaint that is addressed in the article “&lt;a href=&quot;https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/&quot;&gt;What Colour is Your Function&lt;/a&gt;” that highlights developers’ responsibility to explicitly colour their function and consciously think about &lt;a href=&quot;https://ryhl.io/blog/async-what-is-blocking/&quot;&gt;blocking vs non-blocking code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Contrast this with Golang, where you simply need to add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go&lt;/code&gt; keyword without thinking about which code might block the execution and use channels to communicate across Goroutines. Golang allows the developer to achieve high performance without much cognitive overhead.&lt;/p&gt;

&lt;p&gt;This is especially important for developers new to Rust. As the lack of experience in async and blocking code can be somewhat of a footgun. In the initial rewrite of Rust, we made an amateur mistake of using a synchronous Redis function to call the Redis cache. It resulted in the application performing poorly until we corrected it with the non-blocking asynchronous version using the Fred redis library.&lt;/p&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;Following the eventful process of rewriting the service from the ground up in Rust, the outcomes proved to be quite intriguing.&lt;/p&gt;

&lt;p&gt;Shadowing traffic to both services as seen in Figure 2, the P99 latency is similar (or perhaps even slightly worse) in the Rust service compared to the original Golang one.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rust-blog/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: P99 latency comparison between the Golang service (purple) and Rust service (blue). 
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Normalising the QPS and resource consumption, we see from Table 2 that Rust consumes ~20% of the resources of the original Golang application, resulting in 5x savings in terms of resource consumption.
&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;&quot; align=&quot;middle&quot;&gt;
&lt;b&gt;Table 2: Comparison of resource consumption between Rust and Golang service.&lt;/b&gt;
&lt;/div&gt;
&lt;table class=&quot;table&quot; style=&quot;&quot; align=&quot;middle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Service&lt;/th&gt;
      &lt;th&gt;Indicative QPS&lt;/th&gt;
      &lt;th&gt;Resources &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Original Golang Service&lt;/td&gt;
      &lt;td&gt;1,000&lt;/td&gt;
      &lt;td&gt;20 Cores&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; New Rust Service&lt;/td&gt;
      &lt;td&gt;1,000&lt;/td&gt;
      &lt;td&gt;4.5 Cores&lt;/td&gt;
    &lt;/tr&gt;
 &lt;/tbody&gt;   
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;learnings-and-conclusion&quot;&gt;Learnings and conclusion&lt;/h2&gt;

&lt;p&gt;The outcomes and insights from this rewrite have been eye-opening, debunking certain myths while also validating others.&lt;/p&gt;

&lt;h3 id=&quot;myth-1-rust-is-blazingly-fast-faster-than-golang&quot;&gt;Myth 1: Rust is blazingly fast! Faster than Golang!&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: Disproved.
Golang is “fast enough” for most use cases. It’s a mature language built with concurrency at its core, and it performs exceptionally well in its intended domain. While Rust can outperform Golang due to its higher performance ceiling and finer-grained control, rewriting a Golang service in Rust solely for performance improvements is unlikely to yield significant benefits.&lt;/p&gt;

&lt;h3 id=&quot;myth-2-rust-is-more-efficient-than-golang&quot;&gt;Myth 2: Rust is more efficient than Golang&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: True.
Rewriting a Golang service in Rust will probably give you 50% savings in compute. Rust does fulfill its promise of being memory safe without garbage collection, allowing it to be one of the more efficient languages out there. This is in line with &lt;a href=&quot;https://aws.amazon.com/blogs/opensource/sustainability-with-rust/&quot;&gt;other discoveries&lt;/a&gt; in the market.&lt;/p&gt;

&lt;h3 id=&quot;myth-3-the-learning-curve-of-rust-is-too-high&quot;&gt;Myth 3: The learning curve of Rust is too high&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: It depends.
Pure synchronous Rust is fine. As long as you don’t overcomplicate the code and only clone what is needed, it is mostly true. The language is easy enough to pick up for most experienced developers. Even with cloning sprinkled in, the code is usually “fast enough”. The compiler is a good teacher, the compiler error messages are amazing, and if your code compiles, it probably works. Also, the Clippy linter is amazing.&lt;/p&gt;

&lt;p&gt;However, introducing async can be challenging. Async is something quite different from what you would encounter in other languages like Go. Improper use of blocking code in async code can result in nuanced bugs that can catch inexperienced Rust developers off-guard.&lt;/p&gt;

&lt;h2 id=&quot;evaluating-the-worth-of-the-rewrite&quot;&gt;Evaluating the worth of the rewrite&lt;/h2&gt;

&lt;p&gt;Yes, the effort was worth it for this service. The trade-off between development effort spent and the cost savings were justified.&lt;/p&gt;

&lt;p&gt;As a side effect, the service is 80% cheaper and probably more bug free, as Rust eliminates a class of common Golang errors like Null pointers and concurrent map writes by virtue of the design of the language. If your code compiles, you usually have the confidence that it will work as you expect due to the language being more explicit.&lt;/p&gt;

&lt;p&gt;Would we encourage choosing Rust over Golang for new microservices? Absolutely, as the resulting service is likely to be at least 50% more efficient than its Go counterpart. However, this decision presents an important and exciting opportunity for management and leaders to invest in empowering their engineers by equipping them with the skills to master Rust’s unique concepts, such as Async and Lifetimes. While the initial development pace might be slower as the team builds proficiency, this investment can unlock long-term benefits. Once the workforce is skilled in Rust, development speed should align with expectations, and the resulting systems are likely to be more stable and secure, thanks to Rust’s inherent safety features.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebrust&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Jun 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/counter-service-how-we-rewrote-it-in-rust</link>
        <guid isPermaLink="true">https://engineering.grab.com/counter-service-how-we-rewrote-it-in-rust</guid>
        
        <category>Database</category>
        
        <category>Rust</category>
        
        <category>Data</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>The complete stream processing journey on FlinkSQL</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the fast-paced world of data analytics, real-time processing has become a necessity. Modern businesses require insights not just quickly, but in real-time to make informed decisions and stay ahead of the competition. &lt;strong&gt;Apache Flink&lt;/strong&gt; has emerged as a powerful tool in this domain, offering state-of-the-art stream processing capabilities. In this blog, we introduce our FlinkSQL interactive solution in accompanying productionising automation, and enhancing our users’ stream processing development journey.&lt;/p&gt;

&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;Last year, we introduced Zeppelin notebooks for Flink, as detailed in our previous post &lt;a href=&quot;https://engineering.grab.com/rethinking-streaming-processing-data-exploration&quot;&gt;Rethinking Stream Processing: Data Exploration&lt;/a&gt; in an effort to enhance data exploration for downstream data users. However, as our use cases evolved over time, we quickly hit a few technical roadblocks.&lt;/p&gt;

&lt;h3 id=&quot;flink-version-maintenance&quot;&gt;Flink version maintenance&lt;/h3&gt;

&lt;p&gt;Zeppelin notebook source code is maintained by a community separate from Flink’s community. As of writing, the latest Flink version supported is 1.17, whilst the latest Flink is already on version 1.20. This discrepancy in version support hinders our Flink upgrading efforts.&lt;/p&gt;

&lt;h3 id=&quot;cluster-start-up-time&quot;&gt;Cluster start up time&lt;/h3&gt;

&lt;p&gt;Our design to spin up a Zeppelin cluster per user on demand invokes a cold start delay, taking roughly around 5 minutes for the notebook to be ready. This delay is not suitable for use cases that require quick insights from production data. We quickly noticed that the user uptake of this solution was not as high as we expected.&lt;/p&gt;

&lt;h3 id=&quot;integration-challenges&quot;&gt;Integration challenges&lt;/h3&gt;

&lt;p&gt;Whilst Zeppelin notebooks were useful for serving individual developers, we experienced difficulty integrating it with other internal platforms. We designed Zeppelin to empower solo data explorers, but other internal platforms like dashboards or automated pipelines needed a way to aggregate data from Kafka and Zeppelin just couldn’t keep up. The notebook setup was too isolated, requiring a workaround to share insights or plug into existing tools. For instance, if a team wanted to pull aggregated real-time metrics into a monitoring system, they had to export data manually, which is far from seamless access that we aimed for.&lt;/p&gt;

&lt;h2 id=&quot;introducing-flinksql-interactive&quot;&gt;Introducing FlinkSQL interactive&lt;/h2&gt;

&lt;p&gt;With those considerations in mind, we decided to swap out our Zeppelin cluster with a shared &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql-gateway/overview/&quot;&gt;FlinkSQL gateway&lt;/a&gt; cluster. We simplified our solution by removing some features our notebooks offered, focusing only on features that promote data democratisation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Shared FlinkSQL gateway architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We split our solution into 3 layers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compute layer&lt;/li&gt;
  &lt;li&gt;Integration layer&lt;/li&gt;
  &lt;li&gt;Query layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Users first interact with our platform portal to submit queries for data from Kafka online store using SQL (1). Upon submission, our backend orchestrator then creates a session for the user (2) and submits the SQL query to our FlinkSQL gateway using their inbuilt REST API (3). The FlinkSQL gateway then packages the SQL query into a Flink job to be submitted to our Flink session cluster (4) before collating its results. The subsequent results would be polled from the query layer to be displayed back to the user.&lt;/p&gt;

&lt;h3 id=&quot;compute-layer&quot;&gt;Compute layer&lt;/h3&gt;

&lt;p&gt;With FlinkSQL gateway acting as the main compute engine for ad-hoc queries, it is now more straightforward to perform Flink version upgrades along with our solution, since the FlinkSQL gateway is packaged along with the main Flink distribution. We do not need to maintain Flink shims for each version as adapters between the Flink compute cluster and Zeppelin notebook cluster.&lt;/p&gt;

&lt;p&gt;Another advantage of using the shared FlinkSQL gateway was the reduced cold start time for each ad-hoc queries. Since all users share the same FlinkSQL cluster instead of having their own Zeppelin cluster, there was no need to wait for cluster startup during initialisation of their sessions. This brought the lead time to the first results displayed down from 5 minutes to 1 minute. There was still lead time involved as the tool provisions task managers on an ad-hoc basis to balance availability of such developer tools and the associated cost.&lt;/p&gt;

&lt;h3 id=&quot;integration-layer&quot;&gt;Integration layer&lt;/h3&gt;

&lt;p&gt;The Integration layer serves as the glue between the user-facing query layer and the underlying compute layer, ensuring seamless communication and security across our ecosystem. With the shift to a shared FlinkSQL gateway, we recognised the need for an intermediary that could handle authentication, authorisation, orchestration, and integration with internal platforms - all while abstracting the complexities of Flink’s native REST API.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: FlinkSQL gateway&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The FlinkSQL gateway’s built-in REST API gets the job done for basic query submission, but it falls short in areas like session management, requiring multiple POST requests just to fetch results. To address this, we extended a custom control plane with its own set of REST APIs, layered on top of the gateway.&lt;/p&gt;

&lt;p&gt;We then extend these sessions and integrate them to our inhouse authentication and authorisation platform. For each query made, the control plane authenticates the user, spins up lightweight sessions and manages the communication between the caller and the Flink Session Cluster. If you are interested, check out our previous blog post, &lt;a href=&quot;https://engineering.grab.com/an-elegant-platform&quot;&gt;An elegant platform&lt;/a&gt;, for more details on the above mentioned streaming platform and its control plane.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl --location &apos;https://example.com/v1/flink/flinksql/interactive&apos; \
--header &apos;Content-Type: application/json&apos; \
--header &apos;Authorization: Bearer ...&apos; \
--data &apos;{
    &quot;environment&quot; : &quot;prd&quot;,
    &quot;namespace&quot; : &quot;flink&quot;,
    &quot;sql&quot; : &quot;SELECT * FROM titanicstream&quot;}&apos;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Example API request for running a FlinkSQL query&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The integration layer also caters to B2B needs via our Headless APIs. By exposing the endpoints, developers are able to integrate real-time processing into their own tools. To run a query, programs can simply make a POST request with the SQL query and an operation ID would be returned. This operation ID could then be used in subsequent GET requests to fetch the paginated results of the unbounded query. This setup is ideal for internal platforms that need to query Kafka data programmatically. By abstracting these complexities, it ensures that users, whether individual analysts or internal platforms—can tap into Kafka data without wrestling with Flink’s raw interfaces.&lt;/p&gt;

&lt;h3 id=&quot;query-layer&quot;&gt;Query layer&lt;/h3&gt;

&lt;p&gt;We then proceed to pair our APIs developed with an Interactive UI to build a Query layer that serves both human workflows. This is where users meet our platform.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Flink query layer’s user flow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Through our platform portal, users land in a clean SQL editor. We used a Hive Metastore (HMS) catalog that translates Kafka topics into tables. Users don’t need to decode stream internals; they can jump straight into it by simply selecting a table to query on. Once a query is submitted, it is then handled by the integration layer which routes it through the control plane to the gateway. Results are then streamed back, appearing in the UI within one minute, a significant improvement from the five minute Zeppelin cold starts.&lt;/p&gt;

&lt;p&gt;This all crystalises into the user flow demonstrated in Figure 3, where we can easily retrieve Titanic data from a Kafka stream with a short command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT COUNT(*) FROM titanicstream WHERE kafkaEventTime &amp;gt; NOW() - INTERVAL &apos;1&apos; HOUR.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This setup enables a few use cases for our teams, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fraud analysts using the real-time data to debug and spot patterns in fraudulent transactions.&lt;/li&gt;
  &lt;li&gt;Data scientists querying live signals to validate their prediction models.&lt;/li&gt;
  &lt;li&gt;Engineers validating the messages sent from their system to confirm they are properly structured and accurately delivered.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;productionising-flinksql&quot;&gt;Productionising FlinkSQL&lt;/h2&gt;

&lt;p&gt;With data being democratised, we see more users building use cases around our online data store and utilising the above tools to build new stream processing pipelines expressed as SQL queries. To simplify the last step of the software development lifecycle of deployment, we have also developed a tool to create a configuration based stream processing pipeline, with the business logic expressed as a SQL statement.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Portal for FlinkSQL pipeline creation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We host connectors for users to connect to other platforms within Grab, such as Kafka and our internal feature stores. Users could simply use them off-the-shelf and configure according to their needs before deploying their stream processing pipeline.&lt;/p&gt;

&lt;p&gt;Users would then proceed to submit their streaming logic as a SQL statement. In the example illustrated in the diagram, the logic expressed is a simple filter on a Kafka stream for sinking the filtered events into a separate Kafka stream.&lt;/p&gt;

&lt;p&gt;Users have the ability to then define the parallelism and associated resources they want to run their Flink jobs with. Upon submission, the associated resources would be provisioned and the Flink pipeline would be automatically deployed. Behind the scenes, we manage the application JAR file that is being used to run the job that dynamically parses these configurations and translates them into a proper Flink job graph to be submitted to the Flink cluster.&lt;/p&gt;

&lt;p&gt;Within 10 minutes, users would have completed deploying their stream processing pipeline to production.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With our full suite of solutions for low code development via FlinkSQL, from exploration and design, to development and then deployment, we have simplified the journey for developing business use cases off online streaming stores. By offering both a user-friendly interface for low-code users and a robust API for developers, these tools empower businesses to harness the full potential of real-time data processing. Whether you are a data analyst looking for quick insights or a developer integrating real-time analytics into your applications, our tools are able to lower the barrier of entry to utilising real-time data.&lt;/p&gt;

&lt;p&gt;After we released these solutions, we quickly saw an uptick in pipelines created as well as the number of interactive queries fired. This result was encouraging and we hope that this would gradually bring upon a paradigm shift, enabling Grab to make data-driven operational decisions on real-time signals, empowering us with the ability to react to ever-changing market conditions in the most efficient manner.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebflinksql&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Jun 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/the-complete-stream-processing-journey-on-flinksql</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-complete-stream-processing-journey-on-flinksql</guid>
        
        <category>Database</category>
        
        <category>FlinkSQL</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Effortless enterprise authentication at Grab: Dex in action</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Grab, Southeast Asia’s leading superapp, has created many internal applications to support its diverse range of internal and external business needs. Authentication&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and authorisation&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; serve as fundamental components of application development, as robust identity and access management are essential for all systems.&lt;/p&gt;

&lt;p&gt;We recognised the need for a centralised internal system to manage access, authentication, and authorisation. This system would streamline access management, ensure compliance with audit requirements, enhance developer velocity, and simplify authentication and authorisation processes for both developers and business operations.&lt;/p&gt;

&lt;p&gt;Grab created Concedo to fulfill this requirement by providing a mechanism for services to configure their access control based on their specific role to permission matrix (R2PM)&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. This allows for quick and easy integration with Concedo, enabling developers to expedite the shipping of their systems without investing excessive time in building the authentication and authorisation module.&lt;/p&gt;

&lt;p&gt;The authentication mechanism, based on Google’s OAuth2.0&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, includes custom features that enhance identity for service integration. However, this customisation isn’t standard, creating integration challenges with external platforms like Databricks and Datadog. These platforms then use their own authentication and authorisation, resulting in a fragmented and undesirable sign-on experience for users.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/undesired-sign-on-experience.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Undesired user sign-on experience due to fragmented authentication approaches.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The inconsistency in user experience also resulted in complications. The lack of standardisation led to difficulties in establishing authentication and authorisation for individual applications. Additionally, it created substantial administrative overhead due to the necessity of managing multiple identities. The absence of standardisation also hindered transparency in access control across all applications.&lt;/p&gt;

&lt;p&gt;This led us to inquire how a standardised protocol could be established to function seamlessly across all applications, regardless of whether they were developed internally or sourced from external platforms.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/desired-state.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Desired state, having something in between the different identity providers (IdP).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;choosing-among-industry-standards&quot;&gt;Choosing among industry standards&lt;/h2&gt;

&lt;p&gt;We wanted to build a platform to serve both authentication and authorisation, providing a seamless integration and user sign-on experience. We then asked ourselves, “What are the current industry standards we can leverage on?”.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Security Assertion Markup Language (SAML)&lt;/strong&gt;: An authentication protocol which leverages heavily on session cookies to manage each authentication session.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Open Authorisation (OAuth)&lt;/strong&gt;: An authorisation protocol which focuses on granting access for particular details rather than providing user identity information.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;OpenID Connect (OIDC)&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;: An authentication protocol built on OAuth 2.0, enabling single sign-on (SSO). OIDC unifies and standardises user authentication, making it a solution for organisations with numerous applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OIDC enhances user experience by redirecting them to an identity provider (IdP) like Google or Microsoft for authentication when accessing an application. Upon successful verification, the IdP sends a secure token with the user’s identity information back to the application, granting access without the need for additional credentials.&lt;/p&gt;

&lt;p&gt;With OIDC, authentication and authorisation are fully implemented, enabling seamless integration across platforms, including mobile, API, and browser-based applications, while also providing SSO functionality.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/desired-state-protocol.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Desired state with the protocol decided.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;OIDC seemed like an ideal solution, but it came with potential drawbacks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OIDC relies on trusting a third-party authentication service. Any disruption to this service could result in downtime.&lt;/li&gt;
  &lt;li&gt;Compromised credentials could affect access to multiple services.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following section, we will explore our strategies in mitigating these challenges effectively.&lt;/p&gt;

&lt;h2 id=&quot;implementing-the-chosen-standard&quot;&gt;Implementing the chosen standard&lt;/h2&gt;

&lt;p&gt;With OIDC chosen as the standard, the focus shifted to implementation.&lt;/p&gt;

&lt;p&gt;We have always been a supporter of open source projects. Rather than building a platform from the ground up, we leveraged existing solutions while seeking opportunities to contribute back to the open source community.&lt;/p&gt;

&lt;p&gt;The team explored Cloud Native Computing Foundation (CNCF) projects and discovered &lt;a href=&quot;https://dexidp.io/&quot;&gt;Dex&lt;/a&gt; - A federated OpenID connect provider that aims to allow integration of any IdP into an application using OIDC. Dex was selected as our open-source platform of choice due to its alignment with our high-level objectives.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/desired-state-platform.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Desired state with Dex as the platform foundation.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;how-dex-works&quot;&gt;How Dex works&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/dex-work.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. High level architecture of Dex. Source: https://dexidp.io/docs/&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When a user or machine tries to access a protected application or service, they are redirected to Dex for authentication. Dex acts as a middleman (identity aggregator) between the user and various IdPs to establish an authenticated session.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/sequence-diagram-dex-auth.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Simplified sequence diagram of how authentication works for Dex.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Dex’s key features include enabling SSO experiences, allowing users to access multiple applications after authenticating through a single provider. Dex also supports multiple IdP use cases and provides standardised OIDC authentication tokens.&lt;/p&gt;

&lt;p&gt;Dex implementation separated application authentication concerns, established a single source of truth for identity, enabled new IdP additions, ensured adherence to security best practices, and provided scalability for deployments of all sizes.&lt;/p&gt;

&lt;h2 id=&quot;how-dex-is-streamlining-authentication-and-authorisation&quot;&gt;How Dex is streamlining authentication and authorisation&lt;/h2&gt;

&lt;h3 id=&quot;token-delegation&quot;&gt;Token delegation&lt;/h3&gt;

&lt;p&gt;When services communicate with each other, one service often assigns an identity to ensure that authorisation can be carried out on a specific service. For example, in figure 7, a service account or robot account is typically used as an identity so that service B can identify the caller.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/service-identification.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Service identification through service account.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Although service accounts are the recommended approach for enabling Service B to identify the caller, they come with challenges that must be addressed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Service account compromise&lt;/strong&gt;: Service accounts often have high-level privileges and typically broad access to Service B. If compromised, they pose a significant security risk, making careful management essential.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Access control issue&lt;/strong&gt;: The other approach creates unnecessary complexity by requiring Service A to handle user-level permissions for Service B. This violates the principle of separation of concerns.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address this issue, Dex introduced a &lt;a href=&quot;https://dexidp.io/docs/guides/token-exchange/&quot;&gt;token exchange&lt;/a&gt; feature.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/token-exchange.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. Token exchange example with trusted peers established.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The token exchange process involves two main components; token minting and trust relationship.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Token minting&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The user (Alice) logs into Service A.&lt;/li&gt;
  &lt;li&gt;Service A, acting as a trusted peer, is authorised to mint tokens.&lt;/li&gt;
  &lt;li&gt;Service A generates a token valid for both Service A and Service B. This is reflected in the token’s “aud” (audience) field: “aud”: “serviceA serviceB”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Trust relationship&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Service B must be configured to trust Service A as a peer.&lt;/li&gt;
  &lt;li&gt;Service B accepts tokens minted by Service A.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach differs from the service account-based scenario by using a trust-based peer relationship. Service A is authorised to mint tokens for Service B providing a more sophisticated but preferred method. The token is properly scoped for both services, ensuring a clear audit trail of token issuance, while reducing token manipulation risks.&lt;/p&gt;

&lt;h3 id=&quot;kill-switch&quot;&gt;Kill switch&lt;/h3&gt;

&lt;p&gt;As highlighted earlier,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;OIDC relies on trusting a third-party authentication service. Any disruption to this service could result in downtime.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dex’s ability to support multiple IdPs enables traffic to be shifted to a different IdP if one, such as Google, experiences an outage. This “kill switch” mechanism ensures that integrated services are not disrupted and do not require any changes to mitigate the issue. It is only triggered during specific IdP outages.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/killswitch.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. Trigger kill switch without having other services changing from their end.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;looking-forward&quot;&gt;Looking forward&lt;/h2&gt;

&lt;p&gt;Following the successful implementation of Dex as the unified authentication provider, the next phase in enhancing our identity and access management infrastructure is to leverage this robust identity foundation to establish a unified and simplified authorisation model. This initiative is driven by the recognition that the current authorisation landscape remains fragmented and complex, leading to potential inefficiencies and security vulnerabilities.&lt;/p&gt;

&lt;p&gt;By centralising authorisation and aligning it with the unified identity provided by Dex, we can streamline access control, improve user experience, and strengthen security across our applications and services. This will involve consolidating authorisation policies, standardising access control mechanisms, and simplifying the management of user permissions.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Shoutout to the awesome Concedo team for driving Dex integration and to our leadership for steering the way toward a simpler, unified authentication and authorisation journey! Special recognition to Arun Ravi for his invaluable contributions to the project—his work on Dex within the Data Tech platform laid the foundation for what eventually evolved into the Concedo Dex.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdex&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h3 id=&quot;definition-of-terms&quot;&gt;Definition of terms&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Authentication: Who you are. Making sure you are who you say you are by verifying your identity. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Authorisation: What you can do. Defining the resources or actions you are allowed to access or perform after your identity has been verified. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Role-to-Permission Matrix (R2PM): A structured framework used to map roles within an organisation to the permissions or access rights each role has in a system, application, or process. This matrix serves as a critical component in access control and identity management, ensuring that users have appropriate access based on their roles while minimising the risk of unauthorised access. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Open Authorisation (OAuth 2.0): Protocol for authorisation. For example, Google Login on third-party portals allows your identity to remain with Google, but third-party portals can obtain limited access to specific data such as your profile photo. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;OpenID Connect (OIDC): Identity protocol built on top of OAuth 2.0. On top of authorisation provided by OAuth 2.0, it verifies and provides a trusted identity. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 23 May 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/dex-in-action</link>
        <guid isPermaLink="true">https://engineering.grab.com/dex-in-action</guid>
        
        <category>Access control</category>
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
