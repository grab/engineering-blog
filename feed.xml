<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 19 Dec 2023 10:46:52 +0000</pubDate>
    <lastBuildDate>Tue, 19 Dec 2023 10:46:52 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Championing CyberSecurity: Grab's bug bounty programme in 2023</title>
        <description>&lt;p&gt;Launched in 2015, &lt;a href=&quot;https://hackerone.com/grab?type%3Dteam&quot;&gt;Grab’s Security bug bounty programme&lt;/a&gt; has achieved remarkable success and forged strong partnerships within a thriving bounty community. By holding quarterly campaigns with HackerOne, Grab has been dedicated to security and giving back to the global security community to research further. Over the years, Grab has paid over $700,000 in cumulative payments to committed security researchers, aiding their research.&lt;/p&gt;

&lt;p&gt;Our journey doesn’t stop there – we’ve also expanded our internal bug bounty team, ensuring that we have the necessary resources to stay at the forefront of security challenges. As we continue to innovate and evolve, it’s critical that our team remains at the cutting edge of security developments.&lt;/p&gt;

&lt;p&gt;Marking its eighth year in 2023, this initiative has achieved new milestones and continues to set the stage for an even more successful ninth year. In 2023, this included a special campaign in Threatcon Nepal, aimed at increasing our bounty engagements. A key development was the enrichment of monetary incentives to honour our hacker community’s remarkable contributions to our programme’s success.&lt;/p&gt;

&lt;p&gt;Let’s look at the key takeaways we gained from the bug bounty programme in 2023.&lt;/p&gt;

&lt;h2 id=&quot;highlights-from-2023&quot;&gt;Highlights from 2023&lt;/h2&gt;

&lt;p&gt;This year, we had some of the highest participation and engagement rates we’ve seen since the programme launched.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We’ve processed ~1000 submissions through our HackerOne bug bounty programme.&lt;/li&gt;
  &lt;li&gt;Impressive record of 400 submissions in the Q1 2023 campaign.&lt;/li&gt;
  &lt;li&gt;We’ve maintained a consistent schedule of campaigns and innovative efforts to enhance hacker engagement.&lt;/li&gt;
  &lt;li&gt;Released a comprehensive report of our seven-year bug bounty journey – check out some key highlights in the image below.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cybersec-bug/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;As Grab expands and transforms its product and service portfolio, we are dedicated to ensuring that our bug bounty programme reflects this growth. In our rigorous pursuit of boosting security, we regularly introduce new areas of focus to our scope. In 2024, expect the inclusion of new scopes, enhanced response times, heightened engagement from the hacker community, and more competitive rewards.&lt;/p&gt;

&lt;p&gt;In the past year, we have incorporated Joint Ventures and Acquisitions into the scope of our bug bounty programme. By doing so, we proactively address emerging security challenges, while fortifying the safety and integrity of our expanding ecosystem. We remain fully dedicated to embracing change and growth as integral parts of our journey to provide a secure and seamless experience for our users.&lt;/p&gt;

&lt;p&gt;On top of that, we continue to improve our methods of motivating researchers through the bug bounty programme. One recent change is to diversify our reward methods by incorporating both financial rewards and recognition. This allows us to cater to different researcher motivations, cultivate stronger relationships, and acknowledge researchers’ contributions.&lt;/p&gt;

&lt;p&gt;That said, we recognise that there’s always room for improvement and the bug bounty programme is uniquely poised for substantial expansion. In the near future, we will be:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Introducing more elements to the scope of our bug bounty programme&lt;/li&gt;
  &lt;li&gt;Enhancing feedback loops on the HackerOne platform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these improvements, we can drive continuous improvement efforts to provide a secure experience for our users while strengthening our connection with the security research community.&lt;/p&gt;

&lt;h2 id=&quot;a-word-of-thanks&quot;&gt;A word of thanks&lt;/h2&gt;

&lt;p&gt;2023 has been an exhilarating year for our team. We’re grateful for the continued support from all the security researchers who’ve actively participated in our programme.&lt;/p&gt;

&lt;p&gt;Here are the top three researchers in 2023:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/damian89&quot;&gt;Damian89&lt;/a&gt; &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/happy_csr&quot;&gt;Happy_csr&lt;/a&gt; &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/mclaren650sspider&quot;&gt;mclaren650sspider&lt;/a&gt; &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As we head into our ninth year, we know there are new opportunities and challenges that await us. We strive to remain dedicated to the values of collaboration and continuous improvement, working hand in hand with the security community to enhance our superapp’s security and deliver an even safer experience for our users.&lt;/p&gt;

&lt;p&gt;We’re gearing up for another exciting year ahead in our programme, and looking forward to interesting submissions from our participants. We extend an open invitation to all researchers to submit reports to our bug bounty programme. Your contributions hold immense value and have a significant impact on the safety and security of our products, our users, and the broader security community. For comprehensive information about the programme scope, rules, and rewards, &lt;a href=&quot;https://hackerone.com/grab?type%3Dteam&quot;&gt;visit our website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Until next year, keep up the great work, and happy hacking!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Dec 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/cybersec-bug</link>
        <guid isPermaLink="true">https://engineering.grab.com/cybersec-bug</guid>
        
        <category>Security</category>
        
        <category>Bug bounty</category>
        
        <category>HackerOne</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Sliding window rate limits in distributed systems</title>
        <description>&lt;p&gt;Like many other companies, Grab uses marketing communications to notify users of promotions or other news. If a user receives these notifications from multiple companies, it would be a form of information overload and they might even start considering these communications as spam. Over time, this could lead to some users revoking their consent to receive marketing communications altogether. Hence, it is important to find a rate-limited solution that sends the right amount of communications to our users.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;In Grab, marketing emails and push notifications are part of carefully designed campaigns to ensure that users get the right notifications (i.e. based on past orders or usage patterns). Trident is Grab’s in-house tool to compose these campaigns so that they run efficiently at scale. An example of a campaign is scheduling a marketing email blast to 10 million users at 4 pm. Read more about Trident’s architecture &lt;a href=&quot;/supporting-large-campaigns-at-scale&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trident relies on Hedwig, another in-house service, to deliver the messages to users. Hedwig does the heavy lifting of delivering large amounts of emails and push notifications to users while maintaining a high query per second (QPS) rate and minimal delay. The following high-level architectural illustration demonstrates the interaction between Trident and Hedwig.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Diagram of data interaction between Trident and Hedwig&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The aim is to regulate the number of marketing comms sent to users daily and weekly, tailored based on their interaction patterns with the Grab superapp.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Based on their interaction patterns with our superapp, we have clustered users into a few segments.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;New: Users recently signed up to the Grab app but haven’t taken any rides yet.
Active: Users who took rides in the past month.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With these metrics, we came up with optimal daily and weekly frequency limit values for each clustered user segment. The solution discussed in this article ensures that the comms sent to a user do not exceed the daily and weekly thresholds for the segment. This is also called frequency capping.&lt;/p&gt;

&lt;p&gt;However, frequency capping can be split into two sub-problems:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Efficient storage of clustered user data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With a huge customer base of over 270 million users, storing the user segment membership information has to be cost-efficient and memory-sleek. Querying the segment to which a user belongs should also have minimal latency.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Persistent tracking of comms sent per user&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To stay within the daily and weekly thresholds, we need to actively track the number of comms sent to each user, which can be referred to make rate limiting decisions. The rate limiting logic should also have minimal latency, be cost efficient, and not take up too much memory storage.&lt;/p&gt;

&lt;h4 id=&quot;optimising-storage-of-user-segment-data&quot;&gt;Optimising storage of user segment data&lt;/h4&gt;

&lt;p&gt;The problem here is figuring out which segment a particular user belongs to and ensuring that the user doesn’t appear in more than one segment. There are two options that suit our needs and we’ll explain more about each option, as well as what was the best option for us.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bloom filter&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/&quot;&gt;Bloom filter&lt;/a&gt; is a space-efficient probabilistic data structure that addresses this problem well. Simply put, Bloom filters internally use arrays to track memberships of the elements.&lt;/p&gt;

&lt;p&gt;For our scenario, each user segment would need its own bloom filter. We used &lt;a href=&quot;https://hur.st/bloomfilter/?n%3D270000000%26p%3D1.0E-7%26m%3D%26k%3D&quot;&gt;this bloom filter&lt;/a&gt; calculator to estimate the memory required for each bloom filter. We found that we needed approximately 1 GB of memory and 23 hash functions to accurately represent the membership information of 270 million users in an array. Additionally, this method guarantees a false positive rate of  1.0E-7, which means 1 in 1 million elements may get wrong membership results because of hash collision.&lt;/p&gt;

&lt;p&gt;With Grab’s existing segments, this approach needs 4GB of memory, which may increase as we increase the number of segments in the future. Moreover, the potential hash collision needs to be handled by increasing the memory size with even more hash functions. Another thing to note is that Bloom filters do not support deletion so every time a change needs to be done, you need to create a new version of the Bloom filter. Although Bloom filters have many advantages, these shortcomings led us to explore another approach.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Roaring bitmaps&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Roaring bitmaps are sets of unsigned integers consisting of containers of disjoint subsets, which can store large amounts of data in a compressed form. Essentially, roaring bitmaps could reduce memory storage significantly and overcome the hash collision problem. To understand the intuition behind this, first, we need to know how bitmaps work and the possible drawbacks behind it.&lt;/p&gt;

&lt;p&gt;To represent a list of numbers as a bitmap, we first need to create an array with a size equivalent to the largest element in the list. For every element in the list, we then mark the bit value as 1 in the corresponding index in the array. While bitmaps work very well for storing integers in closer intervals, they occupy more space and become sparse when storing integer ranges with uneven distribution, as shown in the image below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Diagram of bitmaps with uneven distribution&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To reduce memory footprint and improve the performance of bitmaps, there are compression techniques such as Run-Length Encoding (RLE), and Word Aligned Hybrid (WAH). However, this would require additional effort to implement, whereas using roaring bitmaps would solve these issues.&lt;/p&gt;

&lt;p&gt;Roaring bitmaps’ hybrid data storage approach offers the following advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster set operations (union, intersection, differencing).&lt;/li&gt;
  &lt;li&gt;Better compression ratio when handling mixed datasets (both dense and sparse data distribution).&lt;/li&gt;
  &lt;li&gt;Ability to scale to large datasets without significant performance loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To summarise, roaring bitmaps can store positive integers from 0 to (2^32)-1. Each positive integer value is converted to a 32-bit binary, where the 16 Most Significant Bits (MSB) are used as the key and the remaining 16 Least Significant Bits (LSB) are represented as the value. The values are then stored in an array, a bitmap, or used to run containers with RLE encoding data structures.&lt;/p&gt;

&lt;p&gt;If the number of integers mapped to the key is less than 4096, then all the integers are stored in an array in sorted order and converted into a bitmap container in the runtime as the size exceeds. Roaring bitmap analyses the distribution of set bits in the bitmap container i.e. if the continuous interval of set bits is more than a given threshold, the bitmap container can be more efficiently represented using the RLE container. Internally, the RLE container uses an array where the even indices store the beginning of the runs and the odd indices represent the length of the runs. This enables the roaring bitmap to dynamically switch between the containers to optimise storage and performance.&lt;/p&gt;

&lt;p&gt;The following diagram shows how a set of elements with different distributions are stored in roaring bitmaps.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Diagram of how roaring bitmaps store elements with different distributions &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Grab, we developed a microservice that abstracts roaring bitmaps implementations and provides an API to check set membership and enumeration of elements in the sets. &lt;a href=&quot;/streamlining-grabs-segmentation-platform&quot;&gt;Check out this blog to learn more about it.&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;distributed-rate-limiting&quot;&gt;Distributed rate limiting&lt;/h4&gt;

&lt;p&gt;The second part of the problem involves rate limiting the number of communication messages sent to users on a daily or weekly basis and each segment has specific daily and weekly limits. By utilising roaring bitmaps, we can determine the segment to which a user belongs. After identifying the appropriate segment, we will apply the personalised limits to the user using a distributed rate limiter, which will be discussed in further detail in the following sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Choosing the right datastore&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Based on our use case, Amazon ElasticCache for Redis and DynamoDB were two viable options for storing the sent communication messages count per user. However, we decided to choose Redis due to a number of factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Higher throughput at lower latency – Redis shards data across nodes in the cluster.&lt;/li&gt;
  &lt;li&gt;Cost-effective – Usage of Lua script reduces unnecessary data transfer overheads.&lt;/li&gt;
  &lt;li&gt;Better at handling spiky rate limiting workloads at scale.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Distributed rate limiter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To appropriately limit the comms our users receive, we needed a rate limiting algorithm, which could execute directly in the datastore cluster, then return the results in the application logic for further processing. The two rate limiting algorithms we considered were the sliding window rate limiter and sliding log rate limiter.&lt;/p&gt;

&lt;p&gt;The sliding window rate limiter algorithm divides time into a fixed-size window (we defined this as 1 minute) and counts the number of requests within each window. On the other hand, the sliding log maintains a log of each request timestamp and counts the number of requests between two timestamp ranges, providing a more fine-grained method of rate limiting. Although sliding log consumes more memory to store the log of request timestamp, we opted for the sliding log approach as the accuracy of the rate limiting was more important than memory consumption.&lt;/p&gt;

&lt;p&gt;The sliding log rate limiter utilises a Redis sorted set data structure to efficiently track and organise request logs. Each timestamp in milliseconds is stored as a unique member in the set. The score assigned to each member represents the corresponding timestamp, allowing for easy sorting in ascending order. This design choice optimises the speed of search operations when querying for the total request count within specific time ranges.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Sliding Log Rate limiter Algorithm:

Input:
  # user specific redis key where the request timestamp logs are stored as sorted set
  keys =&amp;gt; user_redis_key

  # limit_value is the limit that needs to be applied for the user
  # start_time_in_millis is the starting point of the time window
  # end_time_in_millis is the ending point of the time window
  # current_time_in_millis is the current time the request is sent
  # eviction_time_in_millis, members in the set whose value is less than this will be evicted from the set

  args =&amp;gt; limit_value, start_time_in_millis, end_time_in_millis, current_time_in_millis, eviction_time_in_millis

Output:
  # 0 means not_allowed and 1 means allowed
  response =&amp;gt; 0 / 1

Logic:
  # zcount fetches the count of the request timestamp logs falling between the start and the end timestamp
  request_count = zcount user_redis_key start_time_in_millis end_time_in_millis

  response = 0
  # if the count of request logs is less than allowed limits then record the usage by adding current timestamp in sorted set

  if request_count &amp;lt; limit_value then
    zadd user_redis_key current_time_in_millis current_time_in_millis
    response = 1

  # zremrangebyscore removes the members in the sorted set whose score is less than eviction_time_in_millis

  zremrangebyscore user_redis_key -inf eviction_time_in_millis
  return response
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This algorithm takes O(log n) time complexity, where n is the number of request logs stored in the sorted set. It is not possible to evict entries in the sorted set like how we have time-to-live (TTL) for Redis keys. To prevent the size of the sorted set from increasing over time, we have a fixed variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eviction_time_in_millis&lt;/code&gt; that is passed to the script. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zremrangebyscore&lt;/code&gt; command then deletes members from the sorted set whose score is less than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eviction_time_in_millis&lt;/code&gt; in O(log n) time complexity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lua script optimisations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Redis Cluster mode, all Redis keys accessed by a Lua script must be present on the same node, and they should be passed as part of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KEYS&lt;/code&gt; input array of the script. If the script attempts to access keys located on different nodes within the cluster, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CROSSSLOT&lt;/code&gt; error will be thrown. Redis keys, or userIDs, are distributed across multiple nodes in the cluster so it is not feasible to send a batch of userIDs within the same Lua script for rate limiting, as this might result in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CROSSSLOT&lt;/code&gt; error.&lt;/p&gt;

&lt;p&gt;Invoking a separate Lua script call for each user is a possible approach, but it incurs a significant number of network calls, which can be optimised further with the following approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Upload the Lua script into the Redis server during the server startup with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SCRIPT LOAD&lt;/code&gt; command and we get the SHA1 hash of the script if the upload is successful.&lt;/li&gt;
  &lt;li&gt;The SHA1 hash can then be used to invoke the Lua script with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVALSHA&lt;/code&gt; command passing the keys and arguments as script input.&lt;/li&gt;
  &lt;li&gt;Redis pipelining takes in multiple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVALSHA&lt;/code&gt; commands that call the Lua script and each invocation corresponds to a userID for getting the rate limiting result.&lt;/li&gt;
  &lt;li&gt;Redis pipelining groups the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVALSHA&lt;/code&gt; Redis commands with Redis keys located on the same nodes internally. It then sends the grouped commands in a single network call to the relevant nodes within the Redis cluster and provides the rate limiting outcome to the client.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since Redis operates on a single thread, any long-running Lua script can cause other Redis commands to be blocked until the script completes execution. Thus, it’s optimal for the Lua script to execute in under 5 milliseconds. Additionally, the current time is passed as an argument to the script to account for potential variations in time when the script is executed on a node’s replica, which could be caused by &lt;a href=&quot;https://medium.com/geekculture/all-things-clock-time-and-order-in-distributed-systems-physical-time-in-depth-3c0a4389a838%23:~:text%3DClock%2520Drift%253A%2520As%2520mentioned%252C%2520no,rate%2520is%2520called%2520clock%2520drift.&quot;&gt;clock drift&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By bringing together roaring bitmaps and the distributed rate limiter, this is what our final solution looks like:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Our final solution using roaring bitmaps and distributed rate limiter&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The roaring bitmaps structure is serialised and stored in an AWS S3 bucket, which is then downloaded in the instance during server startup. After which, triggering a user segment membership check can simply be done with a local method call. The configuration service manages the mapping information between the segment and allowed rate limiting values.&lt;/p&gt;

&lt;p&gt;Whenever a marketing message needs to be sent to a user, we first find the segment to which the user belongs, retrieve the defined rate limiting values from the configuration service, then execute the Lua script to get the rate limiting decision. If there is enough quota available for the user, we send the comms.&lt;/p&gt;

&lt;p&gt;The architecture of the messaging service looks something like this:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Architecture of the messaging service&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;In addition to decreasing the unsubscription rate, there was a significant enhancement in the latency of sending communications. Eliminating redundant communications also alleviated the system load, resulting in a reduction of the delay between the scheduled time and the actual send time of comms.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Applying rate limiters to safeguard our services is not only a standard practice but also a necessary process. Many times, this can be achieved by configuring the rate limiters at the instance level. The need for rate limiters for business logic may not be as common, but when you need it, the solution must be lightning-fast, and capable of seamlessly operating within a distributed environment.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Dec 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/frequency-capping</link>
        <guid isPermaLink="true">https://engineering.grab.com/frequency-capping</guid>
        
        <category>Data</category>
        
        <category>Big data</category>
        
        <category>Rate limiting</category>
        
        <category>Frequency capping</category>
        
        <category>Distributed systems</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>An elegant platform</title>
        <description>&lt;p&gt;Coban is Grab’s real-time data streaming platform team. As a platform team, we thrive on providing our internal users from all verticals with self-served data-streaming resources, such as &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; topics, &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; and &lt;a href=&quot;https://www.confluent.io/learn/change-data-capture/&quot;&gt;Change Data Capture&lt;/a&gt; (CDC) pipelines, various kinds of &lt;a href=&quot;https://docs.confluent.io/platform/current/connect/&quot;&gt;Kafka-Connect&lt;/a&gt; connectors, as well as &lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;Apache Zeppelin&lt;/a&gt; notebooks, so that they can effortlessly leverage real-time data to build intelligent applications and services.&lt;/p&gt;

&lt;p&gt;In this article, we present our journey from pure Infrastructure-as-Code (IaC) towards a more sophisticated control plane that has revolutionised the way data streaming resources are self-served at Grab. This change also leads to improved scalability, stability, security, and user adoption of our data streaming platform.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;In the early ages of public cloud, it was a common practice to create virtual resources by clicking through the web console of a cloud provider, which is sometimes referred to as &lt;em&gt;ClickOps&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;ClickOps&lt;/em&gt; has many downsides, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inability to review, track, and audit changes to the infrastructure.&lt;/li&gt;
  &lt;li&gt;Inability to massively scale the infrastructure operations.&lt;/li&gt;
  &lt;li&gt;Inconsistencies between environments, e.g. staging and production.&lt;/li&gt;
  &lt;li&gt;Inability to quickly recover from a disaster by re-creating the infrastructure at a different location.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That said, &lt;em&gt;ClickOps&lt;/em&gt; has one tremendous advantage; it makes creating resources using a graphical User Interface (UI) fairly easy for anyone like Infrastructure Engineers, Software Engineers, Data Engineers etc. This also leads to a high iteration speed towards innovation in general.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
IaC resolved many of the limitations of &lt;em&gt;ClickOps&lt;/em&gt;, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Changes are committed to a Version Control System (VCS) like Git: They can be reviewed by peers before being merged. The full history of all changes is available for investigating issues and for audit.&lt;/li&gt;
  &lt;li&gt;The infrastructure operations scale better: Code for similar pieces of infrastructure can be modularised. Changes can be rolled out automatically by Continuous Integration (CI) pipelines in the VCS system, when a change is merged to the main branch.&lt;/li&gt;
  &lt;li&gt;The same code can be used to deploy the staging and production environments consistently.&lt;/li&gt;
  &lt;li&gt;The infrastructure can be re-created anytime from its source code, in case of a disaster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, IaC unwittingly posed a new entry barrier too, requiring the learning of new tools like Ansible, Puppet, Chef, Terraform, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Some organisations set up dedicated Site Reliability Engineer (SRE) teams to centrally manage, operate, and support those tools and the infrastructure as a whole, but that soon created the potential of new bottlenecks in the path to innovation.&lt;/p&gt;

&lt;p&gt;On the other hand, others let engineering teams manage their own infrastructure, and Grab adopted that same approach. We use &lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt; to manage infrastructure, and all teams are expected to have select engineers who have received Terraform training and have a clear understanding of it.&lt;/p&gt;

&lt;p&gt;In this context, Coban’s platform initially started as a handful of Git repositories where users had to submit their Merge Requests (MR) of Terraform code to create their data streaming resources. Once reviewed by a Coban engineer, those Terraform changes would be applied by a CI pipeline running &lt;a href=&quot;https://www.runatlantis.io/&quot;&gt;Atlantis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
While this was a meaningful first step towards self-service and platformisation of Coban’s offering within Grab, it had several significant downsides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Due to the lack of control on the Terraform changes, the CI pipeline was prone to human errors and frequent failures. For example, users would initiate a new Terraform project by duplicating an existing one, but then would forget to change the location of the remote Terraform state, leading to the in-place replacement of an existing resource.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The Coban team needed to review all MRs and provide ad hoc support whenever the pipeline failed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: In the absence of Identity and Access Management (IAM), MRs could potentially contain changes pertaining to other teams’ resources, or even changes to Coban’s core infrastructure, with code review as the only guardrail.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited user growth&lt;/strong&gt;: We could only acquire users who were well-versed in Terraform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It soon became clear that we needed to build a layer of abstraction between our users and the Terraform code, to increase the level of control and lower the entry barrier to our platform, while still retaining all of the benefits of IaC under the hood.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;We designed and built an in-house three-tier control plane made of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Coban UI&lt;/strong&gt;, a front-end web interface, providing our users with a seamless ClickOps experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Heimdall&lt;/strong&gt;, the Go back-end of the web interface, transforming ClickOps into IaC.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Khone&lt;/strong&gt;, the storage and provisioner layer, a Git repository storing Terraform code and metadata of all resources as well as the CI pipelines to plan and apply the changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the next sections, we will deep dive in those three components.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 1 Simplified architecture of a request flowing from the user to the Coban infrastructure, via the three components of the control plane: the Coban UI, Heimdall, and Khone.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Although we designed the user journey to start from the Coban UI, our users can still opt to communicate with Heimdall and with Khone directly, e.g. for batch changes, or just because many engineers love Git and we want to encourage broad adoption. To make sure that data is eventually consistent across the three systems, we made Khone the only persistent storage layer. Heimdall regularly fetches data from Khone, caches it, and presents it to the Coban UI upon each query.&lt;/p&gt;

&lt;p&gt;We also continued using Terraform for all resources, instead of mixing various declarative infrastructure approaches (e.g. Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&quot;&gt;Custom Resource Definition&lt;/a&gt;, &lt;a href=&quot;https://helm.sh/docs/topics/charts/&quot;&gt;Helm charts&lt;/a&gt;), for the sake of consistency of the logic in Khone’s CI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;coban-ui&quot;&gt;Coban UI&lt;/h3&gt;

&lt;p&gt;The Coban UI is a &lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt; &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Glossary/SPA&quot;&gt;Single Page Application&lt;/a&gt; (React SPA) designed by our partner team Chroma, a dedicated team of front-end engineers who thrive on building legendary UIs and reusable components for platform teams at Grab.&lt;/p&gt;

&lt;p&gt;It serves as a comprehensive self-service portal, enabling users to effortlessly create data streaming resources by filling out web forms with just a few clicks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image7.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 2 Screen capture of a new Kafka topic creation in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to facilitating resource creation and configuration, the Coban UI is seamlessly integrated with multiple monitoring systems. This integration allows for real-time monitoring of critical metrics and health status for Coban infrastructure components, including Kafka clusters, Kafka topic bytes in/out rates, and more. Under the hood, all this information is exposed by Heimdall APIs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 3 Screen capture of the metrics of a Kafka cluster in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In terms of infrastructure, the Coban UI is hosted in &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html&quot;&gt;AWS S3 website hosting&lt;/a&gt;. All dynamic content is generated by querying the APIs of the back-end: Heimdall.&lt;/p&gt;

&lt;h3 id=&quot;heimdall&quot;&gt;Heimdall&lt;/h3&gt;

&lt;p&gt;Heimdall is the Go back-end of the Coban UI. It serves a collection of APIs for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Managing the data streaming resources of the Coban platform with Create, Read, Update and Delete (CRUD) operations, treating the Coban UI as a first-class citizen.&lt;/li&gt;
  &lt;li&gt;Exposing the metadata of all Coban resources, so that they can be used by other platforms or searched in the Coban UI.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All operations are authenticated and authorised. Read more about Heimdall’s access control in &lt;a href=&quot;/migrating-to-abac&quot;&gt;Migrating from Role to Attribute-based Access Control&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the next sections, we are going to dive deeper into these two features.&lt;/p&gt;

&lt;h4 id=&quot;managing-the-data-streaming-resources&quot;&gt;Managing the data streaming resources&lt;/h4&gt;

&lt;p&gt;First and foremost, Heimdall enables our users to self-manage their data streaming resources. It primarily relies on Khone as its storage and provisioner layer for actual resource management via Git CI pipelines. Therefore, we designed Heimdall’s resource management workflow to leverage the underlying Git flow.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 4 Diagram flow of a request in Heimdall.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 4 shows the diagram flow of a typical request in Heimdall to create, update, or delete a resource.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An authenticated user initiates a request, either by navigating in the Coban UI or by calling the Heimdall API directly. At this stage, the request state is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Initiated&lt;/code&gt; on Heimdall.&lt;/li&gt;
  &lt;li&gt;Heimdall validates the request against multiple validation rules. For example, if an ongoing change request exists for the same resource, the request fails. If all tests succeed, the request state moves to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ongoing&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Heimdall then creates an MR in Khone, which contains the Terraform files describing the desired state of the resource, as well as an in-house metadata file describing the key attributes of both resource and requester.&lt;/li&gt;
  &lt;li&gt;After the MR has been created successfully, Heimdall notifies the requester via Slack and shares the MR URL.&lt;/li&gt;
  &lt;li&gt;After that, Heimdall starts polling the status of the MR in a loop.&lt;/li&gt;
  &lt;li&gt;For changes pertaining to production resources, an approver who is code owner in the repository of the resource has to approve the MR. Typically, the approver is an immediate teammate of the requester. Indeed, as a platform team, we empower our users to manage their own resources in a self-service fashion. Ultimately, the requester would merge the MR to trigger the CI pipeline applying the actual Terraform changes. Note that for staging resources, this entire step 6 is automatically performed by Heimdall.&lt;/li&gt;
  &lt;li&gt;Depending on the MR status and the status of its CI pipeline in Khone, the final state of the request can be:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Failed&lt;/code&gt; if the CI pipeline has failed in Khone.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Completed&lt;/code&gt; if the CI pipeline has succeeded in Khone.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cancelled&lt;/code&gt; if the MR was closed in Khone.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Heimdall exposes APIs to let users track the status of their requests. In the Coban UI, a page queries those APIs to elegantly display the requests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 5 Screen capture of the Coban UI showing all requests.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;exposing-the-metadata&quot;&gt;Exposing the metadata&lt;/h4&gt;

&lt;p&gt;Apart from managing the data streaming resources, Heimdall also centralises and exposes the metadata pertaining to those resources so other Grab systems can fetch and use it. They can make various queries, for example, listing the producers and consumers of a given Kafka topic, or determining if a database (DB) is the data source for any CDC pipeline.&lt;/p&gt;

&lt;p&gt;To make this happen, Heimdall not only retains the metadata of all of the resources that it creates, but also regularly ingests additional information from a variety of upstream systems and platforms, to enrich and make this metadata comprehensive.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 6 Diagram showing some of Heimdall's upstreams (on the left) and downstreams (on the right) for metadata collection, enrichment, and serving. The arrows show the data flow. The network connection (client -&amp;gt; server) is actually the other way around.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
On the left side of Fig. 6, we illustrate Heimdall’s ingestion mechanism with several examples (step 1):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The metadata of all Coban resources is ingested from Khone. This means the metadata of the resources that were created directly in Khone is also available in Heimdall.&lt;/li&gt;
  &lt;li&gt;The list of Kafka producers is retrieved from our monitoring platform, where most of them emit metrics.&lt;/li&gt;
  &lt;li&gt;The list of Kafka consumers is retrieved directly from the respective Kafka clusters, by listing the &lt;a href=&quot;https://docs.confluent.io/platform/current/clients/consumer.html#consumer-groups&quot;&gt;consumer groups&lt;/a&gt; and respective &lt;a href=&quot;https://developer.confluent.io/faq/apache-kafka/kafka-clients/#kafka-clients-what-is-clientid-in-kafka&quot;&gt;Client IDs&lt;/a&gt; of each partition.&lt;/li&gt;
  &lt;li&gt;The metadata of all DBs, that are used as a data source for CDC pipelines, is fetched from Grab’s internal DB management platform.&lt;/li&gt;
  &lt;li&gt;The Kafka stream schemas are retrieved from the Coban schema repository.&lt;/li&gt;
  &lt;li&gt;The Kafka stream configuration of each stream is retrieved from Grab Universal Configuration Management platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all of this ingested data, Heimdall can provide comprehensive and accurate information about all data streaming resources to any other Grab platforms via a set of dedicated APIs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The right side of Fig. 6 shows some examples (step 2) of Heimdall’s serving mechanism:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As a downstream of Heimdall, the Coban UI enables our direct users to conveniently browse their data streaming resources and access their attributes.&lt;/li&gt;
  &lt;li&gt;The entire resource inventory is ingested into the broader Grab inventory platform, based on &lt;a href=&quot;https://backstage.io/&quot;&gt;backstage.io&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The Kafka streams are ingested into Grab’s internal data discovery platform, based on &lt;a href=&quot;https://datahubproject.io/&quot;&gt;DataHub&lt;/a&gt;, where users can discover and trace the lineage of any piece of data.&lt;/li&gt;
  &lt;li&gt;The CDC connectors pertaining to DBs are ingested by Grab internal DB management platform, so that they are made visible in that platform when users are browsing their DBs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the downstream platforms that ingest data from Heimdall each expose a particular view of the Coban inventory that serves their purpose, but the Coban platform remains the only source of truth for any data streaming resource at Grab.&lt;/p&gt;

&lt;p&gt;Lastly, Heimdall leverages an internal MySQL DB to support quick data query and exploration. The corresponding API is called by the Coban UI to let our users conveniently search globally among all resources’ attributes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 7 Screen capture of the global search feature in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;khone&quot;&gt;Khone&lt;/h3&gt;

&lt;p&gt;Khone is the persistent storage layer of our platform, as well as the executor for actual resource creation, changes, and deletion. Under the hood, it is actually a GitLab repository of Terraform code in typical &lt;a href=&quot;https://about.gitlab.com/topics/gitops/&quot;&gt;GitOps&lt;/a&gt; fashion, with CI pipelines to plan and apply the Terraform changes automatically. In addition, it also stores a metadata file for each resource.&lt;/p&gt;

&lt;p&gt;Compared to letting the platform create the infrastructure directly and keep track of the desired state in its own way, relying on a standard IaC tool like Terraform for the actual changes to the infrastructure presents two major advantages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The Terraform code can directly be used for disaster recovery. In case of a disaster, any entitled Cobaner with a local copy of the main branch of the Khone repository is able to recreate all our platform resources directly from their machine. There is no need to rebuild the entire platform’s control plane, thus reducing our Recovery Time Objective (RTO).&lt;/li&gt;
  &lt;li&gt;Minimal effort required to follow the API changes of our infrastructure ecosystem (AWS, Kubernetes, Kafka, etc.). When such a change happens, all we need to do is to update the corresponding Terraform provider.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’d like to read more about Khone, check out &lt;a href=&quot;/securing-gitops-pipeline&quot;&gt;Securing GitOps pipelines&lt;/a&gt;. In this section, we will only focus on Khone’s features that are relevant from the platform perspective.&lt;/p&gt;

&lt;h4 id=&quot;lightweightterraform&quot;&gt;Lightweight Terraform&lt;/h4&gt;

&lt;p&gt;In Khone, each resource is stored as a Terraform definition. There are two major differences from a normal Terraform project:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No Terraform environment, such as the required Terraform providers and the location of the remote Terraform state file. They are automatically generated by the CI pipeline via a simple wrapper.&lt;/li&gt;
  &lt;li&gt;Only vetted Khone Terraform modules can be used. This is controlled and enforced by the CI pipeline via code inspection. There is one such Terraform module for each kind of supported resource of our platform (e.g. Kafka topic, Flink pipeline, Kafka Connect mirror source connector etc.). Furthermore, those in-house Terraform modules are designed to automatically derive their key variables (e.g. resource name, cluster name, environment) from the relative path of the parent Terraform project in the Khone repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those characteristics are designed to limit the risk and blast radius of human errors. They also make sure that all resources created in Khone are supported by our platform, so that they can also be discovered and managed in Heimdall and the Coban UI. Lastly, by generating the Terraform environment on the fly, we can destroy resources simply by deleting the directory of the project in the code base – this would not be possible otherwise.&lt;/p&gt;

&lt;h4 id=&quot;resource-metadata&quot;&gt;Resource metadata&lt;/h4&gt;

&lt;p&gt;All resource metadata is stored in a YAML file that is present in the Terraform directory of each resource in the Khone repository. This is mainly used for ownership and cost attribution.&lt;/p&gt;

&lt;p&gt;With this metadata, we can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Better communicate with our users whenever their resources are impacted by an incident or an upcoming maintenance operation.&lt;/li&gt;
  &lt;li&gt;Help teams understand the costs of their usage of our platform, a significant step towards cost efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two different ways resource metadata can be created:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Automatically through Heimdall: The YAML metadata file is automatically generated by Heimdall.&lt;/li&gt;
  &lt;li&gt;Through Khone by a human user: The user needs to prepare the YAML metadata file and include it in the MR. This file is then verified by the CI pipeline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outcome&quot;&gt;Outcome&lt;/h2&gt;

&lt;p&gt;The initial version of the three-tier Coban platform, as described in this article, was internally released in March 2022, supporting only Kafka topic management at the time. Since then, we have added support for Flink pipelines, four kinds of Kafka Connect connectors, CDC pipelines, and more recently, Apache Zeppelin notebooks. At the time of writing, the Coban platform manages about 5000 data streaming resources, all described as IaC under the hood.&lt;/p&gt;

&lt;p&gt;Our platform also exposes enriched metadata that includes the full data lineage from Kafka producers to Kafka consumers, as well as ownership information, and cost attribution.&lt;/p&gt;

&lt;p&gt;With that, our monthly active users have almost quadrupled, truly moving the needle towards democratising the usage of real-time data within all Grab verticals.&lt;/p&gt;

&lt;p&gt;In spite of that user growth, the end-to-end workflow success rate for self-served resource creation, change or deletion, remained well above 90% in the first half of 2023, while the Heimdall API uptime was above 99.95%.&lt;/p&gt;

&lt;h2 id=&quot;challenges-faced&quot;&gt;Challenges faced&lt;/h2&gt;

&lt;p&gt;A common challenge for platform teams resides in the misalignment between the Service Level Objective (SLO) of the platform, and the various environments (e.g. staging, production) of the managed resources and upstream/downstream systems and platforms.&lt;/p&gt;

&lt;p&gt;Indeed, the platform aims to guarantee the same level of service, regardless of whether it is used to create resources in the staging or the production environment. From the platform team’s perspective, the platform as a whole is considered production-grade, as soon as it serves actual users.&lt;/p&gt;

&lt;p&gt;A naive approach to address this challenge is to let the production version of the platform manage all resources regardless of their respective environments. However, doing so does not permit a hermetic segregation of the staging and production environments across the organisation, which is a good security practice, and often a requirement for compliance. For example, the production version of the platform would have to connect to upstream systems in the staging environment, e.g. staging Kafka clusters to collect their consumer groups, in the case of Heimdall. Conversely, the staging version of certain downstreams would have to connect to the production version of Heimdall, to fetch the metadata of relevant staging resources.&lt;/p&gt;

&lt;p&gt;The alternative approach, generally adopted across Grab, is to instantiate all platforms in each environment (staging and production), while still considering both instances as production-grade and guaranteeing tight SLOs in both environments.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 8 Architecture of the Coban platform, broken down by environment.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Fig. 8, both instances of Heimdall have equivalent SLOs. The caveat is that all upstream systems and platforms must also guarantee a strict SLO in both environments. This obviously comes with a cost, for example, tighter maintenance windows for the operations pertaining to the Kafka clusters in the staging environment.&lt;/p&gt;

&lt;p&gt;A strong “platform” culture is required for platform teams to fully understand that their instance residing in the staging environment is not their own staging environment and should not be used for testing new features.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Currently, users creating, updating, or deleting production resources in the Coban UI (or directly by calling Heimdall API) receive the URL of the generated GitLab MR in a Slack message. From there, they must get the MR approved by a code owner, typically another team member, and finally merge the MR, for the requested change to be actually implemented by the CI pipeline.&lt;/p&gt;

&lt;p&gt;Although this was a fairly easy way to implement a maker/checker process that was immediately compliant with our regulatory requirements for any changes in production, the user experience is not optimal. In the near future, we plan to bring the approval mechanism into Heimdall and the Coban UI, while still providing our more advanced users with the option to directly create, approve, and merge MRs in GitLab. In the longer run, we would also like to enhance the Coban UI with the output of the Khone CI jobs that include the Terraform plan and apply results.&lt;/p&gt;

&lt;p&gt;There is another aspect of the platform that we want to improve. As Heimdall regularly polls the upstream platforms to collect their metadata, this introduces a latency between a change in one of those platforms and its reflection in the Coban platform, which can hinder the user experience. To refresh resource metadata in Heimdall in near real time, we plan to leverage an existing Grab-wide event stream, where most of the configuration and code changes at Grab are produced as events. Heimdall will soon be able to consume those events and update the metadata of the affected resources immediately, without waiting for the next periodic refresh.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/an-elegant-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/an-elegant-platform</guid>
        
        <category>Data</category>
        
        <category>Data streaming</category>
        
        <category>Real-time streaming</category>
        
        <category>Platformisation</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Road localisation in GrabMaps</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2022, Grab achieved self-sufficiency in its Geo services. As part of this transition, one crucial step was moving towards using an internally-developed map tailored specifically to the market in which Grab operates. Now that we have full control over the map layer, we can add more data to it or improve it according to the needs of the services running on top. One key aspect that this transition unlocked for us was the possibility of creating hyperlocal data at map level.&lt;/p&gt;

&lt;p&gt;For instance, by determining the country to which a road belongs, we can now automatically infer the official language of that country and display the street name in that language. In another example, knowing the country for a specific road, we can automatically infer the driving side (left-handed or right-handed) leading to an improved navigation experience. Furthermore, this capability also enables us to efficiently handle various scenarios. For example, if we know that a road is part of a gated community, an area where our driver partners face restricted access, we can prevent the transit through that area.&lt;/p&gt;

&lt;p&gt;These are just some examples of the possibilities from having full control over the map layer. By having an internal map, we can align our maps with specific markets and provide better experiences for our driver-partners and customers.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;For all these to be possible, we first needed to localise the roads inside the map. Our goal was to include hyperlocal data into the map, which refers to data that is specific to a certain area, such as a country, city, or even a smaller part of the city like a gated community. At the same time, we aimed to deliver our map with a high cadence, thus, we needed to find the right way to process this large amount of data while continuing to create maps in a cost-effective manner.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;In the following sections of this article, we will use an extract from the Southeast Asia map to provide visual representations of the concepts discussed.&lt;/p&gt;

&lt;p&gt;In Figure 1, Image 1 shows a visualisation of the road network, the roads belonging to this area. The coloured lines in Image 2 represent the borders identifying the countries in the same area. Overlapping the information from Image 1 and Image 2, we can extrapolate and say that the entire surface included in a certain border could have the same set of common properties as shown in Image 3. In Image 4, we then proceed with adding localised roads for each area.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/localisation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Map of Southeast Asia&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For this to be possible, we have to find a way to localise each road and identify its associated country. Once this localisation process is complete, we can replicate all this information specific to a given border onto each individual road. This information includes details such as the country name, driving side, and official language. We can go even further and infer more information, and add hyperlocal data. For example, in Vietnam, we can automatically prevent motorcycle access on the motorways.&lt;/p&gt;

&lt;p&gt;Assigning each road on the map to a specific area, such as a country, service area, or subdivision, presents a complex task. So, how can we efficiently accomplish this?&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;The most straightforward approach would be to test the inclusion of each road into each area boundary, but that is easier said than done. With close to 30 million road segments in the Southeast Asia map and over 10 thousand areas, the computational cost of determining inclusion or intersection between a polyline and a polygon is expensive.&lt;/p&gt;

&lt;p&gt;Our solution to this challenge involves replacing the expensive yet precise operation with a decent approximation. We introduce a proxy entity, the geohash, and we use it to approximate the areas and also to localise the roads.&lt;/p&gt;

&lt;p&gt;We replace the geometrical inclusion with a series of simpler and less expensive operations. First, we conduct an inexpensive precomputation where we identify all the geohases that belong to a certain area or within a defined border. We then identify the geohashes to which the roads  belong to. Finally, we use these precomputed values to assign roads to their respective areas. This process is also computationally inexpensive.&lt;/p&gt;

&lt;p&gt;Given the large area we process, we leverage big data techniques to distribute the execution across multiple nodes and thus speed up the operation. We want to deliver the map daily and this is one of the many operations that are part of the map-making process.&lt;/p&gt;

&lt;h3 id=&quot;what-is-a-geohash&quot;&gt;What is a geohash?&lt;/h3&gt;

&lt;p&gt;To further understand our implementation we will first explain the &lt;a href=&quot;https://en.wikipedia.org/wiki/Geohash&quot;&gt;geohash concept&lt;/a&gt;. A geohash is a unique identifier of a specific region on the Earth. The basic idea is that the Earth is divided into regions of user-defined size and each region is assigned a unique id, which is known as its geohash. For a given location on earth, the geohash algorithm converts its latitude and longitude into a string.&lt;/p&gt;

&lt;p&gt;Geohashes uses a Base-32 alphabet encoding system comprising characters ranging from  0 to 9 and A to Z, excluding “A”, “I”, “L” and “O”. Imagine dividing the world into a grid with 32 cells. The first character in a geohash identifies the initial location of one of these 32 cells. Each of these cells are then further subdivided into 32 smaller cells.This subdivision process continues and refines to specific areas in the world. Adding characters to the geohash sub-divides a cell, effectively zooming in to a more detailed area.&lt;/p&gt;

&lt;p&gt;The precision factor of the geohash determines the size of the cell. For instance, a precision factor of one creates a cell 5,000 km high and 5,000 km wide. A precision factor of six creates a cell 0.61km high and 1.22 km wide. Furthermore, a precision factor of nine creates a cell 4.77 m high and 4.77 m wide. It is important to note that cells are not always square and can have varying dimensions.&lt;/p&gt;

&lt;p&gt;In Figure 2,  we have exemplified a geohash 6 grid and its code is &lt;strong&gt;wsdt33&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-code-wsdt33.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - An example of geohash code wsdt33&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;using-less-expensive-operations&quot;&gt;Using less expensive operations&lt;/h3&gt;

&lt;p&gt;Calculating the inclusion of the roads inside a certain border is an expensive operation. However, quantifying the exact expense is challenging as it depends on several factors. One factor is the complexity of the border. Borders are usually irregular and very detailed, as they need to correctly reflect the actual border. The complexity of the road geometry is another factor that plays an important role as roads are not always straight lines.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/roads-to-localise.png&quot; alt=&quot;&quot; style=&quot;width:30%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Roads to localise&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Since this operation is expensive both in terms of cloud cost and time to run, we need to identify a cheaper and faster way that would yield similar results. Knowing that the complexity of the border lines is the cause of the problem, we tried using a different alternative, a rectangle. Calculating the inclusion of a polyline inside a rectangle is a cheaper operation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/roads-inside-rectangle.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Roads inside a rectangle&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;So we transformed this large, one step operation, where we test each road segment for inclusion in a border, into a series of smaller operations where we perform the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify all the geohashes that are part of a certain area or belong to a certain border. In this process we include additional areas to make sure that we cover the entire surface inside the border.&lt;/li&gt;
  &lt;li&gt;For each road segment, we identify the list of geohashes that it belongs to. A road, depending on its length or depending on its shape, might belong to multiple geohashes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In Figure 5, we identify that the road belongs to two geohashes and that the two geohashes are part of the border we use.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-proxy.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Geohashes as proxy&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Now, all we need to do is join the two data sets together. This kind of operation is a great candidate for a big data approach, as it allows us to run it in parallel and speed up the processing time.&lt;/p&gt;

&lt;h2 id=&quot;precision-tradeoff&quot;&gt;Precision tradeoff&lt;/h2&gt;

&lt;p&gt;We mentioned earlier that, for the sake of argument, we replace precision with a decent approximation. Let’s now delve into the real tradeoff by adopting this approach.&lt;/p&gt;

&lt;p&gt;The first thing that stands out with this approach is that we traded precision for cost. We are able to reduce the cost as this approach uses less hardware resources and computation time. However, this reduction in precision suffers, particularly for roads located near the borders as they might be wrongly classified.&lt;/p&gt;

&lt;p&gt;Going back to the initial example, let’s take the case of the external road, on the left side of the area. As you can see in Figure 6, it is clear that the road does not belong to our border. But when we apply the geohash approach it gets included into the middle geohash.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/wrong-road-localisation.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Wrong road localisation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Given that just a small part of the geohash falls inside the border, the entire geohash will be classified as belonging to that area, and, as a consequence, the road that belongs to that geohash will be wrongly localised and we’ll end up adding the wrong localisation information to that road. This is clearly a consequence of the precision tradeoff. So, how can we solve this?&lt;/p&gt;

&lt;h3 id=&quot;geohash-precision&quot;&gt;Geohash precision&lt;/h3&gt;

&lt;p&gt;One option is to increase the geohash precision. By using smaller and smaller geohashes, we can better reflect the actual area. As we go deeper and we further split the geohash, we can accurately follow the border. However, a high geohash precision also equates to a computationally intensive operation bringing us back to our initial situation. Therefore, it is crucial to find the right balance between the geohash size and the complexity of operations.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-precision.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Geohash precision&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;geohash-coverage-percentage&quot;&gt;Geohash coverage percentage&lt;/h3&gt;

&lt;p&gt;To find a balance between precision and data loss, we looked into calculating the geohash coverage percentage. For example, in Figure 8, the blue geohash is entirely within the border.  Here we can say that it has a 100% geohash coverage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-inside-border.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8 - Geohash inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However, take for example the geohash in Figure 9. It touches the border and has only around 80% of its surface inside the area. Given that most of its surface is within the border, we still can say that it belongs to the area.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-partial-border.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9 - Geohash partially inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Let’s look at another example. In Figure 10, only a small part of the geohash is within the border. We can say that the geohash coverage percentage here is around 5%. For these cases, it becomes difficult for us to determine whether the geohash does belong to the area. What would be a good tradeoff in this case?&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-barely-border.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10 - Geohash barely inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;border-shape&quot;&gt;Border shape&lt;/h3&gt;

&lt;p&gt;To go one step further, we can consider a mixed solution, where we use the border shape but only for the geohashes touching the border. This would still be an intensive computational operation but the number of roads located in these geohashes will be much smaller, so it is still a gain.&lt;/p&gt;

&lt;p&gt;For the geohashes with full coverage inside the area, we’ll use the geohash for the localisation, the simpler operation. For the geohashes that are near the border, we’ll use a different approach. To increase the precision around the borders, we can cut the geohash following the  border’s shape. Instead of having a rectangle, we’ll use a more complex shape which is still simpler than the initial border shape.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-border-shape.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 11 - Geohash following a border’s shape&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;We began with a simple approach and we enhanced it to improve precision. This also increased the complexity of the operation. We then asked, what are the actual gains? Was it worthwhile to go through all this process? In this section, we put this to the test.&lt;/p&gt;

&lt;p&gt;We first created a benchmark by taking a small sample of the data and ran the localisation process on a laptop. The sample comprised approximately 2% of the borders and 0.0014% of the roads. We ran the localisation process using two approaches.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;With the first approach, we calculated the intersection between all the roads and borders. The entire operation took around 38 minutes.&lt;/li&gt;
  &lt;li&gt;For the second approach, we optimised the operation using geohashes. In this approach, the runtime was only 78 seconds (1.3 minutes).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, it is important to note that this is not an apples-to-apples comparison. The operation that we measured was the localisation of the roads but we did not include the border filling operation where we fill the borders with geohashes. This is because this operation does not need to be run every time. It can be run once and reused multiple times.&lt;/p&gt;

&lt;p&gt;Though not often required, it is still crucial to understand and consider the operation of precomputing areas and filling borders with geohashes. The precomputation process depends on several factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number and shape of the borders - The more borders and the more complex the borders are, the longer the operation will take.&lt;/li&gt;
  &lt;li&gt;Geohash precision - How accurate do we need our localisation to be? The more accurate it needs to be, the longer it will take.&lt;/li&gt;
  &lt;li&gt;Hardware availability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Going back to our hypothesis, although this precomputation might be expensive, it is rarely run as the borders don’t change often and can be triggered only when needed. However, regular computation, where we find the area to which each road belongs to, is often run as the roads change constantly. In our system, we run this localisation for each map processing.&lt;/p&gt;

&lt;p&gt;We can also further optimise this process by applying the opposite approach. Geohashes that have full coverage inside a border can be merged together into larger geohashes thus simplifying the computation inside the border. In the end, we can have a solution that is fully optimised for our needs with the best cost-to-performance ratio.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/optimised-geohash.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 12 - Optimised geohashes&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Although geohashes seem to be the right solution for this kind of problem, we also need to monitor their content. One consideration is the road density inside a geohash. For example, a geohash inside a city centre usually has a lot of roads while one in the countryside may have much less. We need to consider this aspect to have a balanced computation operation and take full advantage of the big data approach. In our case, we achieve this balance by considering the number of road kilometres within a geohash.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/unbalanced-data.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 13 - Unbalanced data&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, the resources that we choose also matter. To optimise time and cost, we need to find the right balance between the running time and resource cost. As shown in Figure 14, based on a sample data we ran, sometimes, we get the best result when using smaller machines.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/cost-vs-runtime.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 14 - Cost vs runtime&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;The achievements and insights showcased in this article are indebted to the contributions made by Mihai Chintoanu. His expertise and collaborative efforts have profoundly enriched the content and findings presented herein.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Fri, 17 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/road-localisation-grabmaps</link>
        <guid isPermaLink="true">https://engineering.grab.com/road-localisation-grabmaps</guid>
        
        <category>Maps</category>
        
        <category>Data</category>
        
        <category>Big Data</category>
        
        <category>Data processing</category>
        
        <category>Hyperlocalisation</category>
        
        <category>GrabMaps</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Graph modelling guidelines</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Graph modelling is a highly effective technique for representing and analysing complex and interconnected data across various domains. By deciphering relationships between entities, graph modelling can reveal insights that might be otherwise difficult to identify using traditional data modelling approaches. In this article, we will explore what graph modelling is and guide you through a step-by-step process of implementing graph modelling to create a social network graph.&lt;/p&gt;

&lt;h2 id=&quot;what-is-graph-modelling&quot;&gt;What is graph modelling?&lt;/h2&gt;

&lt;p&gt;Graph modelling is a method for representing real-world entities and their relationships using nodes, edges, and properties. It employs graph theory, a branch of mathematics that studies graphs, to visualise and analyse the structure and patterns within complex datasets. Common applications of graph modelling include social network analysis, recommendation systems, and biological networks.&lt;/p&gt;

&lt;h2 id=&quot;graph-modelling-process&quot;&gt;Graph modelling process&lt;/h2&gt;

&lt;h3 id=&quot;step-1-define-your-domain&quot;&gt;Step 1: Define your domain&lt;/h3&gt;
&lt;p&gt;Before diving into graph modelling, it’s crucial to have a clear understanding of the domain you’re working with. This involves getting acquainted with the relevant terms, concepts, and relationships that exist in your specific field. To create a social network graph, familiarise yourself with terms like users, friendships, posts, likes, and comments.&lt;/p&gt;

&lt;h3 id=&quot;step-2-identify-entities-and-relationships&quot;&gt;Step 2: Identify entities and relationships&lt;/h3&gt;
&lt;p&gt;After defining your domain, you need to determine the entities (nodes) and relationships (edges) that exist within it. Entities are the primary objects in your domain, while relationships represent how these entities interact with each other. In a social network graph, users are entities, and friendships are relationships.&lt;/p&gt;

&lt;h3 id=&quot;step-3-establish-properties&quot;&gt;Step 3: Establish properties&lt;/h3&gt;
&lt;p&gt;Each entity and relationship may have a set of properties that provide additional information. In this step, identify relevant properties based on their significance to the domain. A user entity might have properties like name, age, and location. A friendship relationship could have a ‘since’ property to denote the establishment of the friendship.&lt;/p&gt;

&lt;h3 id=&quot;step-4-choose-a-graph-model&quot;&gt;Step 4: Choose a graph model&lt;/h3&gt;
&lt;p&gt;Once you’ve identified the entities, relationships, and properties, it’s time to choose a suitable graph model. Two common models are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Property graph&lt;/strong&gt;: A versatile model that easily accommodates properties on both nodes and edges. It’s well-suited for most applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Description Framework (RDF)&lt;/strong&gt;: A World Wide Web Consortium (W3C) standard model, using triples of subject-predicate-object to represent data. It is commonly used in semantic web applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a social network graph, a property graph model is typically suitable. This is because user entities have many attributes and features. Property graphs provide a clear representation of the relationships between people and their attribute profiles.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-modelling-guidelines/graph.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Social network graph&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;step-5-develop-a-schema&quot;&gt;Step 5: Develop a schema&lt;/h3&gt;
&lt;p&gt;Although not required, developing a schema can be helpful for large-scale projects and team collaborations. A schema defines the structure of your graph, including entity types, relationships, and properties. In a social network graph, you might have a schema that specifies the types of nodes (users, posts) and the relationships between them (friendships, likes, comments).&lt;/p&gt;

&lt;h3 id=&quot;step-6-import-or-generate-data&quot;&gt;Step 6: Import or generate data&lt;/h3&gt;
&lt;p&gt;Next, acquire the data needed to populate your graph. This can come in the form of existing datasets or generated data from your application. For a social network graph, you can import user information from a CSV file and generate simulated friendships, posts, likes, and comments.&lt;/p&gt;

&lt;h3 id=&quot;step-7-implement-the-graph-using-a-graph-database-or-other-storage-options&quot;&gt;Step 7: Implement the graph using a graph database or other storage options&lt;/h3&gt;
&lt;p&gt;Finally, you need to store your graph data using a suitable graph database. Neo4j, Amazon Neptune, or Microsoft Azure Cosmos DB are examples of graph databases. Alternatively, depending on your specific requirements, you can use a non-graph database or an in-memory data structure to store the graph.&lt;/p&gt;

&lt;h3 id=&quot;step-8-analyse-and-visualise-the-graph&quot;&gt;Step 8: Analyse and visualise the graph&lt;/h3&gt;
&lt;p&gt;After implementing the graph, you can perform various analyses using graph algorithms, such as shortest path, centrality, or community detection. In addition, visualising your graph can help you gain insights and facilitate communication with others.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By following these steps, you can effectively create and analyse graph models for your specific domain. Remember to adjust the steps according to your unique domain and requirements, and always ensure that confidential and sensitive data is properly protected.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://neo4j.com/developer/graph-database/&quot;&gt;What is a Graph Database?&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 08 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/graph-modelling-guidelines</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-modelling-guidelines</guid>
        
        <category>Graph technology</category>
        
        <category>Graphs</category>
        
        <category>Graph networks</category>
        
        <category>Security</category>
        
        <category>Data</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>LLM-powered data classification for data entities at scale</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we deal with PetaByte-level data and manage countless data entities ranging from database tables to Kafka message schemas. Understanding the data inside is crucial for us, as it not only streamlines the data access management to safeguard the data of our users, drivers and merchant-partners, but also improves the data discovery process for data analysts and scientists to easily find what they need.&lt;/p&gt;

&lt;p&gt;The Caspian team (Data Engineering team) collaborated closely with the Data Governance team on automating governance-related metadata generation. We started with Personal Identifiable Information (PII) detection and built an orchestration service using a third-party classification service. With the advent of the Large Language Model (LLM), new possibilities dawned for metadata generation and sensitive data identification at Grab. This prompted the inception of the project, which aimed to integrate LLM classification into our existing service. In this blog, we share insights into the transformation from what used to be a tedious and painstaking process to a highly efficient system, and how it has empowered the teams across the organisation.&lt;/p&gt;

&lt;p&gt;For ease of reference, here’s a list of terms we’ve used and their definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Entity&lt;/strong&gt;: An entity representing a schema that contains rows/streams of data, for example, database tables, stream messages, data lake tables.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Refers to the model’s output given a data entity, unverified manually.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Classification&lt;/strong&gt;: The process of classifying a given data entity, which in the context of this blog, involves generating tags that represent sensitive data or Grab-specific types of data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metadata Generation&lt;/strong&gt;: The process of generating the metadata for a given data entity. In this blog, since we limit the metadata to the form of tags, we often use this term and data classification interchangeably.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;: Refers to the level of confidentiality of data. High sensitivity means that the data is highly confidential. The lowest level of sensitivity often refers to public-facing or publicly-available data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;When we first approached the data classification problem, we aimed to solve something more specific - Personal Identifiable Information (PII) detection. Initially, to protect sensitive data from accidental leaks or misuse, Grab implemented manual processes and campaigns targeting data producers to tag schemas with sensitivity tiers. These tiers ranged from Tier 1, representing schemas with highly sensitive information, to Tier 4, indicating no sensitive information at all. As a result, half of all schemas were marked as Tier 1, enforcing the strictest access control measures.&lt;/p&gt;

&lt;p&gt;The presence of a single Tier 1 table in a schema with hundreds of tables justifies classifying the entire schema as Tier 1. However, since Tier 1 data is rare, this implies that a large volume of non-Tier 1 tables, which ideally should be more accessible, have strict access controls.&lt;/p&gt;

&lt;p&gt;Shifting access controls from the schema-level to the table-level could not be done safely due to the lack of table classification in the data lake. We could have conducted more manual classification campaigns for tables, however this was not feasible for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The volume, velocity, and variety of data had skyrocketed within the organisation, so it took significantly more time to classify at table level compared to schema level. Hence, a programmatic solution was needed to streamline the classification process, reducing the need for manual effort.&lt;/li&gt;
  &lt;li&gt;App developers, despite being familiar with the business scope of their data, interpreted internal data classification policies and external data regulations differently, leading to inconsistencies in understanding.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A service called Gemini &lt;em&gt;(named before Google announced the Gemini model!)&lt;/em&gt; was built internally to automate the tag generation process using a third party data classification service. Its purpose was to scan the data entities in batches and generate column/field level tags. These tags would then go through a review process by the data producers. The data governance team provided classification rules and used regex classifiers, alongside the third-party tool’s own machine learning classifiers, to discover sensitive information.&lt;/p&gt;

&lt;p&gt;After the implementation of the initial version of Gemini, a few challenges remained.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The third-party tool did not allow customisations of its machine learning classifiers, and the regex patterns produced too many false positives during our evaluation.&lt;/li&gt;
  &lt;li&gt;Building in-house classifiers would require a dedicated data science team to train a customised model. They would need to invest a significant amount of time to understand data governance rules thoroughly and prepare datasets with manually labelled training data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;LLM came up on our radar following its recent &lt;em&gt;“iPhone moment”&lt;/em&gt; with ChatGPT’s explosion onto the scene. It is trained using an extremely large corpus of text and contains trillions of parameters. It is capable of conducting natural language understanding tasks, writing code, and even analysing data based on requirements. The LLM naturally solves the mentioned pain points as it provides a natural language interface for data governance personnel. They can express governance requirements through text prompts, and the LLM can be customised effortlessly without code or model training.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;In this section, we dive into the implementation details of the data classification workflow. Please refer to the diagram below for a high-level overview:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/data_classification_workflow.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Overview of data classification workflow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This diagram illustrates how data platforms, the metadata generation service (Gemini), and data owners work together to classify and verify metadata. Data platforms trigger scan requests to the Gemini service to initiate the tag classification process. After the tags are predicted, data platforms consume the predictions, and the data owners are notified to verify these tags.&lt;/p&gt;

&lt;h3 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/arch_diagram_orchestration.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Architecture diagram of the orchestration service Gemini&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our orchestration service, Gemini, manages the data classification requests from data platforms. From the diagram, the architecture contains the following components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data platforms: These platforms are responsible for managing data entities and initiating data classification requests.&lt;/li&gt;
  &lt;li&gt;Gemini: This orchestration service communicates with data platforms, schedules and groups data classification requests.&lt;/li&gt;
  &lt;li&gt;Classification engines: There are two available engines (a third-party classification service and GPT3.5) for executing the classification jobs and return results. Since we are still in the process of evaluating two engines, both of the engines are working concurrently.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the orchestration service receives requests, it helps aggregate the requests into reasonable mini-batches. Aggregation is achievable through the message queue at fixed intervals. In addition, a rate limiter is attached at the workflow level. It allows the service to call the Cloud Provider APIs with respective rates to prevent the potential throttling from the service providers.&lt;/p&gt;

&lt;p&gt;Specific to LLM orchestration, there are two limits to be mindful of. The first one is the context length. The input length cannot surpass the context length, which was 4000 tokens for GPT3.5 at the time of development (or around 3000 words). The second one is the overall token limit (since both the input and output share the same token limit for a single request). Currently, all Azure OpenAI model deployments share the same quota under one account, which is set at 240K tokens per minute.&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section, we focus on LLM-powered column-level tag classification. The tag classification process is defined as follows:&lt;/p&gt;

&lt;p&gt;Given a data entity with a defined schema, we want to tag each field of the schema with metadata classifications that follow an internal classification scheme from the data governance team. For example, the field can be tagged as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular kind of business metric&amp;gt;&lt;/code&gt; or a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular type of personally identifiable information (PII)&lt;/code&gt;. These tags indicate that the field contains a business metric or PII.&lt;/p&gt;

&lt;p&gt;We ask the language model to be a column tag generator and to assign the most appropriate tag to each column. Here we showcase an excerpt of the prompt we use:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;You are a database column tag classifier, your job is to assign the most appropriate tag based on table name and column name. The database columns are from a company that provides ride-hailing, delivery, and financial services. Assign one tag per column. However not all columns can be tagged and these columns should be assigned &amp;lt;None&amp;gt;. You are precise, careful and do your best to make sure the tag assigned is the most appropriate.

The following is the list of tags to be assigned to a column. For each line, left hand side of the : is the tag and right hand side is the tag definition

…
&amp;lt;Personal.ID&amp;gt; : refers to government-provided identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar. This tag should absolutely not be assigned to columns named &quot;id&quot;, &quot;merchant id&quot;, &quot;passenger id&quot;, “driver id&quot; or similar since these are not government-provided identification numbers. This tag should be very rarely assigned.

&amp;lt;None&amp;gt; : should be used when none of the above can be assigned to a column.
…

Output Format is a valid json string, for example:

[{
        &quot;column_name&quot;: &quot;&quot;,
        &quot;assigned_tag&quot;: &quot;&quot;
}]

Example question

`These columns belong to the &quot;deliveries&quot; table

        1. merchant_id
        2. status
        3. delivery_time`

Example response

[{
        &quot;column_name&quot;: &quot;merchant_id&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;Personal.ID&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;status&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;delivery_time&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also curated a tag library for LLM to classify. Here is an example:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column-level Tag&lt;/th&gt;
      &lt;th&gt;Definition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.ID&lt;/td&gt;
      &lt;td&gt;Refers to external identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Name &lt;/td&gt;
      &lt;td&gt;Refers to the name or username of a person and should be assigned to columns containing &quot;name&quot;, &quot;username&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Contact_Info&lt;/td&gt;
      &lt;td&gt;Refers to the contact information of a person and should be assigned to columns containing &quot;email&quot;, &quot;phone&quot;, &quot;address&quot;, &quot;social media&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Geo.Geohash&lt;/td&gt;
      &lt;td&gt;Refers to a geohash and should be assigned to columns containing &quot;geohash&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;Should be used when none of the above can be assigned to a column.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The output of the language model is typically in free text format, however, we want the output in a fixed format for downstream processing. Due to this nature, prompt engineering is a crucial component to make sure downstream workflows can process the LLM’s output.&lt;/p&gt;

&lt;p&gt;Here are some of the techniques we found useful during our development:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Articulate the requirements: The requirement of the task should be as clear as possible, LLM is only instructed to do what you ask it to do.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/en-gb/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#few-shot-learning&quot;&gt;Few-shot learning&lt;/a&gt;: By showing the example of interaction, models understand how they should respond.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/TypeChat&quot;&gt;Schema Enforcement&lt;/a&gt;: Leveraging its ability of understanding code, we explicitly provide the DTO (Data Transfer Object) schema to the model so that it understands that its output must conform to it.&lt;/li&gt;
  &lt;li&gt;Allow for confusion: In our prompt we specifically added a default tag – the LLM is instructed to output the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;None&amp;gt;&lt;/code&gt; tag when it cannot make a decision or is confused.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Regarding classification accuracy, we found that it is surprisingly accurate with its great semantic understanding. For acknowledged tables, users on average change less than one tag. Also, during an internal survey done among data owners at Grab in September 2023, 80% reported that this new tagging process helped them in tagging their data entities.&lt;/p&gt;

&lt;h3 id=&quot;publish-and-verification&quot;&gt;Publish and verification&lt;/h3&gt;

&lt;p&gt;The predictions are published to the Kafka queue to downstream data platforms. The platforms inform respective users weekly to verify the classified tags to improve the model’s correctness and to enable iterative prompt improvement. Meanwhile, we plan to remove the verification mandate for users once the accuracy reaches a certain level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/verification_message.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Verification message shown in the data platform for user to verify the tags&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since the new system was rolled out, we have successfully integrated this with Grab’s metadata management platform and production database management platform. Within a month since its rollout, we have scanned more than 20,000 data entities, averaging around 300-400 entities per day.&lt;/p&gt;

&lt;p&gt;Using a quick back-of-the-envelope calculation, we can see the significant time savings achieved through automated tagging. Assuming it takes a data owner approximately 2 minutes to classify each entity, we are saving approximately 360 man-days per year for the company. This allows our engineers and analysts to focus more on their core tasks of engineering and analysis rather than spending excessive time on data governance.&lt;/p&gt;

&lt;p&gt;The classified tags pave the way for more use cases downstream. These tags, in combination with rules provided by data privacy office in Grab, enable us to determine the sensitivity tier of data entities, which in turn will be leveraged for enforcing the Attribute-based Access Control (ABAC) policies and enforcing Dynamic Data Masking for downstream queries. To learn more about the benefits of ABAC, readers can refer to another engineering &lt;a href=&quot;https://engineering.grab.com/migrating-to-abac&quot;&gt;blog&lt;/a&gt; posted earlier.&lt;/p&gt;

&lt;p&gt;Cost wise, for the current load, it is extremely affordable contrary to common intuition. This affordability enables us to scale the solution to cover more data entities in the company.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;h3 id=&quot;prompt-improvement&quot;&gt;Prompt improvement&lt;/h3&gt;

&lt;p&gt;We are currently exploring feeding sample data and user feedback to greatly increase accuracy. Meanwhile, we are experimenting on outputting the confidence level from LLM for its own classification. With confidence level output, we would only trouble users when the LLM is uncertain of its answers. Hopefully this can remove even more manual processes in the current workflow.&lt;/p&gt;

&lt;h3 id=&quot;prompt-evaluation&quot;&gt;Prompt evaluation&lt;/h3&gt;

&lt;p&gt;To track the performance of the prompt given, we are building analytical pipelines to calculate the metrics of each version of the prompt. This will help the team better quantify the effectiveness of prompts and iterate better and faster.&lt;/p&gt;

&lt;h3 id=&quot;scaling-out&quot;&gt;Scaling out&lt;/h3&gt;

&lt;p&gt;We are also planning to scale out this solution to more data platforms to streamline governance-related metadata generation to more teams. The development of downstream applications using our metadata is also on the way. These exciting applications are from various domains such as security, data discovery, etc.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Oct 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-powered-data-classification</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-powered-data-classification</guid>
        
        <category>Data</category>
        
        <category>Machine Learning</category>
        
        <category>Generative AI</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Scaling marketing for merchants with targeted and intelligent promos</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A promotional campaign is a marketing effort that aims to increase sales, customer engagement, or brand awareness for a product, service, or company. The target is to have more orders and sales by assigning promos to consumers within a given budget during the campaign period.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/customer-feedback.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Merchant feedback on marketing&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;From our research, we found that merchants have specific goals for the promos they are willing to offer. They want a simple and cost-effective way to achieve their specific business goals by providing well-designed offers to target the correct customers. From Grab’s perspective, we want to help merchants set up and run campaigns efficiently, and help them achieve their specific business goals.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;
&lt;p&gt;One of Grab’s platform offerings for merchants is the ability to create promotional campaigns. With the emergence of AI technologies, we found that there are opportunities for us to further optimise the platform. The following are the gaps and opportunities we identified:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Globally assigned promos without smart targeting&lt;/strong&gt;: The earlier method targeted every customer, so everyone could redeem until the promo reached the redemption limits. However, this method did not accurately meet business goals or optimise promo spending. The promotional campaign should intelligently target the best promo for each customer to increase sales and better utilise promo spending.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No customised promos for every merchant&lt;/strong&gt;: To better optimise sales for each merchant, merchants should offer customised promos based on their historical consumer trends, not just a general offer set. For example, for a specific merchant, a 27% discount may be the appropriate offer to uplift revenue and sales based on user bookings. However, merchants do not always have the expertise to decide which offer to select to increase profit.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No AI-driven optimisation&lt;/strong&gt;: Without AI models, it was harder for merchants to assign the right promos at scale to each consumer and optimise their business goals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As shown in the following figure, AI-driven promotional campaigns are expected to bring higher sales with more promo spend than heuristic ones. Hence, at Grab we looked to introduce an automated, AI-driven tool that helps merchants intelligently target consumers with appropriate promos, while optimising sales and promo spending. That’s where Bullseye comes in.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/ai-campaign-graph.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Graph showing the sales expectations for AI-driven pomotional campaigns&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Bullseye is an automated, AI-driven promo assignment system that leverages the following capabilities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Automated user segmentation&lt;/strong&gt;: Enables merchants to target new, churned, and active users or all users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automatic promo design&lt;/strong&gt;: Enables a merchant-level promo design framework to customise promos for each merchant or merchant group according to their business goals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assign each user the optimal promo&lt;/strong&gt;: Users will receive promos selected from an array of available promos based on the merchant’s business objective.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Achieve different Grab and merchant objectives&lt;/strong&gt;: Examples of objectives are to increase merchant sales and decrease Grab promo spend.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Flexibility to optimise for an individual merchant brand or group of merchant brands&lt;/strong&gt;: For promotional campaigns, targeting and optimisation can be performed for a single or group of merchants (e.g. enabling GrabFood to run cuisine-oriented promo campaigns).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/architecture.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Bullseye architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The Bullseye architecture consists of a user interface (UI) and a backend service to handle requests. To use Bullseye, our operations team inputs merchant information into the Bullseye UI. The backend service will then interact with APIs to process the information using the AI model. As we work with a large customer population, data is stored in S3 and the API service triggering Chimera Spark job is used to run the prediction model and generate promo assignments. During the assignment, the Spark job parses the input parameters, pre-validates the input, makes some predictions, and then returns the promo assignment results to the backend service.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;The key components in Bullseye are shown in the following figure:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/implementation.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Key components of Bullseye&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Eater Segments Identifier&lt;/strong&gt;: Identifies each user as active, churned, or new based on their historical orders from target merchants.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Promo Designer&lt;/strong&gt;: We constructed a promo variation design framework to adaptively design promo variations for each campaign request as shown in the diagram below.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Offer Content Candidate Generation&lt;/strong&gt;: Generates variant settings of promos based on the promo usage history.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Campaign Impact Simulator&lt;/strong&gt;: Predicts business metrics such as revenue, sales, and cost based on the user and merchant profiles and offer features.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Optimal Promo Selection&lt;/strong&gt;: Selects the optimal offer based on the predicted impact and the given campaign objective. The optimal would be based on how you define optimal. For example, if the goal is to maximise merchant sales, the model selects the top candidate which can bring the highest revenue. Finally, with the promo selection, the service returns the promo set to be used in the target campaign.&lt;/p&gt;

        &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/promo-designer.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Optimal Promo Selection&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Customer Response Model&lt;/strong&gt;: Predicts customer responses such as order value, redemption, and take-up rate if assigning a specific promo. Bullseye captures various user attributes and compares it with an offer’s attributes. Examples of attributes are cuisine type, food spiciness, and discount amount. When there is a high similarity in the attributes, there is a higher probability that the user will take up the offer.&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/scaling-marketing-for-merchants/customer-response-model.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Customer Response Model&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Hyper-parameter Selection&lt;/strong&gt;: Optimises toward multiple business goals. Tuning of hyper-parameters allows the AI assignment model to learn how to meet success criteria such as cost per merchant sales (cpSales) uplift and sales uplift. The success criteria is the achieving of business goals. For example, the merchant wants the sales uplift after assigning promo, but cpSales uplift cannot be higher than 10%. With tuning, the optimiser can find optimal points to meet business goals and use AI models to search for better settings with high efficiency compared to manual specification. We need to constantly tune and iterate models and hyper-parameters to adapt to ever-evolving business goals and the local landscape.&lt;/p&gt;

    &lt;p&gt;As shown in the image below, AI assignments without hyper-parameter tuning (HPT) leads to a high cpSales uplift but low sales uplift (red dot). So the  hyper-parameters would help to fine-tune the assignment result to be in the optimal space such as the blue dot, which may have lower sales than the red dot but meet the success criteria.&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/scaling-marketing-for-merchants/hpt.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Graph showing the impact of using AI assignments with HPT&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;We started using Bullseye in 2021. From its use we found that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hyper-parameters tuning and auto promo design can increase sales and reduce promo spend for food campaigns.&lt;/li&gt;
  &lt;li&gt;Promo Designer optimises budget utilisation and increases the number of promo redemptions for food campaigns.&lt;/li&gt;
  &lt;li&gt;The Customer Response Model reduced promo spending for Mart promotional campaigns.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have seen positive results with the implementation of Bullseye such as reduced promo spending and maximised budget spending returns. In our efforts to serve our merchants better and help them achieve their business goals, we will continue to improve Bullseye. In the next phase, we plan to implement a more intelligent service, enabling reinforcement learning, and online assignment. We also aim to scale AI adoption by onboarding regional promotional campaigns as much as possible.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to William Wu, Rui Tan, Rahadyan Pramudita, Krishna Murthy, and Jiesin Chia for making this project a success.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Oct 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/scaling-marketing-for-merchants</link>
        <guid isPermaLink="true">https://engineering.grab.com/scaling-marketing-for-merchants</guid>
        
        <category>Data</category>
        
        <category>Advertising</category>
        
        <category>Scalability</category>
        
        <category>Data science</category>
        
        <category>Marketing</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Stepping up marketing for advertisers: Scalable lookalike audience</title>
        <description>&lt;p&gt;The advertising industry is constantly evolving, driven by advancements in technology and changes in consumer behaviour. One of the key challenges in this industry is reaching the right audience, reaching people who are most likely to be interested in your product or service. This is where the concept of a lookalike audience comes into play. By identifying and targeting individuals who share similar characteristics with an existing customer base, businesses can significantly improve the effectiveness of their advertising campaigns.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image3.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However, as the scale of Grab advertisements grows, there are several optimisations needed to maintain the efficacy of creating lookalike audiences such as high service level agreement (SLA), high cost of audience creation, and unstable data ingestion.&lt;/p&gt;

&lt;p&gt;The need for an even more efficient and scalable solution for creating lookalike audiences was the motivation behind the development of the scalable lookalike audience platform. By developing a high-performance in-memory lookalike audience retrieval service and embedding-based lookalike audience creation and updating pipelines, t​his improved platform builds on the existing system and provides an even more effective tool for advertisers to reach their target audience.&lt;/p&gt;

&lt;h2 id=&quot;constant-optimisation-for-greater-precision&quot;&gt;Constant optimisation for greater precision&lt;/h2&gt;

&lt;p&gt;In the dynamic world of digital advertising, the ability to quickly and efficiently reach the right audience is paramount and a key strategy is targeted advertising. As such, we have to constantly find ways to improve our current approach to creating lookalike audiences that impacts both advertisers and users. Some of the gaps we identified included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Long SLA&lt;/strong&gt; for audience creation. Earlier, the platform stored results on Segmentation Platform (SegP) and it took two working days to generate a lookalike audience list. This is because inserting a single audience into SegP took three times longer than generating the audience. Extended creation times impacted the effectiveness of advertising campaigns, as it limited the ability of advertisers to respond quickly to changing market dynamics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low scalability&lt;/strong&gt;. As the number of onboarded merchant-partners increased, the time and cost of generating lookalike audiences also increased proportionally. This limited the availability of lookalike audience generation for all advertisers, particularly those with large customer bases or rapidly changing audience profiles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low updating frequency&lt;/strong&gt; of lookalike audiences. With automated updates only occurring on a weekly basis, this increased the likelihood that audiences may become outdated and ineffective. This meant there was scope to further improve to help advertisers more effectively reach their campaign goals, by targeting individuals who fit the desired audience profile.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High cost of creation&lt;/strong&gt;. The cost of producing one segment can add up quickly for advertisers who need to generate multiple audiences. This could impact scalability for advertisers as they could hesitate to effectively use multiple lookalike audiences in their campaigns.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image4.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To efficiently identify the top N lookalike audiences for each Grab user from our pool of millions of users, we developed a solution that leverages user and audience representations in the form of embeddings. Embeddings are vector representations of data that utilise linear distances to capture structure from the original datasets. With embeddings, large sets of data are compressed and easily processed without affecting data integrity. This approach ensures high accuracy, low latency, and low cost in retrieving the most relevant audiences.&lt;/p&gt;

&lt;p&gt;Our solution takes into account the fact that representation drift varies among entities as data is added. For instance, merchant-partner embeddings are more stable than passenger embeddings. By acknowledging this reality, we optimised our process to minimise cost while maintaining a desirable level of accuracy. Furthermore, we believe that having a strong representation learning strategy in the early stages reduced the need for complex models in the following stages.&lt;/p&gt;

&lt;p&gt;Our solution comprises two main components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Real-time lookalike audience retrieving&lt;/strong&gt;: We developed an in-memory high-performance retrieving service that stores passenger embeddings, audience embeddings, and audience score thresholds. To further reduce cost, we designed a passenger embedding compression algorithm that reduces the memory needs of passenger embeddings by around 90%.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Embedding-based audience creation and updating&lt;/strong&gt;: The output of this part of the project is an online retrieving model that includes passenger embeddings, audience embeddings, and thresholds. To minimise costs, we leverage the passenger embeddings that are also utilised by other projects within Grab, beyond advertising, thus sharing the cost. The audience embeddings and thresholds are produced with a low-cost small neural network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In summary, our approach to creating scalable lookalike audiences is designed to be cost-effective, accurate, and efficient, leveraging the power of embeddings and smart computational strategies to deliver the best possible audiences for our advertisers.&lt;/p&gt;

&lt;h3 id=&quot;solution-architecture&quot;&gt;Solution architecture&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image2.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The advertiser creates a campaign with a custom audience, which triggers the audience creation process. During this process, the audience service stores the audience metadata provided by advertisers in a message queue.&lt;/li&gt;
  &lt;li&gt;A scheduled Data Science (DS) job then retrieves the pending audience metadata, creates the audience, and updates the TensorFlow Serving (TFS) model.&lt;/li&gt;
  &lt;li&gt;During the serving period, the Backend (BE) service calls the DS service to retrieve all audiences that include the target user. Ads that are targeting these audiences are then selected by the Click-Through Rate (CTR) model to be displayed to the user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;To ensure the efficiency of the lookalike audience retrieval model and minimise the costs associated with audience creation and serving, we’ve trained the user embedding model using billions of user actions. This extensive training allows us to employ straightforward methods for audience creation and serving, while still maintaining high levels of accuracy.&lt;/p&gt;

&lt;h4 id=&quot;creating-lookalike-audiences&quot;&gt;Creating lookalike audiences&lt;/h4&gt;

&lt;p&gt;The Audience Creation Job retrieves the audience metadata from the online audience service, pulls the passenger embeddings, and then averages these embeddings to generate the audience embedding.&lt;/p&gt;

&lt;p&gt;We use the cosine score of a user and the audience embedding to identify the audiences the user belongs to. Hence, it’s sufficient to store only the audience embedding and score threshold. Additionally, a global &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target-all-pax&lt;/code&gt; Audience list is stored to return these audiences for each online request.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image1.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;serving-lookalike-audiences&quot;&gt;Serving lookalike audiences&lt;/h4&gt;

&lt;p&gt;The online audience service is also tasked with returning all the audiences to which the current user belongs. This is achieved by utilising the cosine score of the user embedding and audience embeddings, and filtering out all audiences that surpass the audience thresholds.&lt;/p&gt;

&lt;p&gt;To adhere to latency requirements, we avoid querying any external feature stores like Redis and instead, store all the embeddings in memory. However, the embeddings of all users are approximately 20 GB, which could affect model loading. Therefore, we devised an embedding compression method based on hash tricks inspired by &lt;a href=&quot;https://brilliant.org/wiki/bloom-filter/#:~:text=A%20bloom%20filter%20is%20a,is%20added%20to%20the%20set&quot;&gt;Bloom Filter&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image5.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;We utilise hash functions to obtain the hash64 value of the paxID, which is then segmented into four 16-bit values. Each 16-bit value corresponds to a 16-dimensional embedding block, and the compressed embedding is the concatenation of these four 16-dimensional embeddings.&lt;/li&gt;
  &lt;li&gt;For each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;paxID&lt;/code&gt;, we have both the original user embedding and the compressed user embedding. The compressed user embeddings are learned by minimising the Mean Square Error loss.&lt;/li&gt;
  &lt;li&gt;We can balance the storage cost and the accuracy by altering the number of hash functions used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Users can see advertisements targeting a new audience within 15 mins after the advertiser creates a campaign.&lt;/li&gt;
  &lt;li&gt;This new system doubled the impressions and clicks, while also improving the CTR, conversion rate, and return on investment.&lt;/li&gt;
  &lt;li&gt;Costs for generating lookalike audiences decreased by 98%.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;To evaluate the effectiveness of our new scalable system besides addressing these issues, we conducted an A/B test to compare it with the earlier system. The results revealed that this new system effectively doubled the number of impressions and clicks while also enhancing the CTR, conversion rate, and return on investment.&lt;/p&gt;

&lt;p&gt;Over the years, we have amassed over billions of user actions, which have been instrumental in training the model and creating a comprehensive representation of user interests in the form of embeddings.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;While this scalable system has proved its effectiveness and demonstrated impressive results in CTR, conversion rate, and return on investment, there is always room for improvement.  &lt;/p&gt;

&lt;p&gt;In the next phase, we plan to explore more advanced algorithms, refine our feature engineering process, and conduct more extensive hyperparameter tuning. Additionally, we will continue to monitor the system’s performance and make necessary adjustments to ensure it remains robust and effective in serving our advertisers’ needs.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.05022.pdf&quot;&gt;Real-time Attention Based Look-alike Model for Recommender System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://brilliant.org/wiki/bloom-filter/#:~:text=A%20bloom%20filter%20is%20a,is%20added%20to%20the%20set&quot;&gt;Bloom Filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3383313.3418481&quot;&gt;Smart Targeting: A Relevance-driven and Configurable Targeting Framework for Advertising System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.05022.pdf&quot;&gt;GUIM - General User and Item Embedding with Mixture of Representation in E-commerce&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Sep 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/scalable-lookalike-audiences</link>
        <guid isPermaLink="true">https://engineering.grab.com/scalable-lookalike-audiences</guid>
        
        <category>Data</category>
        
        <category>Advertising</category>
        
        <category>Scalability</category>
        
        <category>Data science</category>
        
        <category>Marketing</category>
        
        <category>Lookalike audience</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Building hyperlocal GrabMaps</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Southeast Asia (SEA) is a dynamic market, very different from other parts of the world. When travelling on the road, you may experience fast-changing road restrictions, new roads appearing overnight, and high traffic congestion. To address these challenges, GrabMaps has adapted to the SEA market by leveraging big data solutions. One of the solutions is the integration of hyperlocal data in GrabMaps.&lt;/p&gt;

&lt;p&gt;Hyperlocal information is oriented around very small geographical communities and obtained from the local knowledge that our map team gathers. The map team is spread across SEA, enabling us to define clear specifications (e.g. legal speed limits), and validate that our solutions are viable.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/map-detection.gif&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Map showing detections from images and probe data, and hyperlocal data.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Hyperlocal inputs make our mapping data even more robust, adding to the details collected from our image and probe detection pipelines. Figure 1 shows how data from our detection pipeline is overlaid with hyperlocal data, and then mapped across the SEA region. If you are curious and would like to check out the data yourself, you can download it &lt;a href=&quot;https://dumps.improveosm.org/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;processing-hyperlocal-data&quot;&gt;Processing hyperlocal data&lt;/h2&gt;

&lt;p&gt;Now let’s go through the process of detecting hyperlocal data.&lt;/p&gt;

&lt;h3 id=&quot;download-data&quot;&gt;Download data&lt;/h3&gt;

&lt;p&gt;GrabMaps is based on &lt;a href=&quot;https://www.openstreetmap.org/&quot;&gt;OpenStreetMap&lt;/a&gt; (OSM). The first step in the process is to download the .pbf file for Asia from &lt;a href=&quot;https://www.geofabrik.de/&quot;&gt;geofabrick.de&lt;/a&gt;. This .pbf file contains all the data that is available on OSM, such as details of places, trees, and roads. Take for example a park, the .pbf file would contain data on the park name, wheelchair accessibility, and many more.&lt;/p&gt;

&lt;p&gt;For this article, we will focus on hyperlocal data related to the road network. For each road, you can obtain data such as the type of road (residential or motorway), direction of traffic (one-way or more), and road name.&lt;/p&gt;

&lt;h3 id=&quot;convert-data&quot;&gt;Convert data&lt;/h3&gt;
&lt;p&gt;To take advantage of big data computing, the next step in the process is to convert the .pbf file into Parquet format using a Parquetizer. This will convert the binary data in the .pbf file into a table format. Each road in SEA is now displayed as a row in a table as shown in Figure 2.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/data-parquet.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Road data in Parquet format.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;identify-hyperlocal-data&quot;&gt;Identify hyperlocal data&lt;/h3&gt;

&lt;p&gt;After the data is prepared, GrabMaps then identifies and inputs all of our hyperlocal data, and delivers a consolidated view to our downstream services. Our hyperlocal data is obtained from various sources, either by looking at geometry, or other attributes in OSM such as the direction of travel and speed limit. We also apply customised rules defined by our local map team, all in a fully automated manner. This enhances the map together with data obtained from our rides and deliveries GPS pings and from KartaView, Grab’s product for imagery collection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/architecture.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Architecture diagram showing how hyperlocal data is integrated into GrabMaps.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;benefit-of-our-hyperlocal-grabmaps&quot;&gt;Benefit of our hyperlocal GrabMaps&lt;/h2&gt;

&lt;p&gt;GrabNav, a turn-by-turn navigation tool available on the Grab driver app, is one of our products that benefits from having hyperlocal data. Here are some hyperlocal data that are made available through our approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Localisation of roads: The country, state/county, or city the road is in&lt;/li&gt;
  &lt;li&gt;Language spoken, driving side, and speed limit&lt;/li&gt;
  &lt;li&gt;Region-specific default speed regulations&lt;/li&gt;
  &lt;li&gt;Consistent name usage using language inference&lt;/li&gt;
  &lt;li&gt;Complex attributes like intersection links&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To further explain the benefits of this hyperlocal feature, we will use intersection links as an example. In the next section, we will explain how intersection links data is used and how it impacts our driver-partners and passengers.&lt;/p&gt;

&lt;h3 id=&quot;identifying-hyperlocal-data---intersection-links&quot;&gt;Identifying hyperlocal data - intersection links&lt;/h3&gt;
&lt;p&gt;An intersection link is when two or more roads meet. Figure 4 and 5 illustrates what an intersection link looks like in a GrabMaps mock and in OSM.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/intersection-link-mock.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Mock of an intersection link. 
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/intersection-link-illustration.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Intersection link illustration from a real road network in OSM.  
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To locate intersection links in a road network, there are computations involved. We would first combine big data processing (which we do using Spark) with graphs. We use geohash as the unit of processing, and for each geohash, a bi-directional graph is created.&lt;/p&gt;

&lt;p&gt;From such resulting graphs, we can determine intersection links if:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Road segments are parallel&lt;/li&gt;
  &lt;li&gt;The roads have the same name&lt;/li&gt;
  &lt;li&gt;The roads are one way roads&lt;/li&gt;
  &lt;li&gt;Angles and the shape of the road are in the intervals or requirements we seek&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each intersection link we identify is tagged in the map as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intersection_links&lt;/code&gt;. Our downstream service teams can then identify them by searching for the tag.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;The impact we create with our intersection link can be explained through the following example.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/impact1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Longer route, without GrabMaps intersection link feature. The arrow indicates where the route should have suggested a U-turn.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/impact2.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Shorter route using GrabMaps by taking a closer link between two main roads.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 6 and Figure 7 show two different routes for the same origin and destination. However, you can see that Figure 7 has a shorter route and this is made available by taking an intersection link early on in the route. The highlighted road segment in Figure 7 is an intersection link, tagged by the process we described earlier. The route is now much shorter making GrabNav more efficient in its route suggestion.&lt;/p&gt;

&lt;p&gt;There are numerous factors that can impact a driver-partner’s trip, and intersection links are just one example. There are many more features that GrabMaps offers across Grab’s services that allow us to “outserve” our partners.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;GrabMaps and GrabNav deliver enriched experiences to our driver-partners. By integrating certain hyperlocal data features, we are also able to provide more accurate pricing for both our driver-partners and passengers. In our mission towards sustainable growth, this is an area that we will keep on improving by leveraging scalable tech solutions.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Aug 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/building-hyperlocal-grabmaps</link>
        <guid isPermaLink="true">https://engineering.grab.com/building-hyperlocal-grabmaps</guid>
        
        <category>Maps</category>
        
        <category>Data</category>
        
        <category>Big Data</category>
        
        <category>Data processing</category>
        
        <category>hyperlocalisation</category>
        
        <category>GrabMaps</category>
        
        <category>navigation</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Streamlining Grab's Segmentation Platform with faster creation and lower latency</title>
        <description>&lt;p&gt;Launched in 2019, Segmentation Platform has been Grab’s one-stop platform for user segmentation and audience creation across all business verticals. User segmentation is the process of dividing passengers, driver-partners, or merchant-partners (users) into sub-groups (segments) based on certain attributes. Segmentation Platform empowers Grab’s teams to create segments using attributes available within our data ecosystem and provides APIs for downstream teams to retrieve them.&lt;/p&gt;

&lt;p&gt;Checking whether a user belongs to a segment (Membership Check) influences many critical flows on the Grab app:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;When a passenger launches the Grab app, our in-house experimentation platform will tailor the app experience based on the segments the passenger belongs to.&lt;/li&gt;
  &lt;li&gt;When a driver-partner goes online on the Grab app, the Drivers service calls Segmentation Platform to ensure that the driver-partner is not blacklisted.&lt;/li&gt;
  &lt;li&gt;When launching marketing campaigns, Grab’s communications platform relies on Segmentation Platform to determine which passengers, driver-partners, or merchant-partners to send communication to.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This article peeks into the current design of Segmentation Platform and how the team optimised the way segments are stored to reduce read latency thus unlocking new segmentation use cases.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;Segmentation Platform comprises two major subsystems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Segment creation&lt;/li&gt;
  &lt;li&gt;Segment serving&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image1.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. Segmentation Platform architecture
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;segment-creation&quot;&gt;Segment creation&lt;/h3&gt;

&lt;p&gt;Segment creation is powered by Spark jobs. When a Grab team creates a segment, a Spark job is started to retrieve data from our data lake. After the data is retrieved, cleaned, and validated, the Spark job calls the serving sub-system to populate the segment with users.&lt;/p&gt;

&lt;h3 id=&quot;segment-serving&quot;&gt;Segment serving&lt;/h3&gt;

&lt;p&gt;Segment serving is powered by a set of Go services. For persistence and serving, we use ScyllaDB as our primary storage layer. We chose to use ScyllaDB as our NoSQL store due to its ability to scale horizontally and meet our &amp;lt;80ms p99 SLA. Users in a segment are stored as rows indexed by the user ID. The table is partitioned by the user ID ensuring that segment data is evenly distributed across the ScyllaDB clusters.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;User ID&lt;/th&gt;
    &lt;th&gt;Segment Name&lt;/th&gt;
    &lt;th&gt;Other metadata columns&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td&gt;1221&lt;/td&gt;
    &lt;td&gt;Segment_A&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;3421&lt;/td&gt;
    &lt;td&gt;Segment_A&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;5632&lt;/td&gt;
    &lt;td&gt;Segment_B&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;7889&lt;/td&gt;
    &lt;td&gt;Segment_B&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With this design, Segmentation Platform handles up to 12K read and 36K write QPS, with a p99 latency of 40ms.&lt;/p&gt;

&lt;h2 id=&quot;problems&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;The existing system has supported Grab, empowering internal teams to create rich and personalised experiences. However, with the increased adoption and use, certain challenges began to emerge:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As more and larger segments are being created, the write QPS became a bottleneck leading to longer wait times for segment creation.&lt;/li&gt;
  &lt;li&gt;Grab services requested even lower latency for membership checks.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;long-segment-creation-times&quot;&gt;Long segment creation times&lt;/h3&gt;

&lt;p&gt;As more segments were created by different teams within Grab, the write QPS was no longer able to keep up with the teams’ demands. Teams would have to wait for hours for their segments to be created, reducing their operational velocity.&lt;/p&gt;

&lt;h3 id=&quot;read-latency&quot;&gt;Read latency&lt;/h3&gt;

&lt;p&gt;Further, while the platform already offers sub-40ms p99 latency for reads, this was still too slow for certain services and their use cases. For example, Grab’s communications platform needed to check whether a user belongs to a set of segments before sending out communication and incurring increased latency for every communication request was not acceptable. Another use case was for Experimentation Platform, where checks must have low latency to not impact the user experience. 
Thus, the team explored alternative ways of storing the segment data with the goals of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reducing segment creation time&lt;/li&gt;
  &lt;li&gt;Reducing segment read latency&lt;/li&gt;
  &lt;li&gt;Maintaining or reducing cost&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;h3 id=&quot;segments-as-bitmaps&quot;&gt;Segments as bitmaps&lt;/h3&gt;

&lt;p&gt;One of the main engineering challenges was scaling the write throughput of the system to keep pace with the number of segments being created. As a segment is stored across multiple rows in ScyllaDB, creating a large segment incurs a huge number of writes to the database. What we needed was a better way to store a large set of user IDs. Since user IDs are represented as integers in our system, a natural solution to storing a set of integers was a bitmap.&lt;/p&gt;

&lt;p&gt;For example, a segment containing the following user IDs: 1, 6, 25, 26, 89 could be represented with a bitmap as follows:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. Bitmap representation of a segment
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To perform a membership check, a bitwise operation can be used to check if the bit at the user ID’s index is 0 or 1. As a bitmap, the segment can also be stored as a single Blob in object storage instead of inside ScyllaDB.&lt;/p&gt;

&lt;p&gt;However, as the number of user IDs in the system is large, a small and sparse segment would lead to prohibitively large bitmaps. For example, if a segment contains 2 user IDs 100 and 200,000,000, it will require a bitmap containing 200 million bits (25MB) where all but 2 of the bits are just 0. Thus, the team needed an encoding to handle sparse segments more efficiently.&lt;/p&gt;

&lt;h4 id=&quot;roaring-bitmaps&quot;&gt;Roaring Bitmaps&lt;/h4&gt;

&lt;p&gt;After some research, we landed on Roaring Bitmaps, which are compressed uint32 bitmaps. With roaring bitmaps, we are able to store a segment with 1 million members in a Blob smaller than 1 megabyte, compared to 4 megabytes required by a naive encoding.&lt;/p&gt;

&lt;p&gt;Roaring Bitmaps achieve good compression ratios by splitting the set into fixed-size (216) integer chunks and using three different data structures (containers) based on the data distribution within the chunk. The most significant 16 bits of the integer are used as the index of the chunk, and the least significant 16 bits are stored in the containers.&lt;/p&gt;

&lt;h5 id=&quot;array-containers&quot;&gt;Array containers&lt;/h5&gt;

&lt;p&gt;Array containers are used when data is sparse (&amp;lt;= 4096 values). An array container is a sorted array of 16-bit integers. It is memory-efficient for sparse data and provides logarithmic-time access.&lt;/p&gt;

&lt;h5 id=&quot;bitmap-containers&quot;&gt;Bitmap containers&lt;/h5&gt;

&lt;p&gt;Bitmap containers are used when data is dense. A bitmap container is a 216 bit container where each bit represents the presence or absence of a value. It is memory-efficient for dense data and provides constant-time access.&lt;/p&gt;

&lt;h5 id=&quot;run-containers&quot;&gt;Run containers&lt;/h5&gt;

&lt;p&gt;Finally, run containers are used when a chunk has long consecutive values. Run containers use run-length encoding (RLE) to reduce the storage required for dense bitmaps. Run containers store a pair of values representing the start and the length of the run. It provides good memory efficiency and fast lookups.&lt;/p&gt;

&lt;p&gt;The diagram below shows how a dense bitmap container that would have required 91 bits can be compressed into a run container by storing only the start (0) and the length (90). It should be noted that run containers are used only if it reduces the number of bytes required compared to a bitmap.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. A dense bitmap container compressed into a run container
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;By using different containers, Roaring Bitmaps are able to achieve good compression across various data distributions, while maintaining excellent lookup performance. Additionally, as segments are represented as Roaring Bitmaps, service teams are able to perform set operations (union, interaction, and difference, etc) on the segments on the fly, which previously required re-materialising the combined segment into the database.&lt;/p&gt;

&lt;h3 id=&quot;caching-with-an-sdk&quot;&gt;Caching with an SDK&lt;/h3&gt;

&lt;p&gt;Even though the segments are now compressed, retrieving a segment from the Blob store for each membership check would incur an unacceptable latency penalty. To mitigate the overhead of retrieving a segment, we developed an SDK that handles the retrieval and caching of segments.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image2.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. How the SDK caches segments
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The SDK takes care of the retrieval, decoding, caching, and watching of segments. Users of the  SDK are only required to specify the maximum size of the cache to prevent exhausting the service’s memory. The SDK provides a cache with a least-recently-used eviction policy to ensure that hot segments are kept in the cache. They are also able to watch for updates on a segment and the SDK will automatically refresh the cached segment when it is updated.&lt;/p&gt;

&lt;h2 id=&quot;hero-teams&quot;&gt;Hero teams&lt;/h2&gt;

&lt;h3 id=&quot;communications-platform&quot;&gt;Communications Platform&lt;/h3&gt;

&lt;p&gt;Communications Platform has adopted the SDK to implement a new feature to control the communication frequency based on which segments a user belongs to. Using the SDK, the team is able to perform membership checks on multiple multi-million member segments, achieving peak QPS 15K/s with a p99 latency of &amp;lt;1ms. With the new feature, they have been able to increase communication engagement and reduce the communication unsubscribe rate.&lt;/p&gt;

&lt;h3 id=&quot;experimentation-platform&quot;&gt;Experimentation Platform&lt;/h3&gt;

&lt;p&gt;Experimentation Platform powers experimentation across all Grab services. Segments are used heavily in experiments to determine a user’s experience. Prior to using the SDK, Experimentation Platform limited the maximum size of the segments that could be used to prevent exhausting a service’s memory.&lt;/p&gt;

&lt;p&gt;After migrating to the new SDK, they were able to lift this restriction due to the compression efficiency of Roaring Bitmaps. Users are now able to use any segments as part of their experiment without worrying that it would require too much memory.&lt;/p&gt;

&lt;h2 id=&quot;closing&quot;&gt;Closing&lt;/h2&gt;

&lt;p&gt;This blog post discussed the challenges that Segmentation Platform faced when scaling and how the team explored alternative storage and encoding techniques to improve segment creation time, while also achieving low latency reads. The SDK allows our teams to easily make use of segments without having to handle the details of caching, eviction, and updating of segments.&lt;/p&gt;

&lt;p&gt;Moving forward, there are still existing use cases that are not able to use the Roaring Bitmap segments and thus continue to rely on segments from ScyllaDB. Therefore, the team is also taking steps to optimise and improve the scalability of our service and database.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Axel, the wider Segmentation Platform team, and Data Technology team for reviewing the post. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Aug 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/streamlining-grabs-segmentation-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/streamlining-grabs-segmentation-platform</guid>
        
        <category>Back End</category>
        
        <category>Performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
