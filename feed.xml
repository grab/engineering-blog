<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 21 Feb 2025 03:40:02 +0000</pubDate>
    <lastBuildDate>Fri, 21 Feb 2025 03:40:02 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Grab AI Gateway: Connecting Grabbers to Multiple GenAI Providers</title>
        <description>&lt;p&gt;The transformative world of Generative AI (GenAI), which refers to artificial intelligence systems capable of creating new content such as text, images, or music that is similar to human-generated content, has become integral to innovation, powering the next generation of AI-enabled applications. At Grab, it is crucial that every Grabber has access to these cutting-edge technologies to build powerful applications to better serve our customers and enhance their experiences. Grab’s AI Gateway aims to provide exactly this. The gateway seamlessly integrates AI providers like OpenAI, Azure, AWS (Bedrock), Google (VertexAI) and many other AI models, to bring seamless access to advanced AI technologies to every Grabber.&lt;/p&gt;

&lt;h2 id=&quot;why-do-we-need-grab-ai-gateway&quot;&gt;Why do we need Grab AI Gateway?&lt;/h2&gt;

&lt;p&gt;Before we begin implementing Grab AI Gateway in our work process, it is important for us to understand the limitations as well as the solutions that Grab AI Gateway provides. Failure to properly implement Grab AI Gateway could lead to roadblocks in development which negatively affect user experience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streamline access&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Each AI provider has its own way of authenticating their services. Some providers use key-based authentication while others require instance roles or cloud credentials. Grab AI Gateway provides a centralised platform that only requires a one-time provider access setup. Grab AI Gateway removes the effort of procuring resources and setting up infrastructure for AI services, such as servers, storage, and other necessary components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Enables experimentation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By providing a simple unified way to access different AI providers, users can experiment with various Large Language Models (LLMs) and choose the one best suited for their task.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost-efficient usage&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Many AI providers allow purchasing of reserved capacity to provide higher throughput and improve cost effectiveness. However, services that require reservation or pre-purchases over a commitment period can lead to wastage.&lt;/p&gt;

&lt;p&gt;Grab AI Gateway overcomes this problem and minimises wastage with a shared capacity pool. A deprecated service would simply free up bandwidth for a new service to utilise. Additionally, Grab AI Gateway provides a global view of usage trends to help platform teams make informed decisions on reallocating reserved capacity according to demand and future trends (eg. an upcoming model replacing an old one).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Auditing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A central setup ensures that use cases undergo a thorough review process to comply with the privacy and cyber security standards before being deployed in production. For instance, a Q&amp;amp;A bot with access to both restricted and non-restricted data could inadvertently reveal sensitive information if authorisation is not set up properly. Therefore, it is important that use cases are reviewed to ensure they follow Grab’s standard for data privacy and protection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Platformisation benefits&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Proper implementation of a central gateway provides platformisation benefits like:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Reduced operational costs.&lt;/li&gt;
  &lt;li&gt;Centralised monitoring and alerts.&lt;/li&gt;
  &lt;li&gt;Cost attribution.&lt;/li&gt;
  &lt;li&gt;Control limits like maximum QPS and cost cap.&lt;/li&gt;
  &lt;li&gt;Enforce guardrail and safety from prompt injection.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture-and-design&quot;&gt;Architecture and design&lt;/h2&gt;

&lt;p&gt;At its core, the AI Gateway is a set of reverse proxies to different external AI providers like Azure, OpenAI, AWS, and others. From the user’s perspective, the AI Gateway acts like the actual provider where users are only required to set the correct base URLs to access the LLMs. The gateway handles functionalities like authentication, authorisation, and rate limiting, allowing users to solely focus on building GenAI enabled applications.&lt;/p&gt;

&lt;p&gt;To form the basis of identity and access management (IAM) in the gateway, API key can be requested by the user for exploration (short-term personal key) or production (long-term service key) usage. The gateway implements a request path based authorisation where certain keys can be granted access to specific providers or features. Once authenticated, the AI Gateway replaces the internal key in request with the provider key and executes the request on behalf of the user.&lt;/p&gt;

&lt;p&gt;The AI Gateway is designed with a minimalist approach, often serving as a lightweight interface between the user and the provider, intervening only when necessary. This has enabled us to keep up with the pace of innovation in the field and to continue expanding the provider catalogue without increasing the ops burden. Similar to requests, responses from the provider are returned to the user with no to minimal processing time. The gateway is not limited to only chat completion API. It exposes other APIs like embedding, image generation, and audio along with functionalities like fine-tuning, file storage, search, and context caching. The gateway also provides access to in-house open source models. This provides a taste of open source software (OSS) capabilities that users can later decide to deploy a dedicated instance using &lt;a href=&quot;https://engineering.grab.com/catwalk-evolution&quot;&gt;Catwalk&lt;/a&gt;’s &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;VLLM&lt;/a&gt; offering.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabaigateway/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: High level architecture of AI Gateway&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;user-journey-and-features&quot;&gt;User journey and features&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Onboarding process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GenAI based applications come with inherent risks like generating offensive or incorrect output and hostile takeover by malicious actors. As software practices and security standards for building GenAI applications are still evolving, it is important for users to be aware of the potential pitfalls. As AI Gateway is the de facto way to access this technology, the platform team shares the responsibility of building such awareness and ensuring compliance. The onboarding process includes a manual review stage. Every new use case requires a mini-RFC (Request For Comments) and a checklist that is reviewed by the platform team. In certain cases, an in-depth review by the AI Governance task force may be requested. To reduce friction, users are encouraged to build prototypes and experiment with APIs using “exploration keys”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exploration keys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At Grab, every Grabber is encouraged to use GenAI technologies to improve productivity and to experiment and learn within this field. The gateway provides exploration keys to make it easier for users to experiment with building chatbots and Retrieval Augmented Generation (RAG). These keys can be requested by Grabbers through a Slack bot. The keys are short-lived with a validity period of a few days, stricter rate limit restrictions, and access limited to only the staging environment. Exploration keys are highly popular, with more than 3,000 Grabbers requesting the key to experiment with APIs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Unified API interface&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In addition to provider specific interface, the gateway also offers a single interface to interact with multiple AI providers. For users, this lowers the barrier of experimenting between different providers/models, as they do not need to learn and rewrite their logic for different SDKs. Providers can be switched simply by changing the “model” parameter in the API request. This also enables easy setup of fallback logic and dynamic routing across providers. Based on popularity, the gateway uses the OpenAI API scheme to provide the unified interface experience. The API handler translates the request payload to the provider specific input scheme. The translated payload is then sent to reverse proxies. The returned response is translated back to the OpenAI response scheme.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabaigateway/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Unified Interface Logic&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Dynamic routing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The AI Gateway plays a crucial role in maintaining usage efficiency of various reserved instance capacities. It provides the control points to dynamically route requests for certain models to a different albeit similar model backed by a reserved instance. Another frequent use case is smart load balancing across different regions to address region-specific constraints related to maximum available quotas. This approach has helped to minimise rate limiting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Auditing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The AI Gateway records each call’s request, response body, and additional metadata like token usage, URL path, and model name into Grab’s data lake. The purpose of doing so is to maintain a trail of usage which can be used for auditing. The archived data can be inspected for security threats like prompt injection or potential data policy violations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost attribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Allocating costs to each use case is important to encourage responsible usage. The cost of calling LLMs tends to increase at higher request rates, therefore understanding the incurred cost is crucial to understanding the feasibility of a use case. The gateway performs cost calculations for each request once the response is received from the provider. The cost is archived in the data lake along with an audit trail. For async usages like fine-tuning and assisting, the cost is calculated through a separate daily job. Finally, a job aggregates the cost for each service which is used for reporting on dashboards and showback. In addition, alerts are configured to notify if a service exceeds the cost threshold.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rate limits&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AI Gateway enforces its own rate limit on top of the global provider limits to make sure quotas are not consumed by a single service. Currently, limits are enforced on the request rate at the key level.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integration with the ML Platform&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At Grab, the ML platform serves as a one-stop shop, facilitating each phase of the model development lifecycle. The AI Gateway is well integrated with systems like &lt;a href=&quot;https://engineering.grab.com/chimera-sandbox&quot;&gt;Chimera notebooks&lt;/a&gt; used for ideation/development to &lt;a href=&quot;https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale&quot;&gt;Catwalk&lt;/a&gt; for deployment. When a user spins up a Chimera notebook, an exploration key is automatically mounted and is ready for use. For model deployments, users can configure the gateway integration which sets up the required environment variables and mounts the key into the app.&lt;/p&gt;

&lt;h2 id=&quot;challenges-faced&quot;&gt;Challenges faced&lt;/h2&gt;

&lt;p&gt;With more than 300 unique use cases onboarded and many of those making it to production, AI Gateway has gained popularity since its inception in 2023. The gateway has come a long way, with many refinements made to the UX and provider offerings. The journey has not been without its challenges. Some of the challenges have become more prominent as the number of apps deployed increases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Keeping up with innovations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With new features or LLMs being released at a rapid pace, the AI Gateway development has required continuous dedicated effort. Reflecting on our experience, it is easy to get overwhelmed by a constant stream of user requests for each new development in the field. However, we have come to realise it is important to balance release timelines and user expectations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fair distribution of quota&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every use case has a different service level objective (SLO). Batch use cases require high throughput but can tolerate failures while online applications are sensitive to latency and rate limits. In many cases, the underlying provider resource is the same. The responsibility falls over to the gateway to ensure fair distribution based on criticality and requests per second (RPS) requirements. As adoption increases, we have encountered issues where batch usage interfered with the uptime of online services. The use of Async APIs does mitigate the issues, but not all use cases can adhere to turnaround time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Maintaining reverse proxies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Building the gateway as a reverse proxy was a key design decision. While the decision has proven to be beneficial, it is not without its complexity. The design ensures that the gateway is compatible with provider-specific SDKs. However, over time, we have encountered edge cases where certain SDK functionalities do not work as expected due to a missing path in the gateway or a missing configuration. These issues are usually ironed out when caught and a suite of integration tests with SDKs are conducted to ensure there are no breaking changes before deploying.&lt;/p&gt;

&lt;h2 id=&quot;current-use-cases-and-applications&quot;&gt;Current use cases and applications&lt;/h2&gt;

&lt;p&gt;Today, the gateway powers many AI-enabled applications. Some examples include real time audio signal analysis for enhancing ride safety, content moderation to block unsafe content, and description generator for menu items and many others.&lt;/p&gt;

&lt;p&gt;Internally, the gateway powers innovative solutions to boost productivity and reduce toil. A few examples are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GenAI portal that is used for translation and language detection tasks, image generation, and file analysis.&lt;/li&gt;
  &lt;li&gt;Text-to-Insights for converting questions into SQL queries.&lt;/li&gt;
  &lt;li&gt;Incident management automation for triaging incidents and creating reports.&lt;/li&gt;
  &lt;li&gt;Support bot for answering user queries in Slack channels using a knowledge base.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;As we continue to add more features, we plan to focus our efforts on these areas:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Catalogue&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With over 50 AI models each suited for a specific task type, finding the correct model to use is becoming complex. Users are often unsure of the difference between models in terms of capabilities, latency, and cost implications. A catalogue can serve as a guideline by listing currently supported models along with the list of metadata like the input/output modality, token limits, provider quota, pricing, and reference guide.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Out of box governance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Currently, all AI-enabled services that process clear text input and output from customers require users to set up their own guardrails and safety measures. By creating a built-in support for security threats like prompt injection and guardrails for filtering input/output, we can save users significant effort.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Smarter rate limits&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At the current time, the gateway supports basic request rate-based limits at key level. While this rudimentary offering has been proven useful, it has its limitations. More advanced rate limiting policies based on token usage or daily/monthly running costs should be introduced to enforce better and fairer limits. These policies can be modified to be applied on different models and providers.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Priscilla Lee, Isella Lim, and Kevin Littlejohn for helping us in the project and Padarn Wilson for his leadership. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/4i8bEnS&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 19 Feb 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/grab-ai-gateway</link>
        <guid isPermaLink="true">https://engineering.grab.com/grab-ai-gateway</guid>
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Optimisation</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Embracing passwordless authentication with Grab’s Passkey</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;This blog post introduces Passkey — our latest addition to the Grab app — a step towards a secure, passwordless future. It provides an in-depth look at this innovative authentication method that allows users to have full control over their security, making authentication seamless and phishing-resistant. By the end of this piece, you will understand why we developed Passkey, how it works, the challenges we overcame, and the benefits brought to us post-launch. Whether you’re a tech enthusiast, a cybersecurity follower, or a Grab user, this piece offers valuable insights into the passwordless authentication sphere and Grab’s commitment to user safety and comfort.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the evolving world of digital security, Grab has always prioritised user account safety. A significant part of this involves exploring more secure and user-friendly authentication methods. Enter Grab’s Passkey — a major step towards passwordless authentication that leverages the Fast IDentity Online &lt;a href=&quot;https://fidoalliance.org/passkeys/&quot;&gt;(FIDO) standard&lt;/a&gt;, giving users full control over their security, and making authentication seamless.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Traditionally, the authentication process primarily relies on passwords — a precarious practice given the vulnerability to various security threats, such as phishing, keystroke logging, and brute-force attacks. This downside leads to the pursuit of safer, more user-friendly alternatives. Among these is the introduction of passwordless authentication.&lt;/p&gt;

&lt;p&gt;A passwordless authentication method eliminates the need for users to enter traditional passwords during the verification process. Instead, it employs alternatives like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Email link&lt;/strong&gt;: A one-time clickable link sent via email.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;One-Time Passcodes (OTPs)&lt;/strong&gt;: Temporary codes sent to users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Social logins&lt;/strong&gt;: Using existing profiles on platforms like Facebook or Google to sign in.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Authenticator apps&lt;/strong&gt;: Software that generates time-sensitive codes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Recognising the limitations and security issues of traditional password-based authentication, we turned to a more secure, user-friendly solution - the passwordless authentication system. Among other methods, we are also enabling Passkey, built on the FIDO standard. This global standard fosters wider adoption and support from consumer brands, making Passkey a secure and convenient choice.&lt;/p&gt;

&lt;h3 id=&quot;why-passkey&quot;&gt;Why Passkey?&lt;/h3&gt;

&lt;p&gt;Given the rapidly evolving security threats in the digital space, we selected Passkey for its unique benefits in providing both enhanced security and a seamless user experience. Passkey offers enhanced security as it is phishing-resistant and doesn’t require secrets to be stored in Grab’s database. Instead, secrets are securely kept within the user’s device, putting the control in their hands and significantly reducing the chances of exposure.&lt;/p&gt;

&lt;h3 id=&quot;fast-paced-adoption-of-passkey&quot;&gt;Fast-paced adoption of Passkey&lt;/h3&gt;

&lt;p&gt;Passkey technology is not only promising in theory but also successful in practice, as evidenced by its wider industry adoption. Consumers are adopting passkeys at a rapid pace in 2024. With large global consumer brands, such as Adobe, Amazon, Apple, Google, Hyatt, Nintendo, PayPal, Playstation, Shopify and TikTok enabling passkey technology for their users, more than 13 billion accounts can now leverage passkeys for sign-in.&lt;/p&gt;

&lt;p&gt;In a recent &lt;a href=&quot;https://fidoalliance.org/content-ebook-consumer-password-and-passkey-trends-wpd-2024/#:~:text=To%20commemorate%20World%20Password%20Day,attitudes%20towards%20authentication%20are%20evolving&quot;&gt;FIDO Alliance independent study&lt;/a&gt; conducted on World Password Day 2024 across the U.S. and UK, findings reveal:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A majority of people are aware of passkey technology (62%).&lt;/li&gt;
  &lt;li&gt;Over half have enabled passkeys on at least one of their accounts (53%).&lt;/li&gt;
  &lt;li&gt;Once they adopt a passkey, nearly a quarter enable a passkey whenever possible (23%).&lt;/li&gt;
  &lt;li&gt;A large number believe passkeys are more secure (61%) and more convenient than passwords (58%).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These trends clearly illustrate why we chose to implement Passkey as our passwordless solution.&lt;/p&gt;

&lt;h2 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h2&gt;

&lt;h3 id=&quot;how-do-passkeys-work&quot;&gt;How do passkeys work?&lt;/h3&gt;

&lt;p&gt;There are three components of the passkey flow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Holds the accounts database storing the public key and other metadata about the passkey.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Communicates with the authenticator and sends requests to the backend.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Authenticator&lt;/strong&gt;: The user’s authenticator creates and stores the passkey. This may be implemented in the operating system underlying the user agent, in external hardware, or a combination of both.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/embracing-passwordless-authentication-with-passkey/high-level-passkey-authentication.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. A high-level overview of the passkey authentication.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;supported-environments&quot;&gt;Supported environments&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Google Password Manager&lt;/strong&gt;: Stores, serves and synchronises passkeys on Android and Chrome. Passkeys are securely backed up and synced between Android devices where the user is signed using the same Google account, and available passkeys are listed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;iCloud Keychain&lt;/strong&gt;: Synchronises the saved passkey to other Apple devices that run macOS, iOS, or iPadOS where the user is signed in using the same iCloud account.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;In this section, we illustrate the usage of passkeys in several scenarios.&lt;/p&gt;

&lt;h3 id=&quot;creating-a-new-passkey&quot;&gt;Creating a new passkey&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/embracing-passwordless-authentication-with-passkey/passkey-registration.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Passkey registration steps in Grab app.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The user signs into the Grab app and selects &lt;strong&gt;Enable Passkey&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Frontend requests user details and a challenge from Backend.&lt;/li&gt;
  &lt;li&gt;Authenticator creates the user’s passkey upon their consent using their device’s screen lock.&lt;/li&gt;
  &lt;li&gt;This passkey, along with other data, is sent back to Frontend.&lt;/li&gt;
  &lt;li&gt;Frontend sends the public key credential to Backend for storage and future authentications.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/embracing-passwordless-authentication-with-passkey/sequence-diagram-passkey-registration.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Sequence diagram of Passkey registration.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;creating-a-passkey---notable-webauthn-parameters&quot;&gt;Creating a passkey - notable Webauthn parameters&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;When the user selects &lt;strong&gt;Enable Passkey&lt;/strong&gt;, Frontend fetches the following information to call &lt;a href=&quot;https://www.w3.org/TR/webauthn-2/#sctn-sample-registration&quot;&gt;navigator.credentials.create()&lt;/a&gt; from Backend:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialcreationoptions-challenge&quot;&gt;challenge&lt;/a&gt;&lt;/strong&gt;: server-generated challenge.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialcreationoptions-user&quot;&gt;user&lt;/a&gt;.id&lt;/strong&gt;: user’s unique ID, stored as ArrayBuffer.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;user.name&lt;/strong&gt;: unique username or email for account recognition.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;user.displayName&lt;/strong&gt;: user-friendly name for the account.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialcreationoptions-excludecredentials&quot;&gt;excludeCredentials&lt;/a&gt;&lt;/strong&gt;: to prevent registering the same device multiple times.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialcreationoptions-rp&quot;&gt;rp&lt;/a&gt;.id&lt;/strong&gt;: Domain or a registrable suffix of an RP’s origin.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;rp.name&lt;/strong&gt;: Name of the RP.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialcreationoptions-pubkeycredparams&quot;&gt;pubKeyCredParams&lt;/a&gt;&lt;/strong&gt;: Specifies RP’s public-key algorithms.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialcreationoptions-authenticatorselection&quot;&gt;authenticatorSelection&lt;/a&gt;.authenticatorAttachment&lt;/strong&gt;: Indicates type of authenticator attachment desired.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;authenticatorSelection.requireResidentKey&lt;/strong&gt;: Indicates if resident key is needed.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;authenticatorSelection.userVerification&lt;/strong&gt;: Indicates if user verification is &lt;em&gt;required&lt;/em&gt;, &lt;em&gt;preferred&lt;/em&gt;, or &lt;em&gt;discouraged&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Frontend invokes WebAuthn API to create a passkey.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; const publicKeyCredentialCreationOptions = {
   challenge: *****,
   rp: {
     name: &quot;Example&quot;,
     id: &quot;example.com&quot;,
   },
   user: {
     id: *****,
     name: &quot;john78&quot;,
     displayName: &quot;John&quot;,
   },
   pubKeyCredParams: [{alg: -7, type: &quot;public-key&quot;},{alg: -257, type: &quot;public-key&quot;}],
   excludeCredentials: [{
     id: *****,
     type: &apos;public-key&apos;,
     transports: [&apos;internal&apos;],
   }],
   authenticatorSelection: {
     authenticatorAttachment: &quot;platform&quot;,
     requireResidentKey: true,
   }
 };

 const credential = await navigator.credentials.create({
   publicKey: publicKeyCredentialCreationOptions
 });

 // Encode and send the credential to the server for verification.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Post user consent, passkey is created and returned along with relevant data to the frontend.&lt;/li&gt;
  &lt;li&gt;Frontend sends the public key credential to Backend where it gets stored for future authentication.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#authenticatorattestationresponse&quot;&gt;PublicKeyCredential&lt;/a&gt; object returned includes properties like &lt;a href=&quot;https://w3c.github.io/webauthn/#credential-id&quot;&gt;id&lt;/a&gt;, &lt;a href=&quot;https://w3c.github.io/webauthn/#credential-id&quot;&gt;rawId&lt;/a&gt;, &lt;a href=&quot;https://w3c.github.io/webauthn/#client-data&quot;&gt;response.clientDataJSON&lt;/a&gt;, &lt;a href=&quot;https://w3c.github.io/webauthn/#attestation-object&quot;&gt;response.attestationObject&lt;/a&gt;, &lt;a href=&quot;https://w3c.github.io/webauthn/#enumdef-authenticatorattachment&quot;&gt;authenticatorAttachment&lt;/a&gt;, and &lt;strong&gt;type&lt;/strong&gt; (“public-key”).&lt;/li&gt;
      &lt;li&gt;Libraries can be used for handling the public key credential object.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Backend receives and processes the object, and information is stored in the database for future use.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;signing-in-with-a-passkey&quot;&gt;Signing in with a passkey&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/embracing-passwordless-authentication-with-passkey/passkey-authentication.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Passkey authentication steps in Grab app.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The user  launches the Grab app and opts to login using their passkey.&lt;/li&gt;
  &lt;li&gt;Frontend requests a challenge from Backend for passkey authentication.&lt;/li&gt;
  &lt;li&gt;The user  is shown their available passkeys.&lt;/li&gt;
  &lt;li&gt;Upon choosing a passkey, the user consents to using their device’s lock screen.&lt;/li&gt;
  &lt;li&gt;Frontend receives the public key credential and some data.&lt;/li&gt;
  &lt;li&gt;Frontend forwards these to the backend, which verifies them against the database and logs the user in.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus, Passkey enhances the login experience, providing an optimal blend of security and seamless usability.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/embracing-passwordless-authentication-with-passkey/sequency-diagram-passkey-authentication.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Sequence diagram of the Passkey authentication.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;signing-in-with-a-passkey---notable-webauthn-parameters&quot;&gt;Signing in with a passkey - notable Webauthn parameters&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Frontend fetches a challenge from Backend.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialcreationoptions-challenge&quot;&gt;challenge&lt;/a&gt;&lt;/strong&gt;: server-generated challenge, crucial to prevent replay attacks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialrequestoptions-allowcredentials&quot;&gt;allowCredentials&lt;/a&gt;&lt;/strong&gt;: array of acceptable credentials for authentication.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialrequestoptions-userverification&quot;&gt;userVerification&lt;/a&gt;&lt;/strong&gt;: indicates whether user verification is required, preferred, or discouraged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Frontend calls &lt;strong&gt;&lt;a href=&quot;https://www.w3.org/TR/webauthn-2/#sctn-sample-authentication&quot;&gt;navigator.credentials.get()&lt;/a&gt;&lt;/strong&gt; to initiate user authentication.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; // To abort a WebAuthn call, instantiate an `AbortController`.

 const abortController = new AbortController();

 const publicKeyCredentialRequestOptions = {
   // Server generated challenge
   challenge: ****,
   // The same RP ID as used during registration
   rpId: &apos;example.com&apos;,
 };

 const credential = await navigator.credentials.get({
   publicKey: publicKeyCredentialRequestOptions,
   signal: abortController.signal,
   // Specify &apos;conditional&apos; to activate conditional UI
   mediation: &apos;conditional&apos;
 });
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Post user consent through their device’s screen lock, a &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#authenticatorassertionresponse&quot;&gt;PublicKeyCredential&lt;/a&gt;&lt;/strong&gt; object is returned to Frontend.&lt;/li&gt;
  &lt;li&gt;The returned &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#authenticatorassertionresponse&quot;&gt;PublicKeyCredential&lt;/a&gt;&lt;/strong&gt; is sent to Backend for verification. Backend looks up matching credential ID and verifies the signature against the stored public key.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-publickeycredentialrequestoptions-rpid&quot;&gt;rp.id&lt;/a&gt;&lt;/strong&gt;: Must match the rp.id used when creating the passkey.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#authenticatorassertionresponse&quot;&gt;PublicKeyCredential&lt;/a&gt;&lt;/strong&gt; object includes properties like &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#credential-id&quot;&gt;id&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#credential-id&quot;&gt;rawId&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#client-data&quot;&gt;response.clientDataJSON&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-authenticatorassertionresponse-authenticatordata&quot;&gt;response.authenticatorData&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-authenticatorassertionresponse-signature&quot;&gt;response.signature&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#dom-authenticatorassertionresponse-userhandle&quot;&gt;response.userHandle&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://w3c.github.io/webauthn/#enumdef-authenticatorattachment&quot;&gt;authenticatorAttachment&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;type&lt;/strong&gt; (“public-key”).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;A frictionless login paints a positive picture for our users. No more waiting for OTPs or struggling with cumbersome two-factor authentication. With the implementation of Passkey, users will enjoy a smoother, faster, and more secure login process.&lt;/p&gt;

&lt;p&gt;In addition to delivering a frictionless user experience, passkeys provide heightened security compared to conventional authentication methods such as OTPs and passwords, which demand active credential management.&lt;/p&gt;

&lt;p&gt;Using passkeys for authentication can lead to cost savings by cutting down or eliminating fees related to third-party authentication services, communication expenses, and messaging platforms. This strategy not only boosts security and user experience but also enhances the financial efficiency of the authentication process.&lt;/p&gt;

&lt;p&gt;Moving forward, our focus is on enhancing, streamlining, and extending the capabilities of Passkey. We are enthusiastic about the evolution of passwordless authentication and are dedicated to ongoing investments in technologies that deliver the utmost user satisfaction and experience.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Leveraging passkeys for authentication provides heightened security, enhanced user experience, cost-effectiveness, decreased vulnerabilities, multi-factor authentication support, and simplified credential management. The future direction involves enhancing and broadening Passkey capabilities, with a dedication to investing in user-centric technologies that advance passwordless authentication. This commitment underscores the focus on delivering secure, efficient, and user-friendly authentication solutions for both existing and prospective users.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;Looking ahead, based on the user adoption of Passkey and its anticipated impact on improving login convenience, we aim to explore the expansion of this feature to web login as well. We envision a scenario where users can leverage the power of their existing phone Passkey, no matter the operating system, thereby creating a truly seamless and secure login experience.
As we gather user feedback, analyse usage data, and delve into Passkey’s impact, we aim to identify growth opportunities and further enhance our understanding of this innovative feature’s transformative effect on app security. Stay tuned for updates on how we are revolutionising our approach to authentication, with a continuous focus on enhancing user convenience and security.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://fidoalliance.org/content-ebook-consumer-password-and-passkey-trends-wpd-2024/#:~:text=To%20commemorate%20World%20Password%20Day,attitudes%20towards%20authentication%20are%20evolving&quot;&gt;Consumer Password and Passkey Trends: World Password Day 2024 - FIDO Alliance.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://passkeys.dev/docs/tools-libraries/libraries/&quot;&gt;Libraries - passkeys.dev&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.w3.org/TR/webauthn/&quot;&gt;Web Authentication: An API for accessing Public Key Credentials - Level 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developers.yubico.com/Passkeys/Quick_overview_of_WebAuthn_FIDO2_and_CTAP.html&quot;&gt;Quick overview of WebAuthn FIDO2 and CTAP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Dec 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/embracing-passwordless-authentication-with-passkey</link>
        <guid isPermaLink="true">https://engineering.grab.com/embracing-passwordless-authentication-with-passkey</guid>
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Turbocharging GrabUnlimited with Temporal</title>
        <description>&lt;p&gt;Welcome to the behind-the-scenes story of &lt;a href=&quot;https://www.grab.com/sg/grabunlimited/&quot;&gt;GrabUnlimited&lt;/a&gt;, Grab’s flagship membership program. We undertook the mammoth task of migrating from our legacy system to a Temporal&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; workflow-based system, enhancing our ability to handle millions of subscribers with increased efficiency and resilience. The result? A whopping 80% reduction in open production incidents, and most importantly - an improved membership experience for our users. In this first part of the series, you will learn how to design a robust and scalable membership system as we delve into our own experience building one.&lt;/p&gt;

&lt;h2 id=&quot;what-is-grabunlimited&quot;&gt;What is GrabUnlimited?&lt;/h2&gt;

&lt;p&gt;The idea behind GrabUnlimited, is pretty simple: you pay a monthly fee, you get monthly benefits as a member (e.g discounted food delivery fee). A membership system plays a key role in enhancing user experience by giving them more value for money, but also by building loyalty, making Grab their go-to app for everyday needs. However, as this program grew and evolved, it brought along unique challenges and opportunities.&lt;/p&gt;

&lt;p&gt;With the initial triumph and significant surge in subscriber count by over 1000% from January 2022 to June 2023 - which we were super proud of! - the architecture that supported GrabUnlimited was starting to show signs of strain. Common subscriber concerns such as not receiving their membership benefits, along with developer issues marked by an increase in service outages highlighted the system’s low resiliency. The culprit? A backend service that, while functional, was not built to efficiently manage the complexities of a rapidly scaling membership model.&lt;/p&gt;

&lt;h2 id=&quot;deep-dive-into-our-previous-system-design&quot;&gt;Deep dive into our previous system design&lt;/h2&gt;

&lt;p&gt;As engineers, we know that deciding to migrate any system to a new one is like changing the engine of a running car. It requires meticulous evaluation of the existing systems, a deep dive into the issues and their root causes, and a thorough analysis of potential solutions and their trade-offs.&lt;/p&gt;

&lt;h3 id=&quot;how-was-grabunlimited-designed&quot;&gt;How was GrabUnlimited designed?&lt;/h3&gt;

&lt;p&gt;Initially, GrabUnlimited systems were designed for an experiment and not a full-fledged regional product. The idea was to try it out as a minimum viable product over a restricted segment of a few hundred thousand users. Let’s first have a look at how the membership program works.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/life-of-a-membership-worfklow.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. GrabUnlimited life of a membership flowchart.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Under the hood, our membership system relies on two main flows&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Membership purchase&lt;/strong&gt;: The user enrols for a certain duration (e.g 3 months), completes the payment through our Payment service, and receives benefits via our Reward service.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Membership renewal&lt;/strong&gt;: A daily cron job&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; checks which memberships need renewal, processes the payment, and delivers the benefits.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We employed a state machine&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; approach to break down the membership process into smaller chunks called state handlers. For instance, a membership might transition through ‘Init’, ‘Charged’, ‘Rewarded’, and ‘Active’ states. To operate these states, we used &lt;a href=&quot;https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html&quot;&gt;Amazon’s Simple Queue Service&lt;/a&gt; (SQS). SQS acts as a manager, delegating state handlers to workers (our service) and monitoring the status of the state handler. If a worker fails to complete a task, SQS reassigns the task to another worker, ensuring no task is lost. The load is also spread across multiple workers, helping with scalability.&lt;/p&gt;

&lt;p&gt;To safeguard our system against duplicate tasks such as charging the user twice, when a worker takes up a task, it would use a &lt;a href=&quot;https://redis.io/glossary/redis-lock/&quot;&gt;Redis lock&lt;/a&gt;&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; mechanism with a time-to-live (TTL) of five minutes preventing any other worker from picking up the same task. If a worker fails or crashes, the lock expires and another worker can pick up the job.&lt;/p&gt;

&lt;p&gt;So far, so good.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/grabunlimited-previous-system-design-overview.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. GrabUnlimited previous system design overview.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;with-our-success-came-many-challenges&quot;&gt;With our success came many challenges&lt;/h3&gt;

&lt;p&gt;As our subscriber base grew, we experienced an increase in system outages. To address this, we scrutinised metrics like the number of support tickets and gauged the toll on our engineering team. This included the time spent patching up issues and the opportunity cost of not developing new features or improvements.&lt;/p&gt;

&lt;p&gt;From our subscribers’ point of view, we saw a steady increase in reported incidents.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Users were blocked because their membership status was corrupted in our database.&lt;/li&gt;
  &lt;li&gt;Memberships were not automatically renewed, or users were not able to resubscribe.&lt;/li&gt;
  &lt;li&gt;Users were not receiving their benefits after renewing their membership.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the engineering team’s perspective, we were dedicating one engineer every week to battle these incidents full time. The on-call engineers were not only tasked with manually fixing all customer reports but were also swamped with frequent system alerts. This situation had three detrimental impacts on our team:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We were constantly putting out fires instead of addressing the root causes.&lt;/li&gt;
  &lt;li&gt;We were spending resources that could have been used to enhance our customers’ experience.&lt;/li&gt;
  &lt;li&gt;Our team’s motivation and confidence was taking a big hit.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;finding-the-architectural-culprit&quot;&gt;Finding the architectural culprit&lt;/h3&gt;

&lt;p&gt;The first step was to clearly identify and understand the issues within our systems. We looked at the frequency of failures and their root cause. From there, we were able to detect recurring patterns, which led us to four major issues in our architecture.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our system’s cron job, which retrieves all daily memberships due for renewal from our database, becomes slower and more resource-intensive as the number of members increases. Despite our attempt to alleviate high database usage by dividing the process into multiple batches and running several cron jobs, we were still experiencing significant surges each time a cron job runs. So our only viable solution was vertical scaling&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; of the database. In other words, we had a serious bottleneck in our system.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/database-qps-membership-renewals.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Database queries per second during membership renewals at night.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Concurrency&lt;/strong&gt;&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Picture this - A user tries to cancel their membership in the middle of the auto-renewal process, and voila, we have what we call a “zombie” state where the membership is both cancelled and renewed. This situation happens due to the limitations of our 5-minute Redis lock. If the renewal process holding the lock doesn’t complete within the timeout, the lock is released, enabling the cancel process to obtain the lock and run concurrently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resiliency&lt;/strong&gt;&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;What happens when the Rewards service faces an outage? The user buys a membership but doesn’t receive the rewards. It’s like throwing a party but the guests never arrive. We had three issues here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the event where upstream services had an outage, we relied on SQS’s maximum number of retries without exponential backoff&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, causing potential overloads on recovering services.&lt;/li&gt;
  &lt;li&gt;Our cron job being housed within the service itself was susceptible to interruptions during outages or service restarts.&lt;/li&gt;
  &lt;li&gt;Over time, the logic to transition between states in our state machine became complex and multi-responsibility as more states were added. This made our retry mechanism unreliable due to potential risks of double charging or double awarding users. Which leads us to our fourth culprit.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Idempotency&lt;/strong&gt;&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Even when some steps could be retried, our system lacked idempotency guarantees - a safety net to ensure that a step could be repeated without unintended side effects. Although our critical upstream systems like Payments and Rewards support idempotency via idempotency keys, our service wasn’t originally designed with this in mind.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Users could be stuck in a state where the payment succeeded but they didn’t receive their benefits or received them twice, requiring manual intervention from engineers.&lt;/li&gt;
  &lt;li&gt;We were not able to auto-retry membership renewals if the cron job, database, or any service had an outage.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/idempotency-issue-old-system-design.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Example of Idempotency issue in our old system design. If a single task fails in a state handler, the whole step would be retried which could lead to a double awarding.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For example, consider a state handler “BenefitsAwarding” that follows these steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate an idempotency key.&lt;/li&gt;
  &lt;li&gt;Calls Reward service to award the first set of benefits to the subscriber using the key.&lt;/li&gt;
  &lt;li&gt;Calls Reward service to award the second set of benefits to the subscriber using the key.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If step 3 fails due to an outage, and the step is retried and re-queued in SQS, it would restart from step 1. This generates a new idempotency key, meaning the Reward system wouldn’t recognize the retry and will award Benefits1 twice. One way to fix this with our current design is to substantially increase the number of states in our SQS state machine, to isolate tasks further rather than handling too much logic in a state handler. However, that would mean having hundreds of states making the whole process difficult to maintain.&lt;/p&gt;

&lt;p&gt;Ultimately, most incidents traced back to one fundamental issue: Our systems were relying on a sequential process that couldn’t be easily replayed if any incident or disturbance happened during execution. We were placing all our bets on the happy path, a risky gamble indeed.&lt;/p&gt;

&lt;h2 id=&quot;the-solution-migrating-our-system-to-temporal&quot;&gt;The Solution: Migrating our system to Temporal&lt;/h2&gt;

&lt;p&gt;Armed with a clear understanding of the problems and their impacts, we set out to explore potential solutions. This journey led us to consider &lt;strong&gt;refactoring our existing system&lt;/strong&gt; or &lt;strong&gt;migrating to a new architecture&lt;/strong&gt; that another team introduced to us: &lt;strong&gt;Temporal&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;enter-temporal&quot;&gt;Enter Temporal&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://temporal.io/&quot;&gt;Temporal&lt;/a&gt; is an open-source workflow orchestration engine. Think of it as a more robust and battle-tested implementation of our previous SQS architecture. It’s designed to run millions of &lt;a href=&quot;https://docs.temporal.io/workflows&quot;&gt;workflows&lt;/a&gt; concurrently and can recover/resume the state of a workflow execution at the exact point of failure even in the event of an outage. It has features like &lt;a href=&quot;https://docs.temporal.io/encyclopedia/retry-policies&quot;&gt;infinite retries&lt;/a&gt;, &lt;a href=&quot;https://docs.temporal.io/encyclopedia/retry-policies#backoff-coefficient&quot;&gt;exponential backoff&lt;/a&gt;, &lt;a href=&quot;https://docs.temporal.io/cloud/nexus/operations#rate-limiting&quot;&gt;rate limiting&lt;/a&gt;, and observability out of the box. This sounded exactly like what we needed! By using Temporal, we could offload the complexity of managing state transitions, retries, and task concurrency, allowing us to focus on our core business logic.&lt;/p&gt;

&lt;p&gt;In order to make the right decision, we meticulously assessed our options over the following criteria:  scalability&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, reliability&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;, resiliency&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;, performance, development effort, cost, security, flexibility&lt;sup id=&quot;fnref:13&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;, and testability&lt;sup id=&quot;fnref:14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;. We realised that most of what we needed to build to compensate for our system design gaps was already built into Temporal. Let’s have a sneak peek on how the architecture looks and how it solves all four major culprits we discussed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/grabunlimited-new-system-design-architecture.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. GrabUnlimited new system design architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;fixing-our-architecture-culprits&quot;&gt;Fixing our architecture culprits&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with the easiest fix, remember our old cron job for membership renewals? We replaced it with &lt;a href=&quot;https://docs.temporal.io/develop/go/timers&quot;&gt;Timer&lt;/a&gt; which allows a workflow to sleep and automatically wake up. Instead of renewing membership by batches, they are now renewed throughout the entire day based on the hour and minute when the user subscribed. What does this mean for us? We no longer need to fetch memberships from our database to trigger renewals. The workflow will resume at the due date to process the renewal, eliminating the database as a bottleneck.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/total-qps-before-after-temporal.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Total queries per second (QPS) on database before and after the migration to Temporal.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Concurrency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our legacy Redis lock mechanism was clearly not enough. However, with Temporal, we have alternative solutions to avoid race conditions. What happens if a user tries to cancel while the membership renewal workflow is being triggered? Temporal allows us to assign the &lt;a href=&quot;https://docs.temporal.io/workflows#workflow-id-reuse-policy&quot;&gt;same workflow ID&lt;/a&gt; to multiple workflows running mutually exclusive operations, ensuring only one operation runs at a time. Basically, we assigned the same workflow ID to both cancellation and renewal workflows, either cancellation happens first, removing the need to renew the consumer membership, or renewal takes the lead, and cancellation only happens after.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/total-corrupted-membership-states.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Total corrupted membership states (zombies) manually handled by engineers significantly decreased during our migration which started in February.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Resiliency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Out of the box, Temporal allowed us to put in place a few key &lt;a href=&quot;https://docs.temporal.io/encyclopedia/retry-policies&quot;&gt;resilience mechanisms&lt;/a&gt; like exponential backoff and infinite retry which was a key gap in our previous SQS architecture. That was great because we didn’t have to implement these mechanisms on our own and it meant that when calling key upstream services like Payment, we were able to precisely set our retry policies without overwhelming the service in case of an outage on their end.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Idempotency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Remember our fourth culprit from above? Our state handlers with SQS were performing too many tasks simultaneously, which made it risky to trust the retry process. This multi-responsibility nature introduced significant risks, including potential database corruption, double charging, and double awarding of benefits. Further breaking down these steps would result in hundreds of intermediary steps, each requiring careful maintenance and correct sequencing. With Temporal, you can imagine a membership as an ever-running workflow consisting of a sequence of steps that are automatically managed and retried in case of failures.&lt;/p&gt;

&lt;p&gt;While this approach didn’t directly resolve idempotency issues, it made the system and the code more readable and allowed us to &lt;a href=&quot;https://docs.temporal.io/activities#idempotency&quot;&gt;design steps with single responsibilities&lt;/a&gt;. This, in turn, made it simpler for us to develop and ensure these steps were idempotent.&lt;/p&gt;

&lt;p&gt;Let’s take a look at our previous example with Temporal.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/temporal-workflow.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. Temporal workflow: If a single task fails, only that task is retried.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Let’s consider the same use case where a member needs to receive their benefits. The tasks remain the same except we don’t need to persist the idempotency key as it will be in the Temporal workflow state instead.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate idempotency keys.&lt;/li&gt;
  &lt;li&gt;Calls Reward service to award the first set of benefits to the subscriber using the key abc1.&lt;/li&gt;
  &lt;li&gt;Calls Reward service to award the second set of benefits to the subscriber using the second key xyz1.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If the “AssignBenefits2” step fails, and the process is retried by Temporal, it will restart directly from that step, thus preventing the double awarding we were experiencing with SQS. Thanks to this approach, we largely improved idempotency and resiliency in our system, which also led to great results in decreasing user reported incidents.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/turbocharging-grabunlimited-with-temporal/total-opi-related-to-membership.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. Total open production incidents reported by users related to membership issues from January to October 2024.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;embracing-temporal-challenges-and-mindset-shift&quot;&gt;Embracing Temporal: Challenges and mindset shift&lt;/h2&gt;

&lt;p&gt;Transitioning to Temporal was quite a paradigm shift for our team. Rather than managing SQS state transitions, we could now focus on our core business logic while Temporal handled the complexities of state management, error handling, and retries. This change allowed us to streamline development, making our processes more intuitive.&lt;/p&gt;

&lt;p&gt;However, this shift wasn’t without its challenges. Temporal features such as Workflow and Activity design, deterministic execution, and built-in retry mechanisms required a steep learning curve. We had to quickly adapt to Temporal’s new way of thinking, and while it took some time to master these tools, they ultimately led to a more robust and scalable system. The transition to Temporal brought not only technical improvements but also a new mindset for solving problems efficiently.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways-and-conclusion&quot;&gt;Key takeaways and conclusion&lt;/h2&gt;

&lt;p&gt;After a thorough analysis, we decided to transition our architecture to Temporal, as it outperformed on nearly every evaluation criteria. Here are the key takeaways from our experience:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Understand the problem, fix it for the future&lt;/strong&gt;: Migrating legacy systems requires more than just patching up issues; it demands a deep dive into the root causes. For us, that meant addressing challenges in scalability, resiliency, and concurrency head-on to prevent future headaches.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Focusing on what matters&lt;/strong&gt;: By adopting Temporal workflow orchestration, we could shift our focus to what really counts, core business logic. The result? An 80% reduction in production incidents and a much smoother post-migration experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resilience and flexibility at scale&lt;/strong&gt;: Temporal provided the infrastructure we needed to handle millions of subscribers with more robust processes for retries, idempotency, and state management. These features played a key role in ensuring the system remained stable and flexible as our user base grew.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The learning curve pays off&lt;/strong&gt;: Every system migration has its challenges, but the payoff was transformative. Despite the initial hiccups, moving to Temporal allowed us to scale GrabUnlimited seamlessly while significantly improving both our development processes and the overall user experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stay tuned for Part 2, where we dive into the challenges of the migration and the lessons learned along the way. How did we seamlessly migrate millions of users to this new architecture without disrupting their memberships? How did we implement Temporal without pausing development for months? And what roadblocks did we encounter as we scaled this solution to all our users? We’ll answer these questions and more in the next post.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Nothing would have been possible without the unwavering support of Abegail Nato Alcantara, Andrys Silalahi, Pavel Sidlo, and Renu Yadav.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h3 id=&quot;definition-of-terms&quot;&gt;Definition of terms&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Temporal: Temporal is an open-source workflow orchestration platform. It allows developers to build scalable and reliable applications using familiar development patterns and easy-to-use tools. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Cron job: A cron job is a time-based job scheduler in Unix-like operating systems. Users can schedule jobs (commands or scripts) to run periodically at fixed times, dates, or intervals. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;State machine: A state machine is a behavioural model used in computer science. It represents a system in terms of states and transitions between those states. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Redis lock mechanism: Redis is an in-memory data structure store that can be used as a database, cache, and message broker. A Redis lock mechanism is a way to ensure that only one computer in a distributed network can process a certain piece of code at a time. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Vertical scaling: also known as “scaling up”, is the process of adding more resources (such as memory, CPUs, or storage) to an existing server or database to enhance its performance and capacity. Which is different from Horizontal scaling, also known as “scaling out”, the process of adding more servers or nodes to a system to handle increased load. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Concurrency: In computing, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Resiliency: refers to the ability of a system or application to quickly recover from failures and continue its intended operation without significant interruption. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Exponential backoff: Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate. In the context of the article, it refers to a strategy for retrying failed tasks with increasing wait times between retries. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Idempotency: An operation is idempotent if the result of performing it once is exactly the same as the result of performing it repeatedly without any intervening actions. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Scalability: The ability of a system to handle increased workload or demand by adding resources. &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Reliability: The capacity of a system to consistently perform its intended functions without failure. &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Resiliency: The ability of a system to recover quickly and effectively from failures or disruptions, ensuring continuity of service. &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Flexibility: The architecture should be flexible enough to accommodate future changes in requirements. &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Testability: The architecture should allow for effective testing to ensure the system works as expected. &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 12 Dec 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/turbocharging-grabunlimited-with-temporal</link>
        <guid isPermaLink="true">https://engineering.grab.com/turbocharging-grabunlimited-with-temporal</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Product</category>
        
        <category>Database</category>
        
        <category>Scalability</category>
        
        
        <category>Engineering</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>How we seamlessly migrated high volume real-time streaming traffic from one service to another with zero data loss and duplication</title>
        <description>&lt;p&gt;At Grab, we continuously enhance our systems to improve scalability, reliability and cost-efficiency. Recently, we undertook a project to split the read and write functionalities of one of our backend services into separate services. This was motivated by the need to independently scale these operations based on their distinct scalability requirements.&lt;/p&gt;

&lt;p&gt;In this post, we will dive deep into how we migrated the stream processing (write) functionality to a new service with zero data loss and duplication. This was accomplished while handling a high volume of real-time traffic averaging 20,000 reads per second from 16 source Kafka streams writing to other output streams and several DynamoDB tables.&lt;/p&gt;

&lt;h2 id=&quot;migration-challenges-and-strategy&quot;&gt;Migration challenges and strategy&lt;/h2&gt;

&lt;p&gt;Migrating the stream processing to the new service while ensuring zero data loss and duplication posed some interesting challenges, especially given the high volume of real-time data. We needed a strategy that would enable us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Migrate streams one by one gradually.&lt;/li&gt;
  &lt;li&gt;Validate the new service’s processing in production before fully switching over.&lt;/li&gt;
  &lt;li&gt;Perform the switchover with no downtime or data inconsistencies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We considered various options for the switchover such as using feature flags via our unified config management and experimental rollout platform. However, these approaches had some limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There could be some data loss or duplication during the deployment time when toggling the flags, which can be up to a few minutes.&lt;/li&gt;
  &lt;li&gt;There might be data inconsistencies as the flag value could be updated on the services (the existing and and the new one) at slightly different times.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ultimately, we decided on a custom time-based switchover logic implemented in shared code between the two services leveraging our monorepo structure. In the following sections, we will walk you through the steps we took to achieve this seamless migration.&lt;/p&gt;

&lt;h2 id=&quot;step-1-preparation&quot;&gt;Step 1: Preparation&lt;/h2&gt;

&lt;p&gt;First, since both the existing and new services reside in our monorepo, we moved the stream processing code from the existing service to a shared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/commons&lt;/code&gt; directory. This allowed both the old and new services to import and use the same code. We added logic in this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commons&lt;/code&gt; package to selectively turn stream processing on or off based on the service processing them.&lt;/p&gt;

&lt;p&gt;Next, we created temporary “sink” resources  such as streams and DynamoDB tables  for the new service to write the processed data. This allowed us to monitor and validate the new service’s behavior in production without impacting the main resources.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/seamless-migration/figure1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. For a short period, both services consumed the incoming streams, but only the old service continued to write to the actual sink resources while the new service wrote to validation sink resources.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;step-2-scheduling-the-switchover&quot;&gt;Step 2: Scheduling the switchover&lt;/h2&gt;

&lt;p&gt;In the shared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/commons&lt;/code&gt; code, we added a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;map[string]time.Time&lt;/code&gt; to schedule the switchover for each stream.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;map[string]time.Time{
  &quot;streamA&quot;: time.Date(2024, 2, 28, 12, 0, 0, 0, time.UTC),
  &quot;streamB&quot;: time.Date(2024, 3, 10, 12, 0, 0, 0, time.UTC),
  // ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When a stream is added to this map, it means it is scheduled for switchover at the specified time. This logic is shared between both services, so the switchover happens simultaneously. The new service starts writing to the main resources while the old service stops, with no overlap or gap.&lt;/p&gt;

&lt;h2 id=&quot;step-3-deployment-and-monitoring&quot;&gt;Step 3: Deployment and monitoring&lt;/h2&gt;

&lt;p&gt;To perform the switchover, we:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Updated the switchover times for the streams.&lt;/li&gt;
  &lt;li&gt;Deployed both services with enough buffer time before the scheduled switch.&lt;/li&gt;
  &lt;li&gt;Closely monitored the process by creating dedicated monitors for the migration process using our observability tools.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/seamless-migration/figure2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. This timeseries graph shows the stream received at the old and the new service (dotted line), facilitating real time monitoring of the stream processing volume across both services during the validation period.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The old service continued consuming the streams for a short monitoring period post-switchover, but without writing anywhere, ensuring no loss or duplication at the output sink resources. Then, the stream consumption was removed from the old service altogether, completing the entire migration process.&lt;/p&gt;

&lt;h2 id=&quot;results-and-learnings&quot;&gt;Results and learnings&lt;/h2&gt;

&lt;p&gt;Using this time-based approach, we were able to seamlessly migrate the high-volume stream processing to the new service with:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Zero data loss or duplication.&lt;/li&gt;
  &lt;li&gt;No downtime or production issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The whole migration, including the gradual stream-by-stream switchover, was completed in about three weeks.&lt;/p&gt;

&lt;p&gt;One learning was that such custom time-based logic, while effective for our use case, has limitations. If a rollback was needed for any of the two services for some unexpected reasons, some data inconsistency would be unavoidable. Generally, such time-based logic should be used with caution as it can lead to unexpected scenarios if the systems fall out of sync. We went ahead with this approach as it was a temporary measure and we had thoroughly tested it before carrying out the switchover.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Dec 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/seamless-migration</link>
        <guid isPermaLink="true">https://engineering.grab.com/seamless-migration</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Data streaming</category>
        
        <category>Real-time streaming</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Supercharging LLM Application Development with LLM-Kit</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we are committed to leveraging the power of technology to deliver the best services to our users and partners. As part of this commitment, we have developed the LLM-Kit, a comprehensive framework designed to supercharge the setup of production-ready Generative AI applications. This blog post will delve into the features of the LLM-Kit, the problems it solves, and the value it brings to our organisation.&lt;/p&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;p&gt;The introduction of the LLM-Kit has significantly addressed the challenges encountered in LLM application development. The involvement of sensitive data in AI applications necessitates that security remains a top priority, ensuring data safety is not compromised during AI application development.&lt;/p&gt;

&lt;p&gt;Concerns such as scalability, integration, monitoring, and standardisation are common issues that any organisation will face in their LLM and AI development efforts.&lt;/p&gt;

&lt;p&gt;The LLM-Kit has empowered Grab to pursue LLM application development and the rollout of Generative AI efficiently and effectively in the long term.&lt;/p&gt;

&lt;h2 id=&quot;introducing-the-llm-kit&quot;&gt;Introducing the LLM-Kit&lt;/h2&gt;

&lt;p&gt;The LLM-Kit is our solution to these challenges. Since the introduction of the LLM Kit, it has helped onboard hundreds of GenAI applications at Grab and has become the de facto choice for developers. It is a comprehensive framework designed to supercharge the setup of production-ready LLM applications. The LLM-Kit provides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pre-configured structure&lt;/strong&gt;: The LLM-Kit comes with a pre-configured structure containing an API server, configuration management, a sample LLM Agent, and tests.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Integrated tech stack&lt;/strong&gt;: The LLM-Kit integrates with Poetry, Gunicorn, FastAPI, LangChain, LangSmith, Hashicorp Vault, Amazon EKS, and Gitlab CI pipelines to provide a robust and end-to-end tech stack for LLM application development.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability&lt;/strong&gt;: The LLM-Kit features built-in observability with Datadog integration and LangSmith, enabling real-time monitoring of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Config &amp;amp; secret management&lt;/strong&gt;: The LLM-Kit utilises Python’s configparser and Vault for efficient configuration and secret management.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt;: The LLM-Kit provides built-in OpenID Connect (OIDC) auth helpers for authentication to Grab’s internal services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;API documentation&lt;/strong&gt;: The LLM-Kit features comprehensive API documentation using Swagger and Redoc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Redis &amp;amp; vector databases integration&lt;/strong&gt;: The LLM-Kit integrates with Redis and Vector databases for efficient data storage and retrieval.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment pipeline&lt;/strong&gt;: The LLM-Kit provides a deployment pipeline for staging and production environments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluations&lt;/strong&gt;: The LLM-Kit seamlessly integrates with LangSmith, utilising its robust evaluations framework to ensure the quality and performance of the LLM applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to these features, the team has also included a cookbook with many commonly used examples within the organisation providing a valuable resource for developers. Our cookbook includes a diverse range of examples, such as persistent memory agents, Slackbot LLM agents, image analysers and full-stack chatbots with user interfaces, showcasing the versatility of the LLM-Kit.&lt;/p&gt;

&lt;h2 id=&quot;the-value-of-the-llm-kit&quot;&gt;The value of the LLM-Kit&lt;/h2&gt;

&lt;p&gt;The LLM-Kit brings significant value to our teams at Grab:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Increased development velocity&lt;/strong&gt;: By providing a pre-configured structure and integrated tech stack, the LLM-Kit accelerates the development of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improved observability&lt;/strong&gt;: With built-in LangSmith and Datadog integration, teams can monitor their LLM applications in real-time, enabling faster issue detection and resolution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhanced security&lt;/strong&gt;: The LLM-Kit’s built-in OIDC auth helpers and secret management using Vault ensure the secure development and deployment of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficient data management&lt;/strong&gt;: The integration with Vector databases facilitates efficient data storage and retrieval, crucial for the performance of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Standardisation&lt;/strong&gt;: The LLM-Kit provides a paved-road framework for building LLM applications, promoting best practices and standardisation across teams.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Through the LLM-Kit, we can save an estimate of 1.5 weeks before teams start working on their first feature.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Project development process before LLM-Kit&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Project development process after LLM-Kit&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;architecture-design-and-technical-implementation&quot;&gt;Architecture design and technical implementation&lt;/h2&gt;

&lt;p&gt;The LLM-Kit is designed with a modular architecture that promotes scalability, flexibility, and ease of use.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. LLM-Kit modules&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;automated-steps&quot;&gt;Automated steps&lt;/h3&gt;

&lt;p&gt;To better illustrate the technical implementation of the LLM-Kit, let’s take a look at figure 4 which outlines the step-by-step process of how an LLM application is generated with the LLM-Kit:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Process of generating LLM apps using LLM-Kit&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The process begins when an engineer submits a form with the application name and other relevant details. This triggers the creation of a GitLab project, followed by the generation of a code scaffold specifically designed for the LLM application. GitLab CI files are then generated within the same repository to handle continuous integration and deployment tasks. The process continues with the creation of staging infrastructure, including components like Elastic Container Registry (ECR) and Elastic Kubernetes Service (EKS). Additionally, a Terraform folder is created to provision the necessary infrastructure, eventually leading to the deployment of production infrastructure. At the end of the pipeline, a GPT token is pushed to a secure Vault path, and the engineer is notified upon the successful completion of the pipeline.&lt;/p&gt;

&lt;h3 id=&quot;scaffold-code-structure&quot;&gt;Scaffold code structure&lt;/h3&gt;

&lt;p&gt;The scaffolded code is broken down into multiple folders:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Agents&lt;/strong&gt;: Contains the code to initialise an agent. We have gone ahead with LangChain as the agent framework; essentially the entry point for the endpoint defined in the Routes folder.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Auth&lt;/strong&gt;: Authentication and authorisation module for executing some of the APIs within Grab.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt;: Includes extracting all configurations (i.e. GPT token) and secret decryption for running the LLM application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Models&lt;/strong&gt;: Used to define the structure for the core LLM APIs within Grab.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Routes:&lt;/strong&gt; REST API endpoint definitions for the LLM Applications. It comes with health check, authentication, authorisation, and a simple agent by default.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Includes connectivity with PGVector, our managed vector database within Grab and database schemas.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Functions which are used as tools for the LLM Agent.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tracing&lt;/strong&gt;: Integration with our tracing and monitoring tools to monitor various metrics for a production application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Utils&lt;/strong&gt;: Default folder for utility functions.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Scaffold code structure&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;infrastructure-provisioning-and-deployment&quot;&gt;Infrastructure provisioning and deployment&lt;/h3&gt;

&lt;p&gt;Within the same codebase, we have integrated a comprehensive pipeline that automatically scaffolds the necessary code for infrastructure provisioning, deployment, and build processes. Using Terraform, the pipeline provisions the required infrastructure seamlessly. The deployment pipelines are defined in the .gitlab-ci.yml file, ensuring smooth and automated deployments. Additionally, the build process is specified in the Dockerfile, allowing for consistent builds. This automated scaffolding streamlines the development workflow, enabling developers to focus on writing business logic without worrying about the underlying infrastructure and deployment complexities.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Pipeline infrastructure &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;rag-scaffolding&quot;&gt;RAG scaffolding&lt;/h3&gt;

&lt;p&gt;At Grab, we’ve established a streamlined process for setting up a vector database (PGVector) and whitelisting the service using the LLM-Kit. Once the form (figure 7) is submitted, you can access the credentials and database host path. The secrets will be automatically added to the Vault path. Engineers will then only need to include the DB host path in the configuration file of the scaffolded LLM-Kit application.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-7.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Form submitted to access credentials and database host path&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The LLM-Kit is a testament to Grab’s commitment to fostering innovation and growth in AI and ML. By addressing the challenges faced by our teams and providing a comprehensive, scalable, and flexible framework for LLM application development, the LLM-Kit is paving the way for the next generation of AI applications at Grab.&lt;/p&gt;

&lt;h2 id=&quot;growth-and-future-plans&quot;&gt;Growth and future plans&lt;/h2&gt;

&lt;p&gt;Looking ahead, the LLM-Kit team aims to significantly enhance the web server’s concurrency and scalability while providing reliable and easy-to-use SDKs. The team plans to offer reusable and composable LLM SDKs, including evaluation and guardrails frameworks, to enable service owners to build feature-rich Generative AI programs with ease. Key initiatives also include the development of a CLI for version updates and dev tooling, as well as a polling-based agent serving function. These advancements are designed to drive innovation and efficiency within the organisation, ultimately providing a more seamless and efficient development experience for engineers.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to acknowledge and thank Pak Zan Tan, Han Su, and Jonathan Ku from the Yoshi team and Chen Fei Lee from the MEKS team for their contribution to this project under the leadership of Padarn George Wilson.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Nov 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/supercharging-llm-application-development-with-llm-kit</link>
        <guid isPermaLink="true">https://engineering.grab.com/supercharging-llm-application-development-with-llm-kit</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we reduced initialisation time of Product Configuration Management SDK</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;GrabX serves as Grab’s central platform for product configuration management. GrabX client services read product configurations through an SDK. This SDK reads the configurations in a way that’s eventually consistent, meaning it takes about a minute for any configuration updates to reach the client SDKs.&lt;/p&gt;

&lt;p&gt;However, some GrabX SDK clients, particularly those that need to read larger configuration data (~400 MB), reported that the SDK takes an extended amount of time to initialise, approximately four minutes. This blog post details how we analysed and addressed this issue.&lt;/p&gt;

&lt;h2 id=&quot;sdk-observations&quot;&gt;SDK Observations&lt;/h2&gt;

&lt;p&gt;GrabX clients have observed that the GrabX SDK requires several minutes to initialise. This results in what is known as ‘cold starts’, where the SDK takes an extended time to begin supporting the reading of configurations at startup. This challenge highlights the importance of efficient SDK start-up management, especially when a service handling a high volume of incoming traffic initiates new SDK instances to manage the load better. However, due to the extended SDK initialisation time, these instances continue to experience stress, potentially leading to service throttling.&lt;/p&gt;

&lt;h2 id=&quot;sdk-initialisation-workflow&quot;&gt;SDK Initialisation Workflow&lt;/h2&gt;

&lt;p&gt;The SDK initialisation flow described below is based on the improvements we proposed to the SDK design in &lt;a href=&quot;reduced-memory-cpu-usage-grabx-sdk/&quot;&gt;our previous post&lt;/a&gt;. In that post, we suggested enhancing the SDK design by:&lt;/p&gt;

&lt;p&gt;A. Implementing service-based data partitioning and storage in the AWS S3 bucket
B. Allowing service-based subscription of data for the SDK&lt;/p&gt;

&lt;p&gt;The following diagram provides a high-level overview of the initialisation process of the GrabX SDK, which can be divided into the following sequential steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set options that drive the behaviour of the SDK.&lt;/li&gt;
  &lt;li&gt;Initialise dependent module clients.&lt;/li&gt;
  &lt;li&gt;Initialise the GrabX client. (Highlighted as A in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download data for the SDK’s subscribed list of services from the AWS S3 bucket and store this data on the SDK instance disk. (Highlighted as B in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download common data needed by the SDK from the AWS S3 bucket and store this data on the SDK instance disk. This data is referred to as ‘common’ because it is required by all different client services.  (Highlighted as C in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download data for the SDK’s subscribed list of services from the AWS S3 bucket and load this data into the SDK instance memory.  (Highlighted as D in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download common data needed by the SDK from the AWS S3 bucket and load this data into the SDK instance memory. (Highlighted as E in the diagram below)&lt;/li&gt;
  &lt;li&gt;Initialise dependent modules for resolving the configuration value.  (Highlighted as F in the diagram below)&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image1.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;proposed-solution&quot;&gt;Proposed Solution&lt;/h2&gt;

&lt;p&gt;In order to address the issue of extended SDK initialisation time, we have decided to enhance the SDK initialisation design in multiple phases. Each phase focused on improving a specific part of the workflow.&lt;/p&gt;

&lt;h3 id=&quot;improvement-phase-1&quot;&gt;Improvement Phase 1&lt;/h3&gt;

&lt;p&gt;As discussed in the previous section, the GrabX SDK needs to load two separate sets of data: the subscribed services data and the common data. These two data sets are currently downloaded from the AWS S3 bucket and sequentially loaded into disk and memory.&lt;/p&gt;

&lt;p&gt;In the first phase of our improvement plan, we decided to change the sequential data load to a concurrent data load for these two data sets, as illustrated in the following diagram. This alteration in the SDK initialisation workflow reduced the initialisation time by approximately 80%.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;improvement-phase-2&quot;&gt;Improvement Phase 2&lt;/h3&gt;

&lt;p&gt;Building on the progress made in Phase 1, we next turned our attention to the issue of large configuration file sizes. As mentioned in the introduction, the extended SDK initialisation time was particularly noticeable for client services that needed to load larger amounts of data.&lt;/p&gt;

&lt;p&gt;In this phase, we decided to implement an SDK design change that allows the SDK to &lt;strong&gt;concurrently&lt;/strong&gt; download data from the AWS S3 bucket and load it into memory for all these large configurations within a subscribed service, as illustrated in the following diagram. This modification to the SDK initialisation workflow further reduced the initialisation time by approximately &lt;strong&gt;6%&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;improvement-phase-3&quot;&gt;Improvement Phase 3&lt;/h3&gt;

&lt;p&gt;Upon examining the SDK’s behaviour, we observed that the SDK is both persisting configuration data downloaded from the AWS S3 bucket to disk and loading the data into memory. We understand that the data is loaded into memory to reduce the latency of configuration reads. The data is stored on disk to support a fallback mechanism, which is activated in a very specific use case: when the client SDK instance restarts and there is a connectivity issue with AWS S3 for downloading configuration files. In this scenario, the SDK will read the configuration data stored on disk. However, this data could be outdated as it is not freshly downloaded from the AWS S3 bucket, and most client services require the most recent data.&lt;/p&gt;

&lt;p&gt;Therefore, we realised that the fallback mechanism, for which data is persisted on disk, actually conflicts with the desired SDK behaviour for most client services. As a result, we decided to eliminate the SDK initialisation step that downloads configuration data from AWS S3 and persists it on disk. If the SDK initialisation fails to connect to the AWS S3 bucket and download data, client services can then take the necessary action, such as retrying initialisation. This modification further reduced the initialisation time by approximately 50% compared to the improvement achieved in Phase 2.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We benchmarked the proposed solution with a variety of services, each having different configuration data sizes. Our findings suggest that the proposed solution has the potential to reduce initialisation time by up to &lt;strong&gt;90%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The following chart illustrates the phase-wise reduction in initialisation time achieved through the improvements made to the GrabX SDK.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Nov 2024 00:00:01 +0000</pubDate>
        <link>https://engineering.grab.com/how-we-reduced-grabx-sdk-initialisation-time</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-we-reduced-grabx-sdk-initialisation-time</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Metasense V2: Enhancing, improving and productionisation of LLM powered data governance</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the initial article, &lt;a href=&quot;https://engineering.grab.com/llm-powered-data-classification&quot;&gt;LLM Powered Data Classification&lt;/a&gt;, we addressed how we integrated Large Language Models (LLM) to automate governance-related metadata generation. The LLM integration enabled us to resolve challenges in Gemini, such as restrictions on the customisation of machine learning classifiers and limitations of resources to train a customised model. Gemini is a metadata generation service built internally to automate the tag generation process using a third-party data classification service. We also focused on LLM-powered column-level tag classifications. The classified tags, combined with Grab’s data privacy rules, allowed us to determine sensitivity tiers of data entities. The affordability of the model also enables us to scale it to cover more data entities in the company. The initial model scanned more than 20,000 data entries, at an average of 300-400 entities per day. Despite its remarkable performance, we were aware that there was room for improvement in the areas of data classification and prompt evaluation.&lt;/p&gt;

&lt;h2 id=&quot;improving-the-model-post-rollout&quot;&gt;Improving the model post-rollout&lt;/h2&gt;

&lt;p&gt;Since its launch in early 2024, our model has gradually grown to cover the entire data lake. To date, the vast majority of our data lake tables have undergone analysis and classification by our model. This has significantly reduced the workload for Grabbers. Instead of manually classifying all new or existing tables, Grabbers can now rely on our model to assign the appropriate classification tier accurately.&lt;/p&gt;

&lt;p&gt;Despite table classification being automated, the data pipeline still requires owners to manually perform verification to prevent any misclassifications. While it is impossible to entirely eliminate human oversight from critical machine learning workflows, the team has dedicated substantial time post-launch to refining the model, thereby safely minimising the need for human intervention.&lt;/p&gt;

&lt;h3 id=&quot;utilising-post-rollout-data&quot;&gt;Utilising post-rollout data&lt;/h3&gt;

&lt;p&gt;Following the deployment of our model and receipt of extensive feedback from table owners, we have accumulated a large dataset to further enhance the model. This data, coupled with the dataset of manual classifications from the Data Governance Office to ensure compliance with information classification protocols, serves as the training and testing datasets for the second iteration of our model.&lt;/p&gt;

&lt;h3 id=&quot;model-improvements-with-prompt-engineering&quot;&gt;Model improvements with prompt engineering&lt;/h3&gt;

&lt;p&gt;Expanding the evaluation and testing data allowed us to uncover weaknesses in the previous model. For instance, we discovered that seemingly innocuous table columns like “business email” could contain entries with Personal Identifiable Information (PII) data.&lt;/p&gt;

&lt;p&gt;An example of this would be a business that uses a personal email address containing a legal name—a discrepancy that would be challenging for even human reviewers to detect. Additionally, we discovered nested JSON structures occasionally included personal names, phone numbers, and email addresses hidden among other non-PII metadata. Lastly, we identified passenger communications with Grab occasionally mentioning legal names, phone numbers, and other PII, despite most of the content being non-PII.&lt;/p&gt;

&lt;p&gt;Ultimately, we hypothesised the model’s main issue was model capacity. The model displayed difficulty focusing on large data samples containing a mixture of PII and non-PII data despite having a good understanding of what constitutes PII. Just like humans, when given high volumes of tasks to work on simultaneously, the model’s effectiveness is reduced. In the original model, 13 out of 21 tags were aimed at distinguishing different types of non-PII data. This took up significant model capacity and distracted the model from its actual task: identifying PII data.&lt;/p&gt;

&lt;p&gt;To prevent the model from being overwhelmed, large tasks are divided into smaller, more manageable tasks, allowing the model to dedicate more attention to each task. The following measures were taken to free up model capacity:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Splitting the model into two parts to make problem solving more manageable.
    &lt;ul&gt;
      &lt;li&gt;One part for adding PII tags.&lt;/li&gt;
      &lt;li&gt;Another part for adding all other types of tags.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reducing the number of tags for the first part from 21 to 8 by removing all non-PII tags. This simplifies the task of differentiating types of data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using clear and concise language, removing unnecessary detail. This was done by reducing word count in prompt from 1,254 to 737 words for better data analysis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Splitting tables with more than 150 columns into smaller tables. Fewer table rows means that the LLM has sufficient capacity to focus on each column.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;enabling-rapid-prompt-experimentation-and-deployment&quot;&gt;Enabling rapid prompt experimentation and deployment&lt;/h3&gt;

&lt;p&gt;In our quest to facilitate swift experimentation with various prompt versions, we have empowered a diverse team of data scientists and engineers to work together effectively on the prompts and service. This has been made possible by upgrading our model architecture to incorporate the LangChain and LangSmith frameworks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt; introduces a novel framework that streamlines the process from raw input to the desired outcome by chaining interoperable components. &lt;strong&gt;LangSmith&lt;/strong&gt;, on the other hand, is a unified DevOps platform that fosters collaboration among various team members and developers, including product managers, data scientists, and software engineers. It simplifies the processes of development, collaboration, testing, deployment, and monitoring for all involved.&lt;/p&gt;

&lt;p&gt;Our new backend leverages LangChain to construct an updated model that supports classification tasks for both non-PII and PII tagging. Integration with LangSmith enables data scientists to directly develop prompt templates and conduct experiments via the LangSmith user interface. In addition, managing the evaluation dataset on LangSmith provides a clear view of the performance of prompts across multiple custom metrics.&lt;/p&gt;

&lt;p&gt;The integration of LangChain and LangSmith has significantly improved our model architecture, fostering collaboration and continuous improvement. This has not only streamlined our processes but also enhanced the transparency of our performance metrics. By harnessing the power of these innovative tools, we are better equipped to deliver high-quality, efficient solutions.&lt;/p&gt;

&lt;p&gt;The benefits of the LangChain and LangSmith framework enhancements in Metasense are summarised as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streamlined prompt optimisation process.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Data scientists can create, update, and evaluate prompts directly on the LangSmith user interface and save them in commit mode. For rapid deployment, the prompt identifier in service configurations can be easily adjusted.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/metasensev2/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Streamlined prompt optimisation process.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Transparent prompt performance metrics.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LangSmith’s capabilities allow us to effortlessly run evaluations on a dataset and obtain performance metrics across multiple dimensions, such as accuracy, latency, and error rate.&lt;/p&gt;

&lt;h3 id=&quot;assuring-quality-in-perpetuity&quot;&gt;Assuring quality in perpetuity&lt;/h3&gt;

&lt;p&gt;With exceptionally low misclassification rates recorded, table owners can place greater trust in the model’s outputs and spend less time reviewing them. Nevertheless, as a prudent safety measure, we have set up alerts to monitor misclassification rates periodically, sounding an internal alarm if the rate crosses a defined threshold. A model improvement protocol has also been set in place for such alarms.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The integration of LLM into our metadata generation process has significantly improved our data classification capabilities, reducing manual workloads and increasing accuracy. Continuous improvements, including the adoption of LangChain and LangSmith frameworks, have streamlined prompt optimisation and enhanced collaboration among our team. With low misclassification rates and robust safety measures, our system is both reliable and scalable, fostering trust and efficiency. In conclusion, these advancements ensure we remain at the forefront of data governance, delivering high-quality solutions and valuable insights to our stakeholders.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to express our sincere gratitude to Infocomm Media Development Authority (IMDA) for supporting this initative.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Nov 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/metasense-v2</link>
        <guid isPermaLink="true">https://engineering.grab.com/metasense-v2</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we reduced peak memory and CPU usage of the product configuration management SDK</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;GrabX is Grab’s central platform for product configuration management. It has the capacity to control any component within Grab’s backend systems through configurations that are hosted directly on GrabX.&lt;/p&gt;

&lt;p&gt;GrabX clients read these configurations through an SDK, which reads the configurations in a way that’s asynchronous and eventually consistent. As a result, it takes about a minute for any updates to the configurations to reach the client SDKs.&lt;/p&gt;

&lt;p&gt;In this article, we discuss our analysis and the steps we took to reduce the peak memory and CPU usage of the SDK.&lt;/p&gt;

&lt;h2 id=&quot;observations-on-potential-sdk-improvements&quot;&gt;Observations on potential SDK improvements&lt;/h2&gt;

&lt;p&gt;Our GrabX clients noticed that the GrabX SDK tended to require high memory and CPU usage. From this, we saw opportunities for further improvements that could:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimise the tail latencies of client services.&lt;/li&gt;
  &lt;li&gt;Enable our clients to use their resources more effectively.&lt;/li&gt;
  &lt;li&gt;Reduce operation costs and improve the efficiency of using the GrabX SDK.&lt;/li&gt;
  &lt;li&gt;Accelerate the adoption of GrabX by Grab’s internal services.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sdk-design&quot;&gt;SDK design&lt;/h2&gt;

&lt;p&gt;At a high-level, creating, updating, and serving configuration values via the GrabX SDK involved the following process:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/previous-grabx-sdk-design.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Previous GrabX SDK design.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The process begins when GrabX clients either create or update configurations. This is done through the GrabX web portal or by making an API call.&lt;/li&gt;
  &lt;li&gt;Once the configurations are created or updated, the GrabX backend module takes over. It stores the new configuration into an SQL database table.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The GrabX backend ensures that the latest configuration data is available to client SDKs.&lt;/p&gt;

    &lt;p&gt;a. The GrabX backend checks every minute for any newly created or updated configurations.&lt;/p&gt;

    &lt;p&gt;b. If there are new or updated configurations, GrabX backend creates a new JSON file. This file contains all existing and newly created configurations. It’s important to note that all configurations across all services are stored in a single JSON file.&lt;/p&gt;

    &lt;p&gt;c. The backend module uploads this newly created JSON file to an AWS S3 bucket.&lt;/p&gt;

    &lt;p&gt;d. The backend module assigns a version number to the new JSON file and updates a text file in the AWS S3 bucket. This text file stores the latest JSON file version number. The client SDK refers to this version file to check if a newer version of the configuration data is available.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The client SDK performs a check on the version file every minute to determine if a newer version is available. This mechanism is crucial to maintain data consistency across all instances of a service. If any instance fell out of sync, it would be brought back in sync within a minute.&lt;/li&gt;
  &lt;li&gt;If a new version of the configuration JSON file is available, the client SDK downloads this new file. Following the download, it loads the configuration data into memory. Storing the configuration data in memory reduces the read latency for the configurations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;areas-of-improvement-for-existing-sdk-design&quot;&gt;Areas of improvement for existing SDK design&lt;/h2&gt;

&lt;p&gt;In this section we outline the areas of improvement we identified within the SDK design.&lt;/p&gt;

&lt;h3 id=&quot;service-based-data-partitioning&quot;&gt;Service-based data partitioning&lt;/h3&gt;

&lt;p&gt;We saw an opportunity for service-based data partitioning. The configuration data for all services was consolidated into a single JSON file. Upon studying the data read patterns of client services, we observed that most services primarily needed to access configuration data specific to their own service. However, the present design required storing configuration data for all other services. This resulted in unnecessary memory consumption.&lt;/p&gt;

&lt;h3 id=&quot;retaining-only-new-version-of-configuration-in-the-same-file&quot;&gt;Retaining only new version of configuration in the same file&lt;/h3&gt;

&lt;p&gt;By using a single JSON file for storing old and new configuration data, we saw a significant increase in the size of the JSON file.&lt;/p&gt;

&lt;p&gt;The SDK only needs the full data when it starts; the more common case is that it needs to stay updated with the latest configuration. Even in that scenario, the SDK needed to fetch a complete new JSON file every minute no matter the size of the updates. Consequently, the process of downloading, decoding, and loading high volumes of data at a high frequency (every minute) caused the client SDK to spike in memory and CPU usage.&lt;/p&gt;

&lt;h3 id=&quot;more-efficient-json-decoding&quot;&gt;More efficient JSON decoding&lt;/h3&gt;

&lt;p&gt;An additional factor which contributed to memory and CPU usage during the decoding phase was the inefficiency of the default JSON decode library to decode this large (&amp;gt;100MB) JSON file. Decoding this JSON file was heavy on available CPU resources, which tended to starve the service of its ability to handle incoming requests. This manifested as increasing the P99 latency of the service.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/graph-increased-p99-latency.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Graph illustrating the increased P99 latency due to CPU throttling for a service.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;implemented-solution&quot;&gt;Implemented solution&lt;/h2&gt;

&lt;p&gt;We proposed modifications to the existing SDK design, which we discuss in this section.&lt;/p&gt;

&lt;h3 id=&quot;partition-data-by-service&quot;&gt;Partition data by service&lt;/h3&gt;

&lt;p&gt;The proposed solution involved partitioning the data based on services. We chose this approach because a single configuration typically belonged to a single service, and most services primarily needed to read configurations that pertained to their own service.&lt;/p&gt;

&lt;p&gt;Upon analysing the distribution of service-configuration, we discovered that 98% of client services required less than 1% of the total configuration data. Despite this, they were required to maintain and reload 100% of the configuration data. Furthermore, the service with the largest number of configurations only required 20% of the total configuration data.&lt;/p&gt;

&lt;p&gt;Therefore, we proposed a shift towards service-based partitioning of configuration data. This allowed individual client services to access only the data they needed to read.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/service-config.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Graph showing the number of services with varying amounts of configurations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;create-separate-json-files-for-each-configuration&quot;&gt;Create separate JSON files for each configuration&lt;/h3&gt;

&lt;p&gt;Our proposal also included creating a separate JSON file for each configuration in a service. Previously, all data was stored in a single JSON file housed in an AWS S3 bucket, which supported a maximum of 3,500 write/update requests and 5,500 read requests per second.&lt;/p&gt;

&lt;p&gt;By storing each configuration in a separate JSON file, we were able to create a different S3 prefix for each configuration file. These S3 prefixes helped us to maximise S3 throughput by enhancing the read/write performance for each configuration. AWS S3 can handle at least 3,500 PUT/COPY/POST/DELETE requests or 5,500 GET/HEAD requests per second for each partitioned Amazon S3 prefix.&lt;/p&gt;

&lt;p&gt;Therefore, with each configuration’s data stored in a separate S3 file with a different prefix, the GrabX platform could achieve a throughput of 5,500 read requests and 3,500 write/update requests per second per configuration. This was beneficial for boosting read/write capacity when needed.&lt;/p&gt;

&lt;h3 id=&quot;implement-a-service-level-changelog&quot;&gt;Implement a service-level changelog&lt;/h3&gt;

&lt;p&gt;We proposed to create a changelog file at the service level. In other words, a changelog file was created for each service. This file was used to keep track of the latest update version, as well as previous service configuration update versions. This file also recorded the configurations which were created or updated in each version. This enables the SDK to accurately identify the configurations that were created or updated in each update version. This was useful to update the specific configurations belonging to a service on the client side.&lt;/p&gt;

&lt;h3 id=&quot;implement-service-based-sdk&quot;&gt;Implement service-based SDK&lt;/h3&gt;

&lt;p&gt;We proposed that SDK client services should be allowed to subscribe to a list of services for which they need to read configuration data. The SDK was initialised with data of the subscribed services and received updates only for configurations corresponding to the subscribed services.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/sdk-lifecycle-flowchart.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. This flowchart shows our proposed service-based SDK implementation.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The SDK only sought updates for the subscribed services. The client SDK needed to read the changelog file for each of the subscribed services, comparing the latest changelog version against the SDK version number. Whenever a newer changelog version was available, the SDK updated the variables with the latest version.&lt;/p&gt;

&lt;p&gt;This approach significantly reduced the volume of data that the SDK needed to download, decode, and load into memory during both initialisation and each subsequent update.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In summary, we identified ways to optimise CPU and memory usage in the GrabX SDK. Our analysis revealed that frequent high resource consumption hindered the wider adoption of GrabX. We proposed a series of modifications, including partitioning data by service and creating separate JSON files for each configuration.&lt;/p&gt;

&lt;p&gt;After benchmarking the proposed solution with a variety of configuration data sizes, we found that the solution has the potential to reduce memory utilisation by up to 70% and decrease the maximum CPU utilisation by more than 50%. These improvements significantly enhance the performance and scalability of the GrabX SDK.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/bar-charts-before-after.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Bar charts showcasing memory(MB) &amp;amp; CPU(%) utilisation for Service A before and after using the discussed solution.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Moving forward, we plan to continue optimising the GrabX SDK by exploring additional improvements, such as reducing its initialisation time. These efforts aim to make GrabX an even more robust and reliable solution for product configuration management within Grab’s ecosystem.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/reduced-memory-cpu-usage-grabx-sdk</link>
        <guid isPermaLink="true">https://engineering.grab.com/reduced-memory-cpu-usage-grabx-sdk</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>LLM-assisted vector similarity search</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As the complexity of data retrieval requirements continue to grow, traditional search methods often struggle to provide relevant and accurate results, especially for nuanced or conceptual queries. Vector similarity search has emerged as a powerful technique for finding semantically similar information. It refers to finding vectors in a large dataset that are most similar to a given query vector, typically using some distance or similarity measure. The concept originated in the 1960s with the work by Minsky and Papert on nearest neighbour search &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Since then, the idea has evolved substantially with modern approaches often using approximate methods to enable fast search in high-dimensional spaces, such as locality-sensitive hashing &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and graph-based indexing &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Recently, vector similarity search has become a crucial component in many machine learning and information retrieval applications. It is one of the key technologies that popularised the idea of Retrieval Augmented Generation (RAG) &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; which increased the applicability of Transformer &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; based Generative Large Language Models (LLMs) &lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; in domain-specific tasks without requiring any further training or fine-tuning. However, the effectiveness of the vector search can be limited when dealing with intricate queries or contextual nuances. For example, from a typical vector similarity search perspective, “I like fishing” and “I do not like fishing” may be quite close to each other, while in reality, they are the exact opposite. In this blog post, we discuss an approach that we experimented with that combines vector similarity search with LLMs to enhance the relevance and accuracy of search results for such complex and nuanced queries. We leverage the strengths of both techniques: vector similarity search for efficient shortlisting of potential matches, and LLMs for their ability to understand natural language queries and rank the shortlisted results based on their contextual relevance.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h2&gt;
&lt;p&gt;The proposed solution involves a two-step process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Vector similarity search: We first perform a vector similarity search on the dataset to obtain a shortlist of potential matches (e.g., top 10-50 results) for the given query. This step leverages the efficiency of vector similarity search to quickly narrow down the search space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LLM-assisted ranking: The shortlisted results from the vector similarity search are then fed into an LLM, which ranks the results based on their relevance to the original query. The LLM’s ability to understand natural language queries and contextual information helps in identifying the most relevant results from the shortlist.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By combining these two steps, we aim to achieve the best of both worlds: the efficiency of vector similarity search for initial shortlisting, and the contextual understanding and ranking capabilities of LLMs for refining the final results.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-assisted-vector-similarity-search/similarity-search.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Similarity search and the proposed LLM-assisted similarity search.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;To evaluate the effectiveness of our proposed solution, we conducted experiments on two small synthetic datasets in CSV format that we curated using GPT-4o &lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Food dataset&lt;/strong&gt;: A collection of 100 dishes with their titles and descriptions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tourist spots dataset&lt;/strong&gt;: A collection of 100 tourist spots in Asia, including their names, cities, countries, and descriptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to note that we primarily focus on performing similarity search on structured data such as description of various entities in a relational database.&lt;/p&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;Our experimental setup included a Python script for vector similarity search leveraging Facebook AI Similarity Search (FAISS) &lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, a library developed by Facebook that offers efficient similarity search, and OpenAI’s embeddings (i.e., text-embedding-ada-002) &lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; to generate the vector embeddings needed for facilitating the vector search. For our proposed solution, an LLM component (i.e., GPT-4o) was included in the setup in addition to the FAISS-based similarity search component.&lt;/p&gt;

&lt;h3 id=&quot;observations&quot;&gt;Observations&lt;/h3&gt;

&lt;p&gt;To compare the performance of the proposed approach of LLM-assisted vector similarity search as outlined in the “Proposed solution” section with the raw vector similarity search, we conducted both techniques on our two synthetic datasets. With the raw vector search, we get the top three matches for a given query. For our proposed technique, we first get a shortlist of 15 entity matches from FAISS for the same query, and supply the shortlist and the original query to LLM with some descriptive instructions in the prompt to find the top three matches from the provided shortlist.&lt;/p&gt;

&lt;p&gt;From the experiments, in simpler cases where the queries were straightforward and directly aligned with the textual content of the data, both the raw similarity search and the LLM-assisted similarity search demonstrated comparable performance. However, as the queries became more complex, involving additional constraints, negations, or conceptual requirements, the LLM-assisted search exhibited a clear advantage over the raw similarity search. The LLM’s ability to understand context and capture subtleties in the queries allowed it to filter out irrelevant results and rank the most appropriate ones higher, leading to improved accuracy.&lt;/p&gt;

&lt;p&gt;Here are a few examples where the LLM-assisted similarity search performed better:&lt;/p&gt;

&lt;h4 id=&quot;food-dataset&quot;&gt;Food dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: “food with no fish or shrimp”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- title: Tempura, description: A Japanese dish of seafood or vegetables that have been battered and deep fried.
- title: Ceviche, description: A seafood dish popular in Latin America, made from fresh raw fish cured in citrus juices.
- title: Sushi, description: A Japanese dish consisting of vinegared rice accompanied by various ingredients such as seafood and vegetables.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;LLM-assisted similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- title: Chicken Piccata, description: Chicken breasts cooked in a sauce of lemon, butter, and capers.
- title: Chicken Alfredo, description: An Italian-American dish of pasta in a creamy sauce made from butter and Parmesan cheese.
- title: Chicken Satay, description: Grilled chicken skewers served with peanut sauce.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;: The LLM correctly filtered out dishes containing fish or shrimp, while the raw similarity search failed to do so, presumably due to the presence of negation in the query.&lt;/p&gt;

&lt;h4 id=&quot;tourist-spots-dataset&quot;&gt;Tourist spots dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: “exposure to wildlife”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Ocean Park, city: Hong Kong, country: Hong Kong, description: Marine mammal park and oceanarium.
- name: Merlion Park, city: Singapore, country: Singapore, description: Iconic statue with the head of a lion and body of a fish.
- name: Manila Bay, city: Manila, country: Philippines, description: A natural harbor known for its sunset views.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;LLM-assisted similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Ocean Park, city: Hong Kong, country: Hong Kong, description: Marine mammal park and oceanarium.
- name: Chengdu Research Base, city: Chengdu, country: China, description: A research center for giant panda breeding.
- name: Mount Hua, city: Shaanxi, country: China, description: Mountain known for its dangerous hiking trails.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;: Two out of the top three matches by the LLM-assisted technique seem relevant to the query while only one result from the raw similarity search is relevant and the other two being somewhat irrelevant to the query. The LLM identified the relevance of a research base for giant panda breeding to the “exposure to wildlife”, which the raw similarity search ignored in its ranking.&lt;/p&gt;

&lt;p&gt;These examples provide a glimpse into the utility of LLMs in finding more relevant matches in scenarios where the queries involved additional context, constraints, or conceptual requirements beyond simple keyword matching. On the other hand, when the queries were more straightforward and focused on specific keywords or phrases present in the data, both approaches demonstrated comparable performance. For instance, queries like “Japanese food” or “beautiful mountains” yielded similar results from both the raw similarity search and the proposed LLM-assisted approach.&lt;/p&gt;

&lt;p&gt;Overall, the LLM-assisted vector search exhibited a clear advantage in handling complex queries, leveraging its ability to understand natural language and contextual information. However, for simpler queries, the raw similarity search remained a viable option, especially when computational efficiency is a concern.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The experiments demonstrated the potential of combining vector similarity search with LLMs to enhance the relevance and accuracy of search results, particularly for complex and nuanced queries. While vector similarity search alone can provide reasonable results for straightforward queries, the LLM-assisted approach shines when dealing with queries that require a deeper understanding of context, nuances, and conceptual relationships. By leveraging the natural language understanding capabilities of LLMs, this approach can better capture the intent behind complex queries and provide more relevant search results.&lt;/p&gt;

&lt;p&gt;Our experiment was limited to using a small volume of structured data (100 data points in each dataset) with a limited number of queries. However, we have witnessed similar enhancement in search result relevance when we deployed this solution internally within Grab for larger datasets, for example, 4500+ rows of data stored in a relational database.&lt;/p&gt;

&lt;p&gt;Nevertheless, it is important to note that the effectiveness of this approach may still depend on the quality and complexity of the data, as well as the specific use case and query patterns. We believe it is still worthwhile to evaluate the proposed approach for more diverse (e.g., beyond CSV) and larger datasets. An interesting future work can be varying the size of the shortlist from the similarity search and observing how it impacts the overall search relevance when using the proposed approach. In addition, for real world applications, the performance implications in terms of additional latency introduced by the additional LLM query must also be considered.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;M. Minsky and S. Papert, Perceptrons: An Introduction to Computational Geometry. MIT Press, 1969. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;P. Indyk and R. Motwani, “Approximate nearest neighbors: Towards removing the curse of dimensionality,” in Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, 1998. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Y. Malkov and D. Yashunin, “Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in Advances in Neural Information Processing Systems, 2020. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A. Vaswani, “Attention is all you need,” in Advances in Neural Information Processing Systems, 2017. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A. Radford, “Improving language understanding by generative pre-training,” 2018. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“Hello GPT-4o,” OpenAI, May 2024. [Online]. Available: https://openai.com/index/hello-gpt-4o/. [Accessed: Oct. 6, 2024]. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P. E. Mazaré, and H. Jégou, “The faiss library,” arXiv preprint arXiv:2401.08281, 2024. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“Embeddings,” OpenAI API. [Online]. Available: https://platform.openai.com/docs/guides/embeddings. [Accessed: Oct. 6, 2024]. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 23 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-assisted-vector-similarity-search</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-assisted-vector-similarity-search</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        <category>Experiment</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Leveraging RAG-powered LLMs for Analytical Tasks</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Retrieval-Augmented Generation (RAG) is a powerful process that is designed to integrate direct function calling to answer queries more efficiently by retrieving relevant information from a broad database. In the rapidly evolving business landscape, Data Analysts (DAs) are struggling with the growing number of data queries from stakeholders. The conventional method of manually writing and running similar queries repeatedly is time-consuming and inefficient. This is where RAG-powered Large Language Models (LLMs) step in, offering a transformative solution to streamline the analytics process and empower DAs to focus on higher value tasks.&lt;/p&gt;

&lt;p&gt;In this article, we will share how the Integrity Analytics team has built out a data solution using LLMs to help automate tedious analytical tasks like generating regular metric reports and performing fraud investigations.&lt;/p&gt;

&lt;p&gt;While LLMs are known for their proficiency in data interpretation and insight generation, they represent just a fragment of the entire solution. For a comprehensive solution, LLMs must be integrated with other essential tools. The following is required in assembling a solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Internally facing LLM tool -&lt;/strong&gt; Spellvault is a platform within Grab that stores, shares, and refines LLM prompts. It features low/no-code RAG capabilities that lower the barrier of entry for people to create LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data -&lt;/strong&gt; with real time or close to real-time latency to ensure accuracy. It has to be in a standardised format to ensure that all LLM data inputs are accurate.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduler -&lt;/strong&gt;  runs LLM applications at regular intervals. Useful for automating routine tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Messaging Tool -&lt;/strong&gt; a user interface where users can interact with LLM by entering a command to receive reports and insights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introducing-data-arks-the-data-middleware-serving-up-relevant-data-to-the-llm-agents&quot;&gt;Introducing Data-Arks, the data middleware serving up relevant data to the LLM agents&lt;/h2&gt;

&lt;p&gt;For most data use cases, DAs are usually running the same set of SQL queries with minor changes to parameters like dates, age or other filter conditions. In most instances, we already have a clear understanding of the required data and format to accomplish a task. Therefore, we need a tool that can execute the &lt;strong&gt;exact SQL query&lt;/strong&gt; and channel the data output to the LLM.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Data-Arks hosts various APIs which can be called to serve data to applications like SpellVault.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-data-arks&quot;&gt;What is Data-Arks?&lt;/h3&gt;

&lt;p&gt;Data-Arks is an in-house Python-based API platform housing several frequently used SQL queries and python functions packaged into individual APIs. Data-Arks is also integrated with Slack, Wiki, and JIRA APIs, allowing users to parse and fetch information and data from these tools as well. The benefits of Data-Arks are summarised as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration:&lt;/strong&gt; Data-Arks service allows users to upload any SQL query or Python script on the platform. These queries are then surfaced as APIs, which can be called to serve data to the LLM agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Versatility: Data-Arks&lt;/strong&gt; can be extended to everyone. Employees from various teams and functions at Grab can self-serve to upload any SQL query that they want onto the platform, allowing this tool to be used for different teams’ use cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;automating-regular-report-generation-and-summarisation-using-data-arks-and-spellvault&quot;&gt;Automating regular report generation and summarisation using Data-Arks and Spellvault&lt;/h2&gt;

&lt;p&gt;LLMs are just one piece of the puzzle, to build a comprehensive solution, they must be integrated with other tools. Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;p&gt;Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Report Summarizer uses various tools to summarise queries and deliver a summarised report through Slack.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 is an example of a summarised report generated by the Report Summarizer using dummy data.  Report Summarizer calls a Data-Arks API to generate the data in a tabular format and LLM helps summarise and generate a short paragraph of key insights. This automated report generation has helped save an estimated 3-4 hours per report.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Sample of a report generated using dummy data extracted from [https://data.gov.my/](https://data.gov.my/). &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;llm-bots-for-fraud-investigations&quot;&gt;LLM bots for fraud investigations&lt;/h2&gt;

&lt;p&gt;LLMs also excel in helping to streamline fraud investigations, as LLMs are able to contextualise several different data points and information and derive useful insights from them.&lt;/p&gt;

&lt;p&gt;Introducing &lt;strong&gt;A* bot&lt;/strong&gt;, the team’s very own LLM fraud investigation helper.&lt;/p&gt;

&lt;p&gt;A set of frequently used queries for fraud investigation is made available as Data-Arks APIs. Upon a user prompt or query, SpellVault selects the most relevant queries using RAG, executes them and provides a summary of the results to users through Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. A* bot uses Data-Arks and Spellvault to get information for fraud investigations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 shows a sample of fraud investigation responses from A* bot. Scaling to multiple queries for a fraud investigation process, what was once a time-consuming fraud investigation can now be reduced to a matter of minutes, as the A* bot is capable of providing all the necessary information simultaneously.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Sample of fraud investigation responses.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;rag-vs-fine-tuning&quot;&gt;RAG vs fine-tuning&lt;/h2&gt;

&lt;p&gt;On deciding between RAG or fine-tuning to improve LLM accuracy, three key factors tipped the scales in favour of the RAG approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Effort and cost considerations&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires significant computational cost as it involves taking a base model and further training it with smaller, domain specific data and context. RAG is computationally less expensive as it relies on retrieving only relevant data and context to augment a model’s response. As the same base model can be used for different use cases, RAG is the preferred choice due to its flexibility and cost efficiency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ability to respond with the latest information&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires model re-training with each new information update, whereas RAG simply retrieves required context and data from a knowledge base to enhance its response. Thus, by using RAG, LLM is able to answer questions using the most current information from our production database, eliminating the need for model re-training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Speed and scalability&lt;/strong&gt;&lt;br /&gt;
Without the burden of model re-training, the team can rapidly scale and build out new LLM applications with a well managed knowledge base.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The potential of using RAG-powered LLM can be limitless as the ability of GPT is correlated with the tools it equips. Hence, the process does not stop here and we will try to onboard more tools or integration to GPT. In the near future, we plan to utilise Data-Arks to provide images to GPT as GPT-4o is a multimodal model that has vision capabilities. We are committed to pushing the boundaries of what’s possible with RAG-powered LLM, and we look forward to unveiling the exciting advancements that lie ahead.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-what-next.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. What’s next?&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to express our sincere gratitude to the following individuals and teams whose invaluable support and contributions have made this project a reality: &lt;br /&gt;- Meichen Lu, a senior data scientist at Grab, for her guidance and assistance in building the MVP and testing the concept.&lt;br /&gt;- The data engineering team, particularly Jia Long Loh and Pu Li, for setting up the necessary services and infrastructure. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Wed, 09 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</link>
        <guid isPermaLink="true">https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Analytics</category>
        
        <category>Data Science</category>
        
      </item>
    
  </channel>
</rss>
