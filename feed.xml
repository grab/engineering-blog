<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 06 Jan 2026 09:19:44 +0000</pubDate>
    <lastBuildDate>Tue, 06 Jan 2026 09:19:44 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>Kinabalu AI SRE - Leveraging AI for scalable diagnostics and alert management (Part 1)</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;If you’ve ever been on-call during an outage, you know the drill: a flood of alerts, five dashboards open, logs streaming from different places, a dozen threads in Slack, and still no clear picture. Context-switching kills velocity, and “where do I even start?” becomes the default question.&lt;/p&gt;

&lt;p&gt;Kinabalu AI Site Reliability Engineering (AI SRE for short) is our attempt to transform this experience. It consolidates the right context in one place, analyzes it with assistive AI agents, and helps us move from alert to action quickly.&lt;/p&gt;

&lt;p&gt;Target audience:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On-call engineers and incident commanders.&lt;/li&gt;
  &lt;li&gt;Service owners validating health, dependencies, and changes.&lt;/li&gt;
  &lt;li&gt;SRE/platform teams standardizing triage and root cause analysis (RCA) quality.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Kinabalu AI SRE is in its experimental stage; features, coverage, and interfaces may change as we iterate.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Incidents today suffer from several issues, including alert overload, fragmented context across tools, slow RCA, operational redundancy from tool-hopping, and scattered runbooks that are hard to find and apply under pressure.&lt;/p&gt;

&lt;p&gt;AI SRE aims to solve these issues by serving a unified view that streamlines diagnostics and correlates signals to recommend the best next actions. This approach accelerates response time, further reducing time-to-resolution (TTR), lowers the cognitive load on on-calls by keeping all relevant context in one place, and strengthens collaboration through evidence-backed updates and clear ownership.&lt;/p&gt;

&lt;h2 id=&quot;a-typical-user-journey&quot;&gt;A typical user journey&lt;/h2&gt;

&lt;p&gt;Kinabalu’s AI SRE is a 24/7 automator reachable via Slack and a Web UI. It takes input in the form of an automated alert or a direct question and responds with an evidence-backed, actionable insight.&lt;/p&gt;

&lt;p&gt;In a hypothetical user journey with AI SRE, the process might begin with a trigger. For instance, if a monitoring alert is triggered by a fivefold increase in a Datadog report and increasing latency for a service, AI SRE initiates an incident thread and gathers the initial context.&lt;/p&gt;

&lt;p&gt;The following components of AI SRE are then executed in sequence:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Component 1&lt;/strong&gt;: Auto-triage with context from incident records, tagging on severity, priority, owner/oncall, as well as issue types.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Component 2&lt;/strong&gt;: AI SRE (static diagnostics) establishes correlations by&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Metrics and dashboards: analyzes recent deltas and compares against time-of-day/week baselines.&lt;/li&gt;
  &lt;li&gt;Dependencies: checks upstream/downstream services to separate causes from symptoms.&lt;/li&gt;
  &lt;li&gt;Changes: retrieves recent deployments, config updates, and feature-flag flips.&lt;/li&gt;
  &lt;li&gt;Logs: clusters error signatures and tracks frequency shifts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Delivers an incident summary with actionable insights, aRCA draft, and concrete recommendations (queries to run, rollback/feature-flag options, runbook links).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Component 3&lt;/strong&gt;: Dynamic conversation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Conversational follow-up where user enters questions in Slack, such as “List owners for impacted services”, or “Compare p95 across top markets”. AI SRE replies with evidence-backed answers and provides links for further drill-down.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;Under the hood, the backend combines a central signal aggregator with Model Context Protocol (MCP) servers for instant search, and a Large Language Model (LLM) powered intelligence layer that analyzes signals to auto-triage incidents and produce actionable insights.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kinabalu-ai-sre/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. SRE AI architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;signal-aggregator-context-engineering&quot;&gt;Signal aggregator: Context engineering&lt;/h3&gt;

&lt;p&gt;We follow a Retrieval Augmented Generation (RAG) approach and are building a knowledge graph that stitches together incident signals across the stack. The aggregator ingests the information as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Datadog (metrics, monitors)&lt;/li&gt;
  &lt;li&gt;Kibana/Elasticsearch (logs)&lt;/li&gt;
  &lt;li&gt;Grafana (dashboards)&lt;/li&gt;
  &lt;li&gt;Hystrix (circuit state)&lt;/li&gt;
  &lt;li&gt;GitLab/Jira (changes/issues)&lt;/li&gt;
  &lt;li&gt;CI/CD and deployment metadata&lt;/li&gt;
  &lt;li&gt;Service/product catalog (ownership, dependencies)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this context, AI SRE agents can provide a clear view of what changed, when it changed, and who owns it, making incident understanding and debugging faster and more reliable in a near-real-time manner.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kinabalu-ai-sre/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Examples of signal aggregation for building context.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;unified-intelligence-an-agentic-approach&quot;&gt;Unified intelligence: An agentic approach&lt;/h3&gt;

&lt;p&gt;Agents can basically “normalize” the alerts and signals, meaning they standardize and interpret them for better understanding. They can semantically search through historical changes that can explain current symptoms, correlate co-occurring signals, and surface likely causes.&lt;/p&gt;

&lt;p&gt;AI SRE uses the SuperAgent and A2A multi-agent frameworks to analyze incidents using two workflows, which can coexist.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For static diagnosis, a separate flow collects all data and logs for services via the MCP toolkit and sends them to A2A multi-agents for a deep-dive investigation.&lt;/li&gt;
  &lt;li&gt;For dynamic analysis, SuperAgent uses the MCP toolkit to investigate and pull real-time data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;static-diagnosis&quot;&gt;Static diagnosis&lt;/h4&gt;

&lt;p&gt;The static diagnostics workflow starts with a trigger from Slack or the Web UI and ends with a comprehensive service health report. It coordinates six domain-specific sub-agents encompassing the areas of incident management, deployment, application, database, infrastructure, and external APIs. Each sub-agent pulls the relevant signals and runs targeted checks, producing detailed findings. The supervisor then synthesizes these into an investigation-ready brief. The brief contains a concise summary of suspects and blast radius, timeline, and recommended next steps. The briefs are grounded in logs and metrics, so engineers can quickly understand the impact and move toward resolution.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kinabalu-ai-sre/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Examples of static diagnosis by AI SRE.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;dynamic-chat&quot;&gt;Dynamic chat&lt;/h4&gt;

&lt;p&gt;Users can inquire via Slack or the Web UI to receive an immediate, evidence-supported action plan. Examples of such questions include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“How many recent deployments touched the food service?”&lt;/li&gt;
  &lt;li&gt;“How many Terraform changes in the past 5 minutes?”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Powered by our SuperAgent and MCP tool layer, dynamic chat queries live systems such as metrics, logs, deploy history, and configs. It then returns cited data, comparisons, and next-best actions. On-call engineers can diagnose issues and pull logs on the fly, before escalating actions (e.g., open a ticket, compare regions, list owners, suggest rollbacks). It’s human-in-the-loop (HITL) by design.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kinabalu-ai-sre/figure-4a.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Example of examining related deployments within the same time frame.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kinabalu-ai-sre/figure-4b.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Example of analyzing Splunk or DataDog alerts to identify the root cause of an issue.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;mcp-toolkit&quot;&gt;MCP toolkit&lt;/h3&gt;

&lt;p&gt;The Kinabalu MCP Toolkit serves as a universal integration layer that empowers AI SRE by unifying 25 operational tools into a single, consistent interface. This comprehensive toolkit spans six key domains:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Incident and communications: Manages historical incidents, Slack thread context, and ticketing.&lt;/li&gt;
  &lt;li&gt;Internal platforms: Includes changelogs, experiments, rollout history, and automated analyses.&lt;/li&gt;
  &lt;li&gt;Knowledge and AI: Facilitates enterprise document search/chat and unstructured data analysis.&lt;/li&gt;
  &lt;li&gt;Service and configuration: Offers topology and configuration introspection.&lt;/li&gt;
  &lt;li&gt;Observability: Provides insights through metrics, logs, and profiling.&lt;/li&gt;
  &lt;li&gt;Deployment: Tracks recent releases and commit history.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Kinabalu MCP Toolkit is designed to provide AI SRE with a 360 degree view of incidents, significantly accelerating root-cause discovery and response.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Our journey highlights the importance of structured context, robust diagnostic layers, and hybrid AI models for dependable incident automation. Through our ongoing development of Kinabalu AI SRE, we’re moving toward an ecosystem where alerts are normalized, evidence is automatically synthesized, and engineers can focus on higher level decision-making rather than firefighting.&lt;/p&gt;

&lt;p&gt;Stay tuned for part 2, where we will cover the challenges, design decisions, and lessons that shaped Kinabalu AI SRE.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://www.grab.careers/en/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Tue, 06 Jan 2026 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/kinabalu-ai-sre</link>
        <guid isPermaLink="true">https://engineering.grab.com/kinabalu-ai-sre</guid>
        
        <category>automation</category>
        
        <category>LLM</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Demystifying user journeys: Revolutionizing troubleshooting with auto tracking</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Troubleshooting critical issues by deciphering a user’s journey on the Grab app is an extremely challenging task. With countless user journeys and multiple paths through the User Interface (UI), it’s akin to searching for a needle in a vast haystack. This challenge frequently resonates with us, the dedicated developers at Grab, as we strive to understand user behaviors, views, and interactions.&lt;/p&gt;

&lt;h2 id=&quot;the-challenge&quot;&gt;The challenge&lt;/h2&gt;

&lt;p&gt;The distinction between resolving an issue effectively versus spending hours on a wild goose chase is understanding our user journey in real-time.&lt;/p&gt;

&lt;p&gt;The development team initially attempted to address the issue of the incomplete user journey tracking by implementing a system where a click stream event would be sent with every user interaction. However, this approach presented significant challenges due to the sheer volume of UI components—often numbering in the hundreds—and the reliance on individual developers to correctly instrument each one.&lt;/p&gt;

&lt;p&gt;A common pitfall was that developers would occasionally overlook or forget to instrument certain user interactions, leading to breaks in the recorded user journey. This created a highly frustrating situation for both the development and product teams, as the integrity of the user journey data was consistently compromised. Despite continuous efforts to patch these bugs and address the omissions, the team found themselves in a perpetual state of reaction, constantly trying to catch up with newly discovered breaches rather than proactively preventing them. This reactive approach consumed valuable resources and hindered the ability to gain a complete and accurate understanding of user behavior.&lt;/p&gt;

&lt;p&gt;Diagnosing system failures, application bugs, or poor user experiences in complex applications becomes inefficient without real-time performance metrics and detailed session tracking. When engineering teams rely on outdated or fragmented data, they are forced to piece together issue narratives reactively, long after the issues occur. This significantly delays the Mean Time To Resolution (MTTR). Such a reactive approach leads to increased downtime, higher operational costs, customer dissatisfaction, and a waste of developers’ time, as they spend more time “hunting” for clues rather than deploying solutions or new features.&lt;/p&gt;

&lt;h2 id=&quot;our-eureka-moment-autotrack-sdk&quot;&gt;Our ‘Eureka’ moment: AutoTrack SDK&lt;/h2&gt;

&lt;p&gt;The pivotal breakthrough that provides our unique advantage was the creation of auto tracking user journeys—our “Eureka” moment. To deliver this, we developed the new Software Development Kit (SDK) called AutoTrack.&lt;/p&gt;

&lt;p&gt;AutoTrack is system that comprehensively records application state, UI view state, as well as user interactions - a solution that pieces together a chronicle of the user journey, from launch to interactions, as they navigate through the screens. AutoTrack SDK is built on the three core pillars:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Application state&lt;/li&gt;
  &lt;li&gt;User interactions&lt;/li&gt;
  &lt;li&gt;UI screens&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s delve deeper into the mechanics of how this operates.&lt;/p&gt;

&lt;h3 id=&quot;application-state&quot;&gt;Application state&lt;/h3&gt;

&lt;p&gt;Understanding the application state is fundamental to comprehending user behavior and, consequently, executing effective troubleshooting. The application state provides crucial insights into how a user interacts with the app, particularly concerning its visibility and how it was initiated. This encompasses tracking when the app moves between the background and foreground, as well as the various launch mechanisms.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Application state user flow.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Key aspects of application state that are vital to monitor include:&lt;br /&gt;
&lt;strong&gt;Application lifecycle transitions:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Background state:&lt;/strong&gt; When the app is running but not actively displayed to the user (e.g., the user switches to another app, or the device is locked). Understanding how frequently and for how long an app resides in the background can inform power consumption analysis and the effectiveness of background tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Foreground state:&lt;/strong&gt; When the app is actively in use and displayed to the user. Monitoring transitions into and out of the foreground provides a real-time view of user engagement.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inactive state:&lt;/strong&gt; A temporary state where the app is in the foreground but not receiving events (e.g., an incoming call temporarily interrupts the app).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Suspended state:&lt;/strong&gt; An app that is in the background and has been explicitly suspended by the operating system to free up resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Terminated state:&lt;/strong&gt; When the app has been completely closed or crashed. Differentiating between intentional termination and crashes is critical for identifying stability issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Application launch mechanisms:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The way an app is launched significantly impacts the initial user experience and can influence subsequent interactions. Tracking these different launch types is essential for understanding user entry points and for debugging issues that might be specific to a particular launch method.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Explicit user launch:&lt;/strong&gt; This is the most straightforward launch mechanism, where the user directly taps on the app icon from their device’s home screen or app drawer. This indicates a deliberate intent to use the app and often signifies a primary entry point for regular users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deeplinks:&lt;/strong&gt; Deeplinks are URLs that, when clicked, open a specific page or section within a mobile app rather than a web page. They are powerful tools for enhancing user experience and engagement by providing direct access to relevant content.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Push notifications:&lt;/strong&gt; Push notifications are messages sent by an app to a user’s device even when the app is not actively in use. Tapping on a push notification often launches the app and directs the user to a specific context related to the notification’s content.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Code sample for tracking application lifecycle transition.&lt;/figcaption&gt; &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;user-interactions&quot;&gt;User interactions&lt;/h3&gt;

&lt;p&gt;Real-time session tracking is a crucial component in understanding user behavior and optimizing app performance. By meticulously tracking a wide array of user interactions, the system provides invaluable insights into how users navigate and engage with the app. This granular data forms the bedrock for constructing comprehensive user journeys, allowing development teams to visualise the path a user takes from their initial entry point to achieving their goals within the app.&lt;/p&gt;

&lt;p&gt;This deep understanding of user interactions is the most important pillar in creating accurate and insightful user journey maps. These maps, in turn, are instrumental in identifying patterns of user behavior, both positive and negative. For instance, tracking helps to identify pain points, bugs, or areas of confusion that might lead to user frustration or abandonment.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Sample code for real-time session tracking.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ui-screen&quot;&gt;UI screen&lt;/h3&gt;

&lt;p&gt;The system leverages lifecycle events from UIViewController (iOS), Activity (Android), and Fragments (Android) to accurately identify and track which specific screen is currently displayed to the user. This granular level of screen tracking is crucial because it significantly enriches the contextual information available to us. By understanding the precise UI that users are interacting with, we can account for the dynamic nature of our app. Different geographical regions, diverse user segments, and varying operational scenarios can lead to distinct user interfaces being presented. This capability ensures that our analysis and troubleshooting efforts are always based on the actual user experience, allowing for more precise problem identification and more effective solutions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-4-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt; &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Sample code of UIViewController configuration.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ui-screen-data&quot;&gt;UI screen data&lt;/h3&gt;
&lt;p&gt;On top of that, whenever the screen appears, we capture the screen metadata where we read the full screen hierarchy. With the Screen hierarchy JSON data at hand, we employ it to train an AI model. This model, consequently, can generate an HTML file, which mirrors the user’s screen and interaction.&lt;/p&gt;

&lt;p&gt;Disclaimer: information is redacted in compliance with GDPR/PDPA, personal data protection laws.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-7.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Screen hierarchy.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;applications-of-autotrack&quot;&gt;Applications of AutoTrack&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Key applications of AutoTrack data:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reconstructing user journeys and reproducing elusive bugs:&lt;/strong&gt; One of the most significant benefits of AutoTrack is its ability to meticulously record user interactions within the app. This detailed session data allows our teams to precisely recreate the user journey that led to a reported issue. For bugs that are notoriously difficult to reproduce, this capability is a game-changer, eliminating hours of manual guesswork and dramatically accelerating the identification and resolution of underlying problems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automated issue assignment:&lt;/strong&gt; When an issue is reported, AutoTrack data can be leveraged to automatically assign it to the most relevant team. By analysing the context of the issue within the recorded session, including the specific features or modules involved, the system can intelligently route the problem to the engineers best equipped to address it. This automation reduces triage time, ensures issues are handled by subject matter experts, and improves overall response efficiency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automating UI test case generation:&lt;/strong&gt; The rich dataset provided by AutoTrack offers a powerful foundation for automating the creation of UI test cases. By observing how users interact with the interface, we can automatically generate test scripts that mimic real-world usage patterns. This not only speeds up the testing phase but also leads to more comprehensive test coverage, identifying edge cases and user flows that might otherwise be missed by manually written tests.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Understanding analytics event triggers:&lt;/strong&gt; AutoTrack data provides a granular view into when and why specific analytics events are triggered within the application. This allows us to validate the accuracy of our analytics instrumentation, ensure that events are firing as expected, and gain deeper insights into user behavior. By understanding the precise context surrounding event triggers, we can refine our data collection strategies and derive more meaningful insights from our analytics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-takeaways-and-whats-next&quot;&gt;Key takeaways and what’s next&lt;/h2&gt;

&lt;p&gt;AutoTrack replaces fragile manual instrumentation with a unified, real-time view of application state, screen context, and user interactions. That end-to-end trace makes elusive bugs reproducible, routes issues to the right owners, and seeds reliable UI tests—turning guesswork into grounded evidence so teams can ship fixes faster and with greater confidence.&lt;/p&gt;

&lt;p&gt;Looking ahead, we are expanding AutoTrack across surfaces and deepening the context it captures—pairing sessions with network and performance signals, strengthening privacy guardrails, and integrating with automated triage and test generation. Look forward to reading more of our deep dives on auto-generated UI tests and how these journeys will power proactive quality across Grab’s app.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://www.grab.careers/en/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Dec 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/auto-track-sdk</link>
        <guid isPermaLink="true">https://engineering.grab.com/auto-track-sdk</guid>
        
        <category>mobile</category>
        
        <category>ios</category>
        
        <category>android</category>
        
        <category>tracking</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>How Grab is accelerating growth with real-time personalization using Customer Data Platform scenarios</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Delivering personalized user experiences in real-time is central to Grab’s strategy, but achieving this at scale poses significant engineering challenges. Grab’s Customer Data Platform (CDP) and Growth team has successfully delivered several real-time campaigns, driving significant business impact through enhanced personalization. These initiatives include high-impact use cases like immediate mall offers, timely traveler recommendations, precise ad retargeting, and proactive interventions during key user journey moments. At the core of these successes is Grab’s CDP, which rapidly deploys advanced real-time personalization via a powerful new capability called “Scenarios.”&lt;/p&gt;

&lt;h2 id=&quot;about-grabs-cdp&quot;&gt;About Grab’s CDP&lt;/h2&gt;

&lt;p&gt;Grab’s CDP is a centralized, reliable repository for user attributes, designed for freshness, governance, and reusability. Built on &lt;a href=&quot;https://engineering.grab.com/signals-market-place&quot;&gt;Grab’s Signal Marketplace&lt;/a&gt; framework, the CDP streamlines data management through automation and integration, supporting seamless interactions with internal services and toolings that power marketing, experimentation, ads, Machine Learning (ML) features, and external platforms, including Facebook, Google Ads, and TikTok.&lt;/p&gt;

&lt;p&gt;The platform currently manages over 1,000 batch user attributes for Passengers, Drivers, and Merchants, powering diverse use cases from targeted marketing campaigns to operational decision-making across Grab’s entire ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-real-time-personalization&quot;&gt;The need for real-time personalization&lt;/h2&gt;

&lt;p&gt;In our current CDP setup, user segments are primarily created for targeting using batch attributes that update once daily. While these batch updates provide valuable historical insights, they are not suitable for scenarios requiring real-time responsiveness. This delay prevents timely engagement with users, particularly when immediate actions can significantly enhance user experiences and conversion rates.&lt;/p&gt;

&lt;p&gt;For example, when travelers land at an airport, they immediately benefit from timely suggestions for rides, dining options, or local attractions. Traditional batch processing cannot deliver the agility and responsiveness required for these dynamic scenarios.&lt;/p&gt;

&lt;p&gt;Historically, real-time personalization at Grab relied heavily on engineering resources, which resulted in limited scalability and agility. Marketers and product teams often found themselves blocked by engineering bandwidth constraints, restricting experimentation and innovation.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;The limitations of Grab’s existing personalization frameworks include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Batch attribute delays&lt;/strong&gt;: Daily updates are insufficient for scenarios requiring immediate user responses.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Limited dynamic enrichment&lt;/strong&gt;: Difficulties in dynamically integrating real-time events with historical user data, weakens personalization effectiveness.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High engineering overhead&lt;/strong&gt;: Custom solutions require extensive resources, limiting agility and innovation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To overcome these challenges and support Grab’s vision for comprehensive personalization – including proactive recommendations and assistance – CDP needed robust real-time capabilities.&lt;/p&gt;

&lt;h2 id=&quot;cdp-scenarios-real-time-personalization-made-simple&quot;&gt;CDP Scenarios: Real-time personalization made simple&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Scenario&lt;/strong&gt; feature revolutionizes real-time targeting within the CDP by utilizing user-initiated events, geo-fencing, historical profile data, and on-the-fly predictions. This empowers the business to deliver easy, quick, and flexible personalization without the need for complex engineering efforts.&lt;/p&gt;

&lt;p&gt;Scenarios enable innovative use cases such as these:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mall personalization&lt;/strong&gt;: Real-time personalized offers upon arrival.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Traveler assistance&lt;/strong&gt;: Immediate recommendations at airports or hotels.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ad retargeting&lt;/strong&gt;: Enhanced real-time ad targeting.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conversion optimization&lt;/strong&gt;: Timely intervention during user drop-off points.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Imagine predicting a user’s intent to drop off at a mall using both real-time and historical context. For instance, when a user books a ride to a mall, factors such as destination, time, cuisine preferences, and past behavior (e.g., affluence level) can help predict whether the user’s purpose is retail therapy, grocery shopping, or dining out. This prediction accounts for elements like time of day, day of the week, and mall location. Grab’s engineering teams can leverage this predicted intent (signal) to offer personalized actions, such as GrabPay discounts for shopping or exclusive dining offers for dinner.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Scenario in CDP.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;key-features&quot;&gt;Key features&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Event-driven personalization&lt;/strong&gt;: Real-time Scenarios triggered by Scribe events (Grab’s comprehensive event collection and tracking platform) combined with geo-fencing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Historical context integration&lt;/strong&gt;: Optionally enrich Scenarios using historical CDP data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Predictive modeling&lt;/strong&gt;: Deploy pre-trained models for instant user behavior predictions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Self-serve graphical user interface (GUI)&lt;/strong&gt;: Enable marketers to create complex event sequences and validate Scenarios with synthetic data processed through Flink pipelines.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Headless application programming interfaces (APIs)&lt;/strong&gt;: Allow programmatic access and management of Scenarios.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-2.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Attributes for a scenario in CDP.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;self-serve-scenario-creation&quot;&gt;Self-serve Scenario creation&lt;/h3&gt;

&lt;p&gt;We designed an intuitive self-serve UI, embedded within the Grab app, empowering marketers to quickly define and deploy Scenarios. Users can specify event triggers, configure geo-fencing, incorporate historical user attributes, and select predictive models. Marketers can also validate Scenarios using synthetic data before deployment, ensuring accurate and realistic outcomes.&lt;/p&gt;

&lt;p&gt;How it works:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Select event triggers&lt;/strong&gt;: Choose predefined events or define custom intra-session sequences via the GUI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Configure geo-fencing&lt;/strong&gt;: Define Scenario activation locations, like airports or malls.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Include historical attributes (optional)&lt;/strong&gt;: Utilize batch attributes from the CDP to enrich Scenarios.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Select predictive models (optional)&lt;/strong&gt;: Train custom classifiers or pick from pre-trained Catwalk models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Define data sink&lt;/strong&gt;: Choose between Amphawa (DynamoDB), Kafka, or both; potentially extendable to external destinations (e.g., Appsflyer).&lt;/li&gt;
  &lt;li&gt;Once configured, metadata synchronizes automatically with our streaming service, and Scenarios become available for real-time consumption within an hour.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;proven-impact-real-world-success&quot;&gt;Proven impact: Real-world success&lt;/h2&gt;

&lt;p&gt;CDP Scenarios are already delivering measurable business results, with over 12 live production implementations. For instance, in a case study addressing Grab Unlimited subscription signup abandonment, we leveraged CDP Scenarios to increase signups by engaging users in real time within 15 minutes of them leaving the signup process.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Grab Unlimited sign-up journey.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To enhance conversion rates, personalized real-time nudges were deployed through Scenarios. For example, users who started the signup process but failed to complete it within 15 minutes received a follow-up notification, prompting them to finalize their registration.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Scenario flow for Grab Unlimited registration.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This scenario alone achieved more than a 3% uplift in subscriber conversions vs non-real-time acquisition campaigns, demonstrating Scenarios’ potential to significantly boost business outcomes.&lt;/p&gt;

&lt;h2 id=&quot;technical-architecture-low-latency-high-reliability&quot;&gt;Technical architecture: Low latency, high reliability&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-6.jpg&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. High-level scenario flow. Scenarios are designed for low latency (under 15 seconds) and high reliability.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Event registration&lt;/strong&gt;: Popular UI events from Scribe are whitelisted and immediately available; custom events are onboarded via the CDP web portal.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scenario creation&lt;/strong&gt;: Users configure Scenarios through a user-friendly GUI, defining events, historical contexts, and predictive models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time Flink processing&lt;/strong&gt;: Incoming events trigger Scenarios, evaluating user historical data via StarRocks and performing real-time predictions using pre-trained models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time data sync&lt;/strong&gt;: Outcomes are synced back to Kafka or Amphawa (Grab’s internal feature store built on AWS DynamoDB), enriching data for use by subsequent services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consumption by downstream services&lt;/strong&gt;: Kafka streams or CDP’s Profile SDK facilitates immediate, personalized user experiences.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;advancing-the-future-of-real-time-personalization&quot;&gt;Advancing the future of real-time personalization&lt;/h2&gt;

&lt;p&gt;As we continue to innovate, we are focused on enhancing the capabilities of CDP Scenarios to support more complex and scalable personalization use cases. Here are some key areas of improvement we are exploring:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Optimized Scenario sharding for scalable processing&lt;/strong&gt;: To accommodate the growing number of use cases, we plan to scale and orchestrate our Flink pipeline fleet in a headless manner. This approach will improve system stability and enable seamless management of complex Scenarios across the pipeline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Enhanced signal distribution across multiple destinations&lt;/strong&gt;: Currently, Scenario outputs are limited to a single topic or sink. To address the increasing diversity of use cases, we aim to expand signal distribution, allowing downstream consumers to access Scenario outcomes through multiple scalable and reliable channels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Advanced scheduling and delayed triggering&lt;/strong&gt;: While real-time computation of Scenario signals is effective, certain use cases require delayed activation for maximum impact. We are exploring ways to compute signals instantly but trigger actions at scheduled times, such as sending a push notification for booking a return Grab ride based on the average wait time at the drop-off location.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-revolutionizing-real-time-personalization&quot;&gt;Conclusion: Revolutionizing real-time personalization&lt;/h2&gt;

&lt;p&gt;The launch of CDP Scenarios represents a significant milestone for Grab, paving the way for scalable, efficient, and user-friendly real-time personalization. Initial successes have demonstrated its immense potential, delivering notable improvements in user engagement and conversion rates. Looking ahead, we are committed to continuously advancing Scenarios by expanding its features, integrations, and applications to further elevate user experiences across the Grab ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://www.grab.careers/en/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Thu, 18 Dec 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/cdp-scenarios</link>
        <guid isPermaLink="true">https://engineering.grab.com/cdp-scenarios</guid>
        
        <category>Database</category>
        
        <category>FlinkSQL</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>A Decade of Defense: Celebrating Grab&apos;s 10th Year Bug Bounty Program</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Ten years ago, we launched our bug bounty program in partnership with &lt;a href=&quot;https://www.hackerone.com/blog&quot;&gt;HackerOne&lt;/a&gt;. Beyond a security initiative, it represented an open invitation to collaborative development.
As pioneers in Southeast Asia, we began the program with 23 initial researchers, and it has since evolved into a global community of security researchers.&lt;/p&gt;

&lt;p&gt;The strategic structure and scope of our Bug Bounty Program, combined with our continuous innovation and experimentation, have successfully captured the attention of the global security research community. Over the past decade, we have partnered with more than 850 active security researchers from HackerOne’s community of over 2 million cybersecurity professionals worldwide. These dedicated researchers work alongside us across borders and time zones, forming a collaborative defense network that helps protect over 187 million users throughout Southeast Asia. Their ongoing participation demonstrates both the maturity of our program and the trust we’ve built within the security research community.&lt;/p&gt;

&lt;p&gt;This milestone reflects the strength of shared purpose and our sustained partnership with the HackerOne platform. It demonstrates the value of human connection and the collective understanding that security is stronger through collaboration. Here’s to a decade of partnership and to many more years of building a safer future, one collaboration at a time!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/decade-of-defense/figure-1.jpg&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Ten years of achievements with our HackerOne partnership.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;evolution-and-growth-adapting-to-a-dynamic-threat-landscape&quot;&gt;Evolution and growth: Adapting to a dynamic threat landscape&lt;/h3&gt;

&lt;p&gt;Over the past ten years, our program has consistently adapted to the dynamic threat landscape and integrated invaluable feedback from our research community. We have grown from a private initiative to a program that consistently ranks among the top 20 worldwide and among the top 3 in Asia on HackerOne. Key milestones from our journey include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expanding our horizons:&lt;/strong&gt; Our scope significantly broadened in 2023-2024, continuously adding new assets and prominently including financial services in Indonesia and AI systems. This expansion provides researchers with more avenues to contribute to Grab’s security.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Focused mobile security:&lt;/strong&gt; We introduced a dedicated bounty table for mobile-specific issues, recognizing the unique challenges of mobile security.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incentivizing excellence:&lt;/strong&gt; We regularly experiment with campaigns of various types and targets, diversifying our reward methods to include both financial rewards and recognition.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evolving vulnerability focus:&lt;/strong&gt; We’ve observed a significant shift in the types of vulnerabilities reported over the decade, moving from foundational issues in early years to more sophisticated and emerging categories recently.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/decade-of-defense/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. The journey of our bug bounty program.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-global-stage-connecting-with-the-best&quot;&gt;The global stage: Connecting with the best&lt;/h3&gt;

&lt;p&gt;Our program’s success is deeply rooted in its vibrant global community, which we actively foster through continuous engagement. Our strategy extends beyond the platform to major live hacking events, including the &lt;strong&gt;ThreatCon Live Hacking Event 2023&lt;/strong&gt; &lt;strong&gt;in Nepal&lt;/strong&gt; and &lt;strong&gt;DEFCON 32’s Live Recon Village 2024 in Las Vegas.&lt;/strong&gt; These initiatives have been instrumental in connecting us with a diverse pool of new talent and strengthening relationships with researchers across different continents. By meeting hackers where they are, we’ve not only brought new expertise into our ecosystem but also demonstrated our commitment to being an accessible and collaborative partner on a global scale.&lt;/p&gt;

&lt;p&gt;The high participation and quality submissions from these events demonstrate the effectiveness of this approach. They’ve expanded our global security testing coverage and strengthened our standing within the worldwide cybersecurity community. Through ongoing interactions and submitted reports, we continue to see that security is a collaborative effort with no borders.&lt;/p&gt;

&lt;h3 id=&quot;exclusive-anniversary-celebrations-global-club-campaigns&quot;&gt;Exclusive anniversary celebrations: Global club campaigns&lt;/h3&gt;

&lt;p&gt;To commemorate our 10th anniversary, we launched three exclusive, invite-only campaigns with HackerOne’s regional clubs in &lt;strong&gt;Germany, Morocco, and India&lt;/strong&gt;. These campaigns served as cultural exchanges, bringing fresh perspectives from outside our core Southeast Asian consumer markets. By engaging with these clubs, we expanded our researcher community and connected with security experts who understand different threat landscapes and methodologies, bringing outside perspectives to our systems.&lt;/p&gt;

&lt;p&gt;In August, we also ran a broader anniversary campaign that drew significant participation from the researcher community, resulting in 461 submissions. &lt;a href=&quot;https://hackerone.com/xchopath?type=user&quot;&gt;xchopath&lt;/a&gt; was awarded the Best Hacker Bonus for their contributions during this campaign.&lt;/p&gt;

&lt;p&gt;These campaigns expanded our global security testing coverage and strengthened relationships with international researcher communities. Beyond vulnerability reports, they functioned as knowledge-sharing initiatives. We connected directly with researchers to learn from their experience and feedback, creating a continuous loop of improvement. This international collaboration also informed our global expansion security strategy by providing insights into how different regions approach digital payments and authentication.&lt;/p&gt;

&lt;p&gt;The anniversary campaigns allowed us to validate our security frameworks against diverse regulatory environments and advanced testing methodologies from established security markets, reinforcing our commitment to maintaining robust security standards.&lt;/p&gt;

&lt;h3 id=&quot;voices-from-our-community&quot;&gt;Voices from our community&lt;/h3&gt;

&lt;p&gt;Behind every vulnerability report is a researcher who chose to help make Grab safer. Their perspectives reveal the human side of our security evolution. These individuals are not just cybersecurity experts; they are partners in our mission to protect millions of users and ensure a safe digital environment. Here are a few testimonies from participants in our past campaigns:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“The triage was very fast despite the time difference, which I really appreciated. The triaging experience was better than other programs. The huge scope and business portal with different user roles made it especially interesting to explore.” – &lt;a href=&quot;https://hackerone.com/artsec?type=user&quot;&gt;&lt;em&gt;ArtSec&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Germany club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“I liked that different countries have different features—this gives me more attack surface to explore. Response time was great, triage was very fast, and I appreciated Grab’s effort in providing fast responses. The scope was huge with a lot of wildcards for reconnaissance.” – &lt;a href=&quot;https://hackerone.com/sicksec?type=user&quot;&gt;&lt;em&gt;Sicksec&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Morocco club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“More than 20 bugs were reported, and was particularly happy that bounties were being paid upon triage. The Germany team spent a lot of time on the educational part, especially for newcomers. Communication overall was very good, and the immediate response even outside working hours was really cool. SSO and authentication is my expertise and I liked that aspect of exploring the platform.” – &lt;a href=&quot;https://hackerone.com/lauritz?type=user&quot;&gt;&lt;em&gt;Lauritz&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Germany club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-road-ahead-our-commitment-to-a-secure-future&quot;&gt;The road ahead: Our commitment to a secure future&lt;/h3&gt;

&lt;p&gt;With a strong community of security researchers across countries and a decade of collaboration, we’ve built meaningful partnerships. Every vulnerability report represents trust, and every discovery reflects dedication to our shared mission. The program demonstrates our choice to build together rather than work in isolation, to protect rather than exploit, and to collaborate rather than compete.&lt;/p&gt;

&lt;p&gt;While we celebrate our external community, the success of our program relies equally on our dedicated internal teams. Our cybersecurity teams form the operational foundation of this initiative. Their consistent responsiveness and researcher-focused approach have enabled vulnerability reporting to evolve into a genuine partnership, maintaining researcher trust and keeping Grab secure.&lt;/p&gt;

&lt;p&gt;The next ten years will bring challenges we can’t yet imagine, from emerging threats in artificial intelligence to novel cryptographic approaches in a quantum-powered world. We will face them together as a community that spans cultures, time zones, and expertise.&lt;/p&gt;

&lt;p&gt;Together, we’ll continue securing Southeast Asia’s digital future, one partnership, one discovery, one shared achievement at a time.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility, and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people every day to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdef&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Dec 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/a-decade-of-defense</link>
        <guid isPermaLink="true">https://engineering.grab.com/a-decade-of-defense</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Real-time data quality monitoring: Kafka stream contracts with syntactic and semantic test</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In today’s data-driven landscape, monitoring data quality has become a critical need for ensuring reliable and efficient data usage across domains. High-quality data is the backbone of AI innovation, driving efficiency and unlocking new opportunities. As decentralized data ownership grows, the ability to effectively monitor data quality is essential for maintaining reliability in data systems.&lt;/p&gt;

&lt;p&gt;Kafka streams, as a vital component of real-time data processing, play a significant role in this ecosystem. However, unreliable data within Kafka streams can lead to errors and inefficiencies for downstream users, and monitoring the quality of data within these streams has always been a challenge. This blog introduces a solution that empowers stream users to define a data contract, specifying the rules that Kafka stream data must adhere to. By leveraging this user-defined data contract, the solution performs automated real-time data quality checks, identifies problematic data as it occurs, and promptly notifies stream owners. This ensures timely action, enabling effective monitoring and management of Kafka stream data quality while supporting the broader goals of data mesh and AI-driven innovation.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;In the past, monitoring Kafka stream data processing lacked an effective solution for data quality validation. This limitation made it challenging to identify bad data, notify users in a timely manner, and prevent the cascading impact on downstream users from further escalating.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Challenges in syntactic and semantic issue identification&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Syntactic issues&lt;/strong&gt;: Refers to schema mismatches between producers and consumers, which can lead to deserialization errors. While schema backward compatibility can be validated upon schema evolution, there are scenarios where the actual data in the Kafka topic does not align with the defined schema. For example, this can occur when a rogue Kafka producer is not using the expected schema for a given Kafka topic. Identifying the specific fields causing these syntactic issues is a typical challenge.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semantic issues&lt;/strong&gt;: Refers to inconsistencies or misalignments between producers and consumers about the expected pattern or significance of each field. Unlike Kafka stream schemas, which act as a data structure contract between producers and consumers, there is no existing framework for stakeholders to define and enforce field-level semantic rules, for example, the expected length or pattern of an identifier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Timeliness challenge in data quality monitoring&lt;/strong&gt;: There is no real-time mechanism to automatically validate data against predefined rules, timely identify quality issues, and promptly alert stream stakeholders. Without real-time stream validation, data quality issues can sometimes persist for periods of time, impacting various online and offline downstream systems before being discovered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Observability challenge for troubleshooting bad data&lt;/strong&gt;: Even when problematic data is identified, stream users face difficulties in pinpointing the exact “poison data” and understanding which fields are incompatible with the schema or violate semantic rules. This lack of visibility complicates Root Cause Analysis and resolution efforts.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Our &lt;a href=&quot;https://engineering.grab.com/an-elegant-platform&quot;&gt;Coban platform&lt;/a&gt; offers a standardized data quality test and observability solution at the platform level, consisting of the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Contract Definition&lt;/strong&gt;: Enables Kafka stream stakeholders to define contracts that include schema agreements, semantic rules that Kafka topic data must comply with, and Kafka stream ownership details for alerting and notifications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automated Test Execution&lt;/strong&gt;: Provides a long running Test Runner to automatically execute real-time tests based on the defined contract.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time Data Quality Issue Identification&lt;/strong&gt;: Detects data issues at both syntactic and semantic levels in real-time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alerts and Result Observability&lt;/strong&gt;: Alerts users, simplifying observation of data quality issues via the platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;p&gt;The solution includes three components: &lt;em&gt;Data Contract Definition, Test Execution &amp;amp; Data Quality Issue Identification, and Result Observability as shown in the architecture diagram in figure 1&lt;/em&gt;. All mentions of “Flow” from here onwards refer to the corresponding processes illustrated in figure 1.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/coban-architecture.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Real-time Kafka Stream Data Quality Monitoring Architecture diagram.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;data-contract-definition&quot;&gt;Data Contract Definition&lt;/h4&gt;

&lt;p&gt;The Coban Platform streamlines the process of defining Kafka stream data contracts, serving as a formal agreement among Kafka stream stakeholders. This includes the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Schema&lt;/strong&gt;: Represents the schema used by the Kafka topic under test and helps the Test Runner to validate schema compatibility across data streams (Flow 1.1).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Configuration&lt;/strong&gt;: Encompasses essential configurations such as the endpoint and topic name, which the platform automatically populates (Flow 1.2).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability Metadata&lt;/strong&gt;: Provides contact information for notifying Kafka stream stakeholders about data quality issues and includes alert configurations for monitoring (Flow 1.3).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Semantic Test Rules&lt;/strong&gt;: Empowers users to define intuitive semantic test rules at the field level. These rules include checks for string patterns, number ranges, constant values, etc. (Flow 1.5).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LLM-Based Semantic Test Rules Recommendation&lt;/strong&gt;: Defining dozens if not hundreds of field-specific test rules can overwhelm users. To simplify this process, the Coban Platform uses LLM-based recommendations to predict semantic test rules using provided Kafka stream schemas and anonymized sample data (Flow 1.4). This feature helps users set up semantic rules efficiently, as demonstrated in the sample UI in figure 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/sample-ui.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Sample UI showcasing LLM-based Kafka stream schema field-level semantic test rules. Note that the data shown is entirely fictional.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;data-contract-transformation&quot;&gt;Data Contract Transformation&lt;/h4&gt;

&lt;p&gt;Once defined, the Coban Platform’s transformation engine converts the data contract into configurations that the Test Runner can interpret (Flow 2.1). This transformation process includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Schema&lt;/strong&gt;: Translates the schema defined in the data contract into a schema reference that the Test Runner can parse.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Configuration&lt;/strong&gt;: Sets up the Kafka stream as a source for the Test Runner.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability metadata&lt;/strong&gt;: Sets contact information as configurations of the Test Runner.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Semantic Test Rules&lt;/strong&gt;: Transforms human-readable semantic test rules into an inverse SQL query to capture the data that violates the defined rules.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/semantic-test-rules.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Illustration of semantic test rules being converted from human-readable formats into inverse SQL queries.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;test-execution--data-quality-issue-identification&quot;&gt;Test Execution &amp;amp; Data Quality Issue Identification&lt;/h3&gt;

&lt;p&gt;Once the Test Configuration Transformation Engine generates the Test Runner configuration (Flow 2.1), the platform automatically deploys the Test Runner.&lt;/p&gt;

&lt;h4 id=&quot;test-runner&quot;&gt;Test Runner&lt;/h4&gt;

&lt;p&gt;The Test Runner utilises FlinkSQL as the compute engine to execute the tests. FlinkSQL was selected for its flexibility in defining test rules as straightforward SQL statements, enabling our platform to efficiently convert data contracts into enforceable rules.&lt;/p&gt;

&lt;h4 id=&quot;test-execution-workflow-and-problematic-data-identification&quot;&gt;Test Execution Workflow And Problematic Data Identification&lt;/h4&gt;

&lt;p&gt;FlinkSQL consumes data from the Kafka topic under test (Flow 2.2) using its own consumer group, ensuring it doesn’t impact other consumers. It runs the inverse SQL query (Flow 2.3) to identify any data that violates the semantic rules or that is syntactically incorrect in the first place. Test Runner captures such data, packages it into a data quality issue event enriched with a test summary, the total count of bad records, and sample bad data, and publishes it to a dedicated Kafka topic (Flow 3.2). Additionally, the platform sinks all such data quality events to an AWS S3 bucket (Flow 3.1) to enable deeper observability and analysis.&lt;/p&gt;

&lt;h3 id=&quot;result-observability&quot;&gt;Result Observability&lt;/h3&gt;

&lt;p&gt;Grab’s in-house data quality observability platform, Genchi, consumes problematic data captured by the Test Runner (Flow 3.3).&lt;/p&gt;

&lt;h4 id=&quot;alerting&quot;&gt;Alerting&lt;/h4&gt;
&lt;p&gt;Genchi sends Slack notifications (Flow 3.5) to stream owners specified in the data contract observability metadata. These notifications include detailed information about stream issues, such as links to sample data in Coban UI, observed windows, counts of bad records, and other relevant details.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/slack-notification.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Sample Slack notifications
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;observability&quot;&gt;Observability&lt;/h4&gt;

&lt;p&gt;Users can access the Coban UI (Flow 3.4), displaying Kafka stream test rules and sample bad records, highlighting fields and values that violate rules.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/sample-test-result.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. In this Sample Test Result, the highlighted fields indicate violations of the semantic test rules.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since its deployment earlier this year, the solution has enabled Kafka stream users to define contracts with syntactic and semantic rules, automate test execution, and alert users when problematic data is detected, prompting timely action. It has been actively monitoring data quality across 100+ critical Kafka topics. The solution offers the capability to immediately identify and halt the propagation of invalid data across multiple streams.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We implemented and rolled out a solution to assist Grab engineers in effectively monitoring data quality in their Kafka streams. This solution empowers them to establish syntactic and semantic tests for their data. Our platform’s automatic testing feature enables real-time tracking of data quality, with instant alerts for any discrepancies. Additionally, we provide detailed visibility into test results, facilitating the easy identification of specific data fields that violate the rules. This accelerates the process of diagnosing and resolving issues, allowing users to swiftly address production data challenges.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;While our current solution emphasizes monitoring the quality of Kafka streaming data, further exploration will focus on tracing producers to pinpoint the origin of problematic data, as well as enabling more advanced semantic tests such as cross-field validations. Additionally, we aim to expand monitoring capabilities to cover broader aspects like data completeness and freshness, and integrate with &lt;a href=&quot;https://www.gable.ai/&quot;&gt;Gable AI&lt;/a&gt; to detect Data Transfer Object (DTO) changes and semantic regressions in Go producers upon committing code to the Git repository. These enhancements will pave the way for a more robust, multidimensional data quality testing solution across a wider range.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.oreilly.com/library/view/driving-data-quality/9781837635009/&quot;&gt;Driving Data Quality with Data Contracts: A Comprehensive Guide to Building Reliable, Trusted, and Effective Data Platforms&lt;/a&gt; by &lt;a href=&quot;https://www.oreilly.com/search?q=author:%22Andrew%20Jones%22&quot;&gt;Andrew Jones&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdatam&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/real-time-data-quality-monitoring</link>
        <guid isPermaLink="true">https://engineering.grab.com/real-time-data-quality-monitoring</guid>
        
        <category>Engineering</category>
        
        <category>Kafka</category>
        
        <category>Performance</category>
        
        <category>Data science</category>
        
        <category>Data processing</category>
        
        <category>Real-time streaming</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>SpellVault’s evolution: Beyond LLM apps, towards the agentic future</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, innovation isn’t just about building new features; it’s about evolving our platforms to meet the changing needs of our users and the broader technological landscape. &lt;a href=&quot;https://www.grab.com/sg/inside-grab/stories/ai-llm-productivity-tool-apps-coding/&quot;&gt;SpellVault&lt;/a&gt;, our internal AI platform, exemplifies this philosophy. When SpellVault was first launched, our vision was straightforward: empower everyone at Grab to effortlessly build and manage AI-powered apps without the need for coding. Built on the principles of Retrieval-Augmented Generation (RAG) and enhanced by plugin support, SpellVault rapidly evolved into a powerful productivity engine for the organization, enabling the creation of thousands of apps that drive automation, foster experimentation, and support production use cases.&lt;/p&gt;

&lt;p&gt;As the AI landscape has evolved, SpellVault has grown alongside it. Initially launched as a straightforward no-code app builder for Large Language Models (LLMs), it has now evolved into a cutting-edge platform that embraces the agentic future—a future where AI goes beyond generating responses to reasoning, acting, and dynamically adapting through the use of tools and contextual understanding.&lt;/p&gt;

&lt;p&gt;This article outlines SpellVault’s journey towards an agentic future and how we empower users to build AI Agents that are smarter, more adaptable, and ready for the future.&lt;/p&gt;

&lt;h2 id=&quot;a-no-code-platform-for-building-llm-apps&quot;&gt;A no-code platform for building LLM apps&lt;/h2&gt;

&lt;p&gt;SpellVault was founded with a clear mission: to democratize access to AI for everyone at Grab, regardless of their technical expertise. Initially launched as a no-code LLM app builder, the platform was built on a foundation of RAG pipelines and basic plugin support.&lt;/p&gt;

&lt;p&gt;Early on, we recognized that the true potential of AI apps extends beyond the capabilities of language models alone. Their real value lies in the ability to seamlessly interact with external systems and diverse data sources. This insight drove our commitment to minimizing barriers and ensuring users could access data from various sources with ease. From the very beginning, we centered our efforts on three key focus areas:&lt;/p&gt;

&lt;h4 id=&quot;comprehensive-rag-solution-with-useful-integrations&quot;&gt;Comprehensive RAG solution with useful integrations&lt;/h4&gt;

&lt;p&gt;From the start, the SpellVault team prioritized enabling users to enhance their LLM apps with data through RAG. Rather than solely relying on the LLM’s internal information, we wanted the apps to ground their responses in up-to-date, contextually relevant, and factual information. SpellVault has built-in integrations with knowledge sources such as Wikis, Google Docs, as well as plain text and PDF uploads. These capabilities empower users to build assistants that reference relevant knowledge and provide more accurate, verifiable answers.&lt;/p&gt;

&lt;h4 id=&quot;plugins-to-fetch-information-on-demand&quot;&gt;Plugins to fetch information on demand&lt;/h4&gt;

&lt;p&gt;To move beyond static knowledge retrieval, we needed a way for apps to act dynamically. This was made possible through SpellVault plugins—modular components that allow apps to interact with internal systems (e.g. service dashboards, incident trackers) and external APIs (e.g. search engines, weather data). Rather than being confined to their initial prompt and data, these plugins can fetch fresh information at runtime. From the available plugin types, users can create their own instances of plugins with custom settings, enabling highly specialized functionality tailored to their specific workflows. For instance, with SpellVault’s HTTP plugin, users can define custom endpoints and credentials, enabling their AI apps to make tailored HTTP calls during runtime. These custom plugins have become the backbone of many of our most impactful apps, empowering teams to seamlessly integrate SpellVault with their existing systems and processes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. SpellVault’s early architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;making-spellvault-accessible-via-common-interfaces-web-slack-api&quot;&gt;Making SpellVault accessible via common interfaces: Web, Slack, API&lt;/h4&gt;

&lt;p&gt;One of our primary goals was to make AI seamlessly accessible and useful within the tools users already use—whether it’s a browser or Slack. With SpellVault, users can make their AI apps in minutes and start using them via browser or Slack messaging immediately and intuitively, without requiring any additional setup. We also exposed APIs that enabled other internal services to integrate with SpellVault apps for a variety of use cases. This multi-channel approach ensured that SpellVault wasn’t just a standalone sandbox but a platform woven into existing tools and processes.&lt;/p&gt;

&lt;p&gt;Users quickly adopted the platform, creating thousands of apps for internal productivity gains, automation, and even production use cases. The platform’s success validated our hypothesis that there was significant demand for democratized AI tools within the organization.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. SpellVault’s web interface for LLM App configuration and chat.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;evolution-over-time&quot;&gt;Evolution over time&lt;/h2&gt;

&lt;p&gt;The AI landscape over the past few years has been defined by relentless change. New frameworks, execution paradigms, and standards have emerged in quick succession, each promising to make AI systems more powerful, more reliable, or more extensible. At Grab, we recognized that for SpellVault to stay relevant, it could not remain static. It needed to evolve in tandem with the ever-changing ecosystem, continuously incorporating valuable advancements while ensuring a seamless experience for our users.&lt;/p&gt;

&lt;p&gt;This philosophy of continuous adaptation has guided SpellVault’s journey. From its early days as a simple RAG-powered app builder with a few plugins, the platform grew to support an extensive number of plugin types, richer execution models, and eventually a unified approach to tools. Each step was a response both to the needs of our users and to the shifting definition of what “building with AI” meant in practice. Rather than opting for a complete overhaul, SpellVault has embraced incremental advancements, ensuring that users can seamlessly benefit from new capabilities without disruption.&lt;/p&gt;

&lt;p&gt;This approach to evolution has naturally positioned SpellVault to transition from a platform for LLM apps to one designed for AI agents. The following section delves into this transition in greater detail.&lt;/p&gt;

&lt;h3 id=&quot;expanding-capabilities&quot;&gt;Expanding capabilities&lt;/h3&gt;

&lt;p&gt;Over time, we introduced numerous new capabilities to SpellVault, driven both by user feedback and our commitment to innovation and staying ahead of industry trends. For instance, we extended support for different plugin types, enabling integrations with tools like Slack and Kibana, and continuously added more integrations to enhance the platform’s versatility. We implemented auto-updates for users’ Knowledge Vaults, ensuring their data remained current. With more users building with the platform, ensuring the trustworthiness of responses generated by SpellVault apps became increasingly important. We included citation capability to mitigate some of that concern. Recognizing the need for more precise answers to mathematical problems, we developed a feature that enabled LLMs to solve such problems using Python runtime. Additionally, many users requested an automated way to trigger their LLM apps, which led to the creation of a Task Scheduler feature that allows LLMs to schedule actions based on natural language user input.&lt;/p&gt;

&lt;p&gt;A significant milestone in SpellVault’s evolution was the introduction of “Workflow,” a drag-and-drop interface within the platform that empowered users to design deterministic workflows. These workflows enabled users to seamlessly combine various components from the SpellVault ecosystem—such as LLM calls, Python code execution, and Knowledge Vault lookups—in a predefined and structured manner. This enabled advanced use cases for many users.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Evolving tools landscape of SpellVault with increasing integrations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;shifting-the-execution-model&quot;&gt;Shifting the execution model&lt;/h3&gt;

&lt;p&gt;As SpellVault evolved, a fundamental shift took place in the way its apps were executed internally. We transitioned from our &lt;a href=&quot;https://python.langchain.com/docs/how_to/agent_executor/&quot;&gt;legacy executor system&lt;/a&gt;, which facilitated one-off information retrieval from the Knowledge Vault or user plugins, to a more advanced &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/low_level/&quot;&gt;graph based executor&lt;/a&gt;. This empowered SpellVault’s app execution with nodes, edges, and states that supported branching, looping, and modularity. This laid the groundwork for more sophisticated agent behaviors, moving beyond the linear input-output paradigm.&lt;/p&gt;

&lt;p&gt;This transformed all existing SpellVault apps into ‘Reasoning and Acting’ agents, better known as &lt;a href=&quot;https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html&quot;&gt;ReAct agents&lt;/a&gt; - a “one size fits many” solution that significantly enhanced the capabilities of these apps. By enabling them to leverage the Knowledge Vault and plugins in a more agentic and dynamic manner, the ReAct agent framework allowed apps to perform more complex tasks while seamlessly preserving their existing functionality, ensuring no disruption to their behavior.&lt;/p&gt;

&lt;p&gt;In addition, the internal decoupling of the executor and prompt engineering components enabled us to design multiple execution pathways with ease. This allowed us to provide generic Deep Research capability to any SpellVault app via a simple UI checkbox, as well as sophisticated internal workflows that cater to high-ROI complex use cases like on-call alert analysis. The Deep Research capability came with SpellVault’s ability to search across internal information repositories (e.g., Slack messages, Wiki, Jira) within Grab, as well as searching online for relevant information.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. SpellVault’s evolved architecture with more dynamic context gathering and advanced interaction modes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;towards-an-agentic-framework&quot;&gt;Towards an agentic framework&lt;/h3&gt;

&lt;p&gt;Over time, several capabilities were added to SpellVault, including features like Python code execution and internal repository search. Initially, these functionalities were integrated directly into the core PromptBuilder class. For users, these features were primarily accessible through simple checkboxes in the user interface. As SpellVault gradually transitioned towards giving more agency to user-crafted apps, we recognized that these capabilities should instead be positioned as “Tools” for LLMs to use with greater autonomy, similar to how ReAct agent–backed apps have been using SpellVault’s user plugins. We also understood that this shift could bring a clearer mental model for users where they were no longer simply toggling features but creating AI agents with access to a defined set of tools. The agents could then decide when and how to use those tools intelligently to accomplish tasks, making the overall experience more natural and intuitive.&lt;/p&gt;

&lt;p&gt;This recognition led to the consolidation of these scattered capabilities into a unified framework called “Native Tools.” These Native Tools, along with SpellVault’s existing user plugins—rebranded as “Community Built Tools”—formed a comprehensive collection of tools that LLMs could dynamically invoke at runtime. Despite being grouped under the same umbrella, a key distinction was maintained: Native Tools required no user-specific configuration (e.g., performing internet searches), whereas Community Built Tools were custom, user-configured entities (e.g., invoking specific HTTP endpoints) created from available plugin types, often requiring credentials or other personalized settings.&lt;/p&gt;

&lt;p&gt;This consolidation of capabilities under a unified Tools abstraction and enabling SpellVault apps to invoke them with greater autonomy marked a pivotal milestone in the platform’s evolution. It meaningfully shifted SpellVault toward making agentic behavior more natural, discoverable, and extensible for every app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. SpellVault’s Unified Tools housing both Native Tools and Community Built Tools.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;spellvault-as-an-mcp-service&quot;&gt;SpellVault as an MCP service&lt;/h3&gt;

&lt;p&gt;As we streamlined SpellVault’s internal capabilities into a unified tools framework, we also turned our focus outward to align with industry standards. The growing adoption of the &lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;Model Context Protocol&lt;/a&gt; (MCP) presented an opportunity for agents and clients to seamlessly interact without requiring custom integrations. To remain at the forefront of innovation, we adapted SpellVault to function as an MCP service, enabling it to actively participate in this evolving ecosystem. This extension brought two key advancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SpellVault apps as MCP tools&lt;/strong&gt;: Each app created in SpellVault can now be exposed through the MCP protocol. This allows other agents or MCP-compatible clients, such as IDEs or external orchestration frameworks, to treat a SpellVault app as a callable tool. Instead of living only inside our web user interface or Slack interface, these apps become accessible building blocks that other systems can invoke dynamically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;RAG as an MCP tool&lt;/strong&gt;: We extended the same idea to our Knowledge Vaults. Through MCP, external clients can search, retrieve, and even add information to Vaults. This effectively turns SpellVault’s RAG pipeline into an MCP-native service, making contextual grounding available to agents beyond SpellVault itself.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While building the SpellVault MCP Server, we also created &lt;a href=&quot;https://github.com/grab/tinymcp&quot;&gt;TinyMCP&lt;/a&gt; - a lightweight open-source Python library that adds MCP capabilities to an existing FastAPI app as just another router, instead of mounting a separate app.&lt;/p&gt;

&lt;p&gt;By exposing both apps and RAG through MCP, we shifted SpellVault from being a self-contained platform to becoming an interoperable service provider in the agentic ecosystem. Users still benefit from the no-code simplicity inside SpellVault. However, the output of their work, apps, and knowledge, are now usable by other agents and tools outside of it.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;SpellVault’s evolution shows how a platform can adapt with the AI landscape while staying true to its original mission of making powerful technology accessible to everyone. What began as a no-code builder for LLM apps has steadily expanded into an agentic platform - one where apps can act with more intelligence, agency, and context and interact with the systems around them.&lt;/p&gt;

&lt;p&gt;This progress wasn’t the result of a single breakthrough, but of steady, incremental improvements that introduced new capabilities while preserving ease of use. By layering in these advancements thoughtfully but boldly, SpellVault has managed to support more sophisticated agentic behaviors without compromising its original goal of democratizing AI at Grab.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebspell&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/spellvault-evolution-beyond-llm</link>
        <guid isPermaLink="true">https://engineering.grab.com/spellvault-evolution-beyond-llm</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Grab&apos;s Mac Cloud Exit supercharges macOS CI/CD</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In our mission to optimize continuous integration and delivery (CI/CD), we have taken a bold step by relocating our infrastructure from a cloud vendor in the US to a colocation cluster within Southeast Asia, closer to our Git server infrastructure. This change has dramatically improved the performance of our macOS builds, primarily by reducing the network traffic delays associated with distant data centers. By bringing our infrastructure closer to home, we have not only accelerated CI/CD job completion times but also massively slashed operational costs.&lt;/p&gt;

&lt;p&gt;Join us as we delve into the Mac Cloud Exit journey and the significant improvements it has brought to our workflows.&lt;/p&gt;

&lt;p&gt;Our macOS CI/CD infrastructure has evolved from 1 Physical Mac Pro running in our office to a cluster of 250 Mac minis fully occupied during peak hours of the day. There were multiple stages in the journey to transition to the current state. The following diagram shows the focus area for this blog post.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Infrastructure transition path&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;before-and-after-visualizing-the-evolution&quot;&gt;Before and after: Visualizing the evolution&lt;/h3&gt;

&lt;p&gt;We began our journey with a much simpler setup.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image2.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Photo of the setup when we started&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Today, that infrastructure has scaled significantly to meet the growing demands of Grab&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image3.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Mac mini cluster today&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;economy-at-scale-the-rent-vs-own-equation&quot;&gt;Economy at scale: The rent vs. own equation&lt;/h2&gt;

&lt;p&gt;At the beginning, it was a no-brainer to rent when our demand for macOS hardware increased from 1 MacPro to 20 times that size. However, when that grew to over 200 machines, the total cost became significant, prompting us to consider:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the desired reliability for this cluster?&lt;/li&gt;
  &lt;li&gt;What would be the total cost of ownership for us to build this cluster ourselves compared to cloud-based options?&lt;/li&gt;
  &lt;li&gt;What kind of operational leverage would it bring us by controlling end-to-end stack by ourselves?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-is-grabs-scale&quot;&gt;What is Grab’s scale&lt;/h3&gt;

&lt;p&gt;At Grab, our iOS build needs have scaled quite significantly, so we went from running some builds on a single Mac Pro to running them on an army of 250+ Mac minis. And so did the cost.&lt;/p&gt;

&lt;h4 id=&quot;active-jobs-trend&quot;&gt;Active jobs trend&lt;/h4&gt;

&lt;p&gt;The total number of jobs trend is one of the data points to understand the demand situation. The following chart is a snapshot from our demand curve in 2022. Peak demand often started to exceed the available supply, creating queues for the jobs.&lt;/p&gt;

&lt;p&gt;We estimated we would need 200+ machines to comfortably supply for the peak demand and projected a demand for 400+ machines in 2025.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Active macOS CI/CD jobs&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-our-workload&quot;&gt;What is our workload&lt;/h3&gt;

&lt;p&gt;We have several iOS apps that share a common macOS compute cluster for their CI/CD workloads. &lt;br /&gt;
This includes, but is not limited to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grab-taxi-ride-food-delivery/id647268330&quot;&gt;Grab app&lt;/a&gt; (Largest iOS code base with approximately 2.5M+ total lines of code)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grab-driver-app-for-partners/id1257641454&quot;&gt;Grab Driver app&lt;/a&gt; (Second largest iOS code base with approximately 0.7M+ total lines of code)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/kartalink/id6450411148&quot;&gt;KartaLink&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grabmerchant/id1282271764&quot;&gt;GrabMerchant&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/kartaview/id1089548849&quot;&gt;KartaView&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/us/app/ovo/id1142114207&quot;&gt;OVO&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/ph/app/move-it-fast-moto-taxi-ride/id1481198245&quot;&gt;Move It: Fast Moto Taxi Ride&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/ph/app/move-it-driver-app/id6446633186&quot;&gt;Move It Driver App&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The workload primarily involves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Building apps&lt;/li&gt;
  &lt;li&gt;Execution of tests&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-evaluation-cloud-vs-colocation-vs-on-premises&quot;&gt;The evaluation: Cloud vs colocation vs on-premises&lt;/h2&gt;

&lt;p&gt;We did a comprehensive comparison and total cost of ownership (TCO) estimation to compare many different options, including cloud vendors and colocation in different places.&lt;/p&gt;

&lt;h3 id=&quot;cost-of-macos-compute&quot;&gt;Cost of macOS compute&lt;/h3&gt;

&lt;p&gt;The expense of macOS compute is notably higher, particularly in continuous integration (CI) setups, posing challenges for optimal configuration. Several factors contribute to these increased costs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apple’s restrictive EULA mandates a minimum lease period of 24 hours for macOS instances, which alters the utilization equation.&lt;/li&gt;
  &lt;li&gt;Economies of scale are not favorable for available macOS hardware configurations compared to alternatives. Optimized server hardware designed for racking offers various configurations that reduce operational costs, unlike macOS options such as Mac Mini and Mac Pro.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, although not a direct comparison, the &lt;a href=&quot;https://docs.github.com/en/billing/managing-billing-for-your-products/managing-billing-for-github-actions/about-billing-for-github-actions#per-minute-rates-for-standard-runners&quot;&gt;pricing for GitHub Actions build minutes&lt;/a&gt; shows macOS is ten times more costly than Linux. This reflects the pricing GitHub can offer after &lt;a href=&quot;https://www.youtube.com/watch?v=I2J2MzKjcqY&quot;&gt;implementing racking optimizations.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Initially, we conducted rough estimations to assess the total cost of ownership differences between cloud, colocation, and on-premises setups. Even with conservative estimates for manpower and engineering costs, colocation or on-premises setups proved more cost-effective at our scale. This cost disparity became even more pronounced when focusing on cloud vendors providing macOS compute physically located in Southeast Asia.&lt;/p&gt;

&lt;p&gt;We opted to conduct an in-depth evaluation of the following options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Establishing a macOS cluster at our headquarters in Singapore, which was swiftly dismissed due to scalability and cost concerns making it an unsuitable long-term solution.&lt;/li&gt;
  &lt;li&gt;Colocating in a Southeast Asian country where we have operational presence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;choice-of-location&quot;&gt;Choice of location&lt;/h3&gt;

&lt;p&gt;As a Southeast Asian company, we maintain offices in each country where we operate, some of which boast advanced data center infrastructures. We focused our location choices on Singapore and Malaysia, assessing them based on several criteria, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The maturity of existing data center infrastructure.&lt;/li&gt;
  &lt;li&gt;The proximity of the data centers to our offices, ensuring staff availability for infrastructure setup.&lt;/li&gt;
  &lt;li&gt;The cost and reliability of power.&lt;/li&gt;
  &lt;li&gt;The proximity to our Git servers and the expense of establishing direct network connections.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Eventually we concluded to go ahead with a decision to colocate in a data center in Malaysia &lt;a href=&quot;https://www.edgeconnex.com/news/edge-blog/southeast-asias-data-center-powerhouse-malaysia/&quot;&gt;which is one of the emerging data center powerhouses in the region&lt;/a&gt; with relatively low energy cost compared to Singapore.&lt;/p&gt;

&lt;h3 id=&quot;choice-of-mac-hardware&quot;&gt;Choice of Mac hardware&lt;/h3&gt;

&lt;p&gt;Our choice of hardware model for our build and test workload was guided by a cost-benefit analysis. We decided to use bare-metal setups without virtualization, simplifying migration processes, which may be revisited in the future. We ensured we neither over-specified nor under-specified the bare-metal hardware. We had a clear understanding of the resource consumption of our most demanding workload on a few reference models, as illustrated in the following graphs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. User and system CPU usage during build&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Memory usage&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;virtualization-vs-bare-metal&quot;&gt;Virtualization vs bare-metal&lt;/h3&gt;

&lt;p&gt;Virtualization offers significant advantages in managing and provisioning clusters, including the flexibility to create ephemeral builds. However, our experience with macOS virtualization has been mixed. While off-the-shelf virtualization solutions provide maintenance benefits, they often come at the cost of performance or stability.&lt;/p&gt;

&lt;p&gt;Key points:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Improved utilization&lt;/strong&gt;: Virtualization can improve resource utilization by consolidating multiple workloads on fewer physical servers, thereby reducing the overall cost.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Performance penalty&lt;/strong&gt;: However, the performance penalty associated with virtualization can sometimes negate these cost benefits. This is particularly true for macOS virtualization, where we have observed trade-offs in performance or stability.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evolution of virtualization&lt;/strong&gt;: The virtualization space has been evolving and making good progress. We may re-evaluate these solutions in the future as they continue to mature and potentially address current performance and stability issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our conclusion was to stick to bare-metal for the time-being as the benefits didn’t justify the downside and cost.&lt;/p&gt;

&lt;h2 id=&quot;execution&quot;&gt;Execution&lt;/h2&gt;

&lt;h3 id=&quot;progressive-migration&quot;&gt;Progressive migration&lt;/h3&gt;

&lt;p&gt;Any disruption to the macOS CI/CD cluster would be hugely disruptive to the company given our scale highlighted above. So, we enabled new cluster partially for part of the workload for a reasonably long period of time and monitored and compared:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Job failure rate&lt;/li&gt;
  &lt;li&gt;Jobs performance&lt;/li&gt;
  &lt;li&gt;Reliability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once we were confident, we made the full switch and terminated vendor contracts at due.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image7.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Total active jobs trend&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;The migration yielded better results overall than our initial conservative estimates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cost savings: Estimated over 2.4 million USD over three years&lt;/li&gt;
  &lt;li&gt;Performance improvement: Between 20-40% depending on the use case&lt;/li&gt;
  &lt;li&gt;Stability: No compromise&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A strategic investment in our mission to drive Southeast Asia forward by onshoring critical Mac infrastructure into the region.&lt;/p&gt;

&lt;h3 id=&quot;cost&quot;&gt;Cost&lt;/h3&gt;

&lt;p&gt;We anticipate a three-year replacement cycle for our hardware. While some equipment may be utilized beyond this period, it provides a reasonable lifespan for cost estimation purposes.&lt;/p&gt;

&lt;p&gt;The lifecycle of networking equipment involves both physical reliability, following the bathtub curve, and technological obsolescence, often necessitating replacement every 3 to 5 years. Mac minis could become outdated after approximately three years, making the opportunity cost of extended use potentially higher than the net replacement cost after benefits.&lt;/p&gt;

&lt;p&gt;Importantly, the experience gained during this cycle could significantly reduce the engineering costs associated with future replacements.&lt;/p&gt;

&lt;p&gt;Overall, we project total cost of ownership savings of approximately 2.4 million USD over a three-year period compared to our last cloud-based setup rented from a vendor.&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;We measured the performance gains in two of our largest iOS apps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Grab app&lt;/li&gt;
  &lt;li&gt;Grab Driver app&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;overall-gains&quot;&gt;Overall gains&lt;/h4&gt;

&lt;p&gt;The following table summarizes the total time measured before and after the migration for total CI pipeline time and building the app codebase. Measurements are presented in 3 percentiles (p50, p75, p95).&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th rowspan=&quot;2&quot;&gt;App / Metric&lt;/th&gt;
            &lt;th rowspan=&quot;2&quot;&gt;&lt;/th&gt;
            &lt;th colspan=&quot;3&quot;&gt;Time (Minutes)&lt;/th&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;th&gt;p50&lt;/th&gt;
            &lt;th&gt;p75&lt;/th&gt;
            &lt;th&gt;p95&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;CI pipeline time trend for the Grab app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;43&lt;/td&gt;
            &lt;td&gt;54&lt;/td&gt;
            &lt;td&gt;67&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;33&lt;/td&gt;
            &lt;td&gt;42&lt;/td&gt;
            &lt;td&gt;49&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;23.26%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;22.22%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;26.87%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;App build time trend for the Grab app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;10.7&lt;/td&gt;
            &lt;td&gt;13.2&lt;/td&gt;
            &lt;td&gt;17.6&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;6.45&lt;/td&gt;
            &lt;td&gt;9&lt;/td&gt;
            &lt;td&gt;10.8&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;39.72%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;31.82%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.64%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;Pipeline time trend for the Grab Driver app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;47&lt;/td&gt;
            &lt;td&gt;50&lt;/td&gt;
            &lt;td&gt;52&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;26&lt;/td&gt;
            &lt;td&gt;31&lt;/td&gt;
            &lt;td&gt;32&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;44.68%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.00%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.46%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;App build time trend for the Grab Driver app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;10&lt;/td&gt;
            &lt;td&gt;13&lt;/td&gt;
            &lt;td&gt;14&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;6&lt;/td&gt;
            &lt;td&gt;8&lt;/td&gt;
            &lt;td&gt;8.5&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;40.00%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.46%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;39.29%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;a-different-perspective-trends&quot;&gt;A different perspective: Trends&lt;/h4&gt;

&lt;p&gt;The following trend illustrations show how the performance of various tasks has improved while we progressively migrated to the new colocation setup.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image8.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. 14 day aggregate percentiles of p50, p75 and p95 for total CI pipeline times for the Grab app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image9.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. Pipeline time pulse for the Grab app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image10.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10. 14 day aggregate percentiles of p50, p75 and p95 for total CI pipeline times for the Grab Driver app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;stability&quot;&gt;Stability&lt;/h3&gt;

&lt;p&gt;We measured overall job failure rates between both clusters for extended periods as a guardrail metric and ensured the stability of the new cluster before shutting down the old one.&lt;/p&gt;

&lt;h2 id=&quot;colocation-setup-and-rack-configuration&quot;&gt;Colocation setup and rack configuration&lt;/h2&gt;

&lt;p&gt;The following table provides an overview of the layout of our new Mac mini cluster.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;Component&lt;/th&gt;
            &lt;th&gt;Description&lt;/th&gt;
            &lt;th&gt;Redundancy&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;Rack&lt;/td&gt;
            &lt;td&gt;We have got four 42RU (600x1200x42RU) racks housing 200+ Mac minis, plus some spare racks to house upcoming scheduled capacity upgrades.&lt;/td&gt;
            &lt;td&gt;Racks have shared resources which have their own redundancy. Generally rack separation does provide some level of redundancy for total compute.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Power&lt;/td&gt;
            &lt;td&gt;2 power sources power the cluster. Each rack is powered by these 2 power sources. It is 1U, 2-post rack mount.&lt;/td&gt;
                &lt;td&gt;Losing 1 power source will reduce 50% of capacity.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Mac Mini&lt;/td&gt;
            &lt;td&gt;We rack 2 Mac minis in a row on a mounting tray, typically racking 70 minis in one rack in total. Except for the first rack which requires extra rack units (RUs) for core switches and firewalls.&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;KVM&lt;/td&gt;
            &lt;td&gt;KVM switches with adaptor for keyboard and mouse emulation when required.&lt;/td&gt;
            &lt;td&gt;N/A&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Networking Setup&lt;/td&gt;
            &lt;td&gt;Networking consists of Core Switches, Access Switches, Firewalls, Internet and Direct Connect Links.&lt;/td&gt;
            &lt;td&gt;Mostly active/active redundancy.&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;provisioning-and-configuration&quot;&gt;Provisioning and configuration&lt;/h2&gt;

&lt;h3 id=&quot;zero-touch-provisioning&quot;&gt;Zero-touch provisioning&lt;/h3&gt;

&lt;p&gt;Zero-touch provisioning is a streamlined method for setting up and configuring devices with minimal manual intervention. This section outlines the process and benefits of zero-touch provisioning using Jamf for Mac minis.&lt;/p&gt;

&lt;p&gt;We have a setup that enables these machines to start accepting jobs once they are racked up and connected (Power and network cables). Here is how it works:&lt;/p&gt;

&lt;h4 id=&quot;mobile-device-management-mdm-configuration-and-automated-device-enrollment-ade&quot;&gt;Mobile Device Management (MDM) configuration and Automated Device Enrollment (ADE)&lt;/h4&gt;

&lt;p&gt;ADE, previously known as Device Enrollment Program (DEP), is an Apple service that facilitates automatic enrollment. When a new Mac Mini is acquired and registered in the organization’s ADE account, it is primed for automatic enrollment. Administrators create a PreStage enrollment configuration within Jamf Pro, encompassing account settings (e.g., creating a local admin account, hiding it in Users &amp;amp; Groups, skipping account creation for the user), configuration profiles (defining device settings, security policies, and restrictions), and enrollment packages (including necessary software and scripts).&lt;/p&gt;

&lt;h4 id=&quot;device-setup-activation-and-redirection&quot;&gt;Device setup: Activation and redirection&lt;/h4&gt;

&lt;p&gt;Upon powering on and connecting to the internet, the Mac Mini communicates with Apple’s activation servers. The activation servers identify the device as part of the organization’s ADE and redirect it to the Jamf MDM server, ensuring automatic enrollment without user input.&lt;/p&gt;

&lt;h4 id=&quot;enrollment-and-configuration&quot;&gt;Enrollment and configuration&lt;/h4&gt;

&lt;p&gt;The Mac Mini enrolls into the Jamf MDM system automatically. Jamf applies predefined configuration profiles to set up the device’s settings, installs required applications based on configured policies, and enforces security policies such as encryption and authentication settings to ensure compliance.&lt;/p&gt;

&lt;h4 id=&quot;key-benefits-of-zero-touch-provisioning&quot;&gt;Key benefits of zero-touch provisioning&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Devices are ready to use right out of the box, reducing the time and effort required by IT staff.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Ensures that all devices are configured uniformly according to organizational policies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Enforces security policies from the moment the device is first powered on, reducing vulnerabilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Easily manage and configure a large number of devices without manual intervention.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learnings-and-insights&quot;&gt;Learnings and insights&lt;/h2&gt;

&lt;h3 id=&quot;supply-chain-is-as-fast-as-the-last-essential-component-you-need&quot;&gt;Supply chain is as fast as the last essential component you need&lt;/h3&gt;

&lt;p&gt;The efficiency of a supply chain hinges on the delivery of its final essential component. Despite being a fundamental principle, it’s worth reiterating. Our timely launch was facilitated by a buffer period for unexpected delays. Interestingly, one of the last critical items to arrive was the rack mounting trays. The brief delay underscored the importance of prioritizing and planning for on-time delivery of every essential component, irrespective of its manufacturing simplicity.&lt;/p&gt;

&lt;h3 id=&quot;consistently-address-the-question-how-will-this-scale&quot;&gt;Consistently address the question: How will this scale?&lt;/h3&gt;

&lt;p&gt;From the outset, our goal was to develop a scalable infrastructure. As the cluster expands, tasks such as preparing Mac minis for job acceptance require increasing manual input, which ultimately impacts costs. Hence, zero-touch provisioning becomes essential, as scalability is not merely a desirable feature but a necessity.&lt;/p&gt;

&lt;h3 id=&quot;plan-and-opt-in-for-a-power-cost-structure-best-suits-for-your-need&quot;&gt;Plan and opt in for a power cost structure best suits for your need&lt;/h3&gt;

&lt;h4 id=&quot;power-cost-structures&quot;&gt;Power cost structures&lt;/h4&gt;

&lt;p&gt;In a colocation setup power costs can be billed in several ways, each with pros and cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Flat rate per circuit&lt;/strong&gt;: A fixed monthly fee, predictable but limits flexibility (e.g., can’t exceed 80% without extra circuits).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Allocated kW&lt;/strong&gt;: Commit to a fixed power amount (e.g., 100 kW), potentially cheaper but with penalties for overages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metered usage&lt;/strong&gt;: Pay for actual consumption (kWh), good for variable loads but may still charge for space.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;All-in Sspace and power&lt;/strong&gt;: Single rate covering both, easy to compare but less flexible for upgrades.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We ultimately opted for an allocated kW commitment, a phased approach based on conservative equipment power ratings and historical usage. We structured this into phases of commitment increases for future capacity growth.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Mac Cloud Exit wasn’t just a technical migration; it was a strategic move that fundamentally enhanced our engineering efficiency. By onshoring our infrastructure into Southeast Asia, we have achieved $2.4 million USD in projected savings and supercharged our CI pipeline, delivering performance gains of 20-40%. This project proves that taking ownership of our core infrastructure can be a major competitive advantage, allowing us to deliver faster and more reliably for our users across the region.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmacos&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 06 Nov 2025 00:00:05 +0000</pubDate>
        <link>https://engineering.grab.com/mac-cloud-exit</link>
        <guid isPermaLink="true">https://engineering.grab.com/mac-cloud-exit</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we built a custom vision LLM to improve document processing at Grab</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the world of digital services, accurate extraction of information from user-submitted documents such as identification (ID) cards, driver’s licenses, and registration certificates is a critical first step for processes like electronic know-your-customer (eKYC). This task is especially challenging in Southeast Asia (SEA) due to the diversity of languages and document formats.&lt;/p&gt;

&lt;p&gt;We began this journey to address the limitations of traditional Optical Character Recognition (OCR) systems, which struggled with the variety of document templates it had to process. While powerful proprietary Large Language Models (LLMs) were an option, they often fell short in understanding SEA languages, produced errors, hallucinations, and had high latency. On the other hand, open-sourced Vision LLMs were more efficient but not accurate enough for production.&lt;/p&gt;

&lt;p&gt;This prompted us to fine-tune and ultimately develop a lightweight, specialized Vision LLM from the ground up. This blog is our account of the entire process.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Simplified overview of how Vision LLM works.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;what-is-a-vision-llm&quot;&gt;What is a Vision LLM?&lt;/h3&gt;

&lt;p&gt;You’ve likely heard of LLMs that process text. You give the LLM a text prompt, and it responds with a text output. A Vision LLM takes this a step further by allowing the model to understand images. The basic architecture involves three key components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Image encoder&lt;/strong&gt;: This component ‘looks’ at an image and converts it into a numerical (vectorized) format.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vision-language projector&lt;/strong&gt;: It acts as a translator, converting the image’s numerical format into a representation that the language model can understand.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Language model&lt;/strong&gt;: The familiar text-based model that processes the combined image and text input to generate a final text output.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Vision LLM basic  architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;choosing-our-base-vision-llm-model&quot;&gt;Choosing our base Vision LLM model&lt;/h3&gt;
&lt;p&gt;We evaluated a range of LLMs capable of performing OCR and Key Information Extraction (KIE). Our exploration of open-source options—including Qwen2VL, miniCPM, Llama3.2 Vision, Pixtral 12B, GOT-OCR2.0, and NVLM 1.0—led us to select Qwen2-VL 2B as our base multimodal LLM. This decision was driven by several critical factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Efficient size&lt;/strong&gt;: It is small enough for full fine-tuning on GPUs with limited VRAM resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SEA language support&lt;/strong&gt;: Its tokenizer is efficient for languages like Thai and Vietnamese, indicating decent native vocabulary coverage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic resolution&lt;/strong&gt;: Unlike models that require fixed-size image inputs, Qwen2-VL can process images in their native resolution. This is crucial for OCR tasks as it prevents the distortion of text characters that can happen when images are resized or cropped.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We benchmarked Qwen2VL and miniCPM on Grab’s dataset. Our initial findings showed low accuracy, mainly due to the limited coverage of SEA languages. This motivated us to fine-tune the model to improve OCR and KIE accuracy. Training the LLM can be a very data-intensive and GPU resource-intensive process. Due to this, we had to address these two concerns before progressing further:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: How do we use open source and internal data effectively to train the model?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: How do we customize the model to reduce latency but keep high accuracy?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-dataset-generation&quot;&gt;Training dataset generation&lt;/h2&gt;

&lt;h3 id=&quot;synthetic-ocr-dataset&quot;&gt;Synthetic OCR dataset&lt;/h3&gt;

&lt;p&gt;We extracted the SEA languages text content from a large online text corpus—&lt;a href=&quot;https://commoncrawl.org/&quot;&gt;Common Crawl&lt;/a&gt; (internet dataset). Then, we used an in-house synthetic data pipeline to generate text images by rendering SEA text contents in various fonts, backgrounds and augmentations.&lt;/p&gt;

&lt;p&gt;The dataset contains text in Bahasa Indonesia, Thai, Vietnamese, and English. Each image has a paragraph of random sentences extracted from the dataset as shown in Figure 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Two synthetic sample images in Thai language used for model training.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;documint-ai-powered-auto-labelling-framework&quot;&gt;Documint: AI-powered, auto-labelling framework&lt;/h3&gt;

&lt;p&gt;Our experiments showed that applying document detection and orientation correction significantly improves OCR and information extraction. Now that we have an OCR dataset, we needed to generate a pre-processing dataset to further improve model training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Documint&lt;/strong&gt; is an internal platform developed by our team that creates an auto‑labelling and pre‑processing framework for document understanding. It prepares high‑quality, labelled datasets. Documint utilizes various submodules to effectively execute the full OCR and KIE task. We then used a pipeline with the large amount of Grab collected cards and documents to extract training labels. The data was further refined by a human reviewer to achieve high label accuracy.&lt;/p&gt;

&lt;p&gt;Documint has four main modules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Detection module&lt;/strong&gt;: Detect the region from the full picture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Orientation module&lt;/strong&gt;: Gives correction angle (e.g. if document is upside down, 180 degrees).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;OCR module&lt;/strong&gt;: Returns text values in unstructured format.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;KIE module&lt;/strong&gt;: Returns JSON values from unstructured text.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:85%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Pipeline overview of Documint.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;experimentation&quot;&gt;Experimentation&lt;/h2&gt;

&lt;h3 id=&quot;phase-1-the-lora-experiment&quot;&gt;Phase 1: The LoRA experiment&lt;/h3&gt;

&lt;p&gt;Our first attempt in fine-tuning a Vision LLM involved fine-tuning an open-source model Qwen2VL, using a technique called Low-Rank Adaptation (LoRA). LoRA is efficient because it allows lightweight updates to the model’s parameters, minimizing the need for extensive computational resources.&lt;/p&gt;

&lt;p&gt;We trained the model on our curated document data, which included various document templates in multiple languages. The performance was promising for documents with Latin scripts. Our experiment of LoRA fine-tuned Qwen2VL-2B achieved high field-level of accuracy for Indonesian documents.&lt;/p&gt;

&lt;p&gt;However, the fine-tuned model still struggled with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Documents containing non-Latin scripts like Thai and Vietnamese.&lt;/li&gt;
  &lt;li&gt;Unstructured layouts with small, dense text.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-2-the-power-of-full-fine-tuning&quot;&gt;Phase 2: The power of full fine-tuning&lt;/h3&gt;

&lt;p&gt;Our experiments revealed a key limitation. While open-source Vision LLMs often have extensive multi-lingual corpus coverage for the LLM decoder’s pre-training, they lack visual text in SEA languages during vision encoder and joint training. This insight drove our decision to pursue full parameter fine-tuning for optimal results.&lt;/p&gt;

&lt;p&gt;Drawing from the &lt;a href=&quot;https://arxiv.org/abs/2304.08485&quot;&gt;Large Language and Vision Assistant (LLAVA)&lt;/a&gt; methodology, we implemented a two-stage training approach illustrated in Figure 5.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: From left to right—two-stage training process.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1 - Continual pre-training&lt;/strong&gt;: We first trained the vision components of the model using synthetic OCR datasets that we created for Bahasa Indonesia, Thai, Vietnamese, and English. This helps the model to learn the unique visual patterns of SEA scripts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2 - Full-parameter fine-tuning&lt;/strong&gt;: We then fine-tuned the entire model—vision encoder, projector, and language model—using our task-specific document data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 1: OCR Field level accuracy between the baseline and Qwen2-VL 2B model. (pp: percentage points).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The fully fine-tuned Qwen2-VL 2B model delivered significant improvement, especially on documents that the LoRA model struggled with.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Thai document accuracy increased &lt;strong&gt;+70pp&lt;/strong&gt; from baseline.&lt;/li&gt;
  &lt;li&gt;Vietnamese document accuracy rose &lt;strong&gt;+40pp&lt;/strong&gt; from baseline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-3-building-a-lightweight-1b-model-from-scratch&quot;&gt;Phase 3: Building a lightweight 1B model from scratch&lt;/h3&gt;

&lt;p&gt;While the Qwen2VL-2B model was a success, the full fine-tuning pushed the limits of GPUs. To optimize resources used and to create a model perfectly tailored to our needs, we decided to build a lightweight Vision LLM (~1B parameters) from scratch.&lt;/p&gt;

&lt;p&gt;Our strategy was to combine the best parts of all models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We took the powerful &lt;strong&gt;vision encoder&lt;/strong&gt; from the larger Qwen2-VL 2B model.&lt;/li&gt;
  &lt;li&gt;We paired it with the compact and efficient &lt;strong&gt;language decoder&lt;/strong&gt; from the Qwen2.5 0.5B model.&lt;/li&gt;
  &lt;li&gt;We connected them with an &lt;strong&gt;adjusted projector layer&lt;/strong&gt; to ensure they could work together seamlessly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This created a custom ~1B parameter Vision LLM optimized for training and deployment.&lt;/p&gt;

&lt;h4 id=&quot;four-stages-in-training-our-custom-model&quot;&gt;Four stages in training our custom model&lt;/h4&gt;

&lt;p&gt;We trained our new model using a comprehensive four-stage process as shown in Figure 6.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6: From left to right— four stages of model training.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1 - Projector alignment&lt;/strong&gt;: The first step was to train the new projector layer to ensure the vision encoder and language decoder could communicate effectively.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2 - Vision tower enhancement&lt;/strong&gt;: We then trained the vision encoder on a vast and diverse set of public multimodal datasets, covering tasks like visual Q&amp;amp;A, general OCR, and image captioning to improve its foundational visual understanding.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 3 - Language-specific visual training&lt;/strong&gt;: We trained the model on two types of synthetic OCR data. Without this stage, performance on non-Latin documents dropped by as much as 10%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 4 - Task-centric fine-tuning&lt;/strong&gt;: Lastly, we performed full-parameter fine-tuning on our custom 1B model using our curated document dataset.&lt;/p&gt;

&lt;h4 id=&quot;the-final-results-are-as-follow&quot;&gt;The final results are as follow:&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It achieved performance comparable to the larger 2B model, &lt;strong&gt;staying within a 3pp accuracy gap across most document types.&lt;/strong&gt;  The model also maintained strong generalization when trained on quality-augmented datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The latency of our model far outperforms the 2B model, as well as traditional OCR models, as well as external APIs like chatGPT or Gemini. One of the biggest weaknesses we identified with external APIs was the P99 latency, which can easily be 3 to 4x the P50 latency, which would not be acceptable for Grab’s large scale rollouts.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 2: Performance comparison between Qwen2-VL 2B and 1B sized Vision LLM.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;

&lt;p&gt;Our work demonstrates that strategic training with high-quality data enables smaller, specialized models to achieve remarkable efficiency and effectiveness. Here are the critical insights from our extensive experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Full fine-tuning is superior&lt;/strong&gt;: For specialized, non-Latin script domains, full-parameter fine-tuning dramatically outperforms LoRA.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lightweight models are effective&lt;/strong&gt;: A smaller model (~1B) built from scratch and trained comprehensively can achieve near state-of-the-art results, validating the custom architecture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Base model matters&lt;/strong&gt;: Starting with a base model that has native support for your target languages is crucial for success.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data is king&lt;/strong&gt;: Meticulous dataset preprocessing and augmentation plays a critical role in achieving consistent and accurate results.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Native resolution is a game changer&lt;/strong&gt;: A model that can handle dynamic image resolutions preserves text integrity, dramatically improves OCR capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our journey demonstrates that specialized Vision LLMs can effectively replace traditional OCR pipelines with a single, unified, highly accurate model—opening new possibilities for document processing at scale.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 3: Comparison of model types .&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;As we continue to enhance our Vision LLM capabilities, exciting developments are underway:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Smarter, more adaptable models&lt;/strong&gt;: We’re developing Chain of Thought-based OCR and KIE models to strengthen generalisation capabilities and tackle even more diverse document scenarios.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Expanding across Southeast Asia&lt;/strong&gt;: We’re extending support to all Grab markets, bringing our advanced document processing to Myanmar, Cambodia, and beyond.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2409.12191&quot;&gt;https://doi.org/10.48550/arXiv.2409.12&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Improved Baselines with Visual Instruction Tuning: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2310.03744&quot;&gt;https://doi.org/10.48550/arXiv.2310.03744&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2107.09313&quot;&gt;https://doi.org/10.48550/arXiv.2107.09313&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2403.13372&quot;&gt;https://doi.org/10.48550/arXiv.2403.13372&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmodel&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 04 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/custom-vision-llm-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/custom-vision-llm-at-grab</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Machine-learning predictive autoscaling for Flink</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As Grab transitions to derive more valuable insights from our wealth of operational data, we are witnessing a steep increase in stream-processing applications. Over the past year, the number of Flink applications grew 2.5 times, driven by interest in real-time stream processing and the improved accessibility of developing such applications with Flink SQL. At this scale, it has become crucial for the internal Flink platform team to provide a &lt;strong&gt;cost-effective&lt;/strong&gt; and &lt;strong&gt;self-service&lt;/strong&gt; offering that supports users of diverse backgrounds.&lt;/p&gt;

&lt;h2 id=&quot;background-flink-at-grab&quot;&gt;Background: Flink at Grab&lt;/h2&gt;

&lt;p&gt;Flink at Grab is deployed in application mode, each pipeline has its own isolated resources for JobManager and TaskManager. Flink pipeline creators control both application logic and deployment configuration that affect throughput and performance, including OSS configurations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of TaskManagers and task slots per TaskManager&lt;/li&gt;
  &lt;li&gt;CPU cores per TaskManager&lt;/li&gt;
  &lt;li&gt;Memory per TaskManager&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As pipeline creation has become more accessible, users of different backgrounds (analyst, data scientist, engineers, etc.) often struggle to choose a set of configurations that work for their applications. Many go through a long process of trial and error and still end up over-provisioning their applications, leading to huge resource waste. Moreover, pipeline behavior changes over time due to changes in application logic or data pattern, invalidating previous efforts in tuning and causing users to repeat the exercise.&lt;/p&gt;

&lt;p&gt;In this article, we focus on addressing the challenge of efficient CPU provisioning for TaskManagers, as CPU constraints are a common bottleneck in our clusters. Our solution specifically targets Flink applications sourcing data from our message bus system (eg. Kafka, Change Data Capture Streams, DynamoDB Streams) , which represents the majority of our use cases. These workloads offer significant opportunities for cost savings due to their clear seasonal patterns, making them an ideal starting point for optimising autoscaling strategies.&lt;/p&gt;

&lt;h2 id=&quot;limits-of-reactive-autoscaling&quot;&gt;Limits of reactive autoscaling&lt;/h2&gt;

&lt;h3 id=&quot;our-initial-reactive-setup&quot;&gt;Our initial reactive setup&lt;/h3&gt;

&lt;p&gt;Our first automated solution relied on Flink’s Adaptive Scheduler in Reactive Mode. In this mode, each Flink application is deployed as its own individual Flink cluster running a dedicated job. The cluster greedily uses all available TaskManagers and scales its job parallelism accordingly. Running on Kubernetes, the cluster relies on Horizon Pod Autoscaler (HPA) to scale the number of TaskManager pods based on metrics such as CPU usage or custom metrics such as the pipeline’s consumer latency. While this solution was helpful initially, we quickly observed multiple issues with it.
It is important to note that while the below issues can be solved by fine-tuning, it is a tedious trial and error effort that only works for specific applications, requiring users to repeat the process for every pipeline they own.&lt;/p&gt;

&lt;h3 id=&quot;restart-spike-root-cause-of-many-issues&quot;&gt;Restart spike: root cause of many issues&lt;/h3&gt;

&lt;p&gt;When autoscaling a Flink pipeline, the job restarts from the last checkpoint. This triggers an immediate spike in load, as the pipeline must reprocess records from the period between the last checkpoint and job restart, along with any new records that were backlogged at the source during the downtime. As a result, CPU usage and P99 consumer latency  typically spikes after scaling events, for example, at 00:05 and 00:55, as shown in Figure 1. These spikes occur even though there is no change in source topic throughput. In this case, CPU usage surges from 0.5 cores to near provision limit of 2.5 cores,  while consumer latency temporarily spiked from sub-second levels to as high as three minutes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/cpu-usage-con-latency-after-restart.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: CPU usage and consumer latency spike after a pipeline restart.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;reactive-spiral-and-fluctuation&quot;&gt;Reactive spiral and fluctuation&lt;/h3&gt;

&lt;p&gt;Typically, HPA scales on metrics such as CPU usage, consumer latency, or backpressure crossing a defined threshold. The challenge arises if these thresholds are misconfigured. The HPA’s reactive nature, when combined with restart spikes, can become detrimental to your Flink application. It piles additional load onto a system that’s already degrading, further amplifying the problem.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/reactive-scaling-fluctuation.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: A reactive scaling incident that demonstrates scaling fluctuations and restarts.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 2 provides us a case study of reactive spiral and fluctuation, assuming we are having a pipeline that consumes a Kafka topic of 300 partitions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;07:00: As the source topic throughput increases, the P99 consumer latency rises due to insufficient processing power.&lt;/li&gt;
  &lt;li&gt;07:15: Reactive scaling is triggered, resulting in a scale out event. This is reflected in the increased TaskManager and task slot count. The pipeline continues to operate, as there is no increase in restart count.&lt;/li&gt;
  &lt;li&gt;07:30: As the P99 consumer latency remains high, reactive scaling continues to scale out incrementally. The records in rate by task rises rapidly as the pipeline reprocesses data from the checkpoint. During this period, the pipeline repeatedly restarts CPU usage drops significantly, and P99 consumer latency spikes to nearly one hour. This marks the onset of a spiral failure.&lt;/li&gt;
  &lt;li&gt;08:00: Reactive scaling reaches its upper limit of 300 slots, corresponding to the number of partitions in the source topic. This halts the spiral effect as it cannot scale out any further. Without disruption from autoscaling restart, the pipeline begins to process the backlog since the last successful checkpoint, as observed by the significant increase in records in rate by task. As the pipeline catches up, it eventually stabilizes, and the P99 consumer latency returns to normal levels.&lt;/li&gt;
  &lt;li&gt;08:30 - 10:15: The P99 consumer latency returns to normal levels, below the threshold. Reactive scaling triggers scale-in events despite the source topic throughput continuing to trend upward. During these scale-in events, P99 latency fluctuates, occasionally spiking up to 15 minutes. However, these fluctuations are not severe enough to prevent the repeated scale in process.&lt;/li&gt;
  &lt;li&gt;10:15: The P99 consumer latency rises again, triggering a scale-out event back to the upper limit of 300 slots.&lt;/li&gt;
  &lt;li&gt;11:15-11:45: Despite the source topic throughput maintaining an upward trend, the pipeline undergoes multiple scale-in events in quick succession, encounters latency issues due to reprocessing data from checkpoints, and scales out again shortly after. This is an example of fluctuation after scaling in, resulting in 6 restarts within a 30 minutes window.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;limited-parallelism-constraints&quot;&gt;Limited parallelism constraints&lt;/h3&gt;

&lt;p&gt;Even with HPA, we frequently encounter a bottleneck when trying to scale our applications’ throughput. This is primarily because some of our connectors, most notably the Kafka connector, don’t inherently support dynamic parallelism changes.
Kafka topics, by design, have a fixed number of partitions. This directly limits the number of parallel consumers we can run. Consequently, once we reach this maximum parallelism for our consumers, we often have to scale up resources, for example, increase memory/CPU per instance instead of scaling out (adding more instances).&lt;/p&gt;

&lt;h2 id=&quot;predictive-resource-advisor&quot;&gt;Predictive Resource Advisor&lt;/h2&gt;

&lt;h3 id=&quot;assumptions-and-hypothesis&quot;&gt;Assumptions and hypothesis&lt;/h3&gt;

&lt;p&gt;To tackle the issue of reactive spirals and fluctuations, the new solution should have the following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vertical scaling: To tackle the issue of limited parallelism with our dependencies, we should be looking at vertical instead of horizontal scaling.&lt;/li&gt;
  &lt;li&gt;Predictive: Adjust CPU to scale up or down before demand spikes or dips occur, ensuring the system is prepared for changes in workload. This prevents artificial workload increases caused by processing backlogs on top of actual workload increase, further straining the system.&lt;/li&gt;
  &lt;li&gt;Deterministic: The CPU configuration must be precisely calculated based on the workload demand, ensuring predictable and consistent resource allocation. For a given workload, the calculated CPU value should remain the same every time, eliminating variability and uncertainty in scaling decisions.&lt;/li&gt;
  &lt;li&gt;Accurate: Determine the optimal CPU configuration required to handle workload demand in a single, precise calculation, avoiding the inefficiencies of multi-step, trial-and-error tuning.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;key-observations&quot;&gt;Key observations&lt;/h3&gt;

&lt;p&gt;Our solution is conceptualized based on key observations of our Flink applications:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The CPU usage of Flink applications is primarily driven by the input load.&lt;/li&gt;
  &lt;li&gt;The input load of our Flink applications can be accurately forecasted using time-series forecasting techniques.&lt;/li&gt;
  &lt;li&gt;Time-based autoscaling that relies solely on historical CPU usage is not robust enough to adapt to evolving workloads. This approach also carries the risk of a negative self-amplifying feedback loop: each autoscaling restart causes a CPU usage spike (as illustrated in Figure 1), which, if anomalies are not properly handled, inflates subsequent CPU calculations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;model-formulation&quot;&gt;Model formulation&lt;/h3&gt;

&lt;p&gt;We then formulate the relationship between CPU usage and input load using a regression model to provide a mathematical framework for predicting CPU requirements based on workload patterns, expressed as:&lt;/p&gt;

&lt;p style=&quot;text-align:center; font-weight:bold;&quot;&gt;C&lt;sub&gt;t&lt;/sub&gt; = f(x&lt;sub&gt;t&lt;/sub&gt;)&lt;/p&gt;

&lt;p&gt;In this equation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;C&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; represents the CPU required at a specific point in time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; represents the input workload at the corresponding point in time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;f()&lt;/strong&gt; represents the regression function that maps the input load to the required CPU capacity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Input load, represented by Kafka source topic throughput in our case, is chosen as the independent variable &lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; because it reflects true business demand and is entirely independent of Flink consumers. This metric is influenced solely by the business logic of upstream producers and remains unaffected by any changes or behaviors in the Flink consumer pipeline.&lt;/p&gt;

&lt;h3 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h3&gt;

&lt;p&gt;Our predictive autoscaler operates through four key stages as shown in Figure 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/predictive-autosclaing-flow.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: The predictive autoscaling system operates through four key stages.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1: Workload forecast model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The workload forecast model is a time-series forecasting model trained on actual workload data, specifically source topic throughput from our Kafka cluster (1). This approach is particularly effective as our workload exhibits seasonal patterns. While historical data could be directly used as input for CPU prediction, time-series forecasting offers a more robust solution by enabling the model to account for organic traffic growth over time. Through periodic retraining, the model adapts to evolving workload trends, ensuring more accurate and reliable predictions for resource provisioning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2: Resource prediction model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This follows the regression-based model &lt;strong&gt;C&lt;sub&gt;t&lt;/sub&gt; = f(x&lt;sub&gt;t&lt;/sub&gt;)&lt;/strong&gt; defined earlier. We use the same source topic throughput from our Kafka cluster (2a) as  input feature &lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;, and  the Flink application’s Kubernetes CPU usage metric (2b) as output label &lt;strong&gt;C&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; for model training. To ensure clean and representative data for model training, we collect CPU usage metrics under conditions that simulate infinite resource availability. We include data  exclusively from periods of continuous and stable operation, as determined by latency, uptime, and restart metrics (2b), eliminating biases caused by hardware limitations or disruptions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 3: Workload forecasting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To prepare for autoscaling, we forecast the workload for the future t-hour window (3) using our trained time-series forecast model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 4: Predict CPU usage&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The forecasted workload (3) is fed into the resource prediction model to estimate the CPU usage required to handle that workload. The predicted value is then refined using custom safety feature adjustments to account for variability and ensure stability. This adjusted prediction is passed to the custom autoscaler controller, which evaluates the current CPU configuration of the TaskManager deployment. If the adjusted predicted value differs from the existing CPU configuration, the controller initiates vertical scaling to update the TaskManager deployment accordingly.&lt;/p&gt;

&lt;h2 id=&quot;proof-of-concept-and-results&quot;&gt;Proof of concept and results&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;Experiment setup&lt;/h3&gt;

&lt;p&gt;To validate our hypothesis, we present a deep dive into one of our experiments. This pipeline features complex business logic, aggregates from multiple Kafka sources, with a checkpoint interval of one minute and a maximum consumer latency of five minutes.&lt;/p&gt;

&lt;p&gt;We set up an experimental pipeline with configurations identical to the production pipeline (the control). Both applications sourced data from the same Kafka topics but sank data to alternative topics to maintain isolation. The Predictive Resource Advisor was enabled on the experimental pipeline, while the control pipeline operated with fixed CPU provisioning.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Figure 4 demonstrates a strong correlation between CPU usage (yellow, green) and the total Kafka topics throughput. The variable CPU provisioning (blue) for the experimental pipeline is calculated by our autoscaler models, which were trained exclusively on data collected from the experiment pipeline. The CPU usage trend of the experimental pipeline closely mirrors that of the control pipeline and remains aligned with the Kafka throughput trend. However, the experimental pipeline’s CPU provisioning is dynamically adjusted to more closely match its actual CPU usage, whereas the control pipeline maintains a static CPU allocation (purple). This illustrates the model’s effectiveness in dynamically adjusting CPU allocation to meet variable workload demands.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/cpu-usage-source-throughput.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: CPU usage closely correlates with source throughput for both the experimental and control pipelines.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Without autoscaler enabled, the control pipeline experienced no disruptions and maintained latency (blue) consistently below one second, which is not visible in Figure 5. On the other hand, the experiment pipeline latency (red) experienced a highest recorded peak latency of just over four minutes during a single disruption window. Other latency spikes observed were comparable to or lower than the three minutes peak latency previously identified as part of the restart spike issue analysis. The varied durations and amplitudes of these spikes showed some correlation with the heavy Kafka topic throughput during those periods. Importantly, there were only nine autoscaling events throughout the day, resulting in nine restarts for the experiment pipeline.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/impact-autoscaling-sla.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: Autoscaling impacts service-level agreement requirements through latency spikes during scaling events.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;outcome&quot;&gt;Outcome&lt;/h3&gt;

&lt;p&gt;The Predictive Resource Advisor solution has been successfully deployed across more than 50% of applicable production applications, specifically those consuming from Kafka topics and exhibiting seasonal workload patterns with some tolerance for disruptions. This implementation has delivered significant results across three key areas, stability, efficiency, and user experience.&lt;/p&gt;

&lt;h4 id=&quot;stability&quot;&gt;Stability&lt;/h4&gt;

&lt;p&gt;With autoscaling becoming more predictable and controllable, our Flink applications experience fewer disruptions caused by autoscaling fluctuations. The machine learning and predictive capabilities of the solution also ensure that applications remain operational during periods of increased workload by automatically learning and adapting to organic growth trends and workload surges.&lt;/p&gt;

&lt;h4 id=&quot;efficiency&quot;&gt;Efficiency&lt;/h4&gt;

&lt;p&gt;Applications powered by the Predictive Resource Advisor demonstrated significant improvements in CPU provisioning, aligning CPU configuration more closely with actual requirements, particularly during low traffic periods. As a result of this optimization, on average, these applications made approximately &amp;gt;35% savings in cloud infrastructure cost.&lt;/p&gt;

&lt;h4 id=&quot;user-experience&quot;&gt;User experience&lt;/h4&gt;

&lt;p&gt;The solution has simplified the deployment process for users, allowing them to simply deploy Flink applications with default configurations. The Predictive Resource Advisor automatically collects data, trains autoscaling models, and applies configuration changes, thus eliminating the need for manual fine-tuning. This significantly enhances the user experience by streamlining pipeline maintenance and enabling self-service capabilities, such as effortless onboarding. It empowers users to explore and derive value from real-time features with minimal effort.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Our journey doesn’t stop here. We’re continuously working to enhance our predictive autoscaler, with the following key areas of focus:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Tackling memory configuration (Predictive Resource Advisor’s next frontier)&lt;/strong&gt; &lt;br /&gt;
Memory is critical yet often misconfigured that can lead to unrecoverable failures for example, OOMKilled. Our next major goal for the Predictive Resource Advisor is to take on memory tuning, completely removing the burden of complex memory configuration from our users and further empowering them.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhancing model accuracy&lt;/strong&gt; &lt;br /&gt;
To further improve the robustness of our predictions, we are actively exploring advanced techniques in input feature engineering and anomaly detection, especially for workloads exhibiting frequent bursting patterns. By refining these aspects, we aim to extend the applicability of our solution to a broader range of Flink applications, including those connected to diverse sources such as change data capture systems or batch-like, spiky workloads, such as the Flink applications powering our real-time data lake.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Streamlining model training&lt;/strong&gt; &lt;br /&gt;
We’re developing a more efficient model training workflow. A particularly exciting avenue we’re investigating is the use of pretrained time-series forecasting models based on large language model architectures.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/resource-providers/standalone/kubernetes/#application-mode&quot;&gt;Flink deployed in Application Mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/elastic_scaling/#reactive-mode&quot;&gt;Flink Elastic Scaling in Reactive Mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/system/files/osdi18-kalavri.pdf&quot;&gt;Three steps is all you need: fast, accurate, automatic scaling decisions for distributed streaming dataflows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmlflink&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Oct 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/ml-predictive-autoscaling-for-flink</link>
        <guid isPermaLink="true">https://engineering.grab.com/ml-predictive-autoscaling-for-flink</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        <category>data-science</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Modernising Grab’s model serving platform with NVIDIA Triton Inference Server</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/catwalk-evolution&quot;&gt;Catwalk&lt;/a&gt; is Grab’s machine learning (ML) model serving platform, designed to enable data scientists and engineers in deploying production-ready inference APIs. Currently, Catwalk powers hundreds of ML models and online deployments. To accommodate this growth, the platform has adapted to the rapidly evolving machine learning technology landscape. This involved progressively integrating support for multiple frameworks such as ONNX, PyTorch, TensorFlow, and vLLM. While this approach initially worked for a limited number of frameworks, it soon became unsustainable as maintaining various inference engines, ensuring backward compatibility, and managing deprecated legacy components (such as the ONNX server) introduced significant technical debt. Over time, this resulted in degraded platform performance: with increased latency, reduced throughput, and escalating costs. These issues began to impact users, as larger models could no longer be served efficiently or cost-effectively by legacy components. Recognising the need for change, the team revisited the platform’s design to address these challenges.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-and-implementation&quot;&gt;Evaluation and implementation&lt;/h2&gt;

&lt;p&gt;After evaluating other industry-leading model serving platforms and studying best practices, we decided to conduct an in-depth analysis of NVIDIA Triton. Triton offers significant advantages as an inference engine, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Multi-framework support&lt;/strong&gt;: Compatibility with major ML frameworks, including ONNX, PyTorch, and TensorFlow, ensuring versatility and broad applicability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unified inference interface&lt;/strong&gt;: Provides a single, consistent API for various ML frameworks, simplifying user interaction and reducing overhead when switching between models or frameworks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Hardware optimisation&lt;/strong&gt;: Optimised for NVIDIA GPUs, Triton delivers strong performance on CPU-only environments and specialised instances like AWS Inferentia.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Up-to-date support&lt;/strong&gt;: Continuously updated by upstream to support the latest optimisation and features from upstream ML frameworks, ensuring access to cutting-edge capabilities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Advanced inference features&lt;/strong&gt;: Includes capabilities like dynamic batching and model ensembling (model pipelining), which enhances throughput and efficiency for complex ML workflows.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our extensive benchmarking demonstrated that NVIDIA Triton delivers substantial enhancements in both performance and service stability compared to our existing solutions.&lt;/p&gt;

&lt;p&gt;We are now working towards consolidating the various inference engines we manage into a unified, all-in-one Triton engine, beginning with ONNX adoption as the first phase of implementation.&lt;/p&gt;

&lt;p&gt;In this blog, we aim to share our journey of adopting Triton. From initial benchmarking results on one of Grab’s core models facing performance challenges, to the development of the “Triton manager”, a component designed to integrate Triton into our platform seamlessly and with minimal user disruption. Ultimately, more than 50% of online deployments were successfully migrated to Triton, with some of our critical systems achieving a 50% improvement in tail latency.&lt;/p&gt;

&lt;h2 id=&quot;exploratory-benchmark-results&quot;&gt;Exploratory benchmark results&lt;/h2&gt;

&lt;p&gt;We conducted rigorous testing of Triton against our existing ONNX server under varying levels of request traffic.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/table-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 1: Benchmark results of Triton against Catwalk ONNX server.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;During testing with a transformer-based model, Triton demonstrated the ability to handle at least 5 times the traffic while maintaining excellent latency. Additionally, its performance was further enhanced with features like batching enabled, and there is potential for even greater optimisation by converting the model to TensorRT, leveraging GPU support.&lt;/p&gt;

&lt;p&gt;Through profiling, we learned that a handful of ONNX Runtime knobs have an outsized impact on throughput. One low-effort, high-return tweak is to set the intra-op thread count to match the number of physical CPU cores. In most cases, this single change yields a healthy performance lift, sparing us from time-consuming, model-by-model micro-optimisation.&lt;/p&gt;

&lt;h2 id=&quot;adopting-triton-at-scale&quot;&gt;Adopting Triton at scale&lt;/h2&gt;

&lt;p&gt;While the benchmark results clearly demonstrate Triton’s advantages, the primary challenge was ensuring a seamless migration, ideally with minimal user reactions. Given the high frequency of migrations within our company, even exceptional performance improvements are often insufficient to fully motivate internal users to adopt new systems. From our point of view, a successful migration required:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Maintaining API compatibility with existing systems.&lt;/li&gt;
  &lt;li&gt;Ensuring zero-downtime.&lt;/li&gt;
  &lt;li&gt;Preserving all existing functionality while adding new capabilities.&lt;/li&gt;
  &lt;li&gt;Minimising disruption to downstream services and users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To streamline the migration process, we opted to manage it centrally within our platform, rather than relying on individual users to address the details themselves.&lt;/p&gt;

&lt;p&gt;We landed on the idea of offering Triton to our users as a drop-in replacement for the old server, with the help of a new component, “Triton manager”. The Triton manager is a critical component that glues Triton to the Catwalk ecosystem. It consists of two major components: Triton server manager and Triton proxy.&lt;/p&gt;

&lt;p&gt;Triton server manager is designed as the entry point of our Catwalk Triton. It downloads the model from remote storage, runs verification on the model files, prepares per-model configurations based on users’ customisation, and lastly it launches the Triton server. It also periodically checks the server’s health and provides observability overlooking the server’s status.&lt;/p&gt;

&lt;p&gt;Triton proxy provides backward compatibility to the existing clients. It hosts endpoints that translate requests from the older API and forward them to the Triton server. The proxy layer plays a crucial role in facilitating a seamless transition from our legacy servers, eliminating the need for user code changes. The conversion logic is designed to prioritise performance, ensuring minimal overhead. Extensive benchmarks were conducted during development to validate and optimise its efficiency.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: High-level architecture for Triton Inference Server (TIS) deployment at Catwalk.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Finally, a special mode in the Triton server manager is implemented to allow the Triton Inference Server (TIS) to be backward compatible with the command line interface of the existing ONNX runtime server used in Catwalk.&lt;/p&gt;

&lt;p&gt;We plan to enhance the Triton Manager to ensure backward compatibility with other ML frameworks, as part of our efforts to onboard additional frameworks seamlessly.&lt;/p&gt;

&lt;h2 id=&quot;rollout-result&quot;&gt;Rollout result&lt;/h2&gt;

&lt;p&gt;Within just 10 days of Triton’s availability, we successfully rolled it out to over 50% of our online model deployments. Thanks to rigorous testing for backward compatibility, the rollout was seamless, with most users unaware of the transition while benefiting from the improved performance.&lt;/p&gt;

&lt;h2 id=&quot;tritons-impacts-on-critical-models&quot;&gt;Triton’s impacts on critical models&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Latency before and after rollout in ms. Blue line: XGBoost-based model. Orange line: transformer-based model. Solid line: average. Dashed line: p99&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We’ve observed significant performance improvements in our business-critical models that have high demands for stability. Latency improvements were consistently observed in all models, especially in the models that suffered from highly volatile request traffic. For some larger transformer models, the p90 latency decreased dramatically from 120ms to 20ms, and the average latency remained steady at 4ms. Smaller XGBoost models maintained their average latency at 2ms across regions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt; Figure 3: Number of pods, before (blue line) and after (purple line) rollout in another model.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Triton has delivered significant cost savings for certain models, with some achieving over 90% reductions due to its advanced optimisations. These improvements have come alongside enhanced performance and reliability.&lt;/p&gt;

&lt;p&gt;It is worth noting that Triton was initially rolled out with limited capabilities to prioritise backward compatibility and ensure a seamless migration. However, we’ve noticed that higher tail latency still remains an issue when facing request spikes for larger models in production. To address this, we are working on enabling batching through Triton to minimise tail latency during traffic surges. This effort will involve close collaboration with model owners to optimise the capacity of each Triton instance further.&lt;/p&gt;

&lt;h2 id=&quot;early-cost-impact-of-the-migration&quot;&gt;Early cost impact of the migration&lt;/h2&gt;

&lt;p&gt;To gauge the financial upside of migrating to Triton, we took a snapshot of 11 production ML services that had already completed the migration. For every ML service, we compared its infrastructure spend over the 14 days before the cut-over with the 14 days after.&lt;/p&gt;

&lt;p&gt;Despite the staggered migration dates, the trend was uniform: average spend fell by ~ 20% across this small cohort within 14 days. As more models and applications migrate, we expect the absolute dollar savings to scale proportionally.&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;Initial results are aligned with our benchmarks for the Triton migration. With improved performance and cost reduction, we expect model owners to either upgrade their model sizes or allow for higher Queries Per Second (QPS). While making further progress with the overall Triton migration, the model serving platform team will continue to monitor cost differences and provide consultation to model owners who seek further optimisation for their deployments.&lt;/p&gt;

&lt;p&gt;Another key takeaway is the painless migration of Triton for our internal users. Rather than asking internal users to make necessary code changes, our team dedicated significant time to providing Triton as a drop-in inference engine to minimise any inconvenience of migration.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Big appreciation to Shengwei Pang from the Geo team, Khai Hung Do, Nhat Minh Nguyen, and Siddharth Pandey from the Catwalk team, along with Richard Ryu from the PM team and Padarn George Wilson for the sponsorship. &lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmodel&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 21 Oct 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/modernising-grab-model-serving-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/modernising-grab-model-serving-platform</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        <category>data-science</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
  </channel>
</rss>
