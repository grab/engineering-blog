<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 27 Jan 2021 13:36:48 +0000</pubDate>
    <lastBuildDate>Wed, 27 Jan 2021 13:36:48 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Serving driver-partners data at scale using mirror cache</title>
        <description>&lt;p&gt;Since the early beginnings, driver-partners have been the centerpiece of the wide-range of  services or features provided by the Grab platform. Over time, many backend microservices were developed to support our driver-partners such as earnings, ratings, insurance, etc. All of these different microservices require certain information, such as name, phone number, email, active car types, and so on, to curate the services provided to the driver-partners.&lt;/p&gt;

&lt;p&gt;We built the &lt;strong&gt;Drivers Data service&lt;/strong&gt; to provide drivers-partners data to other microservices. The service attracts a high QPS and handles 10K requests during peak hours. Over the years, we have tried different strategies to serve driver-partners data in a resilient and cost-effective manner, while accounting for low response time. In this blog post, we talk about &lt;strong&gt;mirror cache&lt;/strong&gt;, an in-memory local caching solution built to serve driver-partners data efficiently.&lt;/p&gt;

&lt;h2 id=&quot;what-we-started-with&quot;&gt;What we started with&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image1.png&quot; alt=&quot;Figure 1. Drivers Data service architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1. Drivers Data service architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Our Drivers Data service previously used MySQL DB as persistent storage and two caching layers - &lt;em&gt;standalone local cache&lt;/em&gt; (RAM of the EC2 instances) as primary cache and &lt;em&gt;Redis&lt;/em&gt; as secondary for eventually consistent reads. With this setup, the cache hit ratio was very low.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image2.png&quot; alt=&quot;Figure 2. Request flow chart&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2. Request flow chart&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We opted for a &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside&quot;&gt;cache aside&lt;/a&gt; strategy. So when a client request comes, the Drivers Data service responds in the following manner:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If data is present in the in-memory cache (local cache), then the service directly sends back the response.&lt;/li&gt;
  &lt;li&gt;If data is not present in the in-memory cache and found in Redis, then the service sends back the response and updates the local cache asynchronously with data from Redis.&lt;/li&gt;
  &lt;li&gt;If data is not present either in the in-memory cache or Redis, then the service responds back with the data fetched from the MySQL DB and updates both Redis and local cache asynchronously.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image3.png&quot; alt=&quot;Figure 3. Percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3. Percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The measurement of the response source revealed that during peak hours &lt;strong&gt;~25% of the requests were being served via standalone local cache&lt;/strong&gt;, &lt;strong&gt;~20% by MySQL DB&lt;/strong&gt;, and &lt;strong&gt;~55% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The low cache hit rate is caused by the driver-partners data loading patterns: &lt;em&gt;low frequency per driver over time but the high frequency in a short amount of time.&lt;/em&gt; When a driver-partner is a candidate for a job or is involved in an ongoing job, different services make multiple requests to the Drivers Data service to fetch that specific driver-partner information. The frequency of calls for a specific driver-partner reduces if he/she is not involved in the job allocation process or is not doing any job at the moment.&lt;/p&gt;

&lt;p&gt;While low frequency per driver over time impacts the Redis cache hit rate, high frequency in short amounts of time mostly contributes to in-memory cache hit rate. In our investigations, we found that local caches of different nodes in the Drivers Data service cluster were making redundant calls to Redis and DB for fetching the same data that are already present in a node local cache.&lt;/p&gt;

&lt;p&gt;Making in-memory cache available on every instance while the data is in active use, we could greatly increase the in-memory cache hit rate, and that‚Äôs what we did.&lt;/p&gt;

&lt;h2 id=&quot;mirror-cache-design-goals&quot;&gt;Mirror cache design goals&lt;/h2&gt;

&lt;p&gt;We set the following design goals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Support a local least recently used (LRU) cache use-case.&lt;/li&gt;
  &lt;li&gt;Support active cache invalidation.&lt;/li&gt;
  &lt;li&gt;Support best effort replication between local cache instances (EC2 instances). If any instance successfully fetches the latest data from the database, then it should try to replicate or mirror this latest data across all the other nodes in the cluster. If replication fails and the item is expired or not found, then the nodes should fetch it from the database.&lt;/li&gt;
  &lt;li&gt;Support async data replication across nodes to ensure updates for the same key happens only with more recent data. For any older updates, the current data in the cache is ignored. The ordering of cache updates is not guaranteed due to the async replication.&lt;/li&gt;
  &lt;li&gt;Ability to handle auto-scaling.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-building-blocks&quot;&gt;The building blocks&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image4.png&quot; alt=&quot;Figure 4. Mirror cache&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4. Mirror cache&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The mirror cache library runs alongside the Drivers Data service inside each of the EC2 instances of the cluster. The two main components are in-memory cache and replicator.&lt;/p&gt;

&lt;h3 id=&quot;in-memory-cache&quot;&gt;In-memory cache&lt;/h3&gt;
&lt;p&gt;The in-memory cache is used to store multiple key/value pairs in RAM. There is a TTL associated with each key/value pair. We wanted to use a cache that can provide high hit ratio, memory bound, high throughput, and concurrency. After evaluating several options, we went with dgraph‚Äôs open-source concurrent caching library &lt;a href=&quot;https://github.com/dgraph-io/ristretto&quot;&gt;Ristretto&lt;/a&gt; as our in-memory local cache. We were particularly impressed by its use of the TinyLFU admission policy to ensure a high hit ratio.&lt;/p&gt;

&lt;h3 id=&quot;replicator&quot;&gt;Replicator&lt;/h3&gt;
&lt;p&gt;The replicator is responsible for mirroring/replicating each key/value entry among all the live instances of the Drivers Data service. The replicator has three main components: Membership Store, Notifier, and gRPC Server.&lt;/p&gt;

&lt;h4 id=&quot;membership-store&quot;&gt;Membership Store&lt;/h4&gt;
&lt;p&gt;The Membership Store registers callbacks with our service discovery service to notify mirror cache in case any nodes are added or removed from the Drivers Data service cluster.&lt;/p&gt;

&lt;p&gt;It maintains two maps - nodes in the same AZ (AWS availability zone) as itself (the current node of the Drivers Data service in which mirror cache is running) and the nodes in the other AZs.&lt;/p&gt;

&lt;h4 id=&quot;notifier&quot;&gt;Notifier&lt;/h4&gt;
&lt;p&gt;Each service (Drivers Data) node runs a single instance of mirror cache. So effectively, each node has one notifier.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Combine several (key/value) pairs updates to form a batch.&lt;/li&gt;
  &lt;li&gt;Propagate the batch updates among all the nodes in the same AZ as itself.&lt;/li&gt;
  &lt;li&gt;Send the batch updates to exactly one notifier (node) in different AZs who, in turn, are responsible for updating all the nodes in their own AZs with the latest batch of data. This communication technique helps to reduce cross AZ data transfer overheads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of auto-scaling, there is a warm-up period during which the notifier doesn‚Äôt notify the other nodes in the cluster. This is done to minimize duplicate data propagation. The warm-up period is configurable.&lt;/p&gt;

&lt;h4 id=&quot;grpc-server&quot;&gt;gRPC Server&lt;/h4&gt;
&lt;p&gt;An exclusive gRPC server runs for mirror cache. The different nodes of the Drivers Data service use this server to receive new cache updates from the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;Here‚Äôs the structure of each cache update entity:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entity&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Key for cache entry.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bytes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Value associated with the key.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Metadata related to the entity.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicate&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Further actions to be undertaken by the mirror cache after updating its own in-memory cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TTL&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// TTL associated with the data.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// If delete is set as true, then mirror cache needs to delete the key from it's local cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enum&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Nothing&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Stop propagation of the request.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SameRZ&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Notify the nodes in the same Region and AZ.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updatedAt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Same as updatedAt time of DB.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The server first checks if the local cache should update this new value or not. It tries to fetch the existing value for the key. If the value is not found, then the new key/value pair is added. If there is an existing value, then it compares the &lt;em&gt;updatedAt&lt;/em&gt; time to ensure that stale data is not updated in the cache.&lt;/p&gt;

&lt;p&gt;If the replicationType is &lt;em&gt;Nothing&lt;/em&gt;, then the mirror cache stops further replication. In case the replicationType is &lt;em&gt;SameRZ&lt;/em&gt; then the mirror cache tries to propagate this cache update among all the nodes in the same AZ as itself.&lt;/p&gt;

&lt;h2 id=&quot;run-at-scale&quot;&gt;Run at scale&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image5.png&quot; alt=&quot;Figure 5. Drivers Data Service new architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5. Drivers Data Service new architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The behavior of the service hasn‚Äôt changed and the requests are being served in the same manner as before. The only difference here is the replacement of the standalone local cache in each of the nodes with mirror cache. It is the responsibility of mirror cache to replicate any cache updates to the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;After mirror cache was fully rolled out to production, we rechecked our metrics related to the response source and saw a huge improvement. The graph showed that during peak hours &lt;strong&gt;~75% of the response was from in-memory local cache&lt;/strong&gt;. About &lt;strong&gt;15% of the response was served by MySQL DB&lt;/strong&gt; and a further &lt;strong&gt;10% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The local cache hit ratio was at &lt;strong&gt;0.75&lt;/strong&gt;, a jump of 0.5 from before and there was a &lt;strong&gt;5% drop in the number of DB calls&lt;/strong&gt; too.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image6.png&quot; alt=&quot;Figure 6. New percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6. New percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations-and-future-improvements&quot;&gt;Limitations and future improvements&lt;/h2&gt;

&lt;p&gt;Mirror cache is &lt;a href=&quot;https://en.wikipedia.org/wiki/Eventual_consistency#:~:text=Eventual%20consistency%20is%20a%20consistency,return%20the%20last%20updated%20value.&quot;&gt;eventually consistent&lt;/a&gt;, so it is not a good choice for systems that need strong consistency.&lt;/p&gt;

&lt;p&gt;Mirror cache stores all the data in volatile memory (RAM) and they are wiped out during deployments, resulting in a temporary load increase to Redis and DB.&lt;/p&gt;

&lt;p&gt;Also, many new driver-partners are added everyday to the Grab system, and we might need to increase the cache size to maintain a high hit ratio. To address these issues we plan to use SSD in the future to store a part of the data and use RAM only to store hot data.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Mirror cache really helped us scale the Drivers Data service better and serve driver-partners data to the different microservices at low latencies. It also helped us achieve our original goal of an increase in the local cache hit ratio.&lt;/p&gt;

&lt;p&gt;We also extended mirror cache in some other services and found similar promising results.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;A huge shout out to Haoqiang Zhang and Roman Atachiants for their inputs into the final design. Special thanks to the Driver Backend team at Grab for their contribution.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Jan 2021 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/mirror-cache-blog</link>
        <guid isPermaLink="true">https://engineering.grab.com/mirror-cache-blog</guid>
        
        <category>Mirror Cache</category>
        
        <category>Data at Scale</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>The GrabMart journey</title>
        <description>&lt;p&gt;Grab is Southeast Asia‚Äôs leading super app, providing everyday services such as ride-hailing, food delivery, payments, and more. In this blog, we‚Äôd like to share our journey in discovering the need for GrabMart and coming together as a team to build it.&lt;/p&gt;

&lt;h2 id=&quot;being-there-in-the-time-of-need&quot;&gt;Being there in the time of need&lt;/h2&gt;

&lt;p&gt;Back in March 2020, as the COVID-19 pandemic was getting increasingly widespread in Southeast Asia, people began to feel the pressing threat of the virus in carrying out their everyday activities. As social distancing restrictions tightened across Southeast Asia, consumers‚Äô reliance on online shopping and delivery services also grew.
Given the ability of our systems to readily adapt to changes, we were able to introduce a new service that our customers needed - GrabMart. By leveraging the GrabFood platform and quickly onboarding retail partners, we can now provide customers with their daily essentials on-demand, within a one hour delivery window.&lt;/p&gt;

&lt;h3 id=&quot;beginning-an-experiment&quot;&gt;Beginning an experiment&lt;/h3&gt;

&lt;p&gt;As early as November 2019, Grab was already piloting the concept of GrabMart in Malaysia and Singapore in light of the growing online grocery shopping trend. Our Product team decided to first launch GrabMart as a category within GrabFood to quickly gather learnings with minimal engineering effort. Through this pilot, we were able to test the operational flow, identify the value proposition to our customers, and expand our merchant selection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage1.png&quot; alt=&quot;GrabMart within the GrabFood flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;GrabMart within the GrabFood flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We learned that customers had difficulty finding specific items as there was no search function available and they had to scroll through the full list of merchants on the app. Drivers who received GrabMart orders were not always prepared to accept the job as the orders - especially larger ones - were not distinguished from GrabFood. Thanks to our agile Engineering teams, we fixed these issues efficiently, ensuring a smoother user experience.&lt;/p&gt;

&lt;h3 id=&quot;redefining-the-mart-experience&quot;&gt;Redefining the mart experience&lt;/h3&gt;

&lt;p&gt;With the exponential growth of GrabMart regionally at 50% week over week (from around April to September), the team was determined to create a new version of GrabMart that better suited the needs of our users.&lt;/p&gt;

&lt;p&gt;Our user research validated our hypothesis that shopping for groceries online is completely different from ordering meals online. Replicating the user flow of GrabFood for GrabMart would have led us to completely miss the natural path customers take at a grocery store on the app. For example, unlike ordering food, grocery shopping begins at an item-level instead of a merchant-level (like with GrabFood). Identifying this distinction led us to highlight item categories on both the GrabMart homepage and search results page. Other important user research highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Item/Store Categories&lt;/strong&gt;. For users that already have a store in mind, they often look for the store directly. This behavior is similar to the offline shopping behavior. Users unsure of where to find an item, search for it directly or navigate to item categories.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Add to Cart&lt;/strong&gt;. When purchasing familiar items, users often add the items to cart without clicking to read more about the product. Product details are only viewed when purchasing newer items.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduled Delivery&lt;/strong&gt;. As far as delivery time goes, every customer has different needs. Some  prefer paying a higher fee for faster  delivery, while others preferred waiting longer if it meant that the delivery fee was reduced.  Hence we decided to offer on-demand delivery for urgent purchases, and scheduled delivery for non-urgent buys.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage2.png&quot; alt=&quot;The New GrabMart Experience&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;The New GrabMart Experience&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In order to meet our timelines, we divided the deliverables into two main releases and got early feedback from internal users through our Grab Early Access (GEA) program. Since GEA gives users a sneak-peek into upcoming app features, we can resolve any issues that they encounter before releasing the product to the general public. In addition, we made some large-scale changes required across multiple Grab systems, such as the order management system to account for the new mart order type, the allocation system to allocate the right type of driver for mart orders, and the merchant app and our Partner APIs to enable merchants to prepare mart orders efficiently.&lt;/p&gt;

&lt;p&gt;Coupled with user research and country insights on grocery shopping behaviour, we ruthlessly prioritised the features to be built. We introduced Item categories to cater to customers who needed urgent restock of a few items, and Store categories for those shopping for their weekly groceries. We developed add-to-cart to make it easier for customers to put items in their basket, especially if they have a long list of products to buy. Furthermore, we included a Scheduled Delivery option for our Indonesian customers who want to receive their orders in person.&lt;/p&gt;

&lt;h2 id=&quot;designing-for-emotional-states&quot;&gt;Designing for emotional states&lt;/h2&gt;

&lt;p&gt;As we implemented multiple product changes, we realised that we could not risk overwhelming our customers with the amount of information we wanted to communicate. Thus, we decided to prominently display product images in the item category page and allocated space only for essential product details, such as price. Overall, we strived for an engaging design that balanced showing a mix of products, merchant offers, and our own data-driven recommendations.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-e-commerce&quot;&gt;The future of e-commerce&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;‚ÄúCOVID-19 has accelerated the adoption of on-demand delivery services across Southeast Asia, and we were able to tap on existing technologies, our extensive delivery network, and operational footprint to quickly scale GrabMart across the region. In a post-COVID19 normal, we anticipate demand for delivery services to remain elevated. We will continue to double down on expanding our GrabMart service to support consumers‚Äô shopping needs,‚Äù&lt;/em&gt; said Demi Yu, Regional Head of GrabFood and GrabMart.&lt;/p&gt;

&lt;p&gt;As the world embraces a new normal, we believe that online shopping will become even more essential in the months to come. Along with Grab‚Äôs Operations team, we continue to grow our partners on GrabMart so that we can become the most convenient and affordable choice for our customers regionally. By enabling more businesses to expand online, we can then reach more of our customers and meet their needs together.&lt;/p&gt;

&lt;p&gt;To learn more about GrabMart and its supported stores and features, click &lt;a href=&quot;https://www.grab.com/sg/campaign/grabmart/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2021 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabmart-product-team-experience</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabmart-product-team-experience</guid>
        
        <category>GrabMart</category>
        
        <category>Product</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Trident - Real-time event processing at scale</title>
        <description>&lt;p&gt;Ever wondered what goes behind the scenes when you receive advisory messages on a confirmed booking? Or perhaps how you are awarded with rewards or points after completing a GrabPay payment transaction? At Grab, thousands of such campaigns targeting millions of users are operated daily by a backbone service called &lt;em&gt;Trident&lt;/em&gt;. In this post, we share how Trident supports Grab‚Äôs daily business, the engineering challenges behind it, and how we solved them.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image8.jpg&quot; alt=&quot;60-minute GrabMart delivery guarantee campaign operated via Trident&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;60-minute GrabMart delivery guarantee campaign operated via Trident&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-trident&quot;&gt;What is Trident?&lt;/h2&gt;

&lt;p&gt;Trident is essentially Grab‚Äôs in-house real-time &lt;a href=&quot;https://en.wikipedia.org/wiki/IFTTT&quot;&gt;if this, then that (IFTTT)&lt;/a&gt; engine, which automates various types of business workflows. The nature of these workflows could either be to create awareness or to incentivize users to use other Grab services.&lt;/p&gt;

&lt;p&gt;If you are an active Grab user, you might have noticed new rewards or messages that appear in your Grab account. Most likely, these originate from a Trident campaign. Here are a few examples of types of campaigns that Trident could support:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;After a user makes a GrabExpress booking, Trident sends the user a message that says something like ‚ÄúTry out GrabMart too‚Äù.&lt;/li&gt;
  &lt;li&gt;After a user makes multiple ride bookings in a week, Trident sends the user a food reward as a GrabFood incentive.&lt;/li&gt;
  &lt;li&gt;After a user is dropped off at his office in the morning, Trident awards the user a ride reward to use on the way back home on the same evening.&lt;/li&gt;
  &lt;li&gt;If ¬†a GrabMart order delivery takes over an hour of waiting time, Trident awards the user a free-delivery reward as compensation.&lt;/li&gt;
  &lt;li&gt;If the driver cancels the booking, then Trident awards points to the user as a compensation.&lt;/li&gt;
  &lt;li&gt;With the current COVID pandemic, when a user makes a ride booking, Trident sends a message to both the passenger and driver reminding about COVID protocols.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trident processes events based on &lt;em&gt;campaigns&lt;/em&gt;, which are basically a logic configuration on &lt;em&gt;what event&lt;/em&gt; should trigger &lt;em&gt;what actions&lt;/em&gt; under &lt;em&gt;what conditions&lt;/em&gt;. To illustrate this better, let‚Äôs take a sample campaign as shown in the image below. This mock campaign setup is taken from the &lt;em&gt;Trident Internal Management&lt;/em&gt; portal.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image6.png&quot; alt=&quot;Trident process flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident process flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;This sample setup basically translates to: for each user, count his/her number of completed GrabMart orders. Once he/she reaches 2 orders, send him/her a message saying ‚ÄúMake one more order to earn a reward‚Äù. And if the user reaches 3 orders, award him/her the reward and send a congratulatory message. üòÅ&lt;/p&gt;

&lt;p&gt;Other than the basic event, condition, and action, Trident also allows more fine-grained configurations such as supporting the overall budget of a campaign, adding limitations to avoid over awarding, experimenting A/B testing, delaying of actions, and so on.&lt;/p&gt;

&lt;p&gt;An IFTTT engine is nothing new or fancy, but building a high-throughput real-time IFTTT system poses a challenge due to the scale that Grab operates at. We need to handle billions of events and run thousands of campaigns on an average day. The amount of actions triggered by Trident is also massive.&lt;/p&gt;

&lt;p&gt;In the month of October 2020, more than 2,000 events were processed every single second during peak hours. Across the entire month, we awarded nearly half a billion rewards, and sent over 2.5 billion communications to our end-users.&lt;/p&gt;

&lt;p&gt;Now that we covered the importance of Trident to the business, let‚Äôs drill down on how we designed the Trident system to handle events at a massive scale and overcame the performance hurdles with optimization.&lt;/p&gt;

&lt;h2 id=&quot;architecture-design&quot;&gt;Architecture design&lt;/h2&gt;

&lt;p&gt;We designed the Trident architecture with the following goals in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Independence&lt;/strong&gt;: It must run independently of other services, and must not bring performance impacts to other services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Robustness&lt;/strong&gt;: All events must be processed exactly once (i.e. no event missed, no event gets double processed).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: It must be able to scale up processing power when the event volume surges and withstand when popular campaigns run.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following diagram depicts how the overall system architecture looks like.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image4.png&quot; alt=&quot;Trident architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Trident consumes events from multiple Kafka streams published by various backend services across Grab (e.g. GrabFood orders, Transport rides, GrabPay payment processing, GrabAds events). Given the nature of Kafka streams, Trident is completely decoupled from all other upstream services.&lt;/p&gt;

&lt;p&gt;Each processed event is given a unique event key and stored in Redis for 24 hours. For any event that triggers an action, its key is persisted in MySQL as well. Before storing records in both Redis and MySQL, we make sure any duplicate event is filtered out. Together with the &lt;strong&gt;at-least-once&lt;/strong&gt; delivery guaranteed by Kafka, we achieve &lt;em&gt;exactly-once event processing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Scalability is a key challenge for Trident. To achieve high performance under massive event volume, we needed to scale on both the server level and data store level. The following mind map shows an outline of our strategies.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image3.png&quot; alt=&quot;Outline of Trident‚Äôs scale strategy&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Outline of Trident‚Äôs scale strategy&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;scale-servers&quot;&gt;Scale servers&lt;/h2&gt;

&lt;p&gt;Our source of events are Kafka streams. There are mostly two factors that could affect the load on our system:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Number of events produced in the streams (more rides, food orders, etc. results in more events for us to process).&lt;/li&gt;
  &lt;li&gt;Number of campaigns running.&lt;/li&gt;
  &lt;li&gt;Nature of campaigns running. The campaigns that trigger actions for more users cause higher load on our system.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are naturally two types of approaches to scale up server capacity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distribute workload among server instances.&lt;/li&gt;
  &lt;li&gt;Reduce load (i.e. reduce the amount of work required to process each event).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;distribute-load&quot;&gt;Distribute load&lt;/h3&gt;

&lt;p&gt;Distributing workload seems trivial with the load balancing and auto-horizontal scaling based on CPU usage that cloud providers offer. However, an additional server sits idle until it can consume from a Kafka partition.&lt;/p&gt;

&lt;p&gt;Each Kafka partition can only be consumed by one consumer within the same consumer group (our auto-scaling server group in this case). Therefore, any scaling in or out requires matching the Kafka partition configuration with the server auto-scaling configuration.&lt;/p&gt;

&lt;p&gt;Here‚Äôs an example of a bad case of load distribution:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image2.png&quot; alt=&quot;Kafka partitions config mismatches server auto-scaling config&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Kafka partitions config mismatches server auto-scaling config&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;And here‚Äôs an example of a good load distribution where the configurations for the Kafka partitions and the server auto-scaling match:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image10.png&quot; alt=&quot;Kafka partitions config matches server auto-scaling config&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Kafka partitions config matches server auto-scaling config&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Within each server instance, we also tried to increase processing throughput while keeping the resource utilization rate in check. Each Kafka partition consumer has multiple goroutines processing events, and the number of active goroutines is dynamically adjusted according to the event volume from the partition and time of the day (peak/off-peak).&lt;/p&gt;

&lt;h3 id=&quot;reduce-load&quot;&gt;Reduce load&lt;/h3&gt;

&lt;p&gt;You may ask how we reduced the amount of processing work for each event. First, we needed to see where we spent most of the processing time. After performing some profiling, we identified that the rule evaluation logic was the major time consumer.&lt;/p&gt;

&lt;h4 id=&quot;what-is-rule-evaluation&quot;&gt;What is rule evaluation?&lt;/h4&gt;

&lt;p&gt;Recall that Trident needs to operate thousands of campaigns daily. Each campaign has a set of rules defined. When Trident receives an event, it needs to check through the rules for all the campaigns to see whether there is any match. This checking process is called &lt;strong&gt;rule evaluation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;More specifically, a rule consists of one or more conditions combined by &lt;code class=&quot;highlighter-rouge&quot;&gt;AND/OR&lt;/code&gt; Boolean operators. A condition consists of an operator with a left-hand side (LHS) and a right-hand side (RHS). The left-hand side is the name of a &lt;em&gt;variable&lt;/em&gt;, and the right-hand side a value. A sample rule in JSON:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Country is Singapore and taxi type is either JustGrab or GrabCar.
  {
    &quot;operator&quot;: &quot;and&quot;,
    &quot;conditions&quot;: [
    {
      &quot;operator&quot;: &quot;eq&quot;,
      &quot;lhs&quot;: &quot;var.country&quot;,
      &quot;rhs&quot;: &quot;sg&quot;
      },
      {
        &quot;operator&quot;: &quot;or&quot;,
        &quot;conditions&quot;: [
        {
          &quot;operator&quot;: &quot;eq&quot;,
          &quot;lhs&quot;: &quot;var.taxi&quot;,
          &quot;rhs&quot;: &amp;lt;taxi-type-id-for-justgrab&amp;gt;
          },
          {
            &quot;operator&quot;: &quot;eq&quot;,
            &quot;lhs&quot;: &quot;var.taxi&quot;,
            &quot;rhs&quot;: &amp;lt;taxi-type-id-for-grabcard&amp;gt;
          }
        ]
      }
    ]
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When evaluating the rule, our system loads the values of the LHS variable, evaluates against the RHS value, and returns as result (&lt;code class=&quot;highlighter-rouge&quot;&gt;true/false&lt;/code&gt;) whether the rule evaluation passed or not.&lt;/p&gt;

&lt;p&gt;To reduce the resources spent on rule evaluation, there are two types of strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid unnecessary rule evaluation&lt;/li&gt;
  &lt;li&gt;Evaluate ‚Äúcheap‚Äù rules first&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We implemented these two strategies with event prefiltering and weighted rule evaluation.&lt;/p&gt;

&lt;h5 id=&quot;event-prefiltering&quot;&gt;Event prefiltering&lt;/h5&gt;

&lt;p&gt;Just like the DB index helps speed up data look-up, having a pre-built map also helped us narrow down the range of campaigns to evaluate. We loaded active campaigns from the DB every few minutes and organized them into an in-memory hash map, with event type as key, and list of corresponding campaigns as the value. The reason we picked event type as the key is that it is very fast to determine (most of the time just a type assertion), and it can distribute events in a reasonably even way.&lt;/p&gt;

&lt;p&gt;When processing events, we just looked up the map, and only ran rule evaluation on the campaigns in the matching hash bucket. This saved us &lt;strong&gt;at least 90%&lt;/strong&gt; of the processing time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image7.png&quot; alt=&quot;Event prefiltering&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event prefiltering&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h5 id=&quot;weighted-rule-evaluation&quot;&gt;Weighted rule evaluation&lt;/h5&gt;

&lt;p&gt;Evaluating different rules comes with different costs. This is because different variables (i.e. LHS) in the rule can have different sources of values:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The value is already available in memory (already consumed from the event stream).&lt;/li&gt;
  &lt;li&gt;The value is the result of a database query.&lt;/li&gt;
  &lt;li&gt;The value is the result of a call to an external service.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These three sources are ranked by cost:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In-memory &amp;lt; database &amp;lt; external service&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We aimed to maximally avoid evaluating expensive rules (i.e. those that require calling external service, or querying a DB) while ensuring the correctness of evaluation results.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;First optimization - Lazy loading&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lazy loading is a common performance optimization technique, which literally means &lt;em&gt;‚Äúdon‚Äôt do it until it‚Äôs necessary‚Äù&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Take the following rule as an example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A &amp;amp; B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we load the variable values for both A and B before passing to evaluation, then we are unnecessarily loading B if A is false. Since most of the time the rule evaluation fails early (for example, the transaction amount is less than the given minimum amount), there is no point in loading all the data beforehand. So we do lazy loading ie. load data only when evaluating that part of the rule.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Second optimization - Add weight&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let‚Äôs take the same example as above, but in a different order.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;B &amp;amp; A
Source of data for A is memory and B is external service
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now even if we are doing lazy loading, in this case, we are loading the external data always even though it potentially may fail at the next condition whose data is in memory.&lt;/p&gt;

&lt;p&gt;Since most of our campaigns are targeted, a popular condition is to check if a user is in a certain segment, which is usually the first condition that a campaign creator sets. This data resides in another service. So it becomes quite expensive to evaluate this condition first even though the next condition‚Äôs data can be already in memory (e.g. if the taxi type is JustGrab).&lt;/p&gt;

&lt;p&gt;So, we did the next phase of optimization here, by sorting the conditions based on weight of the source of data (low weight if data is in memory, higher if it‚Äôs in our database and highest if it‚Äôs in an external system). If AND was the only logical operator we supported, then it would have been quite simple. But the presence of OR made it complex. We came up with an algorithm that sorts the evaluation based on weight keeping in mind the AND/OR. Here‚Äôs what the flowchart looks like:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image9.png&quot; alt=&quot;Event flowchart&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event flowchart&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;An example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Conditions: A &amp;amp; ( B | C ) &amp;amp; ( D | E )

Actual result: true &amp;amp; ( false | false ) &amp;amp; ( true | true ) --&amp;gt; false

Weight: B &amp;lt; D &amp;lt; E &amp;lt; C &amp;lt; A

Expected check order: B, D, C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Firstly, we start validating B which is false. Apparently, we cannot skip the sibling conditions here since B and C are connected by &lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt;. Next, we check D. D is true and its only sibling E is connected by &lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt; so we can mark E ‚Äúskip‚Äù. Then, we check E but since E has been marked ‚Äúskip‚Äù, we just skip it. Still, we cannot get the final result yet, so we need to continue validating C which is false. Now we know (&lt;code class=&quot;highlighter-rouge&quot;&gt;B | C&lt;/code&gt;) is false so the whole condition is false too. We can stop now.&lt;/p&gt;

&lt;h4 id=&quot;sub-streams&quot;&gt;Sub-streams&lt;/h4&gt;

&lt;p&gt;After investigation, we learned that we consumed a particular stream that produced terabytes of data per hour. It caused our CPU usage to shoot up by &lt;strong&gt;30%&lt;/strong&gt;. We found out that we process only a handful of event types from that stream. So we introduced a sub-stream in between, which contains the event types we want to support. This stream is populated from the main stream by another server, thereby reducing the load on Trident.&lt;/p&gt;

&lt;h3 id=&quot;protect-downstream&quot;&gt;Protect downstream&lt;/h3&gt;

&lt;p&gt;While we scaled up our servers wildly, we needed to keep in mind that there were many downstream services that received more traffic. For example, we call the GrabRewards service for awarding rewards or the &lt;em&gt;LocaleService&lt;/em&gt; for checking the user‚Äôs locale. It is crucial for us to have control over our outbound traffic to avoid causing any stability issues in Grab.&lt;/p&gt;

&lt;p&gt;Therefore, we implemented rate limiting. There is a total rate limit configured for calling each downstream service, and the limit varies in different time ranges (e.g. tighter limit for calling critical service during peak hour).&lt;/p&gt;

&lt;h4 id=&quot;scale-data-store&quot;&gt;Scale data store&lt;/h4&gt;

&lt;p&gt;We have two types of storage in Trident: &lt;em&gt;cache storage (Redis)&lt;/em&gt; and &lt;em&gt;persistent storage (MySQL and others)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Scaling cache storage is straightforward, since Redis Cluster already offers everything we need:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Known to be fast and efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scaling capability&lt;/strong&gt;: New shards can be added at any time to spread out the load.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault tolerance&lt;/strong&gt;: Data replication makes sure that data does not get lost when any single Redis instance fails, and auto election mechanism makes sure the cluster can always auto restore itself in case of any single instance failure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All we needed to make sure is that our cache keys can be hashed evenly into different shards.&lt;/p&gt;

&lt;p&gt;As for scaling persistent data storage, we tackled it in two ways just like we did for servers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distribute load&lt;/li&gt;
  &lt;li&gt;Reduce load (both overall and per query)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;distribute-load-1&quot;&gt;Distribute load&lt;/h3&gt;

&lt;p&gt;There are two levels of load distribution for persistent storage: &lt;em&gt;infra level&lt;/em&gt; and &lt;em&gt;DB level&lt;/em&gt;. On the infra level, we split data with different access patterns into different types of storage. Then on the DB level, we further distributed read/write load onto different DB instances.&lt;/p&gt;

&lt;h4 id=&quot;infra-level&quot;&gt;Infra level&lt;/h4&gt;

&lt;p&gt;Just like any typical online service, Trident has two types of data in terms of access pattern:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Online data&lt;/strong&gt;: Frequent access. Requires quick access. Medium size.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Offline data&lt;/strong&gt;: Infrequent access. Tolerates slow access. Large size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For online data, we need to use a high-performance database, while for offline data, we can ¬†just use cheap storage. The following table shows Trident‚Äôs online/offline data and the corresponding storage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/table.png&quot; alt=&quot;Trident‚Äôs online/offline data and storage&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident‚Äôs online/offline data and storage&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Writing of offline data is done asynchronously to minimize performance impact as shown below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image5.png&quot; alt=&quot;Online/offline data split&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Online/offline data split&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;For retrieving data for the users, we have high timeout for such APIs.&lt;/p&gt;

&lt;h4 id=&quot;db-level&quot;&gt;DB level&lt;/h4&gt;

&lt;p&gt;We further distributed load on the MySQL DB level, mainly by introducing replicas, and redirecting all read queries that can tolerate slightly outdated data to the replicas. This &lt;strong&gt;relieved more than 30% of the load from the master instance&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Going forward, we plan to segregate the single MySQL database into multiple databases, based on table usage, to further distribute load if necessary.&lt;/p&gt;

&lt;h3 id=&quot;reduce-load-1&quot;&gt;Reduce load&lt;/h3&gt;

&lt;p&gt;To reduce the load on the DB, we reduced the overall number of queries and removed unnecessary queries. We also optimized the schema and query, so that query completes faster.&lt;/p&gt;

&lt;h4 id=&quot;query-reduction&quot;&gt;Query reduction&lt;/h4&gt;

&lt;p&gt;We needed to track usage of a campaign. The tracking is just incrementing the value against a unique key in the MySQL database. For a popular campaign, it‚Äôs possible that multiple increment (a write query) queries are made to the database for the same key. If this happens, it can cause an IOPS burst. So we came up with the following algorithm to reduce the number of queries.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have a fixed number of threads per instance that can make such a query to the DB.&lt;/li&gt;
  &lt;li&gt;The increment queries are queued into above threads.&lt;/li&gt;
  &lt;li&gt;If a thread is idle (not busy in querying the database) then proceed to write to the database then itself.&lt;/li&gt;
  &lt;li&gt;If the thread is busy, then increment in memory.&lt;/li&gt;
  &lt;li&gt;When the thread becomes free, increment by the above sum in the database.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To prevent accidental over awarding of benefits (rewards, points, etc), we require campaign creators to set the limits. However, there are some campaigns that don‚Äôt need a limit, so the campaign creators just specify a large number. Such popular campaigns can cause very high QPS to our database. We had a brilliant trick to address this issue- we just don‚Äôt track if the number is high. Do you think people really want to limit usage when they set the per user limit to 100,000? ;)&lt;/p&gt;

&lt;h4 id=&quot;query-optimization&quot;&gt;Query optimization&lt;/h4&gt;

&lt;p&gt;One of our requirements was to track the usage of a campaign - overall as well as per user (and more like daily overall, daily per user, etc). We used the following query for this purpose:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INSERT INTO ‚Ä¶ ON DUPLICATE KEY UPDATE value = value + inc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The table had a unique key index (combining multiple columns) along with a usual auto-increment integer primary key. We encountered performance issues arising from MySQL gap locks when high write QPS hit this table (i.e. when popular campaigns ran). After testing out a few approaches, we ended up making the following changes to solve the problem:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Removed the auto-increment integer primary key.&lt;/li&gt;
  &lt;li&gt;Converted the secondary unique key to the primary key.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Trident is Grab‚Äôs in-house real-time IFTTT engine, which processes events and operates business mechanisms on a massive scale. In this article, we discussed the strategies we implemented to achieve large-scale high-performance event processing. The overall ideas of distributing and reducing load may be straightforward, but there were lots of thoughts and learnings shared in detail. If you have any comments or questions about Trident, feel free to leave a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All the examples of campaigns given in the article are for demonstration purpose only, they are not real live campaigns.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2021 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/trident-real-time-event-processing-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/trident-real-time-event-processing-at-scale</guid>
        
        <category>A/B Testing</category>
        
        <category>Event Processing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Pharos - Searching nearby drivers on road network at scale</title>
        <description>&lt;p&gt;Have you ever wondered what happens when you click on the book button when arranging a ride home? Actually, many things happen behind this simple action and it would take days and nights to talk about all of them. Perhaps, we should rephrase this question to be more precise. ¬†So, let‚Äôs try again - have you ever thought about how Grab stores and uses driver locations to allocate a driver to you? If so, you will surely find this blog post interesting as we cover how it all works in the backend.&lt;/p&gt;

&lt;h2 id=&quot;what-problems-are-we-going-to-solve&quot;&gt;What problems are we going to solve?&lt;/h2&gt;

&lt;p&gt;One of the fundamental problems of the ride-hailing and delivery industry is to locate the nearest moving drivers in real-time. There are two challenges from serving this request in real time.&lt;/p&gt;

&lt;h3 id=&quot;fast-moving-vehicles&quot;&gt;Fast-moving vehicles&lt;/h3&gt;

&lt;p&gt;Vehicles are constantly moving and sometimes the drivers go at the speed of over 20 meters per second. As shown in Figure 1a and Figure 1b, the two nearest drivers to the pick-up point (blue dot) change as time passes. To provide a high-quality allocation service, it is important to constantly track the objects and update object locations at high frequency (e.g. per second).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/fast-moving-drivers.png&quot; alt=&quot;Figure 1: Fast-moving drivers&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1: Fast-moving drivers&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;routing-distance-calculation&quot;&gt;Routing distance calculation&lt;/h3&gt;

&lt;p&gt;To satisfy business requirements, K nearest objects need to be calculated based on the routing distance instead of straight-line distance. Due to the complexity of the road network, the driver with the shortest straight-line distance may not be the optimal driver as it could reach the pick-up point with a longer routing distance due to detour.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image1.png&quot; alt=&quot;Figure 2: Straight line vs routing&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2: Straight line vs routing&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;As shown in Figure 2, the driver at the top is deemed as the nearest one to pick-up point by straight line distance. However, the driver at the bottom should be the true nearest driver by routing distance. Moreover, routing distance helps to infer the estimated time of arrival (ETA), which is an important factor for allocation, as shorter ETA reduces passenger waiting time thus reducing order cancellation rate and improving order completion rate.&lt;/p&gt;

&lt;p&gt;Searching for the K nearest drivers with respect to a given POI is a well studied topic for all ride-hailing companies, which can be treated as a &lt;em&gt;K Nearest Neighbour (KNN) problem&lt;/em&gt;. Our predecessor, Sextant, searches nearby drivers with the &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;haversine&lt;/a&gt;&lt;/em&gt; distance from driver locations to the pick-up point. By partitioning the region into grids and storing them in a distributed manner, Sextant can handle large volumes of requests with low latency. However, nearest drivers found by the haversine distance may incur long driving distance and ETA as illustrated in Figure 2. For more information about Sextant, kindly refer to the paper, &lt;em&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/8788742&quot;&gt;Sextant: Grab‚Äôs Scalable In-Memory Spatial Data Store for Real-Time K-Nearest Neighbour Search&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To better address the challenges mentioned above, we present the next-generation solution, &lt;strong&gt;Pharos&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image7.png&quot; alt=&quot;Figure 3: Lighthouse of Alexandria&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3: Lighthouse of Alexandria&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-pharos&quot;&gt;What is Pharos?&lt;/h2&gt;

&lt;p&gt;Pharos means lighthouse in Greek. At Grab, it is a scalable in-memory solution that supports large-volume, real-time K nearest search by driving distance or ETA with high object update frequency.&lt;/p&gt;

&lt;p&gt;In Pharos, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenStreetMap&quot;&gt;OpenStreetMap&lt;/a&gt; (OSM) graphs to represent road networks. To support hyper-localized business requirements, the graph is partitioned by cities and verticals (e.g. the road network for a four-wheel vehicle is definitely different compared to a motorbike or a pedestrian). We denote this partition key as &lt;em&gt;map ID&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Pharos loads the graph partitions at service start and stores drivers‚Äô spatial data in memory in a distributed manner to alleviate the scalability issue when the graph or the number of drivers grows. These data are distributed into multiple instances (i.e. machines) with replicas for high stability. Pharos exploits &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2016/papers/leis-icde2013.pdf&quot;&gt;Adaptive Radix Trees&lt;/a&gt; (ART) to store objects‚Äô locations along with their metadata.&lt;/p&gt;

&lt;p&gt;To answer the KNN query by routing distance or ETA, Pharos uses &lt;a href=&quot;http://www.vldb.org/pvldb/vol9/p492-abeywickrama.pdf&quot;&gt;Incremental Network Expansion&lt;/a&gt; (INE) starting from the road segment of the query point. During the expansion, drivers stored along the road segments are incrementally retrieved as candidates and put into the results. As the expansion actually generates an isochrone map, it can be terminated by reaching a predefined radius of distance or ETA, or even simply a maximum number of candidates.&lt;/p&gt;

&lt;p&gt;Now that you have an ¬†overview of Pharos, we would like to go into the design details of it, starting with its architecture.&lt;/p&gt;

&lt;h3 id=&quot;pharos-architecture&quot;&gt;Pharos architecture&lt;/h3&gt;

&lt;p&gt;As a microservice, Pharos receives requests from the upstream, performs corresponding actions and then returns the result back. As shown in Figure 4, the Pharos architecture can be broken down into three layers: &lt;em&gt;Proxy&lt;/em&gt;, &lt;em&gt;Node&lt;/em&gt;, and &lt;em&gt;Model&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Proxy layer&lt;/strong&gt;. This layer helps to pass down the request to the right node, especially when the Node is on another machine.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Node layer&lt;/strong&gt;. This layer stores the index of map IDs to models and distributes the request to the right model for execution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model layer&lt;/strong&gt;. This layer is, where the business logic is implemented, executes the operations and returns the result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a distributed in-memory driver storage, Pharos is designed to handle load balancing, fault tolerance, and fast recovery.&lt;/p&gt;

&lt;p&gt;Taking Figure 4 as an example, Pharos consists of three instances. Each individual instance is able to handle any request from the upstream. Whenever there is a request coming from the upstream, it is distributed into one of the three instances, which achieves the purpose of load balancing.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image2.png&quot; alt=&quot;Figure 4: Pharos architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4: Pharos architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In Pharos, each model has two replicas and they are stored on different instances and different availability zones. If one instance is down, the other two instances are still up for service. The fault tolerance module in Pharos automatically detects the reduction of replicas and creates new instances to load graphs and build the models of missing replicas. This proves the reliability of Pharos even under extreme situations.&lt;/p&gt;

&lt;p&gt;With the architecture of Pharos in mind, let‚Äôs take a look at how it stores driver information.&lt;/p&gt;

&lt;h3 id=&quot;driver-storage&quot;&gt;Driver storage&lt;/h3&gt;

&lt;p&gt;Pharos acts as a driver storage, and rather than being an external storage, it adopts in-memory storage which is faster and more adequate to handle frequent driver position updates and retrieve driver locations for nearby driver queries. Without loss of generality, drivers are assumed to be located on the vertices, i.e. &lt;a href=&quot;https://github.com/Project-OSRM/osrm-backend/wiki/Graph-representation&quot;&gt;Edge Based Nodes&lt;/a&gt; (EBN) of an edge-based graph.&lt;/p&gt;

&lt;p&gt;Model is in charge of the driver storage in Pharos. Driver objects are passed down from upper layers to the model layer for storage. Each driver object contains several fields such as driver ID and metadata, containing the driver‚Äôs business related information e.g. driver status and particular allocation preferences.&lt;/p&gt;

&lt;p&gt;There is also a &lt;em&gt;Latitude and Longitude (LatLon) pair&lt;/em&gt; contained in the object, which indicates the driver‚Äôs current location. Very often, this LatLon pair sent from the driver is off the road (not on any existing road). The computation of routing distance between the query point and drivers is based on the road network. Thus, we need to infer which road segment (EBN) the driver is most probably on.&lt;/p&gt;

&lt;p&gt;To convert a LatLon pair to an exact location on a road is called &lt;strong&gt;Snapping&lt;/strong&gt;. Model begins with finding EBNs which are close to the driver‚Äôs location. After that, as illustrated in Figure 5, the driver‚Äôs location is projected to those EBNs, by drawing perpendicular lines from the location to the EBNs. The projected point is denoted as a &lt;strong&gt;phantom node&lt;/strong&gt;. As the name suggests, these nodes do not exist in the graph. They are merely memory representations of the snapped driver.&lt;/p&gt;

&lt;p&gt;Each phantom node contains information about its projected location such as the ID of EBN it is projected to, projected LatLon and projection ratio, etc. Snapping returns a list of phantom nodes ordered by the haversine distance from the driver‚Äôs LatLon to the phantom node in ascending order. The nearest phantom node is bound with the original driver object to provide information about the driver‚Äôs snapped location.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image12.png&quot; alt=&quot;Figure 5: Snapping and phantom nodes&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5: Snapping and phantom nodes&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To efficiently index drivers from the graph, Pharos uses ART for driver storage. Two ARTs are maintained by each model: &lt;em&gt;Driver ART&lt;/em&gt; and &lt;em&gt;EBN ART&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Driver ART&lt;/strong&gt; is used to store the index of driver IDs to corresponding driver objects, while &lt;strong&gt;EBN ART&lt;/strong&gt; is used to store the index of EBN IDs to the root of an ART, which stores the drivers on that EBN.&lt;/p&gt;

&lt;p&gt;Bi-directional indexing between EBNs and drivers are built because an efficient retrieval from driver to EBN is needed as driver locations are constantly updated. In practice, as index keys, driver IDs, and EBN IDs are both numerical. ART has a better throughput for dense keys (e.g. numerical keys) in contrast to sparse keys such as alphabetical keys, and when compared to other in-memory look-up tables (e.g. hash table). It also incurs less memory than other tree-based methods.&lt;/p&gt;

&lt;p&gt;Figure 6 gives an example of driver ART assuming that the driver ID only has three digits.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image9.png&quot; alt=&quot;Figure 6: Driver ART&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6: Driver ART&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;After snapping, this new driver object is wrapped into an update task for execution. During execution, the model firstly checks if this driver already exists using its driver ID. If it does not exist, the model directly adds it to driver ART and EBN ART. If the driver already exists, the new driver object replaces the old driver object on driver ART. For EBN ART, the old driver object on the previous EBN needs to be deleted first before adding the new driver object to the current EBN.&lt;/p&gt;

&lt;p&gt;Every insertion or deletion modifies both ARTs, which might cause changes to roots. The model only stores the roots of ARTs, and in order to prevent race conditions, a lock is used to prevent other read or write operations to access the ARTs while changing the ART roots.&lt;/p&gt;

&lt;p&gt;Whenever a driver nearby request comes in, it needs to get a snapshot of driver storage, i.e. the roots of two ARTs. A simple example (Figure 7a and 7b) is used to explain how synchronization is achieved during concurrent driver update and nearby requests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/art-synchronization.png&quot; alt=&quot;Figure 7: How ARTs change roots for synchronization&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7: How ARTs change roots for synchronization&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Currently there are two drivers A and B stored and these two drivers reside on the same EBN. When there is a nearby request, the current roots of the two ARTs are returned. When processing this nearby request, there could be driver updates coming and modifying the ARTs, e.g. a new root is resulted due to update of driver C. This driver update has no impact on ongoing driver nearby requests as they are using different roots. Subsequent nearby requests will use the new ART roots to find the nearby drivers. Once the current roots are not used by any nearby request, these roots and their child nodes are ready to be garbage collected.&lt;/p&gt;

&lt;p&gt;Pharos does not delete drivers actively. A deletion of expired drivers is carried out every midnight by populating two new ARTs with the same driver update requests for a duration of driver‚Äôs &lt;em&gt;Time To Live (TTL)&lt;/em&gt;, and then doing a switch of the roots at the end. Drivers with expired TTLs are not referenced and they are ready to be garbage collected. In this way, expired drivers are removed from the driver storage.&lt;/p&gt;

&lt;h3 id=&quot;driver-update-and-nearby&quot;&gt;Driver update and nearby&lt;/h3&gt;

&lt;p&gt;Pharos mainly has two external endpoints: &lt;em&gt;Driver Update&lt;/em&gt; and &lt;em&gt;Driver Nearby&lt;/em&gt;. The following describes how the business logic is implemented in these two operations.&lt;/p&gt;

&lt;h4 id=&quot;driver-update&quot;&gt;Driver update&lt;/h4&gt;

&lt;p&gt;Figure 8 demonstrates the life cycle of a driver update request from upstream. Driver update requests from upstream are distributed to each proxy by a load balancer. The chosen proxy firstly constructs a driver object from the request body.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;RouteTable&lt;/em&gt;, a structure in proxy, stores the index between map IDs and replica addresses. Proxy then uses map ID in the request as the key to check its RouteTable and gets the IP addresses of all the instances containing the model of that map ID.&lt;/p&gt;

&lt;p&gt;Then, proxy forwards the update to other replicas that reside in other instances. Those instances, upon receiving the message, know that the update is forwarded from another proxy. Hence they directly pass down the driver object to the node.&lt;/p&gt;

&lt;p&gt;After receiving the driver object, Node sends it to the right model by checking the index between map ID and model. The remaining part of the update flow is the same as described in Driver Storage. Sometimes the driver updates to replicas are not successful, e.g. request lost or model does not exist, Pharos will not react to such kinds of scenarios.&lt;/p&gt;

&lt;p&gt;It can be observed that data storage in Pharos does not guarantee strong consistency. In practice, Pharos favors high throughput over strong consistency of KNN query results as the update frequency is high and slight inconsistency does not affect allocation performance significantly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image6.png&quot; alt=&quot;Figure 8: Driver update flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 8: Driver update flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;driver-nearby&quot;&gt;Driver nearby&lt;/h4&gt;

&lt;p&gt;Similar to driver update, after a driver nearby request comes from the upstream, it is distributed to one of the machines by the load balancer. In a nearby request, a set of filter parameters is used to match with driver metadata in order to support KNN queries with various business requirements. Note that driver metadata also carries an update timestamp. During the nearby search, drivers with an expired timestamp are filtered.&lt;/p&gt;

&lt;p&gt;As illustrated in Figure 9, upon receiving the nearby request, a nearby object is built and passed to the proxy layer. The proxy first checks RouteTable by map ID to see if this request can be served on the current instance. If so, the nearby object is passed to the Node layer. Otherwise, this nearby request needs to be forwarded to the instances that contain this map ID.&lt;/p&gt;

&lt;p&gt;In this situation, a round-robin fashion is applied to select the right instance for load balancing. After receiving the request, the proxy of the chosen instance directly passes the nearby object to the node. Once the node layer receives the nearby object, it looks for the right model using the map ID as key. Eventually, the nearby object goes to the model layer where K-nearest-driver computation takes place. Model snaps the location of the request to some phantom nodes as described previously - these nodes are used as start nodes for expansion later.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image5.png&quot; alt=&quot;Figure 9: Driver nearby flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 9: Driver nearby flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;k-nearest-driver-search&quot;&gt;K nearest driver search&lt;/h4&gt;

&lt;p&gt;Starting from the phantom nodes found in the &lt;em&gt;Driver Nearby&lt;/em&gt; flow, the K nearest driver search begins. Two priority queues are used during the search: &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; is used to keep track of the nearby EBNs, while &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; keeps track of drivers found during expansion by their driving distance to the query point.&lt;/p&gt;

&lt;p&gt;At first, a snapshot of the current driver storage is taken (using roots of current ARTs) and it shows the driver locations on the road network at the time when the nearby request comes in. From each start node, the parent EBN is found and drivers on these EBNs are appended to driverPQ. After that, KNN search expands to adjacent EBNs and appends these EBNs to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. After iterating all start nodes, there will be some initial drivers in driverPQ and adjacent EBNs waiting to be expanded in &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Each time the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;, drivers located on this EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. After that, the closest driver is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. If the driver satisfies all filtering requirements, it is appended to the array of qualified drivers. This step repeats until driverPQ becomes empty. During this process, if the size of qualified drivers reaches the maximum driver limit, the KNN search stops right away and qualified drivers are returned.&lt;/p&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; becomes empty, adjacent EBNs of the current one are to be expanded and those within the predefined range, e.g. three kilometers, are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. Then the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; and drivers on that EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; again. The whole process continues until &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; becomes empty. The driver array is returned as the result of the nearby query.&lt;/p&gt;

&lt;p&gt;Figure 10 shows the pseudo code of this KNN algorithm.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image8.png&quot; alt=&quot;Figure 10: KNN search algorithm&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 10: KNN search algorithm&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What‚Äôs next?&lt;/h2&gt;

&lt;p&gt;Currently, Pharos is running on the production environment, where it handles requests with &lt;strong&gt;P99 latency time of 10ms for driver update&lt;/strong&gt; and &lt;strong&gt;50ms for driver nearby&lt;/strong&gt;, respectively. Even though the performance of Pharos is quite satisfying, we still see some potential areas of improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pharos uses ART for driver storage. Even though ART proves its ability to handle large volumes of driver update and driver nearby requests, the write operations (driver update) are not carried out in parallel. Hence, we plan to explore other data structures that can achieve high concurrency of read and write, eg. concurrent hash table.&lt;/li&gt;
  &lt;li&gt;Pharos uses OSM &lt;a href=&quot;https://www.google.com/url?q=https://i11www.iti.kit.edu/_media/teaching/theses/ba-hamme-13.pdf&amp;amp;sa=D&amp;amp;ust=1608637926094000&amp;amp;usg=AOvVaw0hO37L1Wqh_C423C51e3S8&quot;&gt;Multi-level Dijkstra&lt;/a&gt; (MLD) graphs to find K nearest drivers. As the predefined range of nearby driver search is often a few kilometers, Pharos does not make use of MLD partitions or support long distance query. Thus, we are interested in exploiting MLD graph partitions to enable Pharos to support long distance query.&lt;/li&gt;
  &lt;li&gt;In Pharos, maps are partitioned by cities and we assume that drivers of a city operate within that city. When finding the nearby drivers, Pharos only allocates drivers of that city to the passenger. Hence, in the future, we want to enable Pharos to support cross city allocation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope this blog helps you to have a closer look at how we store driver locations and how we use these locations to find nearby drivers around you.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;
&lt;p&gt;We would like to thank Chunda Ding, Zerun Dong, and Jiang Liu for their contributions to the distributed layer used in Pharos. Their efforts make Pharos reliable and fault tolerant.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3, Lighthouse of Alexandria is taken from &lt;a href=&quot;https://www.britannica.com/topic/lighthouse-of-Alexandria%23/media/1/455210/187239&quot;&gt;https://www.britannica.com/topic/lighthouse-of-Alexandria#/media/1/455210/187239&lt;/a&gt; authored by Sergey Kamshylin.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5, Snapping and Phantom Nodes, is created by Minbo Qiu. We would like to thank him for the insightful elaboration of the snapping mechanism.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cover Photo by Kevin Huang on Unsplash&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Dec 2020 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</guid>
        
        <category>Real-Time K Nearest Neighbour Search</category>
        
        <category>Spatial Data Store</category>
        
        <category>Distributed System</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Reflecting on the five years of Bug Bounty at Grab</title>
        <description>&lt;p&gt;Security has always been a top-priority at Grab; our product security team works round-the-clock to ensure that our customers‚Äô data remains safe. Five years ago, we launched our private bug bounty program on &lt;a href=&quot;https://hackerone.com/grab&quot;&gt;HackerOne&lt;/a&gt;, which evolved into a public program in August 2017. The idea was to complement the security efforts our team has been putting through to keep Grab secure. We were a pioneer in South East Asia to implement a public bug bounty program, and now we stand among the &lt;a href=&quot;https://www.hackerone.com/resources/e-book/top-20-public-bug-bounty-programs&quot;&gt;Top 20 programs on HackerOne&lt;/a&gt; worldwide.&lt;/p&gt;

&lt;p&gt;We started as a private bug bounty program which provided us with fantastic results, thus encouraging us to increase our reach and benefit from the vibrant security community across the globe which have helped us iron-out security issues 24x7 in our products and infrastructure. We then publicly launched our bug bounty program offering competitive rewards and hackers can even earn additional bonuses if their report is well-written and display an innovative approach to testing.&lt;/p&gt;

&lt;p&gt;In 2019, we also enrolled ourselves in the &lt;a href=&quot;https://hackerone.com/googleplay&quot;&gt;Google Play Security Reward Program (GPSRP)&lt;/a&gt;, Offered by Google Play, GPSRP allows researchers to re-submit their resolved mobile security issues directly and get additional bounties if the report qualifies under the GPSRP rules. A selected number of Android applications are eligible, including Grab‚Äôs Android mobile application. Through the participation in GPSP, we hope to give researchers the recognition they deserve for their efforts.&lt;/p&gt;

&lt;p&gt;In this blog post, we‚Äôre going to share our journey of running a bug bounty program, challenges involved and share the learnings we had on the way to help other companies in SEA and beyond to establish and build a successful bug bounty program.&lt;/p&gt;

&lt;h2 id=&quot;transitioning-from-private-to-a-public-program&quot;&gt;Transitioning from Private to a Public Program&lt;/h2&gt;

&lt;p&gt;At Grab, before starting the private program, we defined &lt;a href=&quot;https://docs.hackerone.com/programs/policy-and-scope.html&quot;&gt;policy and scope&lt;/a&gt;, allowing us to communicate the objectives of our bug bounty program and list the targets that can be tested for security issues. We did a security sweep of the targets to eliminate low-hanging security issues, assigned people from the security team to take care of incoming reports, and then launched the program in private mode on HackerOne with a few chosen researchers having demonstrated a history of submitting quality submissions.&lt;/p&gt;

&lt;p&gt;One of the benefits of running a &lt;a href=&quot;https://docs.hackerone.com/programs/private-vs-public-programs.html&quot;&gt;private bug bounty program&lt;/a&gt; is to have some control over the number of incoming submissions of potential security issues and researchers who can report issues. This ensures the quality of submissions and helps to control the volume of bug reports, thus avoiding overwhelming a possibly small security team with a deluge of issues so that they won‚Äôt be overwhelming for the people triaging potential security issues. The invited researchers to the program are limited, and it is possible to invite researchers with a known track record or with a specific skill set, further working in the program‚Äôs favour.&lt;/p&gt;

&lt;p&gt;The results and lessons from our private program were valuable, making our program and processes mature enough to &lt;a href=&quot;https://www.techinasia.com/grab-public-bug-bounty&quot;&gt;open the bug bounty program&lt;/a&gt; to security researchers across the world. We still did another security sweep, reworded the policy, redefined the targets by expanding the scope, and allocated enough folks from our security team to take on the initial inflow of reports which was anticipated to be in tune with other public programs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reflecting-on-the-five-years-of-bug-bounty-at-grab/image1.png&quot; alt=&quot;Submissions&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Noticeable spike in the number of incoming reports as we went public in July 2017.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned-from-the-public-program&quot;&gt;Lessons Learned from the Public Program&lt;/h2&gt;

&lt;p&gt;Although we were running our bug bounty program in private for sometime before going public, we still had not worked much on building standard operating procedures and processes for managing our bug bounty program up until early 2018. Listed below, are our key takeaways from 2018 till July 2020 in terms of improvements, challenges, and other insights.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Response Time&lt;/strong&gt;: No researcher wants to work with a bug bounty team that doesn‚Äôt respect the time that they are putting into reporting bugs to the program. We initially didn‚Äôt have a formal process around response times, because we wanted to encourage all security engineers to pick-up reports. Still, we have been consistently delivering a first response to reports in a matter of hours, which is significantly lower than the top 20 bug bounty programs running on HackerOne. Know what structured (or unstructured) processes work for your team in this area, because your program can see significant rewards from fast response times.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time to Bounty&lt;/strong&gt;: In most bug bounty programs the payout for a bug is made in one of the following ways: full payment after the bug has been resolved, full payment after the bug has been triaged, or paying a portion of the bounty after triage and the remaining after resolution. We opt to pay the full bounty after triage. While we‚Äôre always working to speed up resolution times, that timeline is in our hands, not the researcher‚Äôs. Instead of making them wait, we pay them as soon as impact is determined to incentivize long-term engagement in the program.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noise Reduction&lt;/strong&gt;: With &lt;a href=&quot;https://www.hackerone.com/services&quot;&gt;HackerOne Triage&lt;/a&gt; and &lt;a href=&quot;https://www.hackerone.com/blog/Double-your-signal-double-your-fun&quot;&gt;Human-Augmented Signal&lt;/a&gt;, we‚Äôre able to focus our team‚Äôs efforts on resolving unique, valid vulnerabilities. Human-Augmented Signal flags any reports that are likely false-positives, and Triage provides a validation layer between our security team and the report inbox. Collaboration with the HackerOne Triage team has been fantastic and ultimately allows us to be more efficient by focusing our energy on valid, actionable reports. In addition, we take significant steps to block traffic coming from networks running automated scans against our Grab infrastructure and we‚Äôre constantly exploring this area to actively prevent automated external scanning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Team Coverage&lt;/strong&gt;: We introduced a team scheduling process, in which we assign a security engineer (chosen during sprint planning) on a weekly basis, whose sole responsibility is to review and respond to bug bounty reports. We have integrated our systems with HackerOne‚Äôs API and PagerDuty to ensure alerts are for valid reports and verified as much as possible.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;In one area we haven‚Äôt been doing too great is ensuring higher rates of participation in our core mobile applications; some of the pain points researchers have informed us about while testing our applications are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Researchers‚Äô accounts are getting blocked due to our &lt;a href=&quot;https://engineering.grab.com/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;anti-fraud checks&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Researchers are not able to register driver accounts (which is understandable as our driver-partners have to go through manual verification process)&lt;/li&gt;
  &lt;li&gt;Researchers who are not residing in the Southeast Asia region are unable to complete end-to-end flows of our applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are open to community feedback and how we can improve. We want to hear from you! Please drop us a note at &lt;a href=&quot;mailto:infosec.bugbounty@grab.com&quot;&gt;infosec.bugbounty@grab.com&lt;/a&gt; for any program suggestions or feedback.&lt;/p&gt;

&lt;p&gt;Last but not least, we‚Äôd like to thank all researchers who have contributed to the Grab program so far. Your immense efforts have helped keep Grab‚Äôs businesses and users safe. Here‚Äôs a shoutout to our program‚Äôs top-earning hackers &lt;a href=&quot;https://emojipedia.org/trophy/%23:~:text%3DThe%2520trophy%2520emoji%2520is%2520a,the%2520bottom%2520detailing%2520the%2520award.%26text%3DTrophy%2520was%2520approved%2520as%2520part,to%2520Emoji%25201.0%2520in%25202015.&quot;&gt;üèÜ&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overall Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/quanyang?type%3Duser&quot;&gt;@quanyang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/ngocdh?type%3Duser&quot;&gt;@ngocdh&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Year 2019/2020 Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/alexeypetrenko?type%3Duser&quot;&gt;@alexeypetrenko&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/chaosbolt?type%3Duser&quot;&gt;@chaosbolt&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lastly, here is a special shoutout to &lt;a href=&quot;https://hackerone.com/bagipro&quot;&gt;@bagipro&lt;/a&gt; who has done some great work and testing on our Grab mobile applications!&lt;/p&gt;

&lt;p&gt;Well done and from everyone on the Grab team, we look forward to seeing you on the program!&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</guid>
        
        <category>Security</category>
        
        <category>HackerOne</category>
        
        <category>Bug Bounty</category>
        
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>How Grab is blazing through the super app Bazel migration</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we build a seamless user experience that addresses more and more of the daily lifestyle needs of people across South East Asia. We‚Äôre proud of our Grab rides, payments, and delivery services, and want to provide a unified experience across these offerings.&lt;/p&gt;

&lt;p&gt;Here is couple of examples of what Grab does for millions of people across South East Asia every day:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image2.jpg&quot; alt=&quot;Grab Service Offerings&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Grab Service Offerings&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The Grab Passenger application reached super app status more than a year ago and continues to provide hundreds of life-changing use cases in dozens of areas for millions of users.&lt;/p&gt;

&lt;p&gt;With the big product scale, it brings with it even bigger technical challenges. Here are a couple of dimensions that can give you a sense of the scale we‚Äôre working with.&lt;/p&gt;

&lt;h3 id=&quot;engineering-and-product-structure&quot;&gt;Engineering and product structure&lt;/h3&gt;

&lt;p&gt;Technical and product teams work in close collaboration to outserve our customers. These teams are combined into dedicated groups to form Tech Families and focus on similar use cases and areas.&lt;/p&gt;

&lt;p&gt;Grab consists of many Tech Families who work on food, payments, transport, and other services, which are supported by hundreds of engineers. The diverse landscape makes the development process complicated and requires the industry‚Äôs best practices and approaches.&lt;/p&gt;

&lt;h3 id=&quot;codebase-scale-overview&quot;&gt;Codebase scale overview&lt;/h3&gt;

&lt;p&gt;The Passenger Applications (Android and iOS) contain more than &lt;strong&gt;2.5 million lines of code&lt;/strong&gt; each and it keeps growing. We have &lt;strong&gt;1000+ modules&lt;/strong&gt; in the Android App and &lt;strong&gt;700+ targets&lt;/strong&gt; in the iOS App. Hundreds of commits are merged by all the mobile engineers on a daily basis.&lt;/p&gt;

&lt;p&gt;To maintain the health of the codebase and product stability, we run &lt;strong&gt;40K+ unit tests&lt;/strong&gt; on Android and &lt;strong&gt;30K+ unit tests&lt;/strong&gt; on iOS, as well as thousands of UI tests and hundreds of end-to-end tests on both platforms.&lt;/p&gt;

&lt;h2 id=&quot;build-time-challenges&quot;&gt;Build time challenges&lt;/h2&gt;

&lt;p&gt;The described complexity and scale do not come without challenges. A huge codebase propels the build process to the ultimate extreme- challenging the efficiency of build systems and hardware used to compile the super app, and creating out of the line challenges to be addressed.&lt;/p&gt;

&lt;h3 id=&quot;local-build-time&quot;&gt;Local build time&lt;/h3&gt;

&lt;p&gt;Local build time (the build on engineers‚Äô laptop) is one of the most obvious challenges. More code goes in the application binary, hence the build system requires more time to compile it.&lt;/p&gt;

&lt;h4 id=&quot;adr-local-build-time&quot;&gt;ADR local build time&lt;/h4&gt;

&lt;p&gt;The Android ecosystem provides a great out-of-the-box tool to build your project called &lt;em&gt;Gradle&lt;/em&gt;. It‚Äôs flexible and user friendly, and ¬†provides huge capabilities for a reasonable cost. But is this always true? It appears to not be the case due to multiple reasons. Let‚Äôs unpack these reasons below.&lt;/p&gt;

&lt;p&gt;Gradle performs well for medium sized projects with say 1 million line of code. Once the code surpasses that 1 million mark (or so), Gradle starts failing in giving engineers a reasonable build time for the given flexibility. And that‚Äôs exactly what we have observed in our Android application.&lt;/p&gt;

&lt;p&gt;At some point in time, the Android local build became ridiculously long. We even encountered cases ¬†where engineers‚Äô laptops simply failed to build the project due to hardware resources limits. Clean builds took by the hours, and incremental builds easily hit dozens of minutes.&lt;/p&gt;

&lt;h4 id=&quot;ios-local-build-time&quot;&gt;iOS local build time&lt;/h4&gt;

&lt;p&gt;Xcode behaved a bit better compared to Gradle. The Xcode build cache was somehow bearable for incremental builds and didn‚Äôt exceed a couple of minutes. Clean builds still took dozens of minutes though. When Xcode failed to provide the valid cache, engineers had to rerun everything as a clean build, which killed the experience entirely.&lt;/p&gt;

&lt;h3 id=&quot;ci-pipeline-time&quot;&gt;CI pipeline time&lt;/h3&gt;

&lt;p&gt;Each time an engineer submits a Merge Request (MR), our CI kicks in running a wide variety of jobs to ensure the commit is valid and doesn‚Äôt introduce regression to the master branch. The feedback loop time is critical here as well, and the pipeline time tends to skyrocket alongside the code base growth. We found ourselves on the trend where the feedback loop came in by the hours, which again was just breaking the engineering experience, and prevented ¬†us from delivering the world‚Äôs best features to our customers.&lt;/p&gt;

&lt;p&gt;As mentioned, we have a large number of unit tests (30K-40K+) and UI tests (700+) that we run on a pre-merge pipeline. This brings us to hours of execution time before we could actually allow MRs to land to the master branch.&lt;/p&gt;

&lt;p&gt;The number of daily commits, which is by the hundreds, adds another stone to the basket of challenges.&lt;/p&gt;

&lt;p&gt;All this clearly indicated the area of improvement. We were missing opportunities in terms of engineering productivity.&lt;/p&gt;

&lt;h2 id=&quot;the-extra-mile&quot;&gt;The extra mile&lt;/h2&gt;

&lt;p&gt;The biggest question for us to answer was how to put all this scale into a reasonable experience with minimal engineering idle time and fast feedback loop.&lt;/p&gt;

&lt;h3 id=&quot;build-time-critical-path-optimization&quot;&gt;Build time critical path optimization&lt;/h3&gt;

&lt;p&gt;The most reasonable thing to do was to pay attention to the utilization of the hardware resources and make the build process optimal.&lt;/p&gt;

&lt;p&gt;This literally boiled down to the simplest approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Decouple building blocks&lt;/li&gt;
  &lt;li&gt;Make building blocks as small as possible&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This approach is valid for any build system and applies ¬†for both iOS and Android. The first thing we focused on was to understand what our build graph looked like, how dependencies were distributed, and which blocks were bottlenecks.&lt;/p&gt;

&lt;p&gt;Given the scale of the apps, it‚Äôs practically not possible to manage a dependency tree manually, thus we created a tool to help us.&lt;/p&gt;

&lt;h4 id=&quot;critical-path-overview&quot;&gt;Critical path overview&lt;/h4&gt;

&lt;p&gt;We introduced the Critical Path concept:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The critical path is the longest (time) chain of sequential dependencies, which must be built one after the other.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image3.png&quot; alt=&quot;Critical Path&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Critical Path build&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Even with an infinite number of parallel processors/cores, the total build time cannot be less than the critical path time.&lt;/p&gt;

&lt;p&gt;We implemented the tool that parsed the dependency trees (for both Android and iOS), aggregated modules/target build time, and calculated the critical path.&lt;/p&gt;

&lt;p&gt;The concept of the critical path introduced a number of action items, which we prioritized:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The critical path must be as short as possible.&lt;/li&gt;
  &lt;li&gt;Any huge module/target on the critical path must be split into smaller modules/targets.&lt;/li&gt;
  &lt;li&gt;Depend on interfaces/bridges rather than implementations to shorten the critical path.&lt;/li&gt;
  &lt;li&gt;The presence of other teams‚Äô implementation modules/targets in the critical path of the given team is a red flag.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image1.png&quot; alt=&quot;Stack representation of the Critical Path build time&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Stack representation of the Critical Path build time&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;projects-scale-factor&quot;&gt;Project‚Äôs scale factor&lt;/h4&gt;

&lt;p&gt;To implement the conceptually easy action items, we ran a Grab-wide program. The program has impacted almost every mobile team at Grab and involved &lt;strong&gt;200+ engineers&lt;/strong&gt; to some degree. The whole implementation took 6 months to complete.&lt;/p&gt;

&lt;p&gt;During this period of time, we assigned engineers who were responsible to review the changes, provide support to the engineers across Grab, and monitor the results.&lt;/p&gt;

&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;

&lt;p&gt;Even though the overall plan seemed to be good on paper, the results were minimal - it just flattened the build time curve of the upcoming trend introduced by the growth of the codebase. The estimated impact was almost the same for both platforms and gave us about a &lt;strong&gt;7%-10% cut in the CI and local build time&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;open-source-plan&quot;&gt;Open source plan&lt;/h4&gt;

&lt;p&gt;The critical path tool proved to be effective to illustrate the projects‚Äô bottlenecks in a dependency tree configuration. It is currently widely used by mobile teams at Grab to analyze their dependencies and cut out or limit an unnecessary impact on the respective scope.&lt;/p&gt;

&lt;p&gt;The tool is currently considered to be open-sourced as we‚Äôd like to hear feedback from other external teams and see what can be built on top of it. We‚Äôll provide more details on this in future posts.&lt;/p&gt;

&lt;h3 id=&quot;remote-build&quot;&gt;Remote build&lt;/h3&gt;

&lt;p&gt;Another pillar of the ¬†build process is the hardware where the build runs. The solution is ¬†really straightforward - put more muscles on your build to get it stronger and to run faster.&lt;/p&gt;

&lt;p&gt;Clearly, our engineers‚Äô laptops could not be considered fast enough. To have a fast enough build we were looking at something with &lt;em&gt;20+ cores, ~200Gb of RAM&lt;/em&gt;. None of the desktop or laptop computers can reach those numbers within reasonable pricing. We hit a bottleneck in hardware. Further parallelization of the build process didn‚Äôt give any significant improvement as all the build tasks were just queueing and waiting for the resources to be released. And that‚Äôs where cloud computing came into the picture where a huge variety of available options is ready to be used.&lt;/p&gt;

&lt;h4 id=&quot;adr-mainframer&quot;&gt;ADR mainframer&lt;/h4&gt;

&lt;p&gt;We took advantage of the &lt;a href=&quot;https://github.com/buildfoundation/mainframer&quot;&gt;Mainframer&lt;/a&gt; tool. When the build must run, the code diff is pushed to the remote executor, gets compiled, and then the generated artifacts are pushed back to the local machine. An engineer might still benefit from indexing, debugging, and other features available in the IDE.&lt;/p&gt;

&lt;p&gt;To make the infrastructure mature enough, we‚Äôve introduced Kubernetes-based autoscaling based on the load. Currently, we have a stable infrastructure that accommodates &lt;strong&gt;100+ Android engineers scaling up and down (saving costs)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This strategy gave us a &lt;strong&gt;40-50% improvement in the local build time&lt;/strong&gt;. Android builds finished, in the extreme case, &lt;strong&gt;x2 faster&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;ios&quot;&gt;iOS&lt;/h4&gt;

&lt;p&gt;Given the success of the Android remote build infrastructure, we have immediately turned our attention to the iOS builds. It was an obvious move for us - we wanted the same infrastructure for iOS builds. The idea looked good on paper and was proven with Android infrastructure, but the reality was a bit different for our iOS builds.&lt;/p&gt;

&lt;p&gt;Our ¬†very first roadblock was that Xcode is not that flexible and the process of delegating build to remote is way more complicated compared to Android. We tackled a series of blockers such as running indexing on a remote machine, sending and consuming build artifacts, and even running the remote build itself.&lt;/p&gt;

&lt;p&gt;The reality was that the remote build was absolutely possible for iOS. There were ¬†minor tradeoffs impacting engineering experience alongside obvious gains from utilizing cloud computing resources. But the problem is that legally iOS builds are only allowed to be built on an Apple machine.&lt;/p&gt;

&lt;p&gt;Even if we get¬†the most powerful hardware - a macPro - ¬†the specs are still not ideal and are unfortunately not optimized for the build process. A &lt;em&gt;24 core, 194Gb RAM macPro&lt;/em&gt; could have given about x2 improvement on the build time, but when it had to ¬†run 3 builds simultaneously for different users, the build efficiency immediately dropped to the baseline value.&lt;/p&gt;

&lt;p&gt;Android remote machines with the above same specs are capable of running up to &lt;strong&gt;8 simultaneous builds&lt;/strong&gt;. This allowed us to accommodate up to &lt;strong&gt;30-35 engineers&lt;/strong&gt; per machine, whereas iOS‚Äô infrastructure would require to keep this balance at &lt;strong&gt;5-6 engineers&lt;/strong&gt; per machine. This solution didn‚Äôt seem to be scalable at all, causing us to abandon the idea of the remote builds for iOS at that moment.&lt;/p&gt;

&lt;h3 id=&quot;test-impact-analysis&quot;&gt;Test impact analysis&lt;/h3&gt;

&lt;p&gt;The other battlefront was the CI pipeline time. Our efforts in dependency tree optimizations complemented with comparably powerful hardware played a good part in achieving a reasonable build time on CI.&lt;/p&gt;

&lt;p&gt;CI validations also include the execution of unit and UI tests and may easily take 50%-60% of the pipeline time. The problem was getting worse as the number of tests was constantly growing. We were to face incredibly huge tests‚Äô execution time in the near future. We could mitigate the problem by a muscle approach - throwing more runners and shredding tests - but it won‚Äôt make finance executives happy.&lt;/p&gt;

&lt;p&gt;So the time for smart solutions came again. It‚Äôs a known fact that the simpler solution is more likely to be correct. The simplest solution was to stop running &lt;em&gt;ALL&lt;/em&gt; tests. The idea was to run only those tests that were impacted by the codebase change introduced in the given MR.&lt;/p&gt;

&lt;p&gt;Behind this simple idea, we‚Äôve found a huge impact. Once the &lt;em&gt;Test Impact Analysis&lt;/em&gt; was applied to the pre-merge pipelines, we‚Äôve managed to cut down the total number of executed tests by up to &lt;strong&gt;90%&lt;/strong&gt; without any impact on the codebase quality or applications‚Äô stability. As a result, &lt;strong&gt;we cut the pipeline for both platforms by more than 30%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Today, the Test Impact Analysis is coupled with our codebase. We are looking to ¬†invest some effort to make it available for open sourcing. We are excited to be ¬†on this path.&lt;/p&gt;

&lt;h2 id=&quot;the-end-of-the-native-build-systems&quot;&gt;The end of the Native Build Systems&lt;/h2&gt;

&lt;p&gt;One might say that our journey was long and we won the battle for the build time.&lt;/p&gt;

&lt;p&gt;Today, we hit a limit to the native build systems‚Äô efficiency and hardware for both Android and iOS. And it‚Äôs clear to us that in our current setup, we would not be able to scale up while delivering high engineering experience.&lt;/p&gt;

&lt;h2 id=&quot;lets-move-to-bazel&quot;&gt;Let‚Äôs move to Bazel&lt;/h2&gt;

&lt;p&gt;To introduce another big improvement to the build time, we needed to make some ground-level changes. And this time, we focused on the ¬†build system itself.&lt;/p&gt;

&lt;p&gt;Native build systems are designed to work well for small and medium-sized projects, however they have not been as successful in large scale projects such as the Grab Passenger applications.&lt;/p&gt;

&lt;p&gt;With these assumptions, we considered options and found the Bazel build system to be a good contender. The deep comparison of build systems disclosed that Bazel was promising better results almost in all key areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bazel enables remote builds out of box&lt;/li&gt;
  &lt;li&gt;Bazel provides sustainable cache capabilities (local and remote). This cache can be reused across all consumers - local builds, CI builds&lt;/li&gt;
  &lt;li&gt;Bazel was designed with the big codebase as a cornerstone requirement&lt;/li&gt;
  &lt;li&gt;The majority of the tooling may be reused across multiple platforms&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ways-of-adopting&quot;&gt;Ways of adopting&lt;/h3&gt;

&lt;p&gt;On paper, Bazel was awesome and shining. All our playground investigations showed positive results:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cache worked great&lt;/li&gt;
  &lt;li&gt;Incremental builds were incredibly fast&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the effort to shift to this new build system was huge. We made sure that we foresee all possible pitfalls and impediments. It took us about 5 months to estimate the impact and put together a sustainable proof of concept, which reflected the majority of our use cases.&lt;/p&gt;

&lt;h4 id=&quot;migration-limitations&quot;&gt;Migration limitations&lt;/h4&gt;

&lt;p&gt;After those 5 months of investigation, we got the endless list of incompatible features and major blockers to be addressed. Those blockers touched even such obvious things as indexing and the &lt;em&gt;jump to definition&lt;/em&gt; IDE feature, which we used to take for granted.&lt;/p&gt;

&lt;p&gt;But the biggest challenge was the need to keep the pace of the product release. There was no compromise of stopping the product development even for a day. The way out appeared to be a &lt;strong&gt;hybrid build&lt;/strong&gt; concept. We figured out how to marry native and Bazel build systems to live together in harmony. This move gave us a chance to start migrating target by target, project by project moving from the bottom to top of the dependency graph.&lt;/p&gt;

&lt;p&gt;This approach was a valid enabler, however we were still faced with a challenge of our app‚Äôs ¬†scale. The codebase of over 2.5 million of LOC cannot be migrated overnight. The initial estimation was based on the idea of manually migrating the whole codebase, which would have required us to invest dozens of person-months.&lt;/p&gt;

&lt;h4 id=&quot;team-capacity-limitations&quot;&gt;Team capacity limitations&lt;/h4&gt;

&lt;p&gt;This approach was immediately pushed back by multiple teams arguing with the priority and concerns about the impact on their own product roadmap.&lt;/p&gt;

&lt;p&gt;We were left with not much ¬†choice. On one hand, we had a pressingly long build time. And on the other hand, we were asking for a huge effort from teams. We clearly needed to get buy-ins from all of our stakeholders to push things forward.&lt;/p&gt;

&lt;h3 id=&quot;getting-buy-in&quot;&gt;Getting buy-in&lt;/h3&gt;

&lt;p&gt;To get all needed buy-ins, all stakeholders were grouped and addressed separately. We defined key factors for each group.&lt;/p&gt;

&lt;h4 id=&quot;key-factors&quot;&gt;Key factors&lt;/h4&gt;

&lt;p&gt;C-level stakeholders:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;. The migration impact must be significant - at least a 40% decrease on the build time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Costs&lt;/strong&gt;. Migration costs must be paid back in a reasonable time and the positive impact is extended to ¬†the future.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Engineering experience&lt;/strong&gt;. The user experience must not be compromised. All tools and features engineers used must be available during migration and even after.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Engineers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Engineering experience&lt;/strong&gt;. Similar to the criteria established at the C-level factor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Early adopters engagement&lt;/strong&gt;. A common ¬†core experience must be created across the mobile engineering community to support other engineers in the later stages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Education&lt;/strong&gt;. Awareness campaigns must be in place. Planned and conducted a series of tech talks and workshops to raise awareness among engineers and cut the learning curve. We wrote hundreds of pages of documentation and guidelines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Product teams:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;No roadmap impact&lt;/strong&gt;. Migration must not affect the product roadmap.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Minimize the engineering effort&lt;/strong&gt;. Migration must not increase the efforts from engineering.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;migration-automation-separate-talks&quot;&gt;Migration automation (separate talks)&lt;/h4&gt;

&lt;p&gt;The biggest concern for the majority of the stakeholders appeared to be the estimated migration effort, which impacted the cost, the product roadmap, and the engineering experience. It became evident that we needed to streamline the process and reduce the effort for migration.&lt;/p&gt;

&lt;p&gt;Fortunately, the actual migration process was routine in nature, so we had opportunities for automation. We investigated ideas on automating the whole migration process.&lt;/p&gt;

&lt;h4 id=&quot;the-tools-weve-created&quot;&gt;The tools we‚Äôve created&lt;/h4&gt;

&lt;p&gt;We found that it‚Äôs relatively easy to create a bunch of tools that read the native project structure and create an equivalent Bazel set up. This was a game changer.&lt;/p&gt;

&lt;p&gt;Things moved pretty smoothly for both Android and iOS projects. We managed to roll out tooling to migrate the codebase in a single click/command (well with some exceptions as of now. Stay tuned for another blog post on this). With this tooling combined with the hybrid build concept, we addressed all the key buy-in factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Migration cost dropped by at least 50%.&lt;/li&gt;
  &lt;li&gt;Less engineers required for the actual migration. There was no need to engage the wide engineering community as a small group of people can manage the whole process.&lt;/li&gt;
  &lt;li&gt;There is no more impact on the product roadmap.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;where-do-we-stand-today&quot;&gt;Where do we stand today&lt;/h2&gt;

&lt;p&gt;When we were in the middle of the actual migration, we decided to take a pragmatic path and migrate our applications in phases to ensure everything was under control and that there were no unforeseen issues.&lt;/p&gt;

&lt;p&gt;The hybrid build time is racing alongside our migration progress. It has a linear dependency on the amount of the migrated code. The figures look positive and we are confident in achieving our impact goal of &lt;strong&gt;decreasing at least 40% of the build time&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;plans-to-open-source&quot;&gt;Plans to open source&lt;/h3&gt;

&lt;p&gt;The automated migration tooling we‚Äôve created is planned to be open sourced. We are doing a bit better on the Android side decoupling it from our applications‚Äô implementation details and plan to open source it in the near future.&lt;/p&gt;

&lt;p&gt;The iOS tooling is a bit behind, and we expect it to be available for open-sourcing by the end of Q1‚Äô2021.&lt;/p&gt;

&lt;h2 id=&quot;is-it-worth-it-all&quot;&gt;Is it worth it all?&lt;/h2&gt;

&lt;p&gt;Bazel is not a silver bullet for the build time and your project. There are a lot of edge cases you‚Äôll never know until it punches you straight in your face.&lt;/p&gt;

&lt;p&gt;It‚Äôs far from industry standard and you might find yourself having difficulty hiring engineers with such knowledge. It has a steep learning curve as well. It‚Äôs absolutely an overhead for small to medium-sized projects, but it‚Äôs undeniably essential once you start playing in a high league of super apps.&lt;/p&gt;

&lt;p&gt;If you were to ask whether we‚Äôd go this path again, the answer would come in a &lt;strong&gt;fast and correct&lt;/strong&gt; way - yes, without any doubts.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Sergii Grechukha on behalf of the Passenger App team at Grab. Special thanks to Madushan Gamage, Mikhail Zinov, Nguyen Van Minh, Mihai Costiug, Arunkumar Sampathkumar, Maryna Shaposhnikova, Pavlo Stavytskyi, Michael Goletto, Nico Liu, and Omar Gawish for their contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Dec 2020 04:30:00 +0000</pubDate>
        <link>https://engineering.grab.com/how-grab-is-blazing-through-the-super-app-bazel-migration</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-grab-is-blazing-through-the-super-app-bazel-migration</guid>
        
        <category>Bazel</category>
        
        <category>Android</category>
        
        <category>iOS</category>
        
        <category>Build Time</category>
        
        <category>Xcode</category>
        
        <category>Gradle</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Democratizing Fare Storage at scale using Event Sourcing</title>
        <description>&lt;p&gt;From humble beginnings, Grab has expanded across different markets in the last couple of years. We‚Äôve added a wide range of features to the Grab platform to continue to delight our customers and driver-partners. We had to incessantly find ways to improve our existing solutions to better support new features.&lt;/p&gt;

&lt;p&gt;In this blog, we discuss how we built &lt;em&gt;Fare Storage&lt;/em&gt;, Grab‚Äôs single source of truth fare data store, and how we overcame the challenges to make it more reliable and scalable to support our expanding features.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image1.jpg&quot; alt=&quot;High-level Flow&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To set some context for this blog, let‚Äôs define some key terms before proceeding. A &lt;em&gt;Fare&lt;/em&gt; is a dollar amount calculated to move someone or something from point A to point B. And, a &lt;em&gt;Fee&lt;/em&gt; is a dollar amount added to or subtracted from the original fare amount for any additional service.&lt;/p&gt;

&lt;p&gt;Now that you‚Äôre acquainted with the key concepts, let‚Äôs look take a look at the following image. It illustrates that features such as &lt;em&gt;Destination Change Fee&lt;/em&gt;, &lt;em&gt;Waiting Fee&lt;/em&gt;, &lt;em&gt;Cancellation Fee&lt;/em&gt;, &lt;em&gt;Tolls&lt;/em&gt;, &lt;em&gt;Promos&lt;/em&gt;, &lt;em&gt;Surcharges&lt;/em&gt;, and many others store additional fee breakdown along with the original fare. This set of information is crucial for generating receipts and debugging processes. However, our legacy storage system wasn‚Äôt designed to host massive quantities of information effectively.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image2.jpg&quot; alt=&quot;Sample Flow with Fee Breakdown&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In our legacy architecture, we stored all the booking and fare-related information in a single relational database table. Adding new fare fields and breakdowns required changes in our critical booking system, making iterations prohibitively expensive and hindering innovation.&lt;/p&gt;

&lt;p&gt;The need to store the fare information and metadata for every additional feature along with other booking information resulted in a bloated booking entity. With millions of bookings created every day at Grab, this posed a scaling and stability threat to our booking service storage. Moreover, the legacy storage only tracked the latest value of fare and lacked a holistic view of all the modifications to the fare. So, debugging the fare was also a massive chore for our Engineering and Tech Operations teams.&lt;/p&gt;

&lt;h2 id=&quot;drafting-a-solution&quot;&gt;Drafting a solution&lt;/h2&gt;

&lt;p&gt;The shortcomings of our legacy system led us to explore options for decoupling the fare and its metadata storage from the booking details. We wanted to build a platform that can store and provide access to both fare and its audit trail.&lt;/p&gt;

&lt;p&gt;High-level functional requirements for the new fare store were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide a platform to store and retrieve fare and associated breakdowns, with no tight coupling between services.&lt;/li&gt;
  &lt;li&gt;Act as a single source-of-truth for fare and associated fees in the Grab ecosystem.&lt;/li&gt;
  &lt;li&gt;Enable clients to access the metadata of fare change events in real-time, enabling the Product team to innovate freely.&lt;/li&gt;
  &lt;li&gt;Provide smooth access to a fare‚Äôs audit trail, improving the response time to our customers‚Äô queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Non-functional requirements for the fare store were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High availability for the read and write APIs, with few milliseconds latency.&lt;/li&gt;
  &lt;li&gt;Handle concurrent updates to the fare gracefully.&lt;/li&gt;
  &lt;li&gt;Detect duplicate events for a booking for the same transaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;storing-change-sequence-with-event-sourcing&quot;&gt;Storing change sequence with Event Sourcing&lt;/h2&gt;

&lt;p&gt;Our legacy storage solution used a defined schema and only stored the latest state of the fare. We needed an audit trail-based storage system with fast querying capabilities that can store and retrieve changes in chronological order.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Event Sourcing pattern&lt;/em&gt; stood out as a flexible architectural pattern as it allowed us to store and query the sequence of changes in the order it occurred. In Martin Fowler‚Äôs &lt;a href=&quot;https://martinfowler.com/eaaDev/EventSourcing.html&quot;&gt;blog&lt;/a&gt;, he described &lt;a href=&quot;https://microservices.io/patterns/data/event-sourcing.html&quot;&gt;Event Sourcing&lt;/a&gt; as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‚ÄúThe fundamental idea of Event Sourcing is to ensure that every change to the state of an application is captured in an event object and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.‚Äù&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the Event Sourcing pattern, we store all the fare changes as events in the order they occurred for a booking. We iterate through these events to retrieve a complete snapshot of the modifications to the fare.&lt;/p&gt;

&lt;p&gt;A sample Fare Event looks like this:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Event&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// type of the event, ADD, SUB, SET, resilient&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;EventType&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// value which was added, subtracted or modified&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// fare for the booking after applying discount&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fare&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// description bytes generated by SDK&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;bytes&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//transactionID for the EventType&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;transactionID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Event Sourcing pattern also enable us to use the Command Query Responsibility Segregation (&lt;a href=&quot;https://martinfowler.com/bliki/CQRS.html&quot;&gt;CQRS&lt;/a&gt;) pattern to decouple the read responsibility for different use cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image4.jpg&quot; alt=&quot;CQRS Pattern&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Clients of the fare life cycle read the current fare and create events to change the fare value as per their logic. Clients can also access fare events, when required. This pattern enable clients to modify fares independently, and give them visibility to the sequence for different business needs.&lt;/p&gt;

&lt;p&gt;The diagram below describes the overall fare life cycle from creation, modification to display using the event store.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image5.png&quot; alt=&quot;Overall Fare Life Cycle&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;architecture-overview&quot;&gt;Architecture overview&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image6.jpg&quot; alt=&quot;Fare Cycle Architecture&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Clients interact with the Fare LifeCycle service through an SDK. The SDK offers various features such as metadata serialization, deserialization, retries, and timeouts configurations, some of which are discussed later.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Fare LifeCycle Store&lt;/em&gt; service uses DynamoDB as Event Store to persist and read the fare change events backed by a cache for eventually consistent reads. For further processing, such as archiving and generation of receipts, the successfully updated events are streamed out to a message queue system.&lt;/p&gt;

&lt;h2 id=&quot;ensuring-the-integrity-of-the-fare-sequence&quot;&gt;Ensuring the integrity of the fare sequence&lt;/h2&gt;

&lt;p&gt;Democratizing the responsibility of fare modification means that multiple services might try to update the fare in parallel without prior synchronization. Concurrent fare updates for the same booking might result in a race condition. Concurrency and consistency problems are always highlights of distributed storage systems.&lt;/p&gt;

&lt;p&gt;Let‚Äôs understand why the ordering of fare updates are important. Business rules for different cities and countries regulate the pricing features based on local market conditions and prevailing laws. For example, in some scenarios, &lt;em&gt;Tolls&lt;/em&gt; and &lt;em&gt;Waiting Fees&lt;/em&gt; may not be eligible for discounts or promotions. The service applying discounts needs to consider this information while applying a discount. Therefore, updates to the fare are not independent of the previous fare events.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image3.jpg&quot; alt=&quot;Fare Integrity&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We needed a mechanism to detect race conditions and handle them appropriately to ensure the integrity of the fare. To handle race conditions based on our use case, we explored &lt;a href=&quot;https://en.wikipedia.org/wiki/Lock_(computer_science)&quot;&gt;Pessimistic and Optimistic locking mechanisms&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All the expected fare change events happen based on certain conditions being true or false. For example, less than 1% of the bookings have a payment change request initiated by passengers during a ride. And, the probability of multiple similar changes happening on the same booking is rather low. &lt;em&gt;Optimistic Locking&lt;/em&gt; offers both efficiency and correctness for our requirements where the chances of race conditions are low, and the records are independent of each other.&lt;/p&gt;

&lt;p&gt;The logic to calculate the fare/surcharge is coupled with the business logic of the system that calculates the fare component or fees. So, handling data race conditions on the data store layer was not an acceptable option either. It made more sense to let the clients handle it and keep the storage system decoupled from the business logic to compute the fare.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image8.jpg&quot; alt=&quot;Optimistic Locking&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To achieve &lt;em&gt;Optimistic Locking&lt;/em&gt;, we store a fare version and increment it on every successful update. The client must pass the version they read to modify the fare. Should there be a version mismatch between the update query and the current fare, the update is rejected. On version mismatches, the clients read the updated checksum(version) and retry with the recalculated fare.&lt;/p&gt;

&lt;h2 id=&quot;idempotency-of-event-updates&quot;&gt;Idempotency of event updates&lt;/h2&gt;

&lt;p&gt;The next challenge we came across was how to handle client retries - ensuring that we do not duplicate the same event for the booking. Clients might encounter sporadic errors as a result of network-related issues, although the update was successful. Under such circumstances, clients retry to update the same event, resulting in duplicate events. Duplicate events not only result in an extra space requirement, but it also impairs the clients‚Äô understanding on whether we‚Äôve taken an action multiple times on the fare.&lt;/p&gt;

&lt;p&gt;As discussed in the previous section, retrying with the same version would fail due to the version mismatch. If the previous attempt successfully modified the fare, it would also update the version.&lt;/p&gt;

&lt;p&gt;However, clients might not know if their update modified the version or if any other clients updated the data. Relying on clients to check for event duplication makes the client-side complex and leaves a chance of error if the clients do not handle it correctly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image9.jpg&quot; alt=&quot;Solution for Duplicate Events&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To handle the duplicate events, we associate each event with a unique UUID (&lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt;) generated from the client-side using a UUID library from the Fare LifeCycle service SDK. We check whether the &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt; is already part of successful transaction IDs before updating the fare. If we identify a non-unique &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt;, we return duplicate event errors to the client.&lt;/p&gt;

&lt;p&gt;For unique &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionIDs&lt;/code&gt;, we append it to the list of transactionIDs and save it to the Event Store along with the event.&lt;/p&gt;

&lt;h2 id=&quot;schema-less-metadata&quot;&gt;Schema-less metadata&lt;/h2&gt;

&lt;p&gt;Metadata are the breakdowns associated with the fare. We require the metadata for specific fee/fare calculation for the generation of receipts and debugging purposes. Thus, for the storage system and multiple clients, they need not know the metadata definition of all events.&lt;/p&gt;

&lt;p&gt;One goal for our data store was to give our clients the flexibility to add new fields to existing metadata or to define new metadata without changing the API. We adopted an SDK-based approach for metadata, where clients interact with the Fare LifeCycle service via SDK. The SDK has the following responsibilities for metadata:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Serialize the metadata into bytes before making an API call to the Fare LifeCycle service.&lt;/li&gt;
  &lt;li&gt;Deserialize the bytes metadata returned from the Fare LifeCycle service into a Go struct for client access.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image7.jpg&quot; alt=&quot;Fare LifeCycle SDK&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Serializing and deserializing the metadata on the client-side decoupled it from the Fare LifeCycle Store API. This helped teams update the metadata without deploying the storage service each time.&lt;/p&gt;

&lt;p&gt;For reading the breakdown, the clients pass the metadata bytes to the SDK along with the Event Type, and then it converts them back into the corresponding proto schema. With this approach, clients can update the metadata without changing the Data Store Service.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Fare LifeCycle service enabled us to revolutionize the fare storage at scale for Grab‚Äôs ecosystem of services. Further benefits realized with the system are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The feature agnostic platform helped us to reduce the time-to-market for our hyper-local features so that we can further outserve our customers and driver-partners.&lt;/li&gt;
  &lt;li&gt;Decoupling the fare information from the booking information also helped us to achieve a better separation of concerns between services.&lt;/li&gt;
  &lt;li&gt;Improve the overall reliability and scalability of the Grab platform by decoupling fare and booking information, allowing them to scale independently of each other.&lt;/li&gt;
  &lt;li&gt;Reduce unnecessary coupling between services to fetch fare related information and update fare.&lt;/li&gt;
  &lt;li&gt;The audit-trail of fare changes in the chronological order reduced the time to debug fare and improved our response to customers for fare-related queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope this post helped you to have a closer look at how we used the Event Source pattern for building a data store and how we handled a few caveats and challenges in the process.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Sourabh Suman on behalf of the Pricing team at Grab. Special thanks to Karthik Gandhi, Kurni Famili, ChandanKumar Agarwal, Adarsh Koyya, Niteesh Mehra, Sebastian Wong, Matthew Saw, Muhammad Muneer, and Vishal Sharma for their contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Nov 2020 06:21:00 +0000</pubDate>
        <link>https://engineering.grab.com/democratizing-fare-storage-at-scale-using-event-sourcing</link>
        <guid isPermaLink="true">https://engineering.grab.com/democratizing-fare-storage-at-scale-using-event-sourcing</guid>
        
        <category>Pricing</category>
        
        <category>Event Sourcing</category>
        
        <category>Fare Storage</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Keeping 170 libraries up to date on a large scale Android App</title>
        <description>&lt;p&gt;To scale up to the needs of our customers, we‚Äôve adopted ways to efficiently deliver our services through our everyday superapp - whether it‚Äôs through continuous process improvements or coding best practices. For one, &lt;a href=&quot;https://en.wikipedia.org/wiki/Library_(computing)&quot;&gt;libraries&lt;/a&gt; have made it possible for us to increase our development velocity. In the Passenger App Android team, we‚Äôve a mix of libraries - from libraries that we‚Äôve built in-house to open source ones.&lt;/p&gt;

&lt;p&gt;Every week, we release a new version of our Passenger App. Each update contains on average between five to ten library updates. In this article, we will explain how we keep all libraries used by our app up to date, and the different actions we take to avoid defect leaks into production.&lt;/p&gt;

&lt;h2 id=&quot;how-many-libraries-are-we-using&quot;&gt;How many libraries are we using?&lt;/h2&gt;

&lt;p&gt;Before we add a new library to a project, it goes through a rigorous assessment process covering many parts, such as security issue detection and usability tests measuring the impact on the app size and app startup time. This process ensures that only libraries up to our standards are added.&lt;/p&gt;

&lt;p&gt;In total, there are more than &lt;strong&gt;170 libraries&lt;/strong&gt; powering the SuperApp, including 55 AndroidX artifacts and 22 libraries used for the sole purpose of writing automation testing (Unit Testing or UI Testing).&lt;/p&gt;

&lt;h2 id=&quot;who-is-responsible-for-updating&quot;&gt;Who is responsible for updating&lt;/h2&gt;

&lt;p&gt;While we do have an internal process on how to update the libraries, it doesn‚Äôt mention who and how often it should be done. In fact, it‚Äôs everyone‚Äôs responsibility to make sure our libraries are up to date. Each team should be aware of the libraries they‚Äôre using and whenever a new version is released.&lt;/p&gt;

&lt;p&gt;However, this isn‚Äôt really the case. We‚Äôve a few developers taking ownership of the libraries as a whole and trying to maintain it. With more than 170 external libraries, we surveyed the Android developer community on how they manage libraries in the company. The result can be summarized as follow:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/keeping-170-libraries-up-to-date/infography.png&quot; alt=&quot;Survey Results&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Survey Results&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;While most developers are aware of updates, they don‚Äôt update a library because the risk of defects leaking into production is too high.&lt;/p&gt;

&lt;h2 id=&quot;risk-management&quot;&gt;Risk management&lt;/h2&gt;
&lt;p&gt;The risk is to have a defect leaking into production. It can cause regressions on existing features or introduce new crashes in the app. In a worst case scenario, if this isn‚Äôt caught before publishing, it can force us to make a hotfix and a certain number of users will be impacted.&lt;/p&gt;

&lt;p&gt;Before updating (bump) a library, we evaluate two metrics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the usage of this library in the codebase.&lt;/li&gt;
  &lt;li&gt;the number of changes introduced in the library between the current version and the targeted version.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The risk needs to be assessed between the number of usages of a certain library and the size of the changes. The following chart illustrate this point.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/keeping-170-libraries-up-to-date/radar.png&quot; alt=&quot;Risk Assessment Radar&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Risk Assessment Radar&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;This arbitrary scale helps us in deciding if we will require additional signoff from the QA team. If the estimation places the item on the bottom-left corner, the update will be less risky while if it‚Äôs on the top-right corner, it means we should follow extra verification to reduce the risk.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A good practice to reduce the risks of updating a library is to update it frequently, decreasing the diffs hence reducing the scope of impact.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;reducing-the-risk&quot;&gt;Reducing the risk&lt;/h2&gt;

&lt;p&gt;The first thing we‚Äôre doing to reduce the risk is to update our libraries on a weekly basis. As described above, small changes are always less risky than large changes even if the usage of this partial library is wide. By following incremental updates, we avoid accumulating potential issues over a longer period of time.&lt;/p&gt;

&lt;p&gt;For example, the Android Jetpack and Firebase libraries follow a two-week release train. So every two weeks, we check for new updates, read the changelogs, and proceed with the update.&lt;/p&gt;

&lt;p&gt;In case of a defect detected, we can easily revert the change until we figure out a proper solution or raise the issue to the library owner.&lt;/p&gt;

&lt;h3 id=&quot;automation&quot;&gt;Automation&lt;/h3&gt;

&lt;p&gt;To reduce risk on any merge request (not limited to library update), we‚Äôve spent a tremendous amount of effort on automating tests. For each new feature we‚Äôve a set of test cases written in Gherkin syntax.&lt;/p&gt;

&lt;p&gt;Automation is implemented as UI tests that run on continuous integration (CI) for every merge request. If those tests fail, we won‚Äôt be able to merge any changes.&lt;/p&gt;

&lt;p&gt;To further elaborate, let‚Äôs take this example: Team A developed a lot of features and now has a total of 1,000 test cases. During regression testing before each release, only a subset of those are executed manually based on the impacted area. With automation in place, team A now has 60% of those tests executed as part of CI. So, when all the tests successfully pass, we‚Äôre already 60% confident that no defect is detected. This tremendously increases our confidence level while reducing manual testing.&lt;/p&gt;

&lt;h3 id=&quot;qa-signoff&quot;&gt;QA signoff&lt;/h3&gt;

&lt;p&gt;When the update is in the risk threshold area and the automation tests are insufficient, the developer works with QA engineers on analyzing impacted areas. They would then execute test cases related to the impacted area.&lt;/p&gt;

&lt;p&gt;For example, if we‚Äôre updating Facebook library, the impacted area would be the ‚ÄúLogin with Facebook‚Äù functionality. QA engineers would then run test cases related to social login.&lt;/p&gt;

&lt;p&gt;A single or multiple team can be involved. In some cases, QA signoff can be required by all the teams if they‚Äôre all affected by the update.&lt;/p&gt;

&lt;p&gt;This process requires a lot of effort from different teams and can affect the current roadmap. To avoid falling into this category, we refine the impacted area analysis to be as specific as possible.&lt;/p&gt;

&lt;h2 id=&quot;update-before-it-becomes-mandatory&quot;&gt;Update before it becomes mandatory&lt;/h2&gt;

&lt;p&gt;Google updates the Google Play requirements regularly to ensure that published apps are fully compatible with the latest Android version.&lt;/p&gt;

&lt;p&gt;For example, starting 1st November 2020 all apps must target API 29. This change causes &lt;a href=&quot;https://developer.android.com/about/versions/10/behavior-changes-10&quot;&gt;behavior changes&lt;/a&gt; for some API. New behavior has to be supported and verified for our code, but also for all the libraries we use. Libraries bundled inside our app are also affected if they‚Äôre using Android API. However, the support for newer API is done by each library maintainer. By keeping our libraries up to date, we ensure compatibility with the latest Android API.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Keep updating your libraries. If they‚Äôre following a release plan, try to match it so it won‚Äôt accumulate too many changes. For every new release at Grab, we ship a new version each week, which includes between 5 to 10 libraries bump.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each update, identify the potential risks on your app and find the correct balance between risk and effort required to mitigate this. Don‚Äôt overestimate the risk, especially if the changes are minimal and only include some minor bug fixing. Some library updates don‚Äôt even change any single line of code and are only documentation updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Invest in robust automation testing to create a high confidence level when making changes, including potentially large changes like a huge library bump.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Lucas Nelaupe on behalf of the Grab Android Development team. Special thanks to Tridip Thrizu and Karen Kue for the design and copyediting contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Oct 2020 04:39:00 +0000</pubDate>
        <link>https://engineering.grab.com/keeping-170-libraries-up-to-date-on-a-large-scale-android-app</link>
        <guid isPermaLink="true">https://engineering.grab.com/keeping-170-libraries-up-to-date-on-a-large-scale-android-app</guid>
        
        <category>Mobile</category>
        
        <category>Android</category>
        
        <category>Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Optimally scaling Kafka consumer applications</title>
        <description>&lt;p&gt;Earlier this year, we took you on a journey on how we built and deployed our event sourcing and stream processing framework at Grab. We‚Äôre happy to share that we‚Äôre able to reliably maintain our uptime and continue to service close to 400 billion events a week. We haven‚Äôt stopped there though. To ensure that we can scale our framework as the Grab business continuously grows, we have spent efforts optimizing our infrastructure.&lt;/p&gt;

&lt;p&gt;In this article, we will dive deeper into our Kubernetes infrastructure setup for our &lt;a href=&quot;https://engineering.grab.com/plumbing-at-scale&quot;&gt;stream processing framework&lt;/a&gt;. We will cover why and how we focus on optimal scalability and availability of our infrastructure.&lt;/p&gt;

&lt;h2 id=&quot;quick-architecture-recap&quot;&gt;Quick Architecture Recap&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Coban Platform Architecture&quot; src=&quot;/img/optimally-scaling-kafka-consumer-applications/image2.png&quot; /&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The Coban platform provides lightweight &lt;a href=&quot;https://medium.com/learning-the-go-programming-language/writing-modular-go-programs-with-plugins-ec46381ee1a9&quot;&gt;Golang plugin&lt;/a&gt; architecture-based data processing pipelines running in Kubernetes. These are essentially Kafka consumer pods that consume data, process it, and then materialize the results into various sinks (RDMS, other Kafka topics).&lt;/p&gt;

&lt;h2 id=&quot;anatomy-of-a-processing-pod&quot;&gt;Anatomy of a Processing Pod&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Anatomy of a Processing Pod&quot; src=&quot;/img/optimally-scaling-kafka-consumer-applications/image1.png&quot; /&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Each stream processing pod (the smallest unit of a pipeline‚Äôs deployment) has three top level components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Trigger&lt;/strong&gt;: An interface that connects directly to the source of the data and converts it into an event channel.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Runtime&lt;/strong&gt;: This is the app‚Äôs entry point and the orchestrator of the pod. It manages the worker pools, triggers, event channels, and lifecycle events.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipeline plugin&lt;/strong&gt;: This is provided by the user, and conforms to a contract that the platform team publishes. It contains the domain logic for the pipeline and houses the pipeline orchestration defined by a user based on our Stream Processing Framework.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-scaling&quot;&gt;Optimal Scaling&lt;/h3&gt;

&lt;p&gt;We initially architected our Kubernetes setup around &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale&quot;&gt;horizontal pod autoscaling&lt;/a&gt; (HPA), which scales the number of pods per deployment based on CPU and memory usage. HPA keeps CPU and memory per pod specified in the deployment manifest and scales horizontally as the load changes.&lt;/p&gt;

&lt;p&gt;These were the areas of application wastage we observed on our platform:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As Grab‚Äôs traffic is uneven, we‚Äôd always have to provision for peak traffic. As users would not (or could not) always account for ramps, they would be fairly liberal with setting limit values (CPU and memory), leading to resource wastage.&lt;/li&gt;
  &lt;li&gt;Pods often had uneven traffic distribution despite fairly even partition load distribution in Kafka. The Stream Processing Framework(SPF) is essentially Kafka consumers consuming from Kafka topics, hence the number of pods scaling in and out resulted in unequal partition load per pod.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vertically-scaling-with-fixed-number-of-pods&quot;&gt;Vertically Scaling with Fixed Number of Pods&lt;/h3&gt;

&lt;p&gt;We initially kept the number of pods for a pipeline equal to the number of partitions in the topic the pipeline consumes from. This ensured even distribution of partitions to each pod providing balanced consumption. In order to abstract this from the end user, we automated the application deployment process to directly call the Kafka API to fetch the number of partitions during runtime.&lt;/p&gt;

&lt;p&gt;After achieving a fixed number of pods for the pipeline, we wanted to move away from HPA. We wanted our pods to scale up and down as the load increases or decreases without any manual intervention. &lt;a href=&quot;https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler&quot;&gt;Vertical pod autoscaling&lt;/a&gt; (VPA) solves this problem as it relieves us from any manual operation for setting up resources for our deployment.&lt;/p&gt;

&lt;p&gt;We just deploy the application and let VPA handle the resources required for its operation. It‚Äôs known to not be very susceptible to quick load changes as it trains its model to monitor the deployment‚Äôs load trend over a period of time before recommending an optimal resource. This process ensures the optimal resource allocation for our pipelines considering the historic trends on throughput.&lt;/p&gt;

&lt;p&gt;We saw a &lt;em&gt;~45%&lt;/em&gt; reduction in our total resource usage vs resource requested after moving to VPA with a fixed number of pods from HPA.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Anatomy of a Processing Pod&quot; src=&quot;/img/optimally-scaling-kafka-consumer-applications/image3.png&quot; /&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;managing-availability&quot;&gt;Managing Availability&lt;/h3&gt;

&lt;p&gt;We broadly classify our workloads as latency sensitive (critical) and latency tolerant (non-critical). As a result, we could optimize scheduling and cost efficiency using &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption&quot;&gt;priority classes&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubernetes-sigs/cluster-proportional-autoscaler&quot;&gt;overprovisioning&lt;/a&gt; on heterogeneous node types on AWS.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-priority-classes&quot;&gt;Kubernetes Priority Classes&lt;/h2&gt;

&lt;p&gt;The main cost of running EKS in AWS is attributed to the EC2 machines that form the worker nodes for the Kubernetes cluster. Running &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand&quot;&gt;On-Demand&lt;/a&gt; brings all the guarantees of instance availability but it is definitely very expensive. Hence, our first action to drive cost optimisation was to include &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html&quot;&gt;Spot instances&lt;/a&gt; in our worker node group.&lt;/p&gt;

&lt;p&gt;With the uncertainty of losing a spot instance, we started assigning priority to our various applications. We then let the user choose the priority of their pipeline depending on their use case. Different priorities would result in different node affinity to different kinds of instance groups (On-Demand/Spot). For example, Critical pipelines (latency sensitive) run on On-Demand worker node groups and Non-critical pipelines (latency tolerant) on Spot instance worker node groups.&lt;/p&gt;

&lt;p&gt;We use priority class as a method of preemption, as well as a node affinity that chooses a certain priority pipeline for the node group to deploy to.&lt;/p&gt;

&lt;h2 id=&quot;overprovisioning&quot;&gt;Overprovisioning&lt;/h2&gt;

&lt;p&gt;With spot instances running we realised a need to make our cluster quickly respond to failures. We wanted to achieve quick rescheduling of evicted pods, hence we added overprovisioning to our cluster. This means we keep some noop pods occupying free space running in our worker node groups for the quick scheduling of evicted or deploying pods.&lt;/p&gt;

&lt;p&gt;The overprovisioned pods are the lowest priority pods, thus can be preempted by any pod waiting in the queue for scheduling. We used cluster proportional autoscaler to decide the right number of these overprovisioned pods, which scales up and down proportionally to cluster size (i.e number of nodes and CPU in worker node group). This relieves us from tuning the number of these noop pods as the cluster scales up or down over the period keeping the free space proportional to current cluster capacity.&lt;/p&gt;

&lt;p&gt;Lastly, overprovisioning also helped improve the deployment time because there is no ¬†dependency on the time required for &lt;a href=&quot;https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html&quot;&gt;Auto Scaling Groups&lt;/a&gt; (ASG) to add a new node to the cluster every time we want to deploy a new application.&lt;/p&gt;

&lt;h2 id=&quot;future-improvements&quot;&gt;Future Improvements&lt;/h2&gt;

&lt;p&gt;Evolution is an ongoing process. In the next few months, we plan to work on custom resources for combining VPA and fixed deployment size. Our current architecture setup works fine for now, but we would like to create a more tuneable in-house &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources&quot;&gt;CRD&lt;/a&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources&quot;&gt;(Custom Resource Definition)&lt;/a&gt; for VPA that incorporates rightsizing our Kubernetes deployment horizontally.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored By Shubham Badkur on behalf of the Coban team at Grab - Ryan Ooi, Karan Kamath, Hui Yang, Yuguang Xiao, Jump Char, Jason Cusick, Shrinand Thakkar, Dean Barlan, Shivam Dixit, Andy Nguyen, and Ravi Tandon.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Tue, 13 Oct 2020 02:13:54 +0000</pubDate>
        <link>https://engineering.grab.com/optimally-scaling-kafka-consumer-applications</link>
        <guid isPermaLink="true">https://engineering.grab.com/optimally-scaling-kafka-consumer-applications</guid>
        
        <category>Event Sourcing</category>
        
        <category>Stream Processing</category>
        
        <category>Kubernetes</category>
        
        <category>Backend</category>
        
        <category>Platform</category>
        
        <category>Go</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Our Journey to Continuous Delivery at Grab (Part 1)</title>
        <description>&lt;p&gt;This blog post is a two-part presentation of the effort that went into improving the &lt;a href=&quot;https://continuousdelivery.com/&quot;&gt;continuous delivery&lt;/a&gt; processes for backend services at Grab in the past two years. In the first part, we take stock of where we started two years ago and describe the software and tools we created while introducing some of the integrations we‚Äôve done to automate our software delivery in our staging environment.
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;post-quotations&quot;&gt;
  &lt;i&gt;Continuous Delivery is the ability to get changes of all types‚Äîincluding new features, configuration changes, bug fixes and experiments‚Äîinto production, or into the hands of users, safely and quickly in a sustainable way.&lt;/i&gt;
  &lt;br /&gt;
  &lt;i&gt;‚Äî &lt;a href=&quot;https://continuousdelivery.com/&quot;&gt;continuousdelivery.com&lt;/a&gt;&lt;/i&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
As a backend engineer at Grab, nothing matters more than the ability to innovate quickly and safely. Around the end of 2018, Grab‚Äôs transportation and deliveries backend architecture consisted of roughly 270 services (the majority being microservices). The deployment process was lengthy, required careful inputs and clear communication. The care needed to push changes in production and the risk associated with manual operations led to the introduction of a Slack bot to coordinate deployments. The bot ensures that deployments occur only during off-peak and within work hours:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Overview of the Grab Delivery Process&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image4.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Overview of the Grab Delivery Process&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Once the build was completed, engineers who desired to deploy their software to the Staging environment would copy release versions from the build logs, and paste them in a Jenkins job‚Äôs parameter. Tests needed to be manually triggered from another dedicated Jenkins job.&lt;/p&gt;

&lt;p&gt;Prior to production deployments, engineers would generate their release notes via a script and update them manually in a wiki document. Deployments would be scheduled through interactions with a Slack bot that controls release notes and deployment windows. Production deployments were made once again by pasting the correct parameters into two dedicated Jenkins jobs, one for the canary (a.k.a. one-box) deployment and the other for the full deployment, spread one hour apart. During the monitoring phase, engineers would continuously observe metrics reported on our dashboards.&lt;/p&gt;

&lt;p&gt;In spite of the fragmented process and risky manual operations impacting our velocity and stability, around 614 builds were running each business day and changes were deployed on our staging environment at an average rate of 300 new code releases per business day, while production changes averaged a rate of 28 new code releases per business day.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Our Deployment Funnel, Towards the End of 2018&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image10.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Our Deployment Funnel, Towards the End of 2018&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These figures meant that, on average, it took 10 business days between each service update in production, and only 10% of the staging deployments were eventually promoted to production.&lt;/p&gt;

&lt;h2 id=&quot;automating-continuous-deployments-at-grab&quot;&gt;Automating Continuous Deployments at Grab&lt;/h2&gt;

&lt;p&gt;With an increased focus on Engineering efficiency, in 2018 we started an internal initiative to address frictions in deployments that became known as Conveyor. To build Conveyor with a small team of engineers, we had to rely on an already mature platform which exhibited properties that are desirable to us to achieve our mission.&lt;/p&gt;

&lt;h3 id=&quot;hands-off-deployments&quot;&gt;Hands-off deployments&lt;/h3&gt;

&lt;p&gt;Deployments should be an afterthought. Engineers should be as removed from the process as possible, and whenever possible, decisions should be taken early, during the code review process. The machine will do the heavy lifting, and only when it can‚Äôt decide for itself, should the engineer be involved. Notifications can be leveraged to ensure that engineers are only informed when something goes wrong and a human decision is required.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Hands-off Deployment Principle&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image12.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Hands-off Deployment Principle&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;confidence-in-deployments&quot;&gt;Confidence in Deployments&lt;/h3&gt;

&lt;p&gt;Grab‚Äôs focus on gathering internal Engineering NPS feedback helped us collect valuable metrics. One of the metrics we cared about was our engineers‚Äô confidence in their production deployments. A team‚Äôs entire deployment process to production could last for more than a day and may extend up to a week for teams with large infrastructures running critical services. The possibility of losing progress in deployments when individual steps may last for hours is detrimental to the improvement of Engineering efficiency in the organisation. The deployment automation platform is the bedrock of that confidence. If the platform itself fails regularly or does provide a path of upgrade that is transparent to end-users, any features built on top of it would suffer from these downtimes and ultimately erode confidence in deployments.&lt;/p&gt;

&lt;h3 id=&quot;tailored-to-most-but-extensible-for-the-few&quot;&gt;Tailored To Most But Extensible For The Few&lt;/h3&gt;

&lt;p&gt;Our backend engineering teams are working on diverse stacks, and so are their deployment processes. Right from the start, we wanted our product to benefit the largest population of engineers that had adopted the same process, so as to maximize returns on our investments. To ease adoption, we decided to tailor a deployment pipeline such that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It would model the exact sequence of manual processes followed by this population of engineers.&lt;/li&gt;
  &lt;li&gt;Switching to use that pipeline should require as little work as possible by service teams.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, in cases where this model would not fit a team‚Äôs specific process, our deployment platform should be open and extensible and support new customizations even when they are not originally supported by the product‚Äôs ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;cloud-agnosticity&quot;&gt;Cloud-Agnosticity&lt;/h3&gt;

&lt;p&gt;While we were going to target a specific process and team, to ensure that our solution would stand the test of time, we needed to ensure that our solution would support the variety of environments currently used in production. This variety was also likely to increase, and we wanted a platform that would mature together with the rest of our ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-conveyor&quot;&gt;Overview Of Conveyor&lt;/h2&gt;

&lt;h3 id=&quot;setting-sail-with-spinnaker&quot;&gt;Setting Sail With Spinnaker&lt;/h3&gt;

&lt;p&gt;Conveyor is based on &lt;a href=&quot;https://spinnaker.io/&amp;amp;usg=AOvVaw1a93_1MJmR_1SZQ0mlu4Ow&quot;&gt;Spinnaker&lt;/a&gt;, an open-source, multi-cloud continuous delivery platform. We‚Äôve chosen Spinnaker over other platforms because it is a mature deployment platform with no single point of failure, supports complex workflows (referred to as pipelines in Spinnaker), and already supports a large array of cloud providers. Since Spinnaker is open-source and extensible, it allowed us to add the features we needed for the specificity of our ecosystem.&lt;/p&gt;

&lt;p&gt;To further ease adoption within our organization, we built a tailored ¬†user interface and created our own domain-specific language (DSL) to manage its pipelines as code.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Outline of Conveyor's Architecture&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image3.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Outline of Conveyor's Architecture&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;onboarding-to-a-simpler-interface&quot;&gt;Onboarding To A Simpler Interface&lt;/h3&gt;

&lt;p&gt;Spinnaker comes with its own interface, it has all the features an engineer would want from an advanced continuous delivery system. However, Spinnaker interface is vastly different from Jenkins and makes for a steep learning curve.&lt;/p&gt;

&lt;p&gt;To reduce our barrier to adoption, we decided early on to create a simple interface for our users. In this interface, deployment pipelines take the center stage of our application. Pipelines are objects managed by Spinnaker, they model the different steps in the workflow of each deployment. Each pipeline is made up of stages that can be assembled like lego-bricks to form the final pipeline. An instance of a pipeline is called an execution.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Conveyor dashboard. Sensitive information like authors and service names are redacted.&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image2.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Conveyor Dashboard&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;With this interface, each engineer can focus on what matters to them immediately: the pipelines they have started, or those started by other teammates working on the same services as they are. Conveyor also provides a search bar (on the top) and filters (on the left) that work in concert to explore all pipelines executed at Grab.&lt;/p&gt;

&lt;p&gt;We adopted a consistent set of colours to model all information in our interface:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;blue: represent stages that are currently running;&lt;/li&gt;
  &lt;li&gt;red: stages that have failed or important information;&lt;/li&gt;
  &lt;li&gt;yellow: stages that require human interaction;&lt;/li&gt;
  &lt;li&gt;and finally, in green: stages that were successfully completed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Conveyor also provides a task and notifications area, where all stages requiring human intervention are listed in one location. Manual interactions are often no more than just YES or NO questions:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Conveyor tasks. Sensitive information like author/service names is redacted.&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image9.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Conveyor Tasks&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Finally, in addition to supporting automated deployments, we greatly simplified the start of manual deployments. Instead of being required to copy/paste information, each parameter can be selected on the interface from a set of predefined items, sorted chronologically, and presented with contextual information to help engineers in their decision.&lt;/p&gt;

&lt;p&gt;Several parameters are required for our deployments and their values are selected from the UI to ensure correctness.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Simplified manual deployments&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image8.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Simplified Manual Deployments&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ease-of-adoption-with-our-pipeline-as-code-dsl&quot;&gt;Ease Of Adoption With Our Pipeline-As-Code DSL&lt;/h3&gt;

&lt;p&gt;Ease of adoption for the team is not simply about the learning curve of the new tools. We needed to make it easy for teams to configure their services to deploy with Conveyor. Since we focused on automating tasks that were already performed manually, we needed only to configure the layer that would enable the integration.&lt;/p&gt;

&lt;p&gt;We set on creating a pipeline-as-code implementation when none were widely being developed in the Spinnaker community. It‚Äôs interesting to see that two years on, this idea has grown in parallel in the community, with the birth of other &lt;a href=&quot;https://docs.armory.io/docs/spinnaker/using-dinghy/&quot;&gt;pipeline-as-code implementations&lt;/a&gt;. Our pipeline-as-code is referred to as the Pipeline DSL, and its configuration is located inside each team‚Äôs repository. Artificer is the name of our Pipeline DSL interpreter and it runs with every change inside our monorepository:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Artificer: Our Pipeline DSL&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image6.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Artificer: Our Pipeline DSL&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Pipelines are being updated at every commit if necessary.&lt;/p&gt;

&lt;p&gt;Creating a &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; file within the service‚Äôs directory of our monorepository with the few lines below is all that‚Äôs required for Artificer to do its work and get the benefits of automation provided by Conveyor‚Äôs pipeline:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'default.libsonnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;service-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
       &lt;span class=&quot;s2&quot;&gt;&quot;group-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample minimal &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration to onboard services.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In this file, engineers simply specify the name of their service and the group that a user should belong to, to have deployment rights for the service.&lt;/p&gt;

&lt;p&gt;Once the build is completed, teams can log in to Conveyor and start manual deployments of their services with our pipelines. Three pipelines are provided by default: the integration pipeline used for tests and developments, the staging pipeline used for pre-production tests, and the production pipeline for production deployment.&lt;/p&gt;

&lt;p&gt;Thanks to the simplicity of this minimal configuration file, we were able to generate these configuration files for all existing services of our monorepository. This resulted in the automatic onboarding of a large number of teams and was a major contributing factor to the adoption of Conveyor throughout our organisation.&lt;/p&gt;

&lt;h2 id=&quot;our-journey-to-engineering-efficiency-for-backend-services&quot;&gt;Our Journey To Engineering Efficiency (for backend services)&lt;/h2&gt;

&lt;p&gt;The sections below relate some of the improvements in engineering efficiency we‚Äôve delivered since Conveyor‚Äôs inception. They were not made precisely in this order but for readability, they have been mapped to each step of the software development lifecycle.&lt;/p&gt;

&lt;h3 id=&quot;automate-deployments-at-build-time&quot;&gt;Automate Deployments at Build Time&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Continuous Integration Job&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image7.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Continuous Integration Job&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Continuous delivery begins with a pushed code commit in our trunk-based development flow. Whenever a developer pushes changes onto their development branch or onto the trunk, a continuous integration job is triggered on Jenkins. The products of this job (binaries, docker images, etc) are all uploaded into our artefact repositories. We‚Äôve made two additions to our continuous integration process.&lt;/p&gt;

&lt;p&gt;The first modification happens at the step ‚ÄúUpload &amp;amp; Register artefacts‚Äù. At this step, each artefact created is now registered in Conveyor with its associated metadata. When and if an engineer needs to trigger a deployment manually, Conveyor can display the list of versions to choose from, eliminating the need for error-prone manual inputs:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot; Staging&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image5.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Staging&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Each selectable version shows contextual information: title, author, version and link to the code change where it originated. During registration, the commit time is also recorded and used to order entries chronologically in the interface. To ensure this integration is not a single point of failure for deployments, manual input is still available optionally.&lt;/p&gt;

&lt;p&gt;The second modification implements one of the essential feature continuous delivery: your deployments should happen often, automatically. Engineers are now given the possibility to start automatic deployments once continuous integration has successfully completed, by simply modifying their project‚Äôs continuous integration settings:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;AfterBuild&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;AutoDeploy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;OnDiff&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;OnLand&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;TYPE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;conveyor&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// other settings...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample settings needed to trigger auto-deployments. &lt;code class=&quot;highlighter-rouge&quot;&gt;Diff&lt;/code&gt; refers to code review submissions, and &lt;code class=&quot;highlighter-rouge&quot;&gt;Land&lt;/code&gt; refers to merged code changes.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;staging-pipeline&quot;&gt;Staging Pipeline&lt;/h3&gt;

&lt;p&gt;Before deploying a new artefact to a service in production, changes are validated on the staging environment. During the staging deployment, we verify that canary (one-box) deployments and full deployments with automated smoke and functional tests suites.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Staging Pipeline&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image1.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Staging Pipeline&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We start by acquiring a deployment lock for this service and this environment. This prevents another deployment of the same service on the same environment to happen concurrently, other deployments will be waiting in a FIFO queue until the lock is released.&lt;/p&gt;

&lt;p&gt;The stage &lt;em&gt;‚ÄúCompute Changeset‚Äù&lt;/em&gt; ensures that the deployment is not a rollback. It verifies that the new version deployed does not correspond to a rollback by comparing the ancestry of the commits provided during the artefact registration at build time: since we automate deployments after the build process has completed, cases of rollback may occur when two changes are created in quick succession and the latest build completes earlier than the older one.&lt;/p&gt;

&lt;p&gt;After the stage &lt;em&gt;‚ÄúDeploy Canary‚Äù&lt;/em&gt; has completed, smoke test run. There are three kinds of tests executed at different stages of the pipeline: smoke, functional and security tests. Smoke tests directly reach the canary instance‚Äôs endpoint, by-passing load-balancers. If the smoke tests fail, the canary is immediately rolled back and this deployment is terminated.&lt;/p&gt;

&lt;p&gt;All tests are generated from the same builds as the artefact being tested and their versions must match during testing. To ensure that the right version of the test run and distinguish between the different kind of tests to perform, we provide additional metadata that will be passed by Conveyor to the tests system, known internally as Gandalf:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'default.libsonnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;service-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;group-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_smoke_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/smoke/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_functional_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/functional/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_security_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/security/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration with integration tests added.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, in parallel to the execution of the smoke tests, the canary is also being monitored from the moment its deployment has completed and for a predetermined duration. We leverage our integration with Datadog to allow engineers to select the alerts to monitor. If an alert is triggered during the monitoring period, and while the tests are executed, the canary is again rolled back, and the pipeline is terminated. Engineers can specify the alerts by adding them to the &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration file together with the monitoring duration:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'default.libsonnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;service-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;group-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_smoke_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/smoke/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_functional_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/functional/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_security_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/security/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;stg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;duration_seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;alarms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;datadog&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;alert_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12345678&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;datadog&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;alert_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23456789&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration with alerts in staging added.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;When the smoke tests and monitor pass and the deployment of new artefacts is completed, the pipeline execution triggers functional and security tests. Unlike smoke tests, functional &amp;amp; security tests run only after that step, as they communicate with the cluster through load-balancers, impersonating other services.&lt;/p&gt;

&lt;p&gt;Before releasing the lock, release notes are generated to inform engineers of the delta of changes between the version they just released and the one currently running in production. Once the lock is released, the stage &lt;em&gt;‚ÄúCheck Policies‚Äù&lt;/em&gt; verifies that the parameters and variable of the deployment obeys a specific set of criteria, for example: if its service metadata is up-to-date in our service inventory, or if the base image used during deployment is sufficiently recent.&lt;/p&gt;

&lt;p&gt;Here‚Äôs how the policy stage, the engine, and the providers interact with each other:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Check Policy Stage&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image11.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Check Policy Stage&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Spinnaker, each event of a pipeline‚Äôs execution updates the pipeline‚Äôs state in the database. The current state of the pipeline can be fetched by its API as a single JSON document, describing all information related to its execution: including its parameters, the contextual information related to each stage or even the response from the various interfacing components. The role of our &lt;em&gt;‚ÄúPolicy Check‚Äù&lt;/em&gt; stage is to query this JSON representation of the pipeline, to extract and transform the variables which are forwarded to our policy engine for validation. Our policy engine gathers judgements passed by different policy providers. If the validation by the policy engine fails, the deployment is not rolled back this time; however, promotion to production is not possible and the pipeline is immediately terminated.&lt;/p&gt;

&lt;p&gt;The journey through staging deployment finally ends with the stage &lt;em&gt;‚ÄúRegister Deployment‚Äù&lt;/em&gt;. This stage registers that a successful deployment was made in our staging environment as an artefact. Similarly to the policy check above, certain parameters of the deployment are picked up and consolidated into this document. We use this kind of artefact as proof for upcoming production deployment.&lt;/p&gt;

&lt;h3 id=&quot;continuing-our-journey-to-engineering-efficiency&quot;&gt;Continuing Our Journey to Engineering Efficiency&lt;/h3&gt;

&lt;p&gt;With the advancements made in continuous integration and deployment to staging, Conveyor has reduced the efforts needed by our engineers to just three clicks in its interface, when automated deployment is used. Even when the deployment is triggered manually, Conveyor gives the assurance that the parameters selected are valid and it does away with copy/pasting and human interactions across heterogeneous tools.&lt;/p&gt;

&lt;p&gt;In the sequel to this blog post, we‚Äôll dive into the improvements that we‚Äôve made to our production deployments and introduce a crucial concept that led to the creation of our proof for successful staging deployment. Finally, we‚Äôll cover the impact that Conveyor had on the continuous delivery of our backend services, by comparing our deployment velocity when we started two years ago versus where we are today.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;All these improvements in efficiency for our engineers would never have been possible without the hard work of all team members involved in the project, past and present: Evan Sebastian, Tanun Chalermsinsuwan, Aufar Gilbran, Deepak Ramakrishnaiah, Repon Kumar Roy (Kowshik), Su Han, Voislav Dimitrijevikj, Qijia Wang, Oscar Ng, Jacob Sunny, Subhodip Mandal, and many others who have contributed and collaborated with them.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Wed, 23 Sep 2020 10:23:44 +0000</pubDate>
        <link>https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab</guid>
        
        <category>Deployment</category>
        
        <category>CI</category>
        
        <category>Continuous Integration</category>
        
        <category>Continuous Deployment</category>
        
        <category>Deployment Process</category>
        
        <category>Cloud Agnostic</category>
        
        <category>Spinnaker</category>
        
        <category>Continuous Delivery</category>
        
        <category>Multi Cloud</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
