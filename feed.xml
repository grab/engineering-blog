<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 11 Oct 2024 05:24:32 +0000</pubDate>
    <lastBuildDate>Fri, 11 Oct 2024 05:24:32 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Leveraging RAG-powered LLMs for Analytical Tasks</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Retrieval-Augmented Generation (RAG) is a powerful process that is designed to integrate direct function calling to answer queries more efficiently by retrieving relevant information from a broad database. In the rapidly evolving business landscape, Data Analysts (DAs) are struggling with the growing number of data queries from stakeholders. The conventional method of manually writing and running similar queries repeatedly is time-consuming and inefficient. This is where RAG-powered Large Language Models (LLMs) step in, offering a transformative solution to streamline the analytics process and empower DAs to focus on higher value tasks.&lt;/p&gt;

&lt;p&gt;In this article, we will share how the Integrity Analytics team has built out a data solution using LLMs to help automate tedious analytical tasks like generating regular metric reports and performing fraud investigations.&lt;/p&gt;

&lt;p&gt;While LLMs are known for their proficiency in data interpretation and insight generation, they represent just a fragment of the entire solution. For a comprehensive solution, LLMs must be integrated with other essential tools. The following is required in assembling a solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Internally facing LLM tool -&lt;/strong&gt; Spellvault is a platform within Grab that stores, shares, and refines LLM prompts. It features low/no-code RAG capabilities that lower the barrier of entry for people to create LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data -&lt;/strong&gt; with real time or close to real-time latency to ensure accuracy. It has to be in a standardised format to ensure that all LLM data inputs are accurate.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduler -&lt;/strong&gt;  runs LLM applications at regular intervals. Useful for automating routine tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Messaging Tool -&lt;/strong&gt; a user interface where users can interact with LLM by entering a command to receive reports and insights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introducing-data-arks-the-data-middleware-serving-up-relevant-data-to-the-llm-agents&quot;&gt;Introducing Data-Arks, the data middleware serving up relevant data to the LLM agents&lt;/h2&gt;

&lt;p&gt;For most data use cases, DAs are usually running the same set of SQL queries with minor changes to parameters like dates, age or other filter conditions. In most instances, we already have a clear understanding of the required data and format to accomplish a task. Therefore, we need a tool that can execute the &lt;strong&gt;exact SQL query&lt;/strong&gt; and channel the data output to the LLM.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Data-Arks hosts various APIs which can be called to serve data to applications like SpellVault.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-data-arks&quot;&gt;What is Data-Arks?&lt;/h3&gt;

&lt;p&gt;Data-Arks is an in-house Python-based API platform housing several frequently used SQL queries and python functions packaged into individual APIs. Data-Arks is also integrated with Slack, Wiki, and JIRA APIs, allowing users to parse and fetch information and data from these tools as well. The benefits of Data-Arks are summarised as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration:&lt;/strong&gt; Data-Arks service allows users to upload any SQL query or Python script on the platform. These queries are then surfaced as APIs, which can be called to serve data to the LLM agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Versatility: Data-Arks&lt;/strong&gt; can be extended to everyone. Employees from various teams and functions at Grab can self-serve to upload any SQL query that they want onto the platform, allowing this tool to be used for different teams’ use cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;automating-regular-report-generation-and-summarisation-using-data-arks-and-spellvault&quot;&gt;Automating regular report generation and summarisation using Data-Arks and Spellvault&lt;/h2&gt;

&lt;p&gt;LLMs are just one piece of the puzzle, to build a comprehensive solution, they must be integrated with other tools. Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;p&gt;Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Report Summarizer uses various tools to summarise queries and deliver a summarised report through Slack.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 is an example of a summarised report generated by the Report Summarizer using dummy data.  Report Summarizer calls a Data-Arks API to generate the data in a tabular format and LLM helps summarise and generate a short paragraph of key insights. This automated report generation has helped save an estimated 3-4 hours per report.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Sample of a report generated using dummy data extracted from [https://data.gov.my/](https://data.gov.my/). &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;llm-bots-for-fraud-investigations&quot;&gt;LLM bots for fraud investigations&lt;/h2&gt;

&lt;p&gt;LLMs also excel in helping to streamline fraud investigations, as LLMs are able to contextualise several different data points and information and derive useful insights from them.&lt;/p&gt;

&lt;p&gt;Introducing &lt;strong&gt;A* bot&lt;/strong&gt;, the team’s very own LLM fraud investigation helper.&lt;/p&gt;

&lt;p&gt;A set of frequently used queries for fraud investigation is made available as Data-Arks APIs. Upon a user prompt or query, SpellVault selects the most relevant queries using RAG, executes them and provides a summary of the results to users through Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. A* bot uses Data-Arks and Spellvault to get information for fraud investigations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 shows a sample of fraud investigation responses from A* bot. Scaling to multiple queries for a fraud investigation process, what was once a time-consuming fraud investigation can now be reduced to a matter of minutes, as the A* bot is capable of providing all the necessary information simultaneously.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Sample of fraud investigation responses.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;rag-vs-fine-tuning&quot;&gt;RAG vs fine-tuning&lt;/h2&gt;

&lt;p&gt;On deciding between RAG or fine-tuning to improve LLM accuracy, three key factors tipped the scales in favour of the RAG approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Effort and cost considerations&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires significant computational cost as it involves taking a base model and further training it with smaller, domain specific data and context. RAG is computationally less expensive as it relies on retrieving only relevant data and context to augment a model’s response. As the same base model can be used for different use cases, RAG is the preferred choice due to its flexibility and cost efficiency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ability to respond with the latest information&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires model re-training with each new information update, whereas RAG simply retrieves required context and data from a knowledge base to enhance its response. Thus, by using RAG, LLM is able to answer questions using the most current information from our production database, eliminating the need for model re-training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Speed and scalability&lt;/strong&gt;&lt;br /&gt;
Without the burden of model re-training, the team can rapidly scale and build out new LLM applications with a well managed knowledge base.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The potential of using RAG-powered LLM can be limitless as the ability of GPT is correlated with the tools it equips. Hence, the process does not stop here and we will try to onboard more tools or integration to GPT. In the near future, we plan to utilise Data-Arks to provide images to GPT as GPT-4o is a multimodal model that has vision capabilities. We are committed to pushing the boundaries of what’s possible with RAG-powered LLM, and we look forward to unveiling the exciting advancements that lie ahead.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-what-next.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. What’s next?&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to express our sincere gratitude to the following individuals and teams whose invaluable support and contributions have made this project a reality: &lt;br /&gt;- Meichen Lu, a senior data scientist at Grab, for her guidance and assistance in building the MVP and testing the concept.&lt;br /&gt;- The data engineering team, particularly Jia Long Loh and Pu Li, for setting up the necessary services and infrastructure. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Wed, 09 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</link>
        <guid isPermaLink="true">https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Analytics</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Evolution of Catwalk: Model serving platform at Grab</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As Southeast Asia’s leading super app, Grab serves millions of users across multiple countries every day. Our services range from ride-hailing and food delivery to digital payments and much more. The backbone of our operations? Machine Learning (ML) models. They power our real-time decision-making capabilities, enabling us to provide a seamless and personalised experience to our users. Whether it’s determining the most efficient route for a ride, suggesting a food outlet based on a user’s preference, or detecting fraudulent transactions, ML models are at the forefront.&lt;/p&gt;

&lt;p&gt;However, serving these ML models at Grab’s scale is no small feat. It requires a robust, efficient, and scalable model serving platform, which is where our ML model serving platform, Catwalk, comes in.&lt;/p&gt;

&lt;p&gt;Catwalk has evolved over time, adapting to the growing needs of our business and the ever-changing tech landscape. It has been a journey of continuous learning and improvement, with each step bringing new challenges and opportunities.&lt;/p&gt;

&lt;h1 id=&quot;evolution-of-the-platform&quot;&gt;Evolution of the platform&lt;/h1&gt;

&lt;h2 id=&quot;phase-0-the-need-for-a-model-serving-platform&quot;&gt;Phase 0: The need for a model serving platform&lt;/h2&gt;

&lt;p&gt;Before Catwalk’s debut as our dedicated model serving platform, data scientists across the company employed various ad-hoc approaches to serve ML models. These included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shipping models online using custom solutions.&lt;/li&gt;
  &lt;li&gt;Relying on backend engineering teams to deploy and manage trained ML models.&lt;/li&gt;
  &lt;li&gt;Embedding ML logic within Go backend services.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These methods, however, led to several challenges, undercovering the need for a unified, company-wide platform for serving machine learning models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Operational overhead&lt;/strong&gt;: Data scientists often lacked the necessary expertise to handle the operational aspects of their models, leading to service outages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource wastage&lt;/strong&gt;: There was frequently low resource utilisation (e.g., 1%) for data science services, leading to inefficient use of resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Friction with engineering teams&lt;/strong&gt;: Differences in release cycles and unclear ownership when code was embedded into backend systems resulted in tension between data scientists and engineers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinventing the wheel&lt;/strong&gt;: Multiple teams independently attempted to solve model serving problems, leading to a duplication of effort.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​​These challenges highlighted the need for a company-wide, centralised platform for serving machine learning models.&lt;/p&gt;

&lt;h2 id=&quot;phase-1-no-code-managed-platform-for-tensorflow-serving-models&quot;&gt;Phase 1: No-code, managed platform for TensorFlow Serving models&lt;/h2&gt;

&lt;p&gt;Our initial foray into model serving was centred around creating a managed platform for deploying TensorFlow Serving models. The process involved data scientists submitting their models to the platform’s engineering admin, who could then deploy the model with an endpoint. Infrastructure and networking were managed using Amazon Elastic Kubernetes Service (EKS) and Helm Charts as illustrated below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This phase of our platform, which we also detailed in our &lt;a href=&quot;https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale&quot;&gt;previous article&lt;/a&gt;, was beneficial for some users. However, we quickly encountered scalability challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Codebase maintenance&lt;/strong&gt;: Applying changes to every TensorFlow Serving (TFS) version was cumbersome and difficult to maintain.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited scalability&lt;/strong&gt;: The fully managed nature of the platform made it difficult to scale.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Admin bottleneck&lt;/strong&gt;: The engineering admin’s limited bandwidth became a bottleneck for onboarding new models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited serving types&lt;/strong&gt;: The platform only supported TensorFlow, limiting its usefulness for data scientists using other frameworks like LightGBM, XGBoost, or PyTorch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After a year of operation, only eight models were onboarded to the platform, highlighting the need for a more scalable and flexible solution.&lt;/p&gt;

&lt;h2 id=&quot;phase-2-from-models-to-model-serving-applications&quot;&gt;Phase 2: From models to model serving applications&lt;/h2&gt;

&lt;p&gt;To address the limitations of Phase 1, we transitioned from deploying individual models to self-contained model serving applications. This “low-code, self-serving” strategy introduced several new components and changes as illustrated in the points and diagram below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Support for multiple serving types&lt;/strong&gt;: Users gained the ability to deploy models trained with a variety of frameworks like Open Neural Network Exchange (ONNX), PyTorch, and TensorFlow.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Self-served platform through CI/CD pipelines&lt;/strong&gt;: Data scientists could self-serve and independently manage their model serving applications through CI/CD pipelines.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;New components&lt;/strong&gt;: We introduced these new components to support the self-serving approach:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Catwalk proxy&lt;/strong&gt;, a managed reverse proxy to various serving types.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Catwalk transformer&lt;/strong&gt;, a low-code component to transform input and output data.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amphawa&lt;/strong&gt;, a feature fetching component to augment model inputs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;api-request-flow&quot;&gt;API request flow&lt;/h4&gt;

&lt;p&gt;The Catwalk proxy acts as the orchestration layer. Clients send requests to Catwalk proxy then it orchestrates calls to different components like transformers, feature-store, and so on. A typical end to end request flow is illustrated below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase2-api-request-flow.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Within a year of implementing these changes, the number of models on the platform increased from 8 to 300, demonstrating the success of this approach. However, new challenges emerged:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Complexity of maintaining Helm chart&lt;/strong&gt;: As the platform continued to grow with new components and functionalities, maintaining the Helm chart became increasingly complex. The readability and flow control became more challenging, making the helm chart updating process prone to errors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Process-level mistakes&lt;/strong&gt;: The self-serving approach led to errors such as pushing empty or incompatible models to production, setting too few replicas, or allocating insufficient resources, which resulted in service crashes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We knew that our work was nowhere near done. We had to keep iterating and explore ways to address the new challenges.&lt;/p&gt;

&lt;h2 id=&quot;phase-3-replacing-helm-charts-with-kubernetes-crds&quot;&gt;Phase 3: Replacing Helm charts with Kubernetes CRDs&lt;/h2&gt;

&lt;p&gt;To tackle the deployment challenges and gain more control, we made the significant decision to replace Helm charts with Kubernetes Custom Resource Definitions (CRDs). This required substantial engineering effort, but the outcomes have been rewarding. This transition gave us improved control over deployment pipelines, enabling customisations such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Smart defaults for AutoML&lt;/li&gt;
  &lt;li&gt;Blue-green deployments&lt;/li&gt;
  &lt;li&gt;Capacity management&lt;/li&gt;
  &lt;li&gt;Advanced scaling&lt;/li&gt;
  &lt;li&gt;Application set groupings&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is an example of a simple model serving CRD manifest:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: ml.catwalk.kubebuilder.io/v1
kind: ModelServing
spec:
  hpa:
    desired: 1
    max: 1
    min: 1
  modelMeta:
    modelName: &quot;my-model&quot;
    modelOwner: john.doe
  proxyLayer:
    enableLogging: true
    logHTTPBody: true
  servingLayer:
    servingType: &quot;tensorflow-serving&quot;
    version: &quot;20&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;model-serving-crd-deployment-state-machine&quot;&gt;Model serving CRD deployment state machine&lt;/h4&gt;

&lt;p&gt;Every model serving CRD submission follows a sequence of steps. If there are failures at any step, the controller keeps retrying after small intervals. The major steps on the deployment cycle are described below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Validate whether the new CRD specs are acceptable. Along with sanity checks, we also enforce a lot of platform constraints through this step.&lt;/li&gt;
  &lt;li&gt;Clean up previous non-ready deployment resources. Sometimes a deployment submission might keep crashing and hence it doesn’t proceed to a ready state. On every submission, it’s important to check and clean up such previous deployments.&lt;/li&gt;
  &lt;li&gt;Create resources for the new deployment and ensure that the new deployment is ready.&lt;/li&gt;
  &lt;li&gt;Switch traffic from old deployment to the new deployment.&lt;/li&gt;
  &lt;li&gt;Clean up resources for old deployment. At this point, traffic is already being served by the new deployment resources. So, we can clean up the old deployment.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;phase-4-transition-to-a-high-code-self-served-process-managed-platform&quot;&gt;Phase 4: Transition to a high-code, self-served, process-managed platform&lt;/h2&gt;

&lt;p&gt;As the number of model serving applications and use cases multiplied, clients sought greater control over orchestrations between different models, experiment executions, traffic shadowing, and responses archiving. To cater to these needs, we introduced several changes and components with the Catwalk Orchestrator, a high code orchestration solution, leading the pack.&lt;/p&gt;

&lt;h4 id=&quot;catwalk-orchestrator&quot;&gt;Catwalk orchestrator&lt;/h4&gt;

&lt;p&gt;The &lt;strong&gt;Catwalk Orchestrator&lt;/strong&gt; is a highly abstracted framework for building ML applications that replaced the catwalk-proxy from previous phases. The key difference is that users can now write their own business/orchestration logic. The orchestrator offers a range of utilities, reducing the need for users to write extensive boilerplate code. Key components of the Catwalk Orchestrator include HTTP server, gRPC server, clients for different model serving flavours (TensorFlow, ONNX, PyTorch, etc), client for fetching features from the feature bank, and utilities for logging, metrics, and data lake ingestion.&lt;/p&gt;

&lt;p&gt;The Catwalk Orchestrator is designed to streamline the deployment of machine learning models. Here’s a typical user journey:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Scaffold a model serving application&lt;/strong&gt;: Users begin by scaffolding a model serving application using a command-line tool.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Write business logic&lt;/strong&gt;: Users then write the business logic for the application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy to staging&lt;/strong&gt;: The application is then deployed to a staging environment for testing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Complete load testing&lt;/strong&gt;: Users test the application in the staging environment and complete load testing to ensure it can handle the expected traffic.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy to production&lt;/strong&gt;: Once testing is completed, the application is deployed to the production environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;bundled-deployments&quot;&gt;Bundled deployments&lt;/h4&gt;

&lt;p&gt;To support multiple ML models as part of a single model serving application, we introduced the concept of &lt;strong&gt;bundled deployments&lt;/strong&gt;. Multiple Kubernetes deployments are bundled together as a single model serving application deployment, allowing each component (e.g., models, catwalk-orchestrator, etc) to have its own Kubernetes deployment and to scale independently.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to the major developments, we implemented other changes to enhance our platform’s efficiency. We made &lt;strong&gt;load testing&lt;/strong&gt; mandatory for all ML application updates to ensure robust performance. This testing process was streamlined with a single command that runs the load test in the staging environment, with the results directly shared with the user.&lt;/p&gt;

&lt;p&gt;Furthermore, we boosted &lt;strong&gt;deployment transparency&lt;/strong&gt; by sharing deployment details through Slack and Datadog. This empowered users to diagnose issues independently, reducing the dependency on on-call support. This transparency not only improved our issue resolution times but also enhanced user confidence in our platform.&lt;/p&gt;

&lt;p&gt;The results of these changes speak for themselves. The Catwalk Orchestrator has evolved into our flagship product. In just two years, we have deployed 200 Catwalk Orchestrators serving approximately 1,400 ML models.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h1&gt;

&lt;p&gt;As we continue to innovate and enhance our model serving platform, we are venturing into new territories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Catwalk serverless&lt;/strong&gt;: We aim to further abstract the model serving experience, making it even more user-friendly and efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Catwalk data serving&lt;/strong&gt;: We are looking to extend Catwalk’s capabilities to serve data online, providing a more comprehensive service.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LLM serving&lt;/strong&gt;: In line with the trend towards generative AI and large language models (LLMs), we’re pivoting Catwalk to support these developments, ensuring we stay at the forefront of the AI and machine learning field.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stay tuned as we continue to advance our technology and bring these exciting developments to life.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Oct 2024 00:00:50 +0000</pubDate>
        <link>https://engineering.grab.com/catwalk-evolution</link>
        <guid isPermaLink="true">https://engineering.grab.com/catwalk-evolution</guid>
        
        <category>Machine Learning</category>
        
        <category>Models</category>
        
        <category>Data Science</category>
        
        <category>TensorFlow</category>
        
        <category>Kubernetes</category>
        
        <category>Docker</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Enabling conversational data discovery with LLMs at Grab</title>
        <description>&lt;p&gt;Imagine a world where finding the right data is like searching for a needle in a haystack. In today’s data-driven landscape, companies are drowning in a sea of information, struggling to navigate through countless datasets to uncover valuable insights. At Grab, we faced a similar challenge. With over 200,000 tables in our data lake, along with numerous Kafka streams, production databases, and ML features, locating the most suitable dataset for our Grabber’s use cases promptly has historically been a significant hurdle.&lt;/p&gt;

&lt;h2 id=&quot;problem-space&quot;&gt;Problem Space&lt;/h2&gt;

&lt;p&gt;Our internal data discovery tool, Hubble, built on top of the popular open-source platform Datahub, was primarily used as a reference tool. While it excelled at providing metadata for known datasets, it struggled with true data discovery due to its reliance on Elasticsearch, which performs well for keyword searches but cannot accept and use user-provided context (i.e., it can’t perform semantic search, at least in its vanilla form). The Elasticsearch parameters provided by Datahub out of the box also had limitations: our monthly average click-through rate was only 82%, meaning that in 18% of sessions, users abandoned their searches without clicking on any dataset. This suggested that the search results were not meeting their needs.&lt;/p&gt;

&lt;p&gt;Another indispensable requirement for efficient data discovery that was missing at Grab was documentation. Documentation coverage for our data lake tables was low, with only 20% of the most frequently queried tables (colloquially referred to as P80 tables) having existing documentation. This made it difficult for users to understand the purpose and contents of different tables, even when browsing through them on the Hubble UI.&lt;/p&gt;

&lt;p&gt;Consequently, data consumers heavily relied on tribal knowledge, often turning to their colleagues via Slack to find the datasets they needed. A survey conducted last year revealed that 51% of data consumers at Grab took multiple days to find the dataset they required, highlighting the inefficiencies in our data discovery process.&lt;/p&gt;

&lt;p&gt;To address these challenges and align with Grab’s ongoing journey towards a data mesh architecture, the Hubble team recognised the importance of improving data discovery. We embarked on a journey to revolutionise the way our employees find and access the data they need, leveraging the power of AI and Large Language Models (LLMs).&lt;/p&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;

&lt;p&gt;Given the historical context, our vision was clear: to remove humans in the data discovery loop by automating the entire process using LLM-powered products. We aimed to reduce the time taken for data discovery from multiple days to mere seconds, eliminating the need for anyone to ask their colleagues data discovery questions ever again.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;goals&quot;&gt;Goals&lt;/h2&gt;

&lt;p&gt;To achieve our vision, we set the following goals for ourselves for the first half of 2024:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Build HubbleIQ:&lt;/strong&gt; An LLM-based chatbot that could serve as the equivalent of a Lead Data Analyst for data discovery. Just as a lead is an expert in their domain and can guide data consumers to the right dataset, we wanted HubbleIQ to do the same across all domains at Grab. We also wanted HubbleIQ to be accessible where data consumers hang out the most: Slack.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improve documentation coverage:&lt;/strong&gt; A new Lead Analyst joining the team would require extensive documentation coverage of very high quality. Without this, they wouldn’t know what data exists and where. Thus, it was important for us to improve documentation coverage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhance Elasticsearch:&lt;/strong&gt; We aimed to tune our Elasticsearch implementation to better meet the requirements of Grab’s data consumers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-systematic-path-to-success&quot;&gt;A Systematic Path to Success&lt;/h2&gt;

&lt;h3 id=&quot;step-1-enhance-elasticsearch&quot;&gt;Step 1: Enhance Elasticsearch&lt;/h3&gt;

&lt;p&gt;Through clickstream analysis and user interviews, the Hubble team identified four categories of data search queries that were seen either on the Hubble UI or in Slack channels:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Exact search:&lt;/strong&gt; Queries belonging to this category were a substring of an existing dataset’s name at Grab, with the query length being at least 40% of the dataset’s name.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Partial search:&lt;/strong&gt; The Levenshtein distance between a query in this category and any existing dataset’s name was greater than 80. This category usually comprised queries that closely resembled an existing dataset name but likely contained spelling mistakes or were shorter than the actual name.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Exact and partial searches accounted for 75% of searches on Hubble (and were non-existent on Slack: as a human, receiving a message that just had the name of a dataset would feel rather odd). Given the effectiveness of vanilla Elasticsearch for these categories, the click rank was close to 0.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inexact search:&lt;/strong&gt; This category comprised queries that were usually colloquial keywords or phrases that may be semantically related to a given table, column, or piece of documentation (e.g., “city” or “taxi type”). Inexact searches accounted for the remaining 25% of searches on Hubble. Vanilla Elasticsearch did not perform well in this category since it relied on pure keyword matching and did not consider any additional context.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Semantic search:&lt;/strong&gt; These were free text queries with abundant contextual information supplied by the user. Hubble did not see any such queries as users rightly expected that Hubble would not be able to fulfil their search needs. Instead, these queries were sent by data consumers to data producers via Slack. Such queries were numerous, but usually resulted in data hunting journeys that spanned multiple days - the root of frustration amongst data consumers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two search types can be seen as “reference” queries, where the data consumer already knows what they are looking for. Inexact and contextual searches are considered “discovery” queries. The Hubble team noticed drop-offs in inexact searches because users learned that Hubble could not fulfil their discovery needs, forcing them to search for alternatives.&lt;/p&gt;

&lt;p&gt;Through user interviews, the team discovered how Elasticsearch should be tuned to better fit the Grab context. They implemented the following optimisations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tagging and boosting P80 tables&lt;/li&gt;
  &lt;li&gt;Boosting the most relevant schemas&lt;/li&gt;
  &lt;li&gt;Hiding irrelevant datasets like PowerBI dataset tables&lt;/li&gt;
  &lt;li&gt;Deboosting deprecated tables&lt;/li&gt;
  &lt;li&gt;Improving the search UI by simplifying and reducing clutter&lt;/li&gt;
  &lt;li&gt;Adding relevant tags&lt;/li&gt;
  &lt;li&gt;Boosting certified tables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result of these enhancements, the click-through rate rose steadily over the course of the half to 94%, a 12 percentage point increase.&lt;/p&gt;

&lt;p&gt;While this helped us make significant improvements to the first three search categories, we knew we had to build HubbleIQ to truly automate the last category - semantic search.&lt;/p&gt;

&lt;h3 id=&quot;step-2-build-a-context-store-for-hubbleiq&quot;&gt;Step 2: Build a Context Store for HubbleIQ&lt;/h3&gt;

&lt;p&gt;To support HubbleIQ, we built a documentation generation engine that used GPT-4 to generate documentation based on table schemas and sample data. We refined the prompt through multiple iterations of feedback from data producers.&lt;/p&gt;

&lt;p&gt;We added a “generate” button on the Hubble UI, allowing data producers to easily generate documentation for their tables. This feature also supported the ongoing Grab-wide initiative to certify tables.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image7.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In conjunction, we took the initiative to pre-populate docs for the most critical tables, while notifying data producers to review the generated documentation. Such docs were visible to data consumers with an “AI-generated” tag as a precaution. When data producers accepted or edited the documentation, the tag was removed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image3.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As a result, documentation coverage for P80 tables increased by 70 percentage points to ~90%. User feedback showed that ~95% of users found the generated docs useful.&lt;/p&gt;

&lt;h3 id=&quot;step-3-build-and-launch-hubbleiq&quot;&gt;Step 3: Build and Launch HubbleIQ&lt;/h3&gt;

&lt;p&gt;With high documentation coverage in place, we were ready to harness the power of LLMs for data discovery. To speed up go-to-market, we decided to leverage &lt;a href=&quot;https://www.glean.com/&quot;&gt;Glean&lt;/a&gt;, an enterprise search tool used by Grab.&lt;/p&gt;

&lt;p&gt;First, we integrated Hubble with Glean, making all data lake tables with documentation available on the Glean platform. Next, we used &lt;a href=&quot;https://www.glean.com/product/apps&quot;&gt;Glean Apps&lt;/a&gt; to create the HubbleIQ bot, which was essentially an LLM with a custom system prompt that could access all Hubble datasets that were catalogued on Glean. Finally, we integrated this bot into Hubble search, such that for any search that is likely to be a semantic search, HubbleIQ results are shown on top, followed by regular search results.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image5.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Recently, we integrated HubbleIQ with Slack, allowing data consumers to discover datasets without breaking their flow. Currently, we are working with analytics teams to add the bot to their “ask” channels (where data consumers come to ask contextual search queries for their domains). After integration, HubbleIQ will act as the first line of defence for answering questions in these channels, reducing the need for human intervention.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The impact of these improvements was significant. A follow-up survey revealed that 73% of respondents found it easy to discover datasets, marking a substantial 17 percentage point increase from the previous survey. Moreover, Hubble reached an all-time high in monthly active users, demonstrating the effectiveness of the enhancements made to the platform.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We’ve made significant progress towards our vision, but there’s still work to be done. Looking ahead, we have several exciting initiatives planned to further enhance data discovery at Grab.&lt;/p&gt;

&lt;p&gt;On the documentation generation front, we aim to enrich the generator with more context, enabling it to produce even more accurate and relevant documentation. We also plan to streamline the process by allowing analysts to auto-update data docs based on Slack threads directly from Slack. To ensure the highest quality of documentation, we will develop an evaluator model that leverages LLMs to assess the quality of both human and AI-written docs. Additionally, we will implement Reflexion, an agentic workflow that utilises the outputs from the doc evaluator to iteratively regenerate docs until a quality benchmark is met or a maximum try-limit is reached.&lt;/p&gt;

&lt;p&gt;As for HubbleIQ, our focus will be on continuous improvement. We’ve already added support for metric datasets and are actively working on incorporating other types of datasets as well. To provide a more seamless user experience, we will enable users to ask follow-up questions to HubbleIQ directly on the HubbleUI, with the system intelligently pulling additional metadata when a user mentions a specific dataset.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;By harnessing the power of AI and LLMs, the Hubble team has made significant strides in improving documentation coverage, enhancing search capabilities, and drastically reducing the time taken for data discovery. While our efforts so far have been successful, there are still steps to be taken before we fully achieve our vision of completely replacing the reliance on data producers for data discovery. Nonetheless, with our upcoming initiatives and the groundwork we have laid, we are confident that we will continue to make substantial progress in the right direction over the next few production cycles.&lt;/p&gt;

&lt;p&gt;As we forge ahead, we remain dedicated to refining and expanding our AI-powered data discovery tools, ensuring that Grabbers have every dataset they need to drive Grab’s success at their fingertips. The future of data discovery at Grab is brimming with possibilities, and the Hubble team is thrilled to be at the forefront of this exciting journey.&lt;/p&gt;

&lt;p&gt;To our readers, we hope that our journey has inspired you to explore how you can leverage the power of AI to transform data discovery within your own organisations. The challenges you face may be unique, but the principles and strategies we have shared can serve as a foundation for your own data discovery revolution. By embracing innovation, focusing on user needs, and harnessing the potential of cutting-edge technologies, you too can unlock the full potential of your data and propel your organisation to new heights. The future of data-driven innovation is here, and we invite you to join us on this exhilarating journey.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Sep 2024 00:00:40 +0000</pubDate>
        <link>https://engineering.grab.com/hubble-data-discovery</link>
        <guid isPermaLink="true">https://engineering.grab.com/hubble-data-discovery</guid>
        
        <category>Data Discovery</category>
        
        <category>AI</category>
        
        <category>LLM</category>
        
        <category>Documentation</category>
        
        <category>Elasticsearch</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Bringing Grab’s Live Activity to Android: Enhancing user experience through custom notifications</title>
        <description>&lt;p&gt;In May 2023, Grab unveiled the Live Activity feature for iOS, which received positive feedback from users. Live Activity is a feature that enhances user experience by displaying a user interface (UI) outside of the app, delivering real-time updates and interactive content. At Grab, we leverage this feature to keep users informed about their order updates without requiring them to manually open the app.&lt;/p&gt;

&lt;p&gt;While Live Activity is a native iOS feature provided by Apple, there is currently no official Android equivalent. However, we are determined to bring this immersive experience to Android users. Inspired by the success of Live Activity on iOS, we have embarked on design explorations and feasibility studies to ensure the seamless integration of Live Activity into the Android platform. Our ultimate goal is to provide Android users with the same level of convenience and real-time updates, elevating their Grab experience.&lt;/p&gt;

&lt;h2 id=&quot;product-exploration&quot;&gt;Product Exploration&lt;/h2&gt;

&lt;p&gt;In July 2023, we took a proactive step by forming a dedicated working group with the specific goal of exploring Live Activity on the Android platform. Our mindset was focused on quickly enabling the MVP (Minimum Viable Product) of this feature for Android users. We focused on enabling Grab users to track food and mart orders on Live Activity as our first use-case. We also designed the Live Activity module as an extendable platform, allowing easy adoption by other Grab internal verticals such as the Express and Transport teams.&lt;/p&gt;

&lt;p&gt;The team kicked off by analysing the current solution and end-to-end flow of Live Activity on iOS. The objective was to uncover opportunities on how we could leverage the existing platform approach.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure1.png&quot; alt=&quot;&quot; style=&quot;width:30%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Grab iOS Live Activity flow.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The first thing that caught our attention was that there is no Live Activity Token (also known as Push Token) concept on Android. Push Token is a token generated from the ActivityKit framework and used to remotely start, update, and end Live Activity notifications on iOS.&lt;/p&gt;

&lt;p&gt;Our goal was to match the Live Activity set-up of iOS in Android, which was a challenge due to the missing Push Token. This required us to think outside the box and develop an innovative workaround. After multiple brainstorming sessions, the team developed two potential solutions, Solution 1 and Solution 2, as illustrated below:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Proposed solutions for Live Activity for Android.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We evaluated the two solutions. The first solution is to substitute the Push Token with a placeholder value, serving as a distinctive notification identifier. Whereas, the second solution involves the Hedwig service, our in-house message delivery service. We proposed to bypass the Live Activity token check process specifically for Android devices. Following extensive discussions, we decided to proceed with the first solution, which ensures consistency in the technical approach between Android and iOS platforms. Additionally, this solution allows us to ensure that notifications are only pushed to the devices that support the Live Activity feature. This decision strikes a good balance between efficiency and compatibility.&lt;/p&gt;

&lt;h3 id=&quot;ui-components&quot;&gt;UI Components&lt;/h3&gt;

&lt;p&gt;Starting with a kick-off project meeting where we showcased our plans and proposed solutions to our stakeholders, the engineering team presented two native Android UI components that could be utilised to replicate Live Activity: the Notification View and the Floating View.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Notification View&lt;/strong&gt; is a component located in the notification drawer (and potentially on the Lock Screen) that fulfils the most basic use-case of the Live Activity feature. It enables Android users to access information without the need to open the app. Since the standard notification template only allows developers to display a single content title, a content subtitle, and one image, it falls short of meeting our Live Activity UI requirements. To overcome this limitation, custom notifications with custom layouts are needed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure3.gif&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Early design spec of Grab’s LA using custom notification.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One of the key advantages of custom notifications is that they do not require any additional new permissions, ensuring a smooth user experience. Additionally, Android users are accustomed to checking their notifications from the notification tray, making it a familiar and intuitive interaction. However, it is important to acknowledge that custom notifications rely on a remote view, which can pose restrictions on rendering only specific views. On top of that, custom notifications provide a limited space for content – limited to 48dp when collapsed and 252dp when expanded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Floating View&lt;/strong&gt; is a component that will appear above all the applications in Android. It adds the convenience of accessing the information when the device is unlocked or when the user is on another app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Early design spec of Grab’s LA using floating view.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The use of a Floating View offers greater flexibility to the view by eliminating the reliance on a remote view. However, it’s important to be aware of the potential limitations associated with this approach. These limitations include the requirement for screen space, which can potentially impact other app functionalities and cause frustration for users. Additionally, if we intend to display multiple order updates, we may require even more space, taking into account that Grab allows users to place multiple orders. Furthermore, the Floating View feature requires an extra “Draw over other apps” permission, a setting that allows an app to display information on top of other apps on your screen.&lt;/p&gt;

&lt;p&gt;After thoughtful deliberation, we concluded that custom notifications provide a more consistent and user-friendly solution for implementing Grab’s Live Activity feature on Android. They offer compatibility, non-intrusiveness, no extra permissions, and the flexibility of silent notifications, ensuring an optimised user experience.&lt;/p&gt;

&lt;h2 id=&quot;building-grab-androids-live-activity&quot;&gt;Building Grab Android’s “Live Activity”&lt;/h2&gt;

&lt;p&gt;We began developing the Live Activity feature by focusing on Food and Mart for the MVP. However, we prioritised potential future use cases for other verticals by examining the existing functionality of the Grab iOS Live Activity feature. By considering these factors from the start, we need to make sure that we build an extendable and flexible solution that caters to different verticals and their various use-cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure5.gif&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Grab’s Android Live Activity.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As we set out to design Grab’s Android Live Activity module, we broke down the task into three key components:&lt;/p&gt;

&lt;h3 id=&quot;registering-live-activity-token&quot;&gt;&lt;strong&gt;Registering Live Activity Token&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In order to enable Hedwig services to send Live Activity notifications to devices, it is necessary to register a Live Activity Token for a specific order to Grab Devices services (refer to figure 1 for the iOS flow). As this use-case is applicable across various verticals in iOS, we have designed a LiveActivityIntegrationManager class specifically to handle this functionality.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;interface LiveActivityIntegrationManager {  
    /\*\*  
     \* To start live activity journey  
     \* @param vertical refers to vertical name  
     \* @param id refers to unique id which is used to differentiate live activity UI instances  
     \* eg: Food will use orderID as id, transport can pass rideID  
     \*\*/  
    fun startLiveActivity(vertical: Vertical, id: String): Completable

    fun updateLiveActivity(id: String, attributes: LiveActivityAttributes)

    fun cancelLiveActivity(id: String)  
}  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our goal is to provide developers with an easy implementation of Live Activity in the Grab app. Developers can simply utilize the startLiveActivity() function to register the token to Grab Devices by passing the vertical name and unique ID as parameters.&lt;/p&gt;

&lt;h3 id=&quot;notification-listener-and-payload-mapping&quot;&gt;&lt;strong&gt;Notification Listener and Payload Mapping&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;To handle Live Activity notifications in Android, it is necessary to listen to the Live Activity notification payload and map it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt;. Taking into consideration the initial Live Activity design (refer to figure 3), we need to analyse the variables necessary for this process. As a result, we break down the Live Activity UI into different UI elements and layouts, as follows:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure6.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Android Live Activity view breakdown.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;App Icon&lt;/strong&gt; – labeled as 1 in Figure 6. &lt;br /&gt;
This view always shows the Grab app icon.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Header Icon&lt;/strong&gt; – labeled as 2 in Figure 6.&lt;br /&gt;
This view is an image view that could be set with icon resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content Title View&lt;/strong&gt; – labeled as 3 in Figure 6. &lt;br /&gt;
This view is a placeholder that could be set with a text or custom remote view.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content Text View&lt;/strong&gt; – labeled as 4 in Figure 6. &lt;br /&gt;
This view is a placeholder that could be set with a text or custom remote view.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Footer View&lt;/strong&gt; – labeled as 5 in Figure 6.&lt;br /&gt;
This view is a placeholder that could be set with icon resources, bitmap, or custom remote view.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Decomposing the UI into different parts allows us to clearly understand of the UI components that need to maintain consistency across different use-cases, as well as the elements that can be easily customised and configured based on specific requirements. As a result, we have designed the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt; class that serves as a container that encompasses all the necessary configurations required for rendering the Live Activity.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 
class LiveActivityAttributes private constructor(  
    val iconRes: Int?,  
    val headerIconRes: Int?,  
    val contentTitle: CharSequence?,  
    val contentTitleStyle: ContentStyle.TitleStyle?,  
    val customContentTitleView: LiveActivityCustomView?,  
    val contentText: CharSequence?,  
    val contentTextStyle: ContentStyle.TextStyle?,  
    val customContentTextView: LiveActivityCustomView?,  
    val footerIconRes: Int?,  
    val footerBitmap: Bitmap?,  
    val footerProgressBarProgress: Float?,  
    val footerProgressBarStyle: ProgressBarStyle?,  
    val footerRatingBarAttributes: RatingBarAttributes?,  
    val customFooterView: LiveActivityCustomView?,  
    val contentIntent: PendingIntent?,  
    …  
)  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;payload-rendering&quot;&gt;&lt;strong&gt;Payload Rendering&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;To ensure a clear separation of responsibilities, we have designed a separate class called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityManager&lt;/code&gt;. This dedicated class is responsible for the mapping of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt; to Notifications. The generated notifications are then utilised by Android’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NotificationManager&lt;/code&gt; class to be posted and displayed accordingly.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
interface LiveActivityManager {  
    /\*\*  
     \* Post a Live Activity to be shown in the status bar, stream, etc.  
     \*  
     \* @param id           the ID of the Live Activity  
     \* @param attributes the LiveActivity to post to the system  
     \*/  
    fun notify(id: Int, attributes: LiveActivityAttributes)

    fun cancel(id: Int)  
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;We are delighted to announce that we have successfully implemented Grab’s Android version of the Live Activity feature for Express and Transport products. Furthermore, we plan to extend this feature to the Driver and Merchant applications as well. We understand the value this feature brings to our users and are committed to enhancing it further. Stay tuned for upcoming updates and enhancements to the Live Activity feature as we continue to improve and expand its capabilities across various verticals.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Mon, 23 Sep 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/live-activity-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/live-activity-2</guid>
        
        <category>Engineering</category>
        
        <category>Android</category>
        
        <category>Exploration</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Unveiling the process: The creation of our powerful campaign builder</title>
        <description>&lt;p&gt;In a previous &lt;a href=&quot;https://engineering.grab.com/trident-real-time-event-processing-at-scale&quot;&gt;blog&lt;/a&gt;, we introduced Trident, Grab’s internal marketing campaign platform. Trident empowers our marketing team to configure If This, Then That (IFTTT) logic and processes real-time events based on that.&lt;/p&gt;

&lt;p&gt;While we mainly covered how we scaled up the system to handle large volumes of real-time events, we did not explain the implementation of the event processing mechanism. This blog will fill up this missing piece. We will walk you through the various processing mechanisms supported in Trident and how they were built.&lt;/p&gt;

&lt;h2 id=&quot;base-building-block-treatment&quot;&gt;Base building block: Treatment&lt;/h2&gt;

&lt;p&gt;In our system, we use the term “treatment” to refer to the core unit of a full IFTTT data structure. A treatment is an amalgamation of three key elements - an event, conditions (which are optional), and actions. For example, consider a promotional campaign that offers “100 GrabPoints for completing a ride paid with GrabPay Credit”. This campaign can be transformed into a treatment in which the event is “ride completion”, the condition is “payment made using GrabPay Credit”, and the action is “awarding 100 GrabPoints”.&lt;/p&gt;

&lt;p&gt;Data generated across various Kafka streams by multiple services within Grab forms the crux of events and conditions for a treatment. Trident processes these Kafka streams, treating each data object as an event for the treatments. It evaluates the set conditions against the data received from these events. If all conditions are met, Trident then executes the actions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/processing-treatments.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Trident processes Kafka streams as events for treatments.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When the Trident user interface (UI) was first established, campaign creators had to grasp the treatment concept and configure the treatments accordingly. As we improved the UI, it became more user-friendly.&lt;/p&gt;

&lt;h2 id=&quot;building-on-top-of-treatment&quot;&gt;Building on top of treatment&lt;/h2&gt;

&lt;p&gt;Campaigns can be more complex than the example we provided earlier. In such scenarios, a single campaign may need transformation into several treatments. All these individual treatments are categorised under what we refer to as a “treatment group”. In this section, we discuss features that we have developed to manage such intricate campaigns.&lt;/p&gt;

&lt;h3 id=&quot;counter&quot;&gt;Counter&lt;/h3&gt;

&lt;p&gt;Let’s say we have a marketing campaign that “rewards users after they complete 4 rides”. For this requirement, it’s necessary for us to keep track of the number of rides each user has completed. To make this possible, we developed a capability known as &lt;strong&gt;counter&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;On the backend, a single counter setup translates into two treatments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treatment 1&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: N/A&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStats&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Treatment 2&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ride Count == 4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this feature, we introduce a new event, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStats&lt;/code&gt; action in &lt;strong&gt;Treatment 1&lt;/strong&gt; triggers the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; event following the update of the user counter. This allows &lt;strong&gt;Treatment 2&lt;/strong&gt; to consume the event and perform subsequent evaluations.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. The end-to-end evaluation process when using the Counter feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt; event is consumed, &lt;strong&gt;Treatment 1&lt;/strong&gt; is evaluated which then executes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStat&lt;/code&gt; action. This action increments the user’s ride counter in the database, gets the latest counter value, and publishes an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; event to Kafka.&lt;/p&gt;

&lt;p&gt;There are also other consumers that listen to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; events. When this event is consumed, &lt;strong&gt;Treatment 2&lt;/strong&gt; is evaluated. This process involves verifying whether the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ride Count&lt;/code&gt; equals to 4. If the condition is satisfied, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt; action is triggered.&lt;/p&gt;

&lt;p&gt;This feature is not limited to counting the number of event occurrences only. It’s also capable of tallying the total amount of transactions, among other things.&lt;/p&gt;

&lt;h3 id=&quot;delay&quot;&gt;Delay&lt;/h3&gt;

&lt;p&gt;Another feature available on Trident is a delay function. This feature is particularly beneficial in situations where we want to time our actions based on user behaviour. For example, we might want to give a ride voucher to a user three hours after they’ve ordered a ride to a theme park. The intention for this is to offer them a voucher they can use for their return trip.&lt;/p&gt;

&lt;p&gt;On the backend, a delay setup translates into two treatments. Given the above scenario, the treatments are as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treatment 1&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dropoff Location == Universal Studio&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduleDelayedEvent&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Treatment 2&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onDelayedEvent&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: N/A&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We introduce a new event, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onDelayedEvent&lt;/code&gt;, which &lt;strong&gt;Treatment 1&lt;/strong&gt; triggers during the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduleDelayedEvent&lt;/code&gt; action. This is made possible by using Simple Queue Service (SQS), given its built-in capability to publish an event with a delay.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process-delay-feature.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. The end-to-end evaluation process when using the Delay feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The maximum delay that SQS supports is 15 minutes; meanwhile, our platform allows for a delay of up to x hours. To address this limitation, we publish the event multiple times upon receiving the message, extending the delay by another 15 minutes each time, until it reaches the desired delay of x hours.&lt;/p&gt;

&lt;h3 id=&quot;limit&quot;&gt;Limit&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;Limit&lt;/strong&gt; feature is used to restrict the number of actions for a specific campaign or user within that campaign. This feature can be applied on a daily basis or for the full duration of the campaign.&lt;/p&gt;

&lt;p&gt;For instance, we can use the &lt;strong&gt;Limit&lt;/strong&gt; feature to distribute 1000 vouchers to users who have completed a ride and restrict it to only one voucher for one user per day. This ensures a controlled distribution of rewards and prevents a user from excessively using the benefits of a campaign.&lt;/p&gt;

&lt;p&gt;In the backend, a limit setup translates into conditions within a single treatment. Given the above scenario, the treatment would be as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TotalUsageCount &amp;lt;= 1000 AND DailyUserUsageCount &amp;lt;= 1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similar to the &lt;strong&gt;Counter&lt;/strong&gt; feature, it’s necessary for us to keep track of the number of completed rides for each user in the database.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process-limit-feature.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. The end-to-end evaluation process when using the Limit feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;a-better-campaign-builder&quot;&gt;A better campaign builder&lt;/h2&gt;

&lt;p&gt;As our campaigns grew more and more complex, the treatment creation quickly became overwhelming. A complex logic flow often required the creation of many treatments, which was cumbersome and error-prone. The need for a more visual and simpler campaign builder UI became evident.&lt;/p&gt;

&lt;p&gt;Our design team came up with a flow-chart-like UI. Figure 5, 6, and 7 show examples of how certain imaginary campaign setup would look like in the new UI.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. When users complete a food order, if they are a gold user, award them with A. However, if they are a silver user, award them with  B.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. When users complete a food or mart order, increment a counter. When the counter reaches 5, send them a message. Once the counter reaches 10, award them with points.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. When a user confirms a ride booking, wait for 1 minute, and then conduct A/B testing by sending a message 50% of the time.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The campaign setup in the new UI can be naturally stored as a node tree structure. The following is how the example in figure 5 would look like in JSON format. We assign each node a unique number ID, and store a map of the ID to node content.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;1&quot;: {
    &quot;type&quot;: &quot;scenario&quot;,
    &quot;data&quot;: { &quot;eventType&quot;: &quot;foodOrderComplete&quot;  },
    &quot;children&quot;: [&quot;2&quot;, &quot;3&quot;]
  },
  &quot;2&quot;: {
    &quot;type&quot;: &quot;condition&quot;,
    &quot;data&quot;: { &quot;lhs&quot;: &quot;var.user.tier&quot;, &quot;operator&quot;: &quot;eq&quot;, &quot;rhs&quot;: &quot;gold&quot; },
    &quot;children&quot;: [&quot;4&quot;]
  },
  &quot;3&quot;: {
    &quot;type&quot;: &quot;condition&quot;,
    &quot;data&quot;: { &quot;lhs&quot;: &quot;var.user.tier&quot;, &quot;operator&quot;: &quot;eq&quot;, &quot;rhs&quot;: &quot;silver&quot; },
    &quot;children&quot;: [&quot;5&quot;]
  },
  &quot;4&quot;: {
    &quot;type&quot;: &quot;action&quot;,
    &quot;data&quot;: {
      &quot;type&quot;: &quot;awardReward&quot;,
      &quot;payload&quot;: { &quot;rewardID&quot;: &quot;ID-of-A&quot;  }
    }
  },
  &quot;5&quot;: {
    &quot;type&quot;: &quot;action&quot;,
    &quot;data&quot;: {
      &quot;type&quot;: &quot;awardReward&quot;,
      &quot;payload&quot;: { &quot;rewardID&quot;: &quot;ID-of-B&quot;  }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conversion-to-treatments&quot;&gt;Conversion to treatments&lt;/h3&gt;

&lt;p&gt;The question then arises, how do we execute this node tree as treatments? This requires a conversion process. We then developed the following algorithm for converting the node tree into equivalent treatments:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// convertToTreatments is the main function
func convertToTreatments(rootNode) -&amp;gt; []Treatment:
  output = []

  for each scenario in rootNode.scenarios:
    // traverse down each branch
    context = createConversionContext(scenario)
    for child in rootNode.children:
      treatments = convertHelper(context, child)
      output.append(treatments)

  return output

// convertHelper is a recursive helper function
func convertHelper(context, node) -&amp;gt; []Treatment:
  output = []
  f = getNodeConverterFunc(node.type)
  treatments, updatedContext = f(context, node)

  output.append(treatments)

  for child in rootNode.children:
    treatments = convertHelper(updatedContext, child)
    output.append(treatments)

  return output
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getNodeConverterFunc&lt;/code&gt; will return different handler functions according to the node type. Each handler function will either update the conversion context, create treatments, or both.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;caption style=&quot;caption-side:bottom&quot;&gt;Table 1. The handler logic mapping for each node type.&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;width:20%&quot;&gt;Node type&lt;/th&gt;
      &lt;th&gt;Logic&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;condition&lt;/td&gt;
      &lt;td&gt;Add conditions into the context and return the updated context.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;action&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and the action itself.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;delay&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and a scheduleDelayedEvent action.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;count&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and an incrementUserStats action.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;count condition&lt;/td&gt;
      &lt;td&gt;Form a condition with the count key from the context, and return an updated context with the condition.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It is important to note that treatments cannot always be reverted to their original node tree structure. This is because different node trees might be converted into the same set of treatments.&lt;/p&gt;

&lt;p&gt;The following is an example where two different node trees setups correspond to the same set of treatments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Food order complete -&amp;gt; if gold user -&amp;gt; then award A&lt;/li&gt;
  &lt;li&gt;Food order complete -&amp;gt; if silver user -&amp;gt; then award B&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/two-node-tree-setup.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. An example of two node tree setups corresponding to the the same set of treatments.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Therefore, we need to store both the campaign node tree JSON and treatments, along with the mapping between the nodes and the treatments. Campaigns are executed using treatments, but displayed using the node tree JSON.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/storing-campaigns.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. For each campaign, we store both the node tree JSON and treatments, along with their mapping.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;how-we-handle-campaign-updates&quot;&gt;How we handle campaign updates&lt;/h3&gt;

&lt;p&gt;There are instances where a marketing user updates a campaign after its creation. For such cases we need to identify:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Which existing treatments should be removed.&lt;/li&gt;
  &lt;li&gt;Which existing treatments should be updated.&lt;/li&gt;
  &lt;li&gt;What new treatments should be added.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can do this by using the node-treatment mapping information we stored. The following is the pseudocode for this process:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func howToUpdateTreatments(oldTreatments []Treatment, newTreatments []Treatment):
  treatmentsUpdate = map[int]Treatment // treatment ID -&amp;gt; updated treatment
  treatmentsRemove = []int // list of treatment IDs
  treatmentsAdd = []Treatment // list of new treatments to be created

  matchedOldTreamentIDs = set()

  for newTreatment in newTreatments:
    matched = false

    // see whether the nodes match any old treatment
    for oldTreatment in oldTreatments:
      // two treatments are considered matched if their linked node IDs are identical
      if isSame(oldTreatment.nodeIDs, newTreatment.nodeIDs):
        matched = true
        treatmentsUpdate[oldTreament.ID] = newTreatment
        matchedOldTreamentIDs.Add(oldTreatment.ID)
        break

    // if no match, that means it is a new treatment we need to create
    if not matched:
      treatmentsAdd.Append(newTreatment)

  // all the non-matched old treatments should be deleted
  for oldTreatment in oldTreatments:
    if not matchedOldTreamentIDs.contains(oldTreatment.ID):
      treatmentsRemove.Append(oldTreatment.ID)

  return treatmentsAdd, treatmentsUpdate, treatmentsRemove
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For a visual illustration, let’s consider a campaign that initially resembles the one shown in figure 10. The node IDs are highlighted in red.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/campaign-node-tree-structure.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10. A campaign in node tree structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This campaign will generate two treatments.&lt;/p&gt;

&lt;table style=&quot;width: 70%&quot; class=&quot;table&quot;&gt;
  &lt;caption style=&quot;caption-side:bottom&quot;&gt;Table 2. The campaign shown in the figure 10 will generated two treatments.&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;width:10%&quot;&gt;ID&lt;/th&gt;
      &lt;th style=&quot;width:50%&quot;&gt;Treatment&lt;/th&gt;
      &lt;th&gt;Linked node IDs&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Event: food order complete &lt;br /&gt; Condition: gold user &lt;br /&gt; Action: award A&lt;/td&gt;
      &lt;td&gt;1, 2, 3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Event: food order complete &lt;br /&gt; Condition: silver user &lt;br /&gt; Action: award B&lt;/td&gt;
      &lt;td&gt;1, 4, 5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;After creation, the campaign creator updates the upper condition branch, deletes the lower branch, and creates a new branch. Note that after node deletion, the deleted node ID will not be reused.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/updated-campaign-node-tree-structure.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 11. An updated campaign in node tree structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;According to our logic in figure 11, the following update will be performed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Update action for treatment 1 to “award C”.&lt;/li&gt;
  &lt;li&gt;Delete treatment 2&lt;/li&gt;
  &lt;li&gt;Create a new treatment: food -&amp;gt; is promo used -&amp;gt; send push&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article reveals the workings of Trident, our bespoke marketing campaign platform. By exploring the core concept of a “treatment” and additional features like Counter, Delay and Limit, we illustrated the flexibility and sophistication of our system.&lt;/p&gt;

&lt;p&gt;We’ve explained changes to the Trident UI that have made campaign creation more intuitive. Transforming campaign setups into executable treatments while preserving the visual representation ensures seamless campaign execution and adaptation.&lt;/p&gt;

&lt;p&gt;Our devotion to improving Trident aims to empower our marketing team to design engaging and dynamic campaigns, ultimately providing excellent experiences to our users.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Sep 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/the-creation-of-our-powerful-campaign-builder</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-creation-of-our-powerful-campaign-builder</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Chimera Sandbox: A scalable experimentation and development platform for Notebook services</title>
        <description>&lt;p&gt;Key to innovation and improvement in machine learning (ML) models is the ability for rapid iteration. Our team, Chimera, part of the Artificial Intelligence (AI) Platform team, provides the essential compute infrastructure, ML pipeline components, and backend services. This support enables our ML engineers, data scientists, and data analysts to efficiently experiment and develop ML solutions at scale.&lt;/p&gt;

&lt;p&gt;With a commitment to leveraging the latest Generative AI (GenAI) technologies, Grab is enhancing productivity tools for all Grabbers. Our Chimera Sandbox, a scalable Notebook platform, facilitates swift experimentation and development of ML solutions, offering deep integration with our AI Gateway. This enables easy access to various Large Language Models (LLMs) (both proprietary and open source), ensuring scalability, compliance, and access control are managed seamlessly.&lt;/p&gt;

&lt;h2 id=&quot;what-is-chimera-sandbox&quot;&gt;What is Chimera Sandbox?&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox is a Notebook service platform. It allows users to launch multiple notebook and visualisation services for experimentation and development. The platform offers an extremely quick onboarding process enabling any Grabber to start learning, exploring and experimenting in just a few minutes. This inclusivity and ease of use have been key in driving the adoption of the platform across different teams within Grab and empowering all Grabbers to be GenAI-ready.&lt;/p&gt;

&lt;p&gt;One significant challenge in harnessing ML for innovation, whether for technical experts or non-technical enthusiasts, has been the accessibility of resources. This includes GPU instances and specialised services for developing LLM-powered applications. Chimera Sandbox addresses this head-on by offering an extensive array of compute instances, both with and without GPU support, thus removing barriers to experimentation. Its deep integration with Grab’s suite of internal ML tools transforms the way users approach ML projects. Users benefit from features like hyperparameter tuning, tracking ML training metadata, accessing diverse LLMs through Grab’s AI Gateway, and experimenting with rich datasets from Grab’s data lake. Chimera Sandbox ensures that users have everything they need at their fingertips. This ecosystem not only accelerates the development process but also encourages innovative approaches to solving complex problems.&lt;/p&gt;

&lt;p&gt;The underlying compute infrastructure of the Chimera Sandbox platform is Grab’s very own battle-tested, highly scalable ML compute infrastructure running on multiple Kubernetes clusters. Each cluster can scale up to thousands of nodes at peak times gracefully. This scalability ensures that the platform can handle the high computational demands of ML tasks. The robustness of Kubernetes ensures that the platform remains stable, reliable, and highly available even under heavy load. At any point in time, there can be hundreds of data scientists, ML engineers and developers experimenting and developing on the Chimera Sandbox platform.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/chimera-sandbox-platform.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Chimera Sandbox Platform.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/ui-starting-chimera.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. UI for Starting Chimera Sandbox.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;best-of-both-worlds&quot;&gt;Best of both worlds&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox is suitable for both new users who want to explore and experiment ML solutions and advanced users who want to have full control over the Notebook services they run. Users can launch Notebook services using default Docker images provided by the Chimera Sandbox platform. These images come pre-loaded with popular data science and ML libraries and various Grab internal systems integrations. Chimera also provides basic Docker images from which the users can use as base images to build their own customised Notebook service Docker images. Once the images are built, the users can configure their Notebook services to use their custom Docker images. This ensures their Notebook environment can be exactly the way they want them to be.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/customise-notebook-packages.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Users are able to customise their Notebook service with additional packages.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;real-time-collaboration&quot;&gt;Real-time collaboration&lt;/h2&gt;
&lt;p&gt;The Chimera Sandbox platform also features a real-time collaboration feature. This feature fosters a collaborative environment where users can exchange ideas and work together on projects.&lt;/p&gt;

&lt;h2 id=&quot;cpu-and-gpu-choices&quot;&gt;CPU and GPU choices&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox offers a wide variety of CPU and GPU choices to cater to specific needs, whether it is a CPU, memory, or GPU intensive experimentation. This flexibility allows users to choose the most suitable computational resources for their tasks, ensuring optimal performance and efficiency.&lt;/p&gt;

&lt;h2 id=&quot;deep-integration-with-spark&quot;&gt;Deep integration with Spark&lt;/h2&gt;

&lt;p&gt;The platform is deeply integrated with internal Spark engines, enabling users to experiment building  extract, transform, and load (ETL) jobs with data from Grab’s data lake. Integrated helpers such as SparkConnect Kernel and %%spark_sql magic cell, provide a faster developer experience, which can execute Spark SQL queries without needing to write additional code to start a Spark session and query.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/magic-cell.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. %%spark_sql magic cell enables users to quickly explore data with Spark.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to Magic Cell, the Chimera Sandbox offers advanced Spark functionalities. Users can write PySpark code using pre-configured and configurable Spark clients in the runtime environment. The underlying computation engine leverages Grab’s custom Spark-on-Kubernetes operator, enabling support for large-scale Spark workloads. This high-code capability complements the low-code Magic Cell feature, providing users with a versatile data processing environment.&lt;/p&gt;

&lt;h2 id=&quot;ai-gallery&quot;&gt;AI Gallery&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox features an AI Gallery to guide and accelerate users to start experimenting with ML solutions or building GenAI-powered applications. This is especially useful for new or novice users who are keen to explore what they can do on the Chimera Sandbox platform. With Chimera Sandbox, users are not just presented with a bare bones compute solution but rather are provided with ways to do ML tasks right from Chimera Sandbox Notebooks. This approach saves users from the hassle of having to piece together the examples from the public internet, which may not work on the platform. These ready-to-run and comprehensive notebooks in the AI Gallery assure users that they can run end-to-end examples without a hitch. Based on these examples, the users can only extend their experimentations and development for their specific needs. Not only that, these tutorials and notebooks exhibit the platform capabilities and integrations available on the platform in an interactive manner rather than having the users refer to a separate documentation.&lt;/p&gt;

&lt;p&gt;Lastly, the AI Gallery encourages contributions from other Grabbers, fostering a collaborative environment. Users who are enthusiastic about creating educational contents on Chimera Sandbox can effectively share their work with other Grabbers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/ai-gallery.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Including AI Gallery in user specified sandbox images.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;integration-with-various-llm-services&quot;&gt;Integration with various LLM services&lt;/h2&gt;

&lt;p&gt;Notebook users on Chimera Sandbox can easily tap into a plethora of LLMs, both open source and proprietary models, without any additional setup via our AI Gateway. The platform takes care of access mechanisms and endpoints for various LLM services so that the users can easily use their favourite libraries to create LLM-powered applications and conduct experimentations. This seamless integration with LLMs enables users to focus on their GAI-powered ideas rather than having to worry about underlying logistics and technicalities of using different LLMs.&lt;/p&gt;

&lt;h2 id=&quot;more-than-a-notebook-service&quot;&gt;More than a notebook service&lt;/h2&gt;

&lt;p&gt;While Notebook is the most popular service on the platform, Chimera Sandbox offers much more than just notebook capabilities. It serves as a comprehensive namespace workspace equipped with a suite of ML/AI tools. Alongside notebooks, users can access essential ML tools such as Optuna for hyperparameter tuning, MLflow for experiment tracking, and other tools including Zeppelin, RStudio, Spark history, Polynote, and LabelStudio. All these services use a shared storage system, creating a tailored workspace for ML and AI tasks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/sandbox-namespace.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. A Sandbox namespace with its out-of-the-box services.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, the Sandbox framework allows for the seamless integration of more services into personal workspaces. This high level of flexibility significantly enhances the capabilities of the Sandbox platform, making it an ideal environment for diverse ML and AI applications.&lt;/p&gt;

&lt;h2 id=&quot;cost-attribution&quot;&gt;Cost attribution&lt;/h2&gt;

&lt;p&gt;For a multi-tenanted platform such as Chimera Sandbox, it is crucial to provide users information on how much they have spent with their experimentations. Cost showback and chargeback capabilities are of utmost importance for a platform on which users can launch Notebook services that use accelerated instances with GPUs. The platform provides cost attribution to individual users, so each user knows exactly how much they are spending on their experimentations and can make budget-conscious decisions. This transparency in cost attribution encourages responsible usage of resources and helps users manage their budgets effectively.&lt;/p&gt;

&lt;h2 id=&quot;growth-and-future-plans&quot;&gt;Growth and future plans&lt;/h2&gt;

&lt;p&gt;In essence, Chimera Sandbox is more than just a tool; it’s a catalyst for innovation and growth, empowering Grabbers to explore the frontiers of ML and AI. By providing an inclusive, flexible, and powerful platform, Chimera Sandbox is helping shape the future of Grab, making every Grabber not just ready but excited to contribute to the AI-driven transformation of our products and services.&lt;/p&gt;

&lt;p&gt;In July and August of this year, teams were given the opportunity to intensively learn and experiment with AI. Since then, we have observed hockey stick growth on the Chimera Sandbox platform. We are enabling massive experimentation across different teams at Grab to experiment and work on different GAI-powered applications.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/daily-active-users.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Chimera Sandbox daily active users.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our future plans include mechanisms for better notebook discovery, collaboration and usability, and the ability to enable users to schedule their notebooks right from Chimera Sandbox. These enhancements aim to improve the user experience and make the platform even more versatile and powerful.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Aug 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/chimera-sandbox</link>
        <guid isPermaLink="true">https://engineering.grab.com/chimera-sandbox</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>How we improved translation experience with cost efficiency</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As COVID restrictions were fully lifted in 2023, the number of tourists grew dramatically. People began to explore the world again, frequently using the Grab app to make bookings outside of their home country. However, we noticed that communication posed a challenge for some users. Despite our efforts to integrate an auto-translation feature in the booking chat, we received feedback about occasional missed or inaccurate translations. You can refer to this &lt;a href=&quot;https://engineering.grab.com/message-center&quot;&gt;blog&lt;/a&gt; for a better understanding of Grab’s chat system.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/improved-translation-experience-with-cost-efficiency/translation-example.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;An example of a bad translation. The correct translation is: &apos;ok sir&apos;.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In an effort to enhance the user experience for travellers using the Grab app, we formed an engineering squad to tackle this problem. The objectives are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure translation is provided when it’s needed.&lt;/li&gt;
  &lt;li&gt;Improve the quality of translation.&lt;/li&gt;
  &lt;li&gt;Maintain the cost of this service within a reasonable range.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ensure-translation-is-provided-when-its-needed&quot;&gt;Ensure translation is provided when it’s needed&lt;/h2&gt;

&lt;p&gt;Originally, we relied on users’ device language settings to determine if translation is needed. For example, if both the passenger and driver’s language setting is set to English, translation is not needed. Interestingly, it turned out that the device language setting did not reliably indicate the language in which a user would send their messages. There were numerous cases where despite having their device language set to English, drivers sent messages in another language.&lt;/p&gt;

&lt;p&gt;Therefore, we needed to detect the language of user messages on the fly to make sure we trigger translation when it’s needed.&lt;/p&gt;

&lt;h3 id=&quot;language-detection&quot;&gt;Language detection&lt;/h3&gt;

&lt;p&gt;Simple as it may seem, language detection is not that straightforward a task. We were unable to find an open-source language detector library that covered all Southeast Asian languages. We looked for Golang libraries as our service was written in Golang. The closest we could find were the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Whatlang: unable to detect Malay&lt;/li&gt;
  &lt;li&gt;Lingua: unable to detect Burmese and Khmer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We decided to choose Lingua over Whatlang as the base detector due to the following factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overall higher accuracy.&lt;/li&gt;
  &lt;li&gt;Capability to provide detection confidence level.&lt;/li&gt;
  &lt;li&gt;We have more users using Malay than those using Burmese or Khmer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a translation request comes in, our first step is to use Lingua for language detection. If the detection confidence level falls below a predefined threshold, we fall back to call the third-party translation service as it can detect all Southeast Asian languages.&lt;/p&gt;

&lt;p&gt;You may ask, why don’t we simply use the third-party service in the first place. It’s because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The third-party service only has a translate API that also does language detection, but it does not provide a standalone language detection API.&lt;/li&gt;
  &lt;li&gt;Using the translate API is costly, so we need to avoid calling it when it’s unnecessary. We will cover more on this in a later section.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another challenge we’ve encountered is the difficulty of distinguishing between Malay and Indonesian languages due to their strong similarities and shared vocabulary. The identical text might convey different meanings in these two languages, which the third-party translation service struggles to accurately detect and translate.&lt;/p&gt;

&lt;p&gt;Differentiating Malay and Indonesian is a tough problem in general. However, in our case, the detection has a very specific context, and we can make use of the context to enhance our detection accuracy.&lt;/p&gt;

&lt;h3 id=&quot;making-use-of-translation-context&quot;&gt;Making use of translation context&lt;/h3&gt;

&lt;p&gt;All our translations are for the messages sent in the context of a booking or order, predominantly between passenger and driver. There are two simple facts that can aid in our language detection:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Booking/order happens in one single country.&lt;/li&gt;
  &lt;li&gt;Drivers are almost always local to that country.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, for a booking that happens in an Indonesian city, if the driver’s message is detected as Malay, it’s highly likely that the message is actually in Bahasa Indonesia.&lt;/p&gt;

&lt;h2 id=&quot;improve-quality-of-translation&quot;&gt;Improve quality of translation&lt;/h2&gt;

&lt;p&gt;Initially, we were entirely dependent on a third-party service for translating our chat messages. While overall powerful, the third-party service is not perfect, and it does generate weird translations from time to time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/improved-translation-experience-with-cost-efficiency/weird-translation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;An example of a weird translation from a third-party service recorded on 19 Dec 2023.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Then, it came to us that we might be able to build an in-house translation model that could translate chat messages better than the third-party service. The reasons being:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The scope of our chat content is highly specific. All the chats are related to bookings or orders. There would not be conversations about life or work in the chat. Maybe a small Machine Learning (ML) model would suffice to do the job.&lt;/li&gt;
  &lt;li&gt;The third-party service is a general translation service. It doesn’t know the context of our messages. We, however, know the whole context. Having the right context gives us a great edge on generating the right translation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training-steps&quot;&gt;Training steps&lt;/h3&gt;

&lt;p&gt;To create our own translation model, we took the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perform topic modelling on Grab chat conversations.&lt;/li&gt;
  &lt;li&gt;Worked with the localisation team to create a benchmark set of translations.&lt;/li&gt;
  &lt;li&gt;Measured existing translation solutions against benchmarks.&lt;/li&gt;
  &lt;li&gt;Used an open source Large Language Model (LLM) to produce synthetic training data.&lt;/li&gt;
  &lt;li&gt;Used synthetic data to train our lightweight translation model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;topic-modelling&quot;&gt;Topic modelling&lt;/h4&gt;

&lt;p&gt;In this step, our aim was to generate a dataset which is both representative of the chat messages sent by our users and diverse enough to capture all of the nuances of the conversations. To achieve this, we took a stratified sampling approach. This involved a random sample of past chat conversation messages stratified by various topics to ensure a comprehensive and balanced representation.&lt;/p&gt;

&lt;h4 id=&quot;developing-a-benchmark&quot;&gt;Developing a benchmark&lt;/h4&gt;

&lt;p&gt;For this step we engaged Grab’s localisation team to create a benchmark for translations. The intention behind this step wasn’t to create enough translation examples to fully train or even finetune a model, but rather, it was to act as a benchmark for translation quality, and also as a set of few-shot learning examples for when we generate our synthetic data.&lt;/p&gt;

&lt;p&gt;This second point was critical! Although LLMs can generate good quality translations, LLMs are highly susceptible to their training examples. Thus, by using a set of handcrafted translation examples, we hoped to produce a set of examples that would teach the model the exact style, level of formality, and correct tone for the context in which we plan to deploy the final model.&lt;/p&gt;

&lt;h4 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h4&gt;

&lt;p&gt;From a theoretical perspective there are two ways that one can measure the performance of a machine translation system. The first is through the computation of some sort of translation quality score such as a BLEU or CHRF++ score. The second method is via subjective evaluation. For example, you could give each translation a score from 1 to 5 or pit two translations against each other and ask someone to assess which they prefer.&lt;/p&gt;

&lt;p&gt;Both methods have their relative strengths and weaknesses. The advantage of a subjective method is that it corresponds better with what we want, a high quality translation experience for our users. The disadvantage of this method is that it is quite laborious. The opposite is true for the computed translation quality scores, that is to say that they correspond less well to a human’s subjective experience of our translation quality, but that they are easier and faster to compute.&lt;/p&gt;

&lt;p&gt;To overcome the inherent limitations of each method, we decided to do the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Set a benchmark score for the translation quality of various translation services using a CHRF++ score.&lt;/li&gt;
  &lt;li&gt;Train our model until its CHRF++ score is significantly better than the benchmark score.&lt;/li&gt;
  &lt;li&gt;Perform a manual A/B test between the newly trained model and the existing translation service.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;synthetic-data-generation&quot;&gt;Synthetic data generation&lt;/h4&gt;

&lt;p&gt;To generate the training data needed to create our model, we had to rely on an open source LLM to generate the synthetic translation data. For this task, we spent considerable effort looking for a model which had both a large enough parameter count to ensure high quality outputs, but also a model which had the correct tokenizer to handle the diverse sets of languages which Grab’s customers speak. This is particularly important for languages which use non-standard character sets such as Vietnamese and Thai. We settled on using a public model from &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; for this task.&lt;/p&gt;

&lt;p&gt;We then used a subset of the previously mentioned benchmark translations to input as few-shot learning examples to our prompt. After many rounds of iteration, we were able to generate translations which were superior to the benchmark CHRF++ scores which we had attained in the previous section.&lt;/p&gt;

&lt;h4 id=&quot;model-fine-tuning&quot;&gt;Model fine tuning&lt;/h4&gt;
&lt;p&gt;We now had one last step before we had something that was production ready! Although we had successfully engineered a prompt capable of generating high quality translations from the public Hugging Face model, there was no way we’d be able to deploy such a model. The model was far too big for us to deploy it in a cost efficient manner and within an acceptable latency. Our solution to this was to fine-tune a smaller bespoke model using the synthetic training data which was derived from the larger model.&lt;/p&gt;

&lt;p&gt;These models were language specific (e.g. English to Indonesian) and built solely for the purpose of language translation. They are 99% smaller than the public model. With approximately 10 Million synthetic training examples, we were able to achieve performance which was 98% as effective as our larger model.&lt;/p&gt;

&lt;p&gt;We deployed our model and ran several A/B tests with it. Our model performed pretty well overall, but we noticed a critical problem: sometimes, numbers got mutated in the translation. These numbers can be part of an address, phone number, price etc. Showing the wrong number in a translation can cause great confusion to the users. Unfortunately, an ML model’s output can never be fully controlled; therefore, we added an additional layer of programmatic check to mitigate this issue.&lt;/p&gt;

&lt;h3 id=&quot;post-translation-quality-check&quot;&gt;Post-translation quality check&lt;/h3&gt;
&lt;p&gt;Our goal is to ensure non-translatable content such as numbers, special symbols, and emojis  in the original message doesn’t get mutated in the translation produced by our in-house model. We extract all the non-translatable content from the original message, count the occurrences of each, and then try to match the same in the translation. If it fails to match, we discard the in-house translation and fall back to using the third-party translation service.&lt;/p&gt;

&lt;h2 id=&quot;keep-cost-low&quot;&gt;Keep cost low&lt;/h2&gt;

&lt;p&gt;At Grab, we try to be as cost efficient as possible in all aspects. In the case of translation, we tried to minimise cost by avoiding unnecessary on-the-fly translations.&lt;/p&gt;

&lt;p&gt;As you would have guessed, the first thing we did was to implement caching. A cache layer is placed before both the in-house translation model and the third-party translation. We try to serve translation from the cache first before hitting the underlying translation service. However, given that translation requests are in free text and can be quite dynamic, the impact of caching is limited. There’s more we need to do.&lt;/p&gt;

&lt;p&gt;For context, in a booking chat, other than the users, Grab’s internal services can also send messages to the chat room. These messages are called system messages. For example,our food service always sends a message with information on the food order when an order is confirmed.&lt;/p&gt;

&lt;p&gt;System messages are all fairly static in nature, however, we saw a very high amount of translation cost attributed to system messages. Taking a deeper look, we noticed the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Many system messages were not sent in the recipient’s language, thus requiring on-the-fly translation.&lt;/li&gt;
  &lt;li&gt;Many system messages, though having the same static structure, contain quite a few variants such as passenger’s name and  food order item name. This makes it challenging to utilise our translation cache effectively as each message is different.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since all system messages are manually prepared, we should be able to get them all manually translated into all the required languages, and avoid on-the-fly translations altogether.&lt;/p&gt;

&lt;p&gt;Therefore, we launched an internal campaign, mandating all internal services that send system messages to chat rooms to get manual translations prepared, and pass in the translated contents. This alone helped us save roughly US$255K a year!&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;At Grab, we firmly believe that our proprietary in-house translation models are not only more cost-effective but cater more accurately to our unique use cases compared to third-party services. We will focus on expanding these models to more languages and countries across our operating regions.&lt;/p&gt;

&lt;p&gt;Additionally, we are exploring opportunities to apply learnings of our chat translations to other Grab content. This strategy aims to guarantee a seamless language experience for our rapidly expanding user base, especially travellers. We are enthusiastically looking forward to the opportunities this journey brings!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Aug 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/improved-translation-experience-with-cost-efficiency</link>
        <guid isPermaLink="true">https://engineering.grab.com/improved-translation-experience-with-cost-efficiency</guid>
        
        <category>Chat</category>
        
        <category>Chat support</category>
        
        <category>Engineering</category>
        
        <category>GrabChat</category>
        
        <category>Messaging</category>
        
        <category>Translation</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>LLM-powered data classification for data entities at scale</title>
        <description>&lt;p&gt;&lt;small class=&quot;credits&quot;&gt; Editor’s note: This post was originally published in October 2023 and has been updated to reflect Grab’s partnership with the Infocomm Media Development Authority as part of its Privacy Enhancing Technology Sandbox that concluded in March 2024.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we deal with PetaByte-level data and manage countless data entities ranging from database tables to Kafka message schemas. Understanding the data inside is crucial for us, as it not only streamlines the data access management to safeguard the data of our users, drivers and merchant-partners, but also improves the data discovery process for data analysts and scientists to easily find what they need.&lt;/p&gt;

&lt;p&gt;The Caspian team (Data Engineering team) collaborated closely with the Data Governance team on automating governance-related metadata generation. We started with Personal Identifiable Information (PII) detection and built an orchestration service using a third-party classification service. With the advent of the Large Language Model (LLM), new possibilities dawned for metadata generation and sensitive data identification at Grab. This prompted the inception of the project, which aimed to integrate LLM classification into our existing service. In this blog, we share insights into the transformation from what used to be a tedious and painstaking process to a highly efficient system, and how it has empowered the teams across the organisation.&lt;/p&gt;

&lt;p&gt;For ease of reference, here’s a list of terms we’ve used and their definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Entity&lt;/strong&gt;: An entity representing a schema that contains rows/streams of data, for example, database tables, stream messages, data lake tables.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Refers to the model’s output given a data entity, unverified manually.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Classification&lt;/strong&gt;: The process of classifying a given data entity, which in the context of this blog, involves generating tags that represent sensitive data or Grab-specific types of data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metadata Generation&lt;/strong&gt;: The process of generating the metadata for a given data entity. In this blog, since we limit the metadata to the form of tags, we often use this term and data classification interchangeably.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;: Refers to the level of confidentiality of data. High sensitivity means that the data is highly confidential. The lowest level of sensitivity often refers to public-facing or publicly-available data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;When we first approached the data classification problem, we aimed to solve something more specific - Personal Identifiable Information (PII) detection. Initially, to protect sensitive data from accidental leaks or misuse, Grab implemented manual processes and campaigns targeting data producers to tag schemas with sensitivity tiers. These tiers ranged from Tier 1, representing schemas with highly sensitive information, to Tier 4, indicating no sensitive information at all. As a result, half of all schemas were marked as Tier 1, enforcing the strictest access control measures.&lt;/p&gt;

&lt;p&gt;The presence of a single Tier 1 table in a schema with hundreds of tables justifies classifying the entire schema as Tier 1. However, since Tier 1 data is rare, this implies that a large volume of non-Tier 1 tables, which ideally should be more accessible, have strict access controls.&lt;/p&gt;

&lt;p&gt;Shifting access controls from the schema-level to the table-level could not be done safely due to the lack of table classification in the data lake. We could have conducted more manual classification campaigns for tables, however this was not feasible for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The volume, velocity, and variety of data had skyrocketed within the organisation, so it took significantly more time to classify at table level compared to schema level. Hence, a programmatic solution was needed to streamline the classification process, reducing the need for manual effort.&lt;/li&gt;
  &lt;li&gt;App developers, despite being familiar with the business scope of their data, interpreted internal data classification policies and external data regulations differently, leading to inconsistencies in understanding.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A service called Gemini &lt;em&gt;(named before Google announced the Gemini model!)&lt;/em&gt; was built internally to automate the tag generation process using a third party data classification service. Its purpose was to scan the data entities in batches and generate column/field level tags. These tags would then go through a review process by the data producers. The data governance team provided classification rules and used regex classifiers, alongside the third-party tool’s own machine learning classifiers, to discover sensitive information.&lt;/p&gt;

&lt;p&gt;After the implementation of the initial version of Gemini, a few challenges remained.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The third-party tool did not allow customisations of its machine learning classifiers, and the regex patterns produced too many false positives during our evaluation.&lt;/li&gt;
  &lt;li&gt;Building in-house classifiers would require a dedicated data science team to train a customised model. They would need to invest a significant amount of time to understand data governance rules thoroughly and prepare datasets with manually labelled training data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;LLM came up on our radar following its recent &lt;em&gt;“iPhone moment”&lt;/em&gt; with ChatGPT’s explosion onto the scene. It is trained using an extremely large corpus of text and contains trillions of parameters. It is capable of conducting natural language understanding tasks, writing code, and even analysing data based on requirements. The LLM naturally solves the mentioned pain points as it provides a natural language interface for data governance personnel. They can express governance requirements through text prompts, and the LLM can be customised effortlessly without code or model training.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;In this section, we dive into the implementation details of the data classification workflow. Please refer to the diagram below for a high-level overview:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/data_classification_workflow.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Overview of data classification workflow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This diagram illustrates how data platforms, the metadata generation service (Gemini), and data owners work together to classify and verify metadata. Data platforms trigger scan requests to the Gemini service to initiate the tag classification process. After the tags are predicted, data platforms consume the predictions, and the data owners are notified to verify these tags.&lt;/p&gt;

&lt;h3 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/arch_diagram_orchestration.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Architecture diagram of the orchestration service Gemini&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our orchestration service, Gemini, manages the data classification requests from data platforms. From the diagram, the architecture contains the following components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data platforms: These platforms are responsible for managing data entities and initiating data classification requests.&lt;/li&gt;
  &lt;li&gt;Gemini: This orchestration service communicates with data platforms, schedules and groups data classification requests.&lt;/li&gt;
  &lt;li&gt;Classification engines: There are two available engines (a third-party classification service and GPT3.5) for executing the classification jobs and return results. Since we are still in the process of evaluating two engines, both of the engines are working concurrently.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the orchestration service receives requests, it helps aggregate the requests into reasonable mini-batches. Aggregation is achievable through the message queue at fixed intervals. In addition, a rate limiter is attached at the workflow level. It allows the service to call the Cloud Provider APIs with respective rates to prevent the potential throttling from the service providers.&lt;/p&gt;

&lt;p&gt;Specific to LLM orchestration, there are two limits to be mindful of. The first one is the context length. The input length cannot surpass the context length, which was 4000 tokens for GPT3.5 at the time of development (or around 3000 words). The second one is the overall token limit (since both the input and output share the same token limit for a single request). Currently, all Azure OpenAI model deployments share the same quota under one account, which is set at 240K tokens per minute.&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section, we focus on LLM-powered column-level tag classification. The tag classification process is defined as follows:&lt;/p&gt;

&lt;p&gt;Given a data entity with a defined schema, we want to tag each field of the schema with metadata classifications that follow an internal classification scheme from the data governance team. For example, the field can be tagged as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular kind of business metric&amp;gt;&lt;/code&gt; or a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular type of personally identifiable information (PII)&lt;/code&gt;. These tags indicate that the field contains a business metric or PII.&lt;/p&gt;

&lt;p&gt;We ask the language model to be a column tag generator and to assign the most appropriate tag to each column. Here we showcase an excerpt of the prompt we use:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;You are a database column tag classifier, your job is to assign the most appropriate tag based on table name and column name. The database columns are from a company that provides ride-hailing, delivery, and financial services. Assign one tag per column. However not all columns can be tagged and these columns should be assigned &amp;lt;None&amp;gt;. You are precise, careful and do your best to make sure the tag assigned is the most appropriate.

The following is the list of tags to be assigned to a column. For each line, left hand side of the : is the tag and right hand side is the tag definition

…
&amp;lt;Personal.ID&amp;gt; : refers to government-provided identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar. This tag should absolutely not be assigned to columns named &quot;id&quot;, &quot;merchant id&quot;, &quot;passenger id&quot;, “driver id&quot; or similar since these are not government-provided identification numbers. This tag should be very rarely assigned.

&amp;lt;None&amp;gt; : should be used when none of the above can be assigned to a column.
…

Output Format is a valid json string, for example:

[{
        &quot;column_name&quot;: &quot;&quot;,
        &quot;assigned_tag&quot;: &quot;&quot;
}]

Example question

`These columns belong to the &quot;deliveries&quot; table

        1. merchant_id
        2. status
        3. delivery_time`

Example response

[{
        &quot;column_name&quot;: &quot;merchant_id&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;Personal.ID&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;status&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;delivery_time&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also curated a tag library for LLM to classify. Here is an example:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column-level Tag&lt;/th&gt;
      &lt;th&gt;Definition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.ID&lt;/td&gt;
      &lt;td&gt;Refers to external identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Name &lt;/td&gt;
      &lt;td&gt;Refers to the name or username of a person and should be assigned to columns containing &quot;name&quot;, &quot;username&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Contact_Info&lt;/td&gt;
      &lt;td&gt;Refers to the contact information of a person and should be assigned to columns containing &quot;email&quot;, &quot;phone&quot;, &quot;address&quot;, &quot;social media&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Geo.Geohash&lt;/td&gt;
      &lt;td&gt;Refers to a geohash and should be assigned to columns containing &quot;geohash&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;Should be used when none of the above can be assigned to a column.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The output of the language model is typically in free text format, however, we want the output in a fixed format for downstream processing. Due to this nature, prompt engineering is a crucial component to make sure downstream workflows can process the LLM’s output.&lt;/p&gt;

&lt;p&gt;Here are some of the techniques we found useful during our development:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Articulate the requirements: The requirement of the task should be as clear as possible, LLM is only instructed to do what you ask it to do.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/en-gb/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#few-shot-learning&quot;&gt;Few-shot learning&lt;/a&gt;: By showing the example of interaction, models understand how they should respond.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/TypeChat&quot;&gt;Schema Enforcement&lt;/a&gt;: Leveraging its ability of understanding code, we explicitly provide the DTO (Data Transfer Object) schema to the model so that it understands that its output must conform to it.&lt;/li&gt;
  &lt;li&gt;Allow for confusion: In our prompt we specifically added a default tag – the LLM is instructed to output the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;None&amp;gt;&lt;/code&gt; tag when it cannot make a decision or is confused.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Regarding classification accuracy, we found that it is surprisingly accurate with its great semantic understanding. For acknowledged tables, users on average change less than one tag. Also, during an internal survey done among data owners at Grab in September 2023, 80% reported that this new tagging process helped them in tagging their data entities.&lt;/p&gt;

&lt;h3 id=&quot;publish-and-verification&quot;&gt;Publish and verification&lt;/h3&gt;

&lt;p&gt;The predictions are published to the Kafka queue to downstream data platforms. The platforms inform respective users weekly to verify the classified tags to improve the model’s correctness and to enable iterative prompt improvement. Meanwhile, we plan to remove the verification mandate for users once the accuracy reaches a certain level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/verification_message.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Verification message shown in the data platform for user to verify the tags&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since the new system was rolled out, we have successfully integrated this with Grab’s metadata management platform and production database management platform. Within a month since its rollout, we have scanned more than 20,000 data entities, averaging around 300-400 entities per day.&lt;/p&gt;

&lt;p&gt;Using a quick back-of-the-envelope calculation, we can see the significant time savings achieved through automated tagging. Assuming it takes a data owner approximately 2 minutes to classify each entity, we are saving approximately 360 man-days per year for the company. This allows our engineers and analysts to focus more on their core tasks of engineering and analysis rather than spending excessive time on data governance.&lt;/p&gt;

&lt;p&gt;The classified tags pave the way for more use cases downstream. These tags, in combination with rules provided by data privacy office in Grab, enable us to determine the sensitivity tier of data entities, which in turn will be leveraged for enforcing the Attribute-based Access Control (ABAC) policies and enforcing Dynamic Data Masking for downstream queries. To learn more about the benefits of ABAC, readers can refer to another engineering &lt;a href=&quot;https://engineering.grab.com/migrating-to-abac&quot;&gt;blog&lt;/a&gt; posted earlier.&lt;/p&gt;

&lt;p&gt;Cost wise, for the current load, it is extremely affordable contrary to common intuition. This affordability enables us to scale the solution to cover more data entities in the company.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;h3 id=&quot;prompt-improvement&quot;&gt;Prompt improvement&lt;/h3&gt;

&lt;p&gt;We are currently exploring feeding sample data and user feedback to greatly increase accuracy. Meanwhile, we are experimenting on outputting the confidence level from LLM for its own classification. With confidence level output, we would only trouble users when the LLM is uncertain of its answers. Hopefully this can remove even more manual processes in the current workflow.&lt;/p&gt;

&lt;h3 id=&quot;prompt-evaluation&quot;&gt;Prompt evaluation&lt;/h3&gt;

&lt;p&gt;To track the performance of the prompt given, we are building analytical pipelines to calculate the metrics of each version of the prompt. This will help the team better quantify the effectiveness of prompts and iterate better and faster.&lt;/p&gt;

&lt;h3 id=&quot;scaling-out&quot;&gt;Scaling out&lt;/h3&gt;

&lt;p&gt;We are also planning to scale out this solution to more data platforms to streamline governance-related metadata generation to more teams. The development of downstream applications using our metadata is also on the way. These exciting applications are from various domains such as security, data discovery, etc.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Grab recently participated in the Singapore government’s regulatory &lt;a href=&quot;https://www.imda.gov.sg/how-we-can-help/data-innovation/privacy-enhancing-technology-sandboxes&quot;&gt;sandbox&lt;/a&gt;, where we successfully demonstrated how LLMs can efficiently and effectively perform data classification, allowing Grab to compound the value of its data for innovative use cases while safeguarding sensitive information such as PII.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Jul 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-powered-data-classification</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-powered-data-classification</guid>
        
        <category>Data</category>
        
        <category>Machine Learning</category>
        
        <category>Generative AI</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Profile-guided optimisation (PGO) on Grab services</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://go.dev/doc/pgo&quot;&gt;Profile-guided optimisation (PGO)&lt;/a&gt; is a technique where CPU profile data for an application is collected and fed back into the next compiler build of Go application. The compiler then uses this CPU profile data to optimise the performance of that build by around &lt;a href=&quot;https://tip.golang.org/doc/pgo#overview&quot;&gt;2-14%&lt;/a&gt; currently (future releases could likely improve this figure further).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/high-level-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;High level view of how PGO works&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;PGO is a &lt;a href=&quot;https://docs.oracle.com/en/graalvm/jdk/22/docs/reference-manual/native-image/optimizations-and-performance/PGO/&quot;&gt;widely used technique&lt;/a&gt; that can be implemented with many programming languages. When it was released in May 2023, PGO was introduced as a preview in Go 1.20.&lt;/p&gt;

&lt;h2 id=&quot;enabling-pgoon-a-service&quot;&gt;Enabling PGO on a service&lt;/h2&gt;

&lt;h3 id=&quot;profile-the-service-to-get-pprof-file&quot;&gt;Profile the service to get pprof file&lt;/h3&gt;

&lt;p&gt;First, make sure that your service is built using Golang version v1.20 or higher, as only these versions support PGO.&lt;/p&gt;

&lt;p&gt;Next, &lt;a href=&quot;https://pkg.go.dev/net/http/pprof&quot;&gt;enable pprof&lt;/a&gt; in your service.&lt;/p&gt;

&lt;p&gt;If it’s already enabled, you can use the following command to capture a 6-minute profile and save it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp/pprof&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl &apos;http://localhost:6060/debug/pprof/profile?seconds=360&apos; -o /tmp/pprof
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;enabled-pgo-on-the-service&quot;&gt;Enabled PGO on the service&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/big-data-real-time-presto-talariadb&quot;&gt;TalariaDB&lt;/a&gt;: TalariaDB is a distributed, highly available, and low latency time-series database for Presto open sourced by Grab.&lt;/p&gt;

&lt;p&gt;It is a service that runs on an EKS cluster and is entirely managed by our team, we will use it as an example here.&lt;/p&gt;

&lt;p&gt;Since the cluster deployment relies on a Docker image, we only need to update the Docker image’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go build&lt;/code&gt; command to include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-PGO=./talaria.PGO&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;talaria.PGO&lt;/code&gt; file is a pprof profile collected from production services over a span of 360 seconds.&lt;/p&gt;

&lt;p&gt;If you’re utilising a &lt;a href=&quot;https://pkg.go.dev/plugin&quot;&gt;go plugin&lt;/a&gt;, &lt;a href=&quot;/faster-using-the-go-plugin-to-replace-Lua-VM&quot;&gt;as we do in TalariaDB&lt;/a&gt;, it’s crucial to ensure that the PGO is also applied to the plugin.&lt;/p&gt;

&lt;p&gt;Here’s our Dockerfile, with the additions to support PGO.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM arm64v8/golang:1.21 AS builder

ARG GO111MODULE=&quot;on&quot;
ARG GOOS=&quot;linux&quot;
ARG GOARCH=&quot;arm64&quot;
ENV GO111MODULE=${GO111MODULE}
ENV GOOS=${GOOS}
ENV GOARCH=${GOARCH}

RUN mkdir -p /go/src/talaria
COPY . src/talaria
#RUN cd src/talaria &amp;amp;&amp;amp; go mod download  &amp;amp;&amp;amp; go build &amp;amp;&amp;amp; test -x talaria
RUN cd src/talaria &amp;amp;&amp;amp; go mod download  &amp;amp;&amp;amp; go build -PGO=./talaria.PGO &amp;amp;&amp;amp; test -x talaria

RUN mkdir -p /go/src/talaria-plugin
COPY ./talaria-plugin  src/talaria-plugin
RUN cd src/talaria-plugin &amp;amp;&amp;amp; make plugin &amp;amp;&amp;amp; test -f talaria-plugin.so
FROM arm64v8/debian:latest AS base

RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ca-certificates &amp;amp;&amp;amp; rm -rf /var/cache/apk/*

WORKDIR /root/ 
ARG GO_BINARY=talaria
COPY  --from=builder /go/src/talaria/${GO_BINARY} .
COPY  --from=builder /go/src/talaria-plugin/talaria-plugin.so .

ADD entrypoint.sh . 
RUN mkdir /etc/talaria/ &amp;amp;&amp;amp; chmod +x /root/${GO_BINARY} /root/entrypoint.sh
ENV TALARIA_RC=/etc/talaria/talaria.rc 
EXPOSE 8027
ENTRYPOINT [&quot;/root/entrypoint.sh&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;result-on-enabling-pgo-on-one-grabx-service&quot;&gt;Result on enabling PGO on one GrabX service&lt;/h3&gt;

&lt;p&gt;It’s important to mention that the pprof utilised for PGO was not captured during peak hours and was limited to a duration of 360 seconds.&lt;/p&gt;

&lt;p&gt;Service &lt;a href=&quot;/big-data-real-time-presto-talariadb&quot;&gt;TalariaDB&lt;/a&gt; has three clusters and the time we enabled PGO for these clusters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We enabled PGO on cluster 0, and deployed on 4 Sep 11.16 AM.&lt;/li&gt;
  &lt;li&gt;We enabled PGO on cluster 1, and deployed on 5 Sep 15:00 PM.&lt;/li&gt;
  &lt;li&gt;We enabled PGO on cluster 2, and deployed on 6 Sep 16:00 PM.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The size of the instances, their quantity, and all other dependencies remained unchanged.&lt;/p&gt;

&lt;h4 id=&quot;cpu-metrics-on-cluster&quot;&gt;CPU metrics on cluster&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/cpu-before-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Cluster CPU usage before enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/cpu-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Cluster CPU usage after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It’s evident that enabling PGO resulted in at least a 10% reduction in CPU usage.&lt;/p&gt;

&lt;h4 id=&quot;memory-metrics-on-cluster&quot;&gt;Memory metrics on cluster&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/mem-before-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Memory usage of the cluster before enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/percentage-free-mem-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Percentage of free memory after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It’s clear that enabling PGO led to a reduction of at least 10GB (30%) in memory usage.&lt;/p&gt;

&lt;h4 id=&quot;volume-metrics-on-cluster&quot;&gt;Volume metrics on cluster&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/persistent-volume-usage-before-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Persistent volume usage on cluster before enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/volume-usage-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Volume usage after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Enabling PGO resulted in a reduction of at least 7GB (38%) in volume usage. This volume is utilised for storing events that are queued for ingestion.&lt;/p&gt;

&lt;h4 id=&quot;ingested-event-countcpumetrics-on-cluster&quot;&gt;Ingested event count/CPU metrics on cluster&lt;/h4&gt;

&lt;p&gt;To gauge the enhancements, I employed the metric of ingested event count per CPU unit (event count / CPU). This approach was adopted to account for the variable influx of events, which complicates direct observation of performance gains.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/count-ingested-event-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Count of ingested events on cluster after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Upon activating PGO, there was a noticeable increase in the ingested event count per CPU, rising from 1.1 million to 1.7 million, as depicted by the blue line in the cluster screenshot.&lt;/p&gt;

&lt;h2 id=&quot;how-we-enabled-pgo-on-a-catwalk-service&quot;&gt;How we enabled PGO on a Catwalk service&lt;/h2&gt;

&lt;p&gt;We also experimented with enabling PGO on certain orchestrators in a Catwalk service. This section covers our findings.&lt;/p&gt;

&lt;h3 id=&quot;enabling-pgo-on-the-test-golang-orch-tfsorchestrator&quot;&gt;Enabling PGO on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test-golang-orch-tfs&lt;/code&gt; orchestrator&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Attempt 1&lt;/em&gt;&lt;/strong&gt;: Take pprof for 59 seconds&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Just 1 pod running with a constant throughput of 420 QPS.&lt;/li&gt;
  &lt;li&gt;Load test started with a non-PGO image at 5:39 PM SGT.&lt;/li&gt;
  &lt;li&gt;Take pprof for 59 seconds.&lt;/li&gt;
  &lt;li&gt;Image with PGO enabled deployed at 5:49 PM SGT.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observation: &lt;strong&gt;CPU usage increased&lt;/strong&gt; after enabling PGO with pprof for 59 seconds.&lt;/p&gt;

&lt;p&gt;We suspected that taking pprof for just 59 seconds may not be sufficient to collect accurate metrics. Hence, we extended the duration to 6 minutes in our second attempt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Attempt 2&lt;/em&gt;&lt;/strong&gt; : Take pprof for 6 minutes&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Just 1 pod running with a constant throughput of 420 QPS.&lt;/li&gt;
  &lt;li&gt;Deployed non PGO image with custom pprof server at 6:13 PM SGT.&lt;/li&gt;
  &lt;li&gt;pprof taken at 6:19 PM SGT for 6 minutes.&lt;/li&gt;
  &lt;li&gt;Image with PGO enabled deployed at 6:29 PM SGT.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observation: &lt;strong&gt;CPU usage decreased&lt;/strong&gt; after enabling PGO with pprof for 6 minutes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/cpu-usage-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;CPU usage after enabling PGO on Catwalk&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/container-mem-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Container memory utilisation after enabling PGO on Catwalk&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Based on this experiment, we found that the impact of PGO is around 5% but the effort involved to enable PGO outweighs the impact. To enable PGO on Catwalk, we would need to create Docker images for each application through CI pipelines.&lt;/p&gt;

&lt;p&gt;Additionally, the Catwalk team would require a workaround to pass the pprof dump, which is not a straightforward task. Hence, we decided to put off the PGO application for Catwalk services.&lt;/p&gt;

&lt;h2 id=&quot;looking-into-pgo-for-monorepo-services&quot;&gt;Looking into PGO for monorepo services&lt;/h2&gt;

&lt;p&gt;From the information provided above, enabling PGO for a service requires the following support mechanisms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A pprof service, which is currently facilitated through Jenkins.&lt;/li&gt;
  &lt;li&gt;A build process that supports PGO arguments and can attach or retrieve the pprof file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For services that are hosted outside the monorepo and are self-managed, the effort required to experiment is minimal. However, for those within the monorepo, we will require support from the build process, which is currently unable to support this.&lt;/p&gt;

&lt;h2 id=&quot;conclusionlearnings&quot;&gt;Conclusion/Learnings&lt;/h2&gt;

&lt;p&gt;Enabling PGO has proven to be highly beneficial for some of our services, particularly TalariaDB. By using PGO, we’ve observed a clear reduction in both CPU usage and memory usage to the tune of approximately 10% and 30% respectively. Furthermore, the volume used for storing queued ingestion events has been reduced by a significant 38%. These improvements definitely underline the benefits and potential of utilising PGO on services.&lt;/p&gt;

&lt;p&gt;Interestingly, applying PGO resulted in an increased rate of ingested event count per CPU unit on TalariaDB, which demonstrates an improvement in the service’s efficiency.&lt;/p&gt;

&lt;p&gt;Experiments with the Catwalk service have however shown that the effort involved to enable PGO might not always justify the improvements gained. In our case, a mere 5% improvement did not appear to be worth the work required to generate Docker images for each application via CI pipelines and create a solution to pass the pprof dump.&lt;/p&gt;

&lt;p&gt;On the whole, it is evident that the applicability and benefits of enabling PGO can &lt;strong&gt;&lt;em&gt;vary across different services&lt;/em&gt;&lt;/strong&gt;. Factors such as application characteristics, current architecture, and available support mechanisms can influence when and where PGO optimisation is feasible and beneficial.&lt;/p&gt;

&lt;p&gt;Moving forward, further improvements to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go-build&lt;/code&gt; and the introduction of PGO support for monorepo services may drive greater adoption of PGO. In turn, this has the potential to deliver powerful system-wide gains that translate to faster response times, lower resource consumption, and improved user experiences. As always, the relevance and impact of adopting new technologies or techniques should be considered on a case-by-case basis against operational realities and strategic objectives.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 05 Jun 2024 00:10:10 +0000</pubDate>
        <link>https://engineering.grab.com/profile-guided-optimisation</link>
        <guid isPermaLink="true">https://engineering.grab.com/profile-guided-optimisation</guid>
        
        <category>Go</category>
        
        <category>optimisation</category>
        
        <category>experiments</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we evaluated the business impact of marketing campaigns</title>
        <description>&lt;p&gt;In a previous &lt;a href=&quot;/supporting-large-campaigns-at-scale&quot;&gt;post&lt;/a&gt;, we introduced our systems for running marketing campaigns. Although we sent millions of messages daily, we had little insight into their effectiveness. Did they engage our users with our promotions? Did they encourage more transactions and bookings?&lt;/p&gt;

&lt;p&gt;As Grab’s business expanded and the number of marketing campaigns increased, understanding the impact of these campaigns became crucial. This knowledge enables campaign managers to design more effective campaigns and avoid wasteful ones that degrade user experience.&lt;/p&gt;

&lt;p&gt;Initially, campaign managers had to consult marketing analysts to gauge the impact of campaigns. However, this approach soon proved unsustainable:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Manual analysis doesn’t scale with an increasing number of campaigns.&lt;/li&gt;
  &lt;li&gt;Different analysts might assess the business impact in slightly different ways, leading to inconsistent results over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, we recognised the need for a centralised solution allowing campaign managers to view their campaign impact analyses.&lt;/p&gt;

&lt;h2 id=&quot;marketing-attribution-model&quot;&gt;Marketing attribution model&lt;/h2&gt;

&lt;p&gt;The marketing analyst team designed a Marketing attribution model (MAM) for estimating the business impact of any campaign that sends messages to users. It quantifies business impact in terms of generated gross merchandise value (GMV), revenue, etc.&lt;/p&gt;

&lt;p&gt;Unlike traditional models that only credit the last touchpoint (i.e. the last message user reads before making a transaction), MAM offers a more nuanced view. It recognises that users are exposed to various marketing messages (emails, pushes, feeds, etc.) throughout their decision-making process. As shown in Fig 1, MAM assigns credit to each touchpoint that influences a conversion (e.g., Grab usage) based on two key factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;: Content directly related to the conversion receives a higher weightage. Imagine a user opening a GrabFood push notification before placing a food order. This push would be considered highly relevant and receive significant credit.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recency&lt;/strong&gt;: Touchpoints closer in time to the conversion hold more weight. For instance, a brand awareness email sent weeks before the purchase would be less impactful than a targeted GrabFood promotion right before the order.
By factoring in both relevance and recency, MAM avoids crediting the same touchpoint twice and provides a more accurate picture of which marketing campaigns are driving higher conversions.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/mam-business-attribution.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. How MAM does business attribution&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;While MAM is effective for comparing the impacts of different campaigns, it struggles with the assessment of a single campaign because it does not account for negative impacts. For example, consider a message stating, “Hey, don’t use Grab.” Clearly, not all messages positively impact business.&lt;/p&gt;

&lt;h2 id=&quot;hold-out-group&quot;&gt;Hold-out group&lt;/h2&gt;

&lt;p&gt;To better evaluate the impact of a single campaign, we divide targeted users into two groups:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hold-out (control): do not send any message&lt;/li&gt;
  &lt;li&gt;Treatment: send the message&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/campaign-hold-out-group.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. Campaign setup with hold-out group&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We then compare the business performance of sending versus not sending messages. For the treatment group, we ideally count only the user transactions potentially linked to the message (i.e., transactions occurring within X days of message receipt). However, since the hold-out group receives no messages, there are no equivalent metrics for comparison.&lt;/p&gt;

&lt;p&gt;The only business metrics available for the hold-out group are the aggregated totals of GMV, revenue, etc., over a given time, divided by the number of users. We must calculate the same for the treatment group to ensure a fair comparison.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/metrics-calculation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. Metrics calculation for both hold-out and treatment group&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The comparison might seem unreliable due to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The metrics are raw aggregations, lacking attribution logic.&lt;/li&gt;
  &lt;li&gt;The aggregated GMV and revenue might be skewed by other simultaneous campaigns involving the same users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, we have to admit that figuring out true business impact is difficult. All we can do is try our best to get as close to the truth as possible. To make the comparison more precise, we employed the following strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stratify the two groups, so that both groups contain roughly the same distribution of users.&lt;/li&gt;
  &lt;li&gt;Calculate statistical significance to rule out the difference caused by random factors.&lt;/li&gt;
  &lt;li&gt;Allow users to narrow down the business metrics to compare according to campaign set-up. For example, we don’t compare ride bookings if the campaign is promoting food.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Statistical significance is a common, yet important technique for evaluating the result of controlled experiments. Let’s see how it’s used in our case.&lt;/p&gt;

&lt;h3 id=&quot;statistical-significance&quot;&gt;Statistical significance&lt;/h3&gt;

&lt;p&gt;When we do an A/B testing, we cannot simply conclude that A is better than B when A’s result is better than B. The difference could be due to other random factors. If you did an A/A test, you will still see differences in the results even without doing anything different to the two groups.&lt;/p&gt;

&lt;p&gt;Statistical significance is a method to calculate the probability that the difference between two groups is really due to randomness. The lower the probability, the more confidently we can say our action is truly making some impact.&lt;/p&gt;

&lt;p&gt;In our case, to derive statistical significance, we assume:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Our hold-out and treatment group are two sets of samples drawn from two populations, A and B.&lt;/li&gt;
  &lt;li&gt;A and B are the same except that B received our message. We can’t 100% prove this, but can reasonably guess this is close to true, since we split with stratification.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Assuming the business metrics we are comparing is food GMV, the base numbers can be formulated as shown in Fig 4.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/calculate-statistical-significance.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. Formulation for calculating statistical significance&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To calculate the probability, we then use a formula derived from the central limit theorem (CLT). The mathematical derivation of the formula is beyond the scope of this post. Programmatically, we use the popular jStat library for the calculation.&lt;/p&gt;

&lt;p&gt;The calculation result of statistical significance as a special notice to the campaign owners is shown in Fig 5.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/business-impact-analysis.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 5. Display of business impact analysis with statistical significance&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;Evaluating the true business impact remains challenging. We continue to refine our methodology and address potential biases, such as the assumption that both groups are of the same distribution, which might not hold true, especially in smaller group sizes. Furthermore, consistently reserving a 10% hold-out in each campaign is impractical for some campaigns, as sometimes campaign owners require messages to reach all targeted users.&lt;/p&gt;

&lt;p&gt;We are committed to advancing our business impact evaluation solutions and will continue improving our existing solutions. We look forward to sharing more insights in future blogs.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 23 May 2024 00:10:10 +0000</pubDate>
        <link>https://engineering.grab.com/evaluate-business-impact-of-marketing-campaigns</link>
        <guid isPermaLink="true">https://engineering.grab.com/evaluate-business-impact-of-marketing-campaigns</guid>
        
        <category>Marketing</category>
        
        <category>Metrics</category>
        
        <category>Optimisation</category>
        
        <category>Statistic</category>
        
        <category>A/B Testing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
