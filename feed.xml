<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 30 Nov 2023 02:55:41 +0000</pubDate>
    <lastBuildDate>Thu, 30 Nov 2023 02:55:41 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>An elegant platform</title>
        <description>&lt;p&gt;Coban is Grab’s real-time data streaming platform team. As a platform team, we thrive on providing our internal users from all verticals with self-served data-streaming resources, such as &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; topics, &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; and &lt;a href=&quot;https://www.confluent.io/learn/change-data-capture/&quot;&gt;Change Data Capture&lt;/a&gt; (CDC) pipelines, various kinds of &lt;a href=&quot;https://docs.confluent.io/platform/current/connect/&quot;&gt;Kafka-Connect&lt;/a&gt; connectors, as well as &lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;Apache Zeppelin&lt;/a&gt; notebooks, so that they can effortlessly leverage real-time data to build intelligent applications and services.&lt;/p&gt;

&lt;p&gt;In this article, we present our journey from pure Infrastructure-as-Code (IaC) towards a more sophisticated control plane that has revolutionised the way data streaming resources are self-served at Grab. This change also leads to improved scalability, stability, security, and user adoption of our data streaming platform.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;In the early ages of public cloud, it was a common practice to create virtual resources by clicking through the web console of a cloud provider, which is sometimes referred to as &lt;em&gt;ClickOps&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;ClickOps&lt;/em&gt; has many downsides, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inability to review, track, and audit changes to the infrastructure.&lt;/li&gt;
  &lt;li&gt;Inability to massively scale the infrastructure operations.&lt;/li&gt;
  &lt;li&gt;Inconsistencies between environments, e.g. staging and production.&lt;/li&gt;
  &lt;li&gt;Inability to quickly recover from a disaster by re-creating the infrastructure at a different location.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That said, &lt;em&gt;ClickOps&lt;/em&gt; has one tremendous advantage; it makes creating resources using a graphical User Interface (UI) fairly easy for anyone like Infrastructure Engineers, Software Engineers, Data Engineers etc. This also leads to a high iteration speed towards innovation in general.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
IaC resolved many of the limitations of &lt;em&gt;ClickOps&lt;/em&gt;, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Changes are committed to a Version Control System (VCS) like Git: They can be reviewed by peers before being merged. The full history of all changes is available for investigating issues and for audit.&lt;/li&gt;
  &lt;li&gt;The infrastructure operations scale better: Code for similar pieces of infrastructure can be modularised. Changes can be rolled out automatically by Continuous Integration (CI) pipelines in the VCS system, when a change is merged to the main branch.&lt;/li&gt;
  &lt;li&gt;The same code can be used to deploy the staging and production environments consistently.&lt;/li&gt;
  &lt;li&gt;The infrastructure can be re-created anytime from its source code, in case of a disaster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, IaC unwittingly posed a new entry barrier too, requiring the learning of new tools like Ansible, Puppet, Chef, Terraform, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Some organisations set up dedicated Site Reliability Engineer (SRE) teams to centrally manage, operate, and support those tools and the infrastructure as a whole, but that soon created the potential of new bottlenecks in the path to innovation.&lt;/p&gt;

&lt;p&gt;On the other hand, others let engineering teams manage their own infrastructure, and Grab adopted that same approach. We use &lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt; to manage infrastructure, and all teams are expected to have select engineers who have received Terraform training and have a clear understanding of it.&lt;/p&gt;

&lt;p&gt;In this context, Coban’s platform initially started as a handful of Git repositories where users had to submit their Merge Requests (MR) of Terraform code to create their data streaming resources. Once reviewed by a Coban engineer, those Terraform changes would be applied by a CI pipeline running &lt;a href=&quot;https://www.runatlantis.io/&quot;&gt;Atlantis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
While this was a meaningful first step towards self-service and platformisation of Coban’s offering within Grab, it had several significant downsides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Due to the lack of control on the Terraform changes, the CI pipeline was prone to human errors and frequent failures. For example, users would initiate a new Terraform project by duplicating an existing one, but then would forget to change the location of the remote Terraform state, leading to the in-place replacement of an existing resource.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The Coban team needed to review all MRs and provide ad hoc support whenever the pipeline failed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: In the absence of Identity and Access Management (IAM), MRs could potentially contain changes pertaining to other teams’ resources, or even changes to Coban’s core infrastructure, with code review as the only guardrail.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited user growth&lt;/strong&gt;: We could only acquire users who were well-versed in Terraform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It soon became clear that we needed to build a layer of abstraction between our users and the Terraform code, to increase the level of control and lower the entry barrier to our platform, while still retaining all of the benefits of IaC under the hood.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;We designed and built an in-house three-tier control plane made of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Coban UI&lt;/strong&gt;, a front-end web interface, providing our users with a seamless ClickOps experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Heimdall&lt;/strong&gt;, the Go back-end of the web interface, transforming ClickOps into IaC.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Khone&lt;/strong&gt;, the storage and provisioner layer, a Git repository storing Terraform code and metadata of all resources as well as the CI pipelines to plan and apply the changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the next sections, we will deep dive in those three components.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 1 Simplified architecture of a request flowing from the user to the Coban infrastructure, via the three components of the control plane: the Coban UI, Heimdall, and Khone.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Although we designed the user journey to start from the Coban UI, our users can still opt to communicate with Heimdall and with Khone directly, e.g. for batch changes, or just because many engineers love Git and we want to encourage broad adoption. To make sure that data is eventually consistent across the three systems, we made Khone the only persistent storage layer. Heimdall regularly fetches data from Khone, caches it, and presents it to the Coban UI upon each query.&lt;/p&gt;

&lt;p&gt;We also continued using Terraform for all resources, instead of mixing various declarative infrastructure approaches (e.g. Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&quot;&gt;Custom Resource Definition&lt;/a&gt;, &lt;a href=&quot;https://helm.sh/docs/topics/charts/&quot;&gt;Helm charts&lt;/a&gt;), for the sake of consistency of the logic in Khone’s CI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;coban-ui&quot;&gt;Coban UI&lt;/h3&gt;

&lt;p&gt;The Coban UI is a &lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt; &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Glossary/SPA&quot;&gt;Single Page Application&lt;/a&gt; (React SPA) designed by our partner team Chroma, a dedicated team of front-end engineers who thrive on building legendary UIs and reusable components for platform teams at Grab.&lt;/p&gt;

&lt;p&gt;It serves as a comprehensive self-service portal, enabling users to effortlessly create data streaming resources by filling out web forms with just a few clicks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image7.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 2 Screen capture of a new Kafka topic creation in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to facilitating resource creation and configuration, the Coban UI is seamlessly integrated with multiple monitoring systems. This integration allows for real-time monitoring of critical metrics and health status for Coban infrastructure components, including Kafka clusters, Kafka topic bytes in/out rates, and more. Under the hood, all this information is exposed by Heimdall APIs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 3 Screen capture of the metrics of a Kafka cluster in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In terms of infrastructure, the Coban UI is hosted in &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html&quot;&gt;AWS S3 website hosting&lt;/a&gt;. All dynamic content is generated by querying the APIs of the back-end: Heimdall.&lt;/p&gt;

&lt;h3 id=&quot;heimdall&quot;&gt;Heimdall&lt;/h3&gt;

&lt;p&gt;Heimdall is the Go back-end of the Coban UI. It serves a collection of APIs for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Managing the data streaming resources of the Coban platform with Create, Read, Update and Delete (CRUD) operations, treating the Coban UI as a first-class citizen.&lt;/li&gt;
  &lt;li&gt;Exposing the metadata of all Coban resources, so that they can be used by other platforms or searched in the Coban UI.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All operations are authenticated and authorised. Read more about Heimdall’s access control in &lt;a href=&quot;/migrating-to-abac&quot;&gt;Migrating from Role to Attribute-based Access Control&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the next sections, we are going to dive deeper into these two features.&lt;/p&gt;

&lt;h4 id=&quot;managing-the-data-streaming-resources&quot;&gt;Managing the data streaming resources&lt;/h4&gt;

&lt;p&gt;First and foremost, Heimdall enables our users to self-manage their data streaming resources. It primarily relies on Khone as its storage and provisioner layer for actual resource management via Git CI pipelines. Therefore, we designed Heimdall’s resource management workflow to leverage the underlying Git flow.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 4 Diagram flow of a request in Heimdall.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 4 shows the diagram flow of a typical request in Heimdall to create, update, or delete a resource.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An authenticated user initiates a request, either by navigating in the Coban UI or by calling the Heimdall API directly. At this stage, the request state is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Initiated&lt;/code&gt; on Heimdall.&lt;/li&gt;
  &lt;li&gt;Heimdall validates the request against multiple validation rules. For example, if an ongoing change request exists for the same resource, the request fails. If all tests succeed, the request state moves to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ongoing&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Heimdall then creates an MR in Khone, which contains the Terraform files describing the desired state of the resource, as well as an in-house metadata file describing the key attributes of both resource and requester.&lt;/li&gt;
  &lt;li&gt;After the MR has been created successfully, Heimdall notifies the requester via Slack and shares the MR URL.&lt;/li&gt;
  &lt;li&gt;After that, Heimdall starts polling the status of the MR in a loop.&lt;/li&gt;
  &lt;li&gt;For changes pertaining to production resources, an approver who is code owner in the repository of the resource has to approve the MR. Typically, the approver is an immediate teammate of the requester. Indeed, as a platform team, we empower our users to manage their own resources in a self-service fashion. Ultimately, the requester would merge the MR to trigger the CI pipeline applying the actual Terraform changes. Note that for staging resources, this entire step 6 is automatically performed by Heimdall.&lt;/li&gt;
  &lt;li&gt;Depending on the MR status and the status of its CI pipeline in Khone, the final state of the request can be:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Failed&lt;/code&gt; if the CI pipeline has failed in Khone.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Completed&lt;/code&gt; if the CI pipeline has succeeded in Khone.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cancelled&lt;/code&gt; if the MR was closed in Khone.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Heimdall exposes APIs to let users track the status of their requests. In the Coban UI, a page queries those APIs to elegantly display the requests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 5 Screen capture of the Coban UI showing all requests.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;exposing-the-metadata&quot;&gt;Exposing the metadata&lt;/h4&gt;

&lt;p&gt;Apart from managing the data streaming resources, Heimdall also centralises and exposes the metadata pertaining to those resources so other Grab systems can fetch and use it. They can make various queries, for example, listing the producers and consumers of a given Kafka topic, or determining if a database (DB) is the data source for any CDC pipeline.&lt;/p&gt;

&lt;p&gt;To make this happen, Heimdall not only retains the metadata of all of the resources that it creates, but also regularly ingests additional information from a variety of upstream systems and platforms, to enrich and make this metadata comprehensive.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 6 Diagram showing some of Heimdall's upstreams (on the left) and downstreams (on the right) for metadata collection, enrichment, and serving. The arrows show the data flow. The network connection (client -&amp;gt; server) is actually the other way around.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
On the left side of Fig. 6, we illustrate Heimdall’s ingestion mechanism with several examples (step 1):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The metadata of all Coban resources is ingested from Khone. This means the metadata of the resources that were created directly in Khone is also available in Heimdall.&lt;/li&gt;
  &lt;li&gt;The list of Kafka producers is retrieved from our monitoring platform, where most of them emit metrics.&lt;/li&gt;
  &lt;li&gt;The list of Kafka consumers is retrieved directly from the respective Kafka clusters, by listing the &lt;a href=&quot;https://docs.confluent.io/platform/current/clients/consumer.html#consumer-groups&quot;&gt;consumer groups&lt;/a&gt; and respective &lt;a href=&quot;https://developer.confluent.io/faq/apache-kafka/kafka-clients/#kafka-clients-what-is-clientid-in-kafka&quot;&gt;Client IDs&lt;/a&gt; of each partition.&lt;/li&gt;
  &lt;li&gt;The metadata of all DBs, that are used as a data source for CDC pipelines, is fetched from Grab’s internal DB management platform.&lt;/li&gt;
  &lt;li&gt;The Kafka stream schemas are retrieved from the Coban schema repository.&lt;/li&gt;
  &lt;li&gt;The Kafka stream configuration of each stream is retrieved from Grab Universal Configuration Management platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all of this ingested data, Heimdall can provide comprehensive and accurate information about all data streaming resources to any other Grab platforms via a set of dedicated APIs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The right side of Fig. 6 shows some examples (step 2) of Heimdall’s serving mechanism:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As a downstream of Heimdall, the Coban UI enables our direct users to conveniently browse their data streaming resources and access their attributes.&lt;/li&gt;
  &lt;li&gt;The entire resource inventory is ingested into the broader Grab inventory platform, based on &lt;a href=&quot;https://backstage.io/&quot;&gt;backstage.io&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The Kafka streams are ingested into Grab’s internal data discovery platform, based on &lt;a href=&quot;https://datahubproject.io/&quot;&gt;DataHub&lt;/a&gt;, where users can discover and trace the lineage of any piece of data.&lt;/li&gt;
  &lt;li&gt;The CDC connectors pertaining to DBs are ingested by Grab internal DB management platform, so that they are made visible in that platform when users are browsing their DBs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the downstream platforms that ingest data from Heimdall each expose a particular view of the Coban inventory that serves their purpose, but the Coban platform remains the only source of truth for any data streaming resource at Grab.&lt;/p&gt;

&lt;p&gt;Lastly, Heimdall leverages an internal MySQL DB to support quick data query and exploration. The corresponding API is called by the Coban UI to let our users conveniently search globally among all resources’ attributes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 7 Screen capture of the global search feature in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;khone&quot;&gt;Khone&lt;/h3&gt;

&lt;p&gt;Khone is the persistent storage layer of our platform, as well as the executor for actual resource creation, changes, and deletion. Under the hood, it is actually a GitLab repository of Terraform code in typical &lt;a href=&quot;https://about.gitlab.com/topics/gitops/&quot;&gt;GitOps&lt;/a&gt; fashion, with CI pipelines to plan and apply the Terraform changes automatically. In addition, it also stores a metadata file for each resource.&lt;/p&gt;

&lt;p&gt;Compared to letting the platform create the infrastructure directly and keep track of the desired state in its own way, relying on a standard IaC tool like Terraform for the actual changes to the infrastructure presents two major advantages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The Terraform code can directly be used for disaster recovery. In case of a disaster, any entitled Cobaner with a local copy of the main branch of the Khone repository is able to recreate all our platform resources directly from their machine. There is no need to rebuild the entire platform’s control plane, thus reducing our Recovery Time Objective (RTO).&lt;/li&gt;
  &lt;li&gt;Minimal effort required to follow the API changes of our infrastructure ecosystem (AWS, Kubernetes, Kafka, etc.). When such a change happens, all we need to do is to update the corresponding Terraform provider.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’d like to read more about Khone, check out &lt;a href=&quot;/securing-gitops-pipeline&quot;&gt;Securing GitOps pipelines&lt;/a&gt;. In this section, we will only focus on Khone’s features that are relevant from the platform perspective.&lt;/p&gt;

&lt;h4 id=&quot;lightweightterraform&quot;&gt;Lightweight Terraform&lt;/h4&gt;

&lt;p&gt;In Khone, each resource is stored as a Terraform definition. There are two major differences from a normal Terraform project:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No Terraform environment, such as the required Terraform providers and the location of the remote Terraform state file. They are automatically generated by the CI pipeline via a simple wrapper.&lt;/li&gt;
  &lt;li&gt;Only vetted Khone Terraform modules can be used. This is controlled and enforced by the CI pipeline via code inspection. There is one such Terraform module for each kind of supported resource of our platform (e.g. Kafka topic, Flink pipeline, Kafka Connect mirror source connector etc.). Furthermore, those in-house Terraform modules are designed to automatically derive their key variables (e.g. resource name, cluster name, environment) from the relative path of the parent Terraform project in the Khone repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those characteristics are designed to limit the risk and blast radius of human errors. They also make sure that all resources created in Khone are supported by our platform, so that they can also be discovered and managed in Heimdall and the Coban UI. Lastly, by generating the Terraform environment on the fly, we can destroy resources simply by deleting the directory of the project in the code base – this would not be possible otherwise.&lt;/p&gt;

&lt;h4 id=&quot;resource-metadata&quot;&gt;Resource metadata&lt;/h4&gt;

&lt;p&gt;All resource metadata is stored in a YAML file that is present in the Terraform directory of each resource in the Khone repository. This is mainly used for ownership and cost attribution.&lt;/p&gt;

&lt;p&gt;With this metadata, we can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Better communicate with our users whenever their resources are impacted by an incident or an upcoming maintenance operation.&lt;/li&gt;
  &lt;li&gt;Help teams understand the costs of their usage of our platform, a significant step towards cost efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two different ways resource metadata can be created:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Automatically through Heimdall: The YAML metadata file is automatically generated by Heimdall.&lt;/li&gt;
  &lt;li&gt;Through Khone by a human user: The user needs to prepare the YAML metadata file and include it in the MR. This file is then verified by the CI pipeline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outcome&quot;&gt;Outcome&lt;/h2&gt;

&lt;p&gt;The initial version of the three-tier Coban platform, as described in this article, was internally released in March 2022, supporting only Kafka topic management at the time. Since then, we have added support for Flink pipelines, four kinds of Kafka Connect connectors, CDC pipelines, and more recently, Apache Zeppelin notebooks. At the time of writing, the Coban platform manages about 5000 data streaming resources, all described as IaC under the hood.&lt;/p&gt;

&lt;p&gt;Our platform also exposes enriched metadata that includes the full data lineage from Kafka producers to Kafka consumers, as well as ownership information, and cost attribution.&lt;/p&gt;

&lt;p&gt;With that, our monthly active users have almost quadrupled, truly moving the needle towards democratising the usage of real-time data within all Grab verticals.&lt;/p&gt;

&lt;p&gt;In spite of that user growth, the end-to-end workflow success rate for self-served resource creation, change or deletion, remained well above 90% in the first half of 2023, while the Heimdall API uptime was above 99.95%.&lt;/p&gt;

&lt;h2 id=&quot;challenges-faced&quot;&gt;Challenges faced&lt;/h2&gt;

&lt;p&gt;A common challenge for platform teams resides in the misalignment between the Service Level Objective (SLO) of the platform, and the various environments (e.g. staging, production) of the managed resources and upstream/downstream systems and platforms.&lt;/p&gt;

&lt;p&gt;Indeed, the platform aims to guarantee the same level of service, regardless of whether it is used to create resources in the staging or the production environment. From the platform team’s perspective, the platform as a whole is considered production-grade, as soon as it serves actual users.&lt;/p&gt;

&lt;p&gt;A naive approach to address this challenge is to let the production version of the platform manage all resources regardless of their respective environments. However, doing so does not permit a hermetic segregation of the staging and production environments across the organisation, which is a good security practice, and often a requirement for compliance. For example, the production version of the platform would have to connect to upstream systems in the staging environment, e.g. staging Kafka clusters to collect their consumer groups, in the case of Heimdall. Conversely, the staging version of certain downstreams would have to connect to the production version of Heimdall, to fetch the metadata of relevant staging resources.&lt;/p&gt;

&lt;p&gt;The alternative approach, generally adopted across Grab, is to instantiate all platforms in each environment (staging and production), while still considering both instances as production-grade and guaranteeing tight SLOs in both environments.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 8 Architecture of the Coban platform, broken down by environment.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Fig. 8, both instances of Heimdall have equivalent SLOs. The caveat is that all upstream systems and platforms must also guarantee a strict SLO in both environments. This obviously comes with a cost, for example, tighter maintenance windows for the operations pertaining to the Kafka clusters in the staging environment.&lt;/p&gt;

&lt;p&gt;A strong “platform” culture is required for platform teams to fully understand that their instance residing in the staging environment is not their own staging environment and should not be used for testing new features.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Currently, users creating, updating, or deleting production resources in the Coban UI (or directly by calling Heimdall API) receive the URL of the generated GitLab MR in a Slack message. From there, they must get the MR approved by a code owner, typically another team member, and finally merge the MR, for the requested change to be actually implemented by the CI pipeline.&lt;/p&gt;

&lt;p&gt;Although this was a fairly easy way to implement a maker/checker process that was immediately compliant with our regulatory requirements for any changes in production, the user experience is not optimal. In the near future, we plan to bring the approval mechanism into Heimdall and the Coban UI, while still providing our more advanced users with the option to directly create, approve, and merge MRs in GitLab. In the longer run, we would also like to enhance the Coban UI with the output of the Khone CI jobs that include the Terraform plan and apply results.&lt;/p&gt;

&lt;p&gt;There is another aspect of the platform that we want to improve. As Heimdall regularly polls the upstream platforms to collect their metadata, this introduces a latency between a change in one of those platforms and its reflection in the Coban platform, which can hinder the user experience. To refresh resource metadata in Heimdall in near real time, we plan to leverage an existing Grab-wide event stream, where most of the configuration and code changes at Grab are produced as events. Heimdall will soon be able to consume those events and update the metadata of the affected resources immediately, without waiting for the next periodic refresh.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/an-elegant-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/an-elegant-platform</guid>
        
        <category>Data</category>
        
        <category>Data streaming</category>
        
        <category>Real-time streaming</category>
        
        <category>Platformisation</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Road localisation in GrabMaps</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2022, Grab achieved self-sufficiency in its Geo services. As part of this transition, one crucial step was moving towards using an internally-developed map tailored specifically to the market in which Grab operates. Now that we have full control over the map layer, we can add more data to it or improve it according to the needs of the services running on top. One key aspect that this transition unlocked for us was the possibility of creating hyperlocal data at map level.&lt;/p&gt;

&lt;p&gt;For instance, by determining the country to which a road belongs, we can now automatically infer the official language of that country and display the street name in that language. In another example, knowing the country for a specific road, we can automatically infer the driving side (left-handed or right-handed) leading to an improved navigation experience. Furthermore, this capability also enables us to efficiently handle various scenarios. For example, if we know that a road is part of a gated community, an area where our driver partners face restricted access, we can prevent the transit through that area.&lt;/p&gt;

&lt;p&gt;These are just some examples of the possibilities from having full control over the map layer. By having an internal map, we can align our maps with specific markets and provide better experiences for our driver-partners and customers.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;For all these to be possible, we first needed to localise the roads inside the map. Our goal was to include hyperlocal data into the map, which refers to data that is specific to a certain area, such as a country, city, or even a smaller part of the city like a gated community. At the same time, we aimed to deliver our map with a high cadence, thus, we needed to find the right way to process this large amount of data while continuing to create maps in a cost-effective manner.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;In the following sections of this article, we will use an extract from the Southeast Asia map to provide visual representations of the concepts discussed.&lt;/p&gt;

&lt;p&gt;In Figure 1, Image 1 shows a visualisation of the road network, the roads belonging to this area. The coloured lines in Image 2 represent the borders identifying the countries in the same area. Overlapping the information from Image 1 and Image 2, we can extrapolate and say that the entire surface included in a certain border could have the same set of common properties as shown in Image 3. In Image 4, we then proceed with adding localised roads for each area.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/localisation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Map of Southeast Asia&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For this to be possible, we have to find a way to localise each road and identify its associated country. Once this localisation process is complete, we can replicate all this information specific to a given border onto each individual road. This information includes details such as the country name, driving side, and official language. We can go even further and infer more information, and add hyperlocal data. For example, in Vietnam, we can automatically prevent motorcycle access on the motorways.&lt;/p&gt;

&lt;p&gt;Assigning each road on the map to a specific area, such as a country, service area, or subdivision, presents a complex task. So, how can we efficiently accomplish this?&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;The most straightforward approach would be to test the inclusion of each road into each area boundary, but that is easier said than done. With close to 30 million road segments in the Southeast Asia map and over 10 thousand areas, the computational cost of determining inclusion or intersection between a polyline and a polygon is expensive.&lt;/p&gt;

&lt;p&gt;Our solution to this challenge involves replacing the expensive yet precise operation with a decent approximation. We introduce a proxy entity, the geohash, and we use it to approximate the areas and also to localise the roads.&lt;/p&gt;

&lt;p&gt;We replace the geometrical inclusion with a series of simpler and less expensive operations. First, we conduct an inexpensive precomputation where we identify all the geohases that belong to a certain area or within a defined border. We then identify the geohashes to which the roads  belong to. Finally, we use these precomputed values to assign roads to their respective areas. This process is also computationally inexpensive.&lt;/p&gt;

&lt;p&gt;Given the large area we process, we leverage big data techniques to distribute the execution across multiple nodes and thus speed up the operation. We want to deliver the map daily and this is one of the many operations that are part of the map-making process.&lt;/p&gt;

&lt;h3 id=&quot;what-is-a-geohash&quot;&gt;What is a geohash?&lt;/h3&gt;

&lt;p&gt;To further understand our implementation we will first explain the &lt;a href=&quot;https://en.wikipedia.org/wiki/Geohash&quot;&gt;geohash concept&lt;/a&gt;. A geohash is a unique identifier of a specific region on the Earth. The basic idea is that the Earth is divided into regions of user-defined size and each region is assigned a unique id, which is known as its geohash. For a given location on earth, the geohash algorithm converts its latitude and longitude into a string.&lt;/p&gt;

&lt;p&gt;Geohashes uses a Base-32 alphabet encoding system comprising characters ranging from  0 to 9 and A to Z, excluding “A”, “I”, “L” and “O”. Imagine dividing the world into a grid with 32 cells. The first character in a geohash identifies the initial location of one of these 32 cells. Each of these cells are then further subdivided into 32 smaller cells.This subdivision process continues and refines to specific areas in the world. Adding characters to the geohash sub-divides a cell, effectively zooming in to a more detailed area.&lt;/p&gt;

&lt;p&gt;The precision factor of the geohash determines the size of the cell. For instance, a precision factor of one creates a cell 5,000 km high and 5,000 km wide. A precision factor of six creates a cell 0.61km high and 1.22 km wide. Furthermore, a precision factor of nine creates a cell 4.77 m high and 4.77 m wide. It is important to note that cells are not always square and can have varying dimensions.&lt;/p&gt;

&lt;p&gt;In Figure 2,  we have exemplified a geohash 6 grid and its code is &lt;strong&gt;wsdt33&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-code-wsdt33.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - An example of geohash code wsdt33&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;using-less-expensive-operations&quot;&gt;Using less expensive operations&lt;/h3&gt;

&lt;p&gt;Calculating the inclusion of the roads inside a certain border is an expensive operation. However, quantifying the exact expense is challenging as it depends on several factors. One factor is the complexity of the border. Borders are usually irregular and very detailed, as they need to correctly reflect the actual border. The complexity of the road geometry is another factor that plays an important role as roads are not always straight lines.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/roads-to-localise.png&quot; alt=&quot;&quot; style=&quot;width:30%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Roads to localise&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Since this operation is expensive both in terms of cloud cost and time to run, we need to identify a cheaper and faster way that would yield similar results. Knowing that the complexity of the border lines is the cause of the problem, we tried using a different alternative, a rectangle. Calculating the inclusion of a polyline inside a rectangle is a cheaper operation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/roads-inside-rectangle.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Roads inside a rectangle&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;So we transformed this large, one step operation, where we test each road segment for inclusion in a border, into a series of smaller operations where we perform the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify all the geohashes that are part of a certain area or belong to a certain border. In this process we include additional areas to make sure that we cover the entire surface inside the border.&lt;/li&gt;
  &lt;li&gt;For each road segment, we identify the list of geohashes that it belongs to. A road, depending on its length or depending on its shape, might belong to multiple geohashes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In Figure 5, we identify that the road belongs to two geohashes and that the two geohashes are part of the border we use.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-proxy.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Geohashes as proxy&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Now, all we need to do is join the two data sets together. This kind of operation is a great candidate for a big data approach, as it allows us to run it in parallel and speed up the processing time.&lt;/p&gt;

&lt;h2 id=&quot;precision-tradeoff&quot;&gt;Precision tradeoff&lt;/h2&gt;

&lt;p&gt;We mentioned earlier that, for the sake of argument, we replace precision with a decent approximation. Let’s now delve into the real tradeoff by adopting this approach.&lt;/p&gt;

&lt;p&gt;The first thing that stands out with this approach is that we traded precision for cost. We are able to reduce the cost as this approach uses less hardware resources and computation time. However, this reduction in precision suffers, particularly for roads located near the borders as they might be wrongly classified.&lt;/p&gt;

&lt;p&gt;Going back to the initial example, let’s take the case of the external road, on the left side of the area. As you can see in Figure 6, it is clear that the road does not belong to our border. But when we apply the geohash approach it gets included into the middle geohash.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/wrong-road-localisation.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Wrong road localisation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Given that just a small part of the geohash falls inside the border, the entire geohash will be classified as belonging to that area, and, as a consequence, the road that belongs to that geohash will be wrongly localised and we’ll end up adding the wrong localisation information to that road. This is clearly a consequence of the precision tradeoff. So, how can we solve this?&lt;/p&gt;

&lt;h3 id=&quot;geohash-precision&quot;&gt;Geohash precision&lt;/h3&gt;

&lt;p&gt;One option is to increase the geohash precision. By using smaller and smaller geohashes, we can better reflect the actual area. As we go deeper and we further split the geohash, we can accurately follow the border. However, a high geohash precision also equates to a computationally intensive operation bringing us back to our initial situation. Therefore, it is crucial to find the right balance between the geohash size and the complexity of operations.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-precision.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Geohash precision&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;geohash-coverage-percentage&quot;&gt;Geohash coverage percentage&lt;/h3&gt;

&lt;p&gt;To find a balance between precision and data loss, we looked into calculating the geohash coverage percentage. For example, in Figure 8, the blue geohash is entirely within the border.  Here we can say that it has a 100% geohash coverage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-inside-border.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8 - Geohash inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However, take for example the geohash in Figure 9. It touches the border and has only around 80% of its surface inside the area. Given that most of its surface is within the border, we still can say that it belongs to the area.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-partial-border.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9 - Geohash partially inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Let’s look at another example. In Figure 10, only a small part of the geohash is within the border. We can say that the geohash coverage percentage here is around 5%. For these cases, it becomes difficult for us to determine whether the geohash does belong to the area. What would be a good tradeoff in this case?&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-barely-border.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10 - Geohash barely inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;border-shape&quot;&gt;Border shape&lt;/h3&gt;

&lt;p&gt;To go one step further, we can consider a mixed solution, where we use the border shape but only for the geohashes touching the border. This would still be an intensive computational operation but the number of roads located in these geohashes will be much smaller, so it is still a gain.&lt;/p&gt;

&lt;p&gt;For the geohashes with full coverage inside the area, we’ll use the geohash for the localisation, the simpler operation. For the geohashes that are near the border, we’ll use a different approach. To increase the precision around the borders, we can cut the geohash following the  border’s shape. Instead of having a rectangle, we’ll use a more complex shape which is still simpler than the initial border shape.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-border-shape.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 11 - Geohash following a border’s shape&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;We began with a simple approach and we enhanced it to improve precision. This also increased the complexity of the operation. We then asked, what are the actual gains? Was it worthwhile to go through all this process? In this section, we put this to the test.&lt;/p&gt;

&lt;p&gt;We first created a benchmark by taking a small sample of the data and ran the localisation process on a laptop. The sample comprised approximately 2% of the borders and 0.0014% of the roads. We ran the localisation process using two approaches.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;With the first approach, we calculated the intersection between all the roads and borders. The entire operation took around 38 minutes.&lt;/li&gt;
  &lt;li&gt;For the second approach, we optimised the operation using geohashes. In this approach, the runtime was only 78 seconds (1.3 minutes).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, it is important to note that this is not an apples-to-apples comparison. The operation that we measured was the localisation of the roads but we did not include the border filling operation where we fill the borders with geohashes. This is because this operation does not need to be run every time. It can be run once and reused multiple times.&lt;/p&gt;

&lt;p&gt;Though not often required, it is still crucial to understand and consider the operation of precomputing areas and filling borders with geohashes. The precomputation process depends on several factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number and shape of the borders - The more borders and the more complex the borders are, the longer the operation will take.&lt;/li&gt;
  &lt;li&gt;Geohash precision - How accurate do we need our localisation to be? The more accurate it needs to be, the longer it will take.&lt;/li&gt;
  &lt;li&gt;Hardware availability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Going back to our hypothesis, although this precomputation might be expensive, it is rarely run as the borders don’t change often and can be triggered only when needed. However, regular computation, where we find the area to which each road belongs to, is often run as the roads change constantly. In our system, we run this localisation for each map processing.&lt;/p&gt;

&lt;p&gt;We can also further optimise this process by applying the opposite approach. Geohashes that have full coverage inside a border can be merged together into larger geohashes thus simplifying the computation inside the border. In the end, we can have a solution that is fully optimised for our needs with the best cost-to-performance ratio.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/optimised-geohash.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 12 - Optimised geohashes&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Although geohashes seem to be the right solution for this kind of problem, we also need to monitor their content. One consideration is the road density inside a geohash. For example, a geohash inside a city centre usually has a lot of roads while one in the countryside may have much less. We need to consider this aspect to have a balanced computation operation and take full advantage of the big data approach. In our case, we achieve this balance by considering the number of road kilometres within a geohash.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/unbalanced-data.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 13 - Unbalanced data&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, the resources that we choose also matter. To optimise time and cost, we need to find the right balance between the running time and resource cost. As shown in Figure 14, based on a sample data we ran, sometimes, we get the best result when using smaller machines.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/cost-vs-runtime.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 14 - Cost vs runtime&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;The achievements and insights showcased in this article are indebted to the contributions made by Mihai Chintoanu. His expertise and collaborative efforts have profoundly enriched the content and findings presented herein.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Fri, 17 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/road-localisation-grabmaps</link>
        <guid isPermaLink="true">https://engineering.grab.com/road-localisation-grabmaps</guid>
        
        <category>Maps</category>
        
        <category>Data</category>
        
        <category>Big Data</category>
        
        <category>Data processing</category>
        
        <category>Hyperlocalisation</category>
        
        <category>GrabMaps</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Graph modelling guidelines</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Graph modelling is a highly effective technique for representing and analysing complex and interconnected data across various domains. By deciphering relationships between entities, graph modelling can reveal insights that might be otherwise difficult to identify using traditional data modelling approaches. In this article, we will explore what graph modelling is and guide you through a step-by-step process of implementing graph modelling to create a social network graph.&lt;/p&gt;

&lt;h2 id=&quot;what-is-graph-modelling&quot;&gt;What is graph modelling?&lt;/h2&gt;

&lt;p&gt;Graph modelling is a method for representing real-world entities and their relationships using nodes, edges, and properties. It employs graph theory, a branch of mathematics that studies graphs, to visualise and analyse the structure and patterns within complex datasets. Common applications of graph modelling include social network analysis, recommendation systems, and biological networks.&lt;/p&gt;

&lt;h2 id=&quot;graph-modelling-process&quot;&gt;Graph modelling process&lt;/h2&gt;

&lt;h3 id=&quot;step-1-define-your-domain&quot;&gt;Step 1: Define your domain&lt;/h3&gt;
&lt;p&gt;Before diving into graph modelling, it’s crucial to have a clear understanding of the domain you’re working with. This involves getting acquainted with the relevant terms, concepts, and relationships that exist in your specific field. To create a social network graph, familiarise yourself with terms like users, friendships, posts, likes, and comments.&lt;/p&gt;

&lt;h3 id=&quot;step-2-identify-entities-and-relationships&quot;&gt;Step 2: Identify entities and relationships&lt;/h3&gt;
&lt;p&gt;After defining your domain, you need to determine the entities (nodes) and relationships (edges) that exist within it. Entities are the primary objects in your domain, while relationships represent how these entities interact with each other. In a social network graph, users are entities, and friendships are relationships.&lt;/p&gt;

&lt;h3 id=&quot;step-3-establish-properties&quot;&gt;Step 3: Establish properties&lt;/h3&gt;
&lt;p&gt;Each entity and relationship may have a set of properties that provide additional information. In this step, identify relevant properties based on their significance to the domain. A user entity might have properties like name, age, and location. A friendship relationship could have a ‘since’ property to denote the establishment of the friendship.&lt;/p&gt;

&lt;h3 id=&quot;step-4-choose-a-graph-model&quot;&gt;Step 4: Choose a graph model&lt;/h3&gt;
&lt;p&gt;Once you’ve identified the entities, relationships, and properties, it’s time to choose a suitable graph model. Two common models are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Property graph&lt;/strong&gt;: A versatile model that easily accommodates properties on both nodes and edges. It’s well-suited for most applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Description Framework (RDF)&lt;/strong&gt;: A World Wide Web Consortium (W3C) standard model, using triples of subject-predicate-object to represent data. It is commonly used in semantic web applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a social network graph, a property graph model is typically suitable. This is because user entities have many attributes and features. Property graphs provide a clear representation of the relationships between people and their attribute profiles.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-modelling-guidelines/graph.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Social network graph&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;step-5-develop-a-schema&quot;&gt;Step 5: Develop a schema&lt;/h3&gt;
&lt;p&gt;Although not required, developing a schema can be helpful for large-scale projects and team collaborations. A schema defines the structure of your graph, including entity types, relationships, and properties. In a social network graph, you might have a schema that specifies the types of nodes (users, posts) and the relationships between them (friendships, likes, comments).&lt;/p&gt;

&lt;h3 id=&quot;step-6-import-or-generate-data&quot;&gt;Step 6: Import or generate data&lt;/h3&gt;
&lt;p&gt;Next, acquire the data needed to populate your graph. This can come in the form of existing datasets or generated data from your application. For a social network graph, you can import user information from a CSV file and generate simulated friendships, posts, likes, and comments.&lt;/p&gt;

&lt;h3 id=&quot;step-7-implement-the-graph-using-a-graph-database-or-other-storage-options&quot;&gt;Step 7: Implement the graph using a graph database or other storage options&lt;/h3&gt;
&lt;p&gt;Finally, you need to store your graph data using a suitable graph database. Neo4j, Amazon Neptune, or Microsoft Azure Cosmos DB are examples of graph databases. Alternatively, depending on your specific requirements, you can use a non-graph database or an in-memory data structure to store the graph.&lt;/p&gt;

&lt;h3 id=&quot;step-8-analyse-and-visualise-the-graph&quot;&gt;Step 8: Analyse and visualise the graph&lt;/h3&gt;
&lt;p&gt;After implementing the graph, you can perform various analyses using graph algorithms, such as shortest path, centrality, or community detection. In addition, visualising your graph can help you gain insights and facilitate communication with others.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By following these steps, you can effectively create and analyse graph models for your specific domain. Remember to adjust the steps according to your unique domain and requirements, and always ensure that confidential and sensitive data is properly protected.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://neo4j.com/developer/graph-database/&quot;&gt;What is a Graph Database?&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 08 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/graph-modelling-guidelines</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-modelling-guidelines</guid>
        
        <category>Graph technology</category>
        
        <category>Graphs</category>
        
        <category>Graph networks</category>
        
        <category>Security</category>
        
        <category>Data</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>LLM-powered data classification for data entities at scale</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we deal with PetaByte-level data and manage countless data entities ranging from database tables to Kafka message schemas. Understanding the data inside is crucial for us, as it not only streamlines the data access management to safeguard the data of our users, drivers and merchant-partners, but also improves the data discovery process for data analysts and scientists to easily find what they need.&lt;/p&gt;

&lt;p&gt;The Caspian team (Data Engineering team) collaborated closely with the Data Governance team on automating governance-related metadata generation. We started with Personal Identifiable Information (PII) detection and built an orchestration service using a third-party classification service. With the advent of the Large Language Model (LLM), new possibilities dawned for metadata generation and sensitive data identification at Grab. This prompted the inception of the project, which aimed to integrate LLM classification into our existing service. In this blog, we share insights into the transformation from what used to be a tedious and painstaking process to a highly efficient system, and how it has empowered the teams across the organisation.&lt;/p&gt;

&lt;p&gt;For ease of reference, here’s a list of terms we’ve used and their definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Entity&lt;/strong&gt;: An entity representing a schema that contains rows/streams of data, for example, database tables, stream messages, data lake tables.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Refers to the model’s output given a data entity, unverified manually.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Classification&lt;/strong&gt;: The process of classifying a given data entity, which in the context of this blog, involves generating tags that represent sensitive data or Grab-specific types of data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metadata Generation&lt;/strong&gt;: The process of generating the metadata for a given data entity. In this blog, since we limit the metadata to the form of tags, we often use this term and data classification interchangeably.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;: Refers to the level of confidentiality of data. High sensitivity means that the data is highly confidential. The lowest level of sensitivity often refers to public-facing or publicly-available data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;When we first approached the data classification problem, we aimed to solve something more specific - Personal Identifiable Information (PII) detection. Initially, to protect sensitive data from accidental leaks or misuse, Grab implemented manual processes and campaigns targeting data producers to tag schemas with sensitivity tiers. These tiers ranged from Tier 1, representing schemas with highly sensitive information, to Tier 4, indicating no sensitive information at all. As a result, half of all schemas were marked as Tier 1, enforcing the strictest access control measures.&lt;/p&gt;

&lt;p&gt;The presence of a single Tier 1 table in a schema with hundreds of tables justifies classifying the entire schema as Tier 1. However, since Tier 1 data is rare, this implies that a large volume of non-Tier 1 tables, which ideally should be more accessible, have strict access controls.&lt;/p&gt;

&lt;p&gt;Shifting access controls from the schema-level to the table-level could not be done safely due to the lack of table classification in the data lake. We could have conducted more manual classification campaigns for tables, however this was not feasible for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The volume, velocity, and variety of data had skyrocketed within the organisation, so it took significantly more time to classify at table level compared to schema level. Hence, a programmatic solution was needed to streamline the classification process, reducing the need for manual effort.&lt;/li&gt;
  &lt;li&gt;App developers, despite being familiar with the business scope of their data, interpreted internal data classification policies and external data regulations differently, leading to inconsistencies in understanding.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A service called Gemini &lt;em&gt;(named before Google announced the Gemini model!)&lt;/em&gt; was built internally to automate the tag generation process using a third party data classification service. Its purpose was to scan the data entities in batches and generate column/field level tags. These tags would then go through a review process by the data producers. The data governance team provided classification rules and used regex classifiers, alongside the third-party tool’s own machine learning classifiers, to discover sensitive information.&lt;/p&gt;

&lt;p&gt;After the implementation of the initial version of Gemini, a few challenges remained.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The third-party tool did not allow customisations of its machine learning classifiers, and the regex patterns produced too many false positives during our evaluation.&lt;/li&gt;
  &lt;li&gt;Building in-house classifiers would require a dedicated data science team to train a customised model. They would need to invest a significant amount of time to understand data governance rules thoroughly and prepare datasets with manually labelled training data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;LLM came up on our radar following its recent &lt;em&gt;“iPhone moment”&lt;/em&gt; with ChatGPT’s explosion onto the scene. It is trained using an extremely large corpus of text and contains trillions of parameters. It is capable of conducting natural language understanding tasks, writing code, and even analysing data based on requirements. The LLM naturally solves the mentioned pain points as it provides a natural language interface for data governance personnel. They can express governance requirements through text prompts, and the LLM can be customised effortlessly without code or model training.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;In this section, we dive into the implementation details of the data classification workflow. Please refer to the diagram below for a high-level overview:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/data_classification_workflow.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Overview of data classification workflow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This diagram illustrates how data platforms, the metadata generation service (Gemini), and data owners work together to classify and verify metadata. Data platforms trigger scan requests to the Gemini service to initiate the tag classification process. After the tags are predicted, data platforms consume the predictions, and the data owners are notified to verify these tags.&lt;/p&gt;

&lt;h3 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/arch_diagram_orchestration.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Architecture diagram of the orchestration service Gemini&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our orchestration service, Gemini, manages the data classification requests from data platforms. From the diagram, the architecture contains the following components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data platforms: These platforms are responsible for managing data entities and initiating data classification requests.&lt;/li&gt;
  &lt;li&gt;Gemini: This orchestration service communicates with data platforms, schedules and groups data classification requests.&lt;/li&gt;
  &lt;li&gt;Classification engines: There are two available engines (a third-party classification service and GPT3.5) for executing the classification jobs and return results. Since we are still in the process of evaluating two engines, both of the engines are working concurrently.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the orchestration service receives requests, it helps aggregate the requests into reasonable mini-batches. Aggregation is achievable through the message queue at fixed intervals. In addition, a rate limiter is attached at the workflow level. It allows the service to call the Cloud Provider APIs with respective rates to prevent the potential throttling from the service providers.&lt;/p&gt;

&lt;p&gt;Specific to LLM orchestration, there are two limits to be mindful of. The first one is the context length. The input length cannot surpass the context length, which was 4000 tokens for GPT3.5 at the time of development (or around 3000 words). The second one is the overall token limit (since both the input and output share the same token limit for a single request). Currently, all Azure OpenAI model deployments share the same quota under one account, which is set at 240K tokens per minute.&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section, we focus on LLM-powered column-level tag classification. The tag classification process is defined as follows:&lt;/p&gt;

&lt;p&gt;Given a data entity with a defined schema, we want to tag each field of the schema with metadata classifications that follow an internal classification scheme from the data governance team. For example, the field can be tagged as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular kind of business metric&amp;gt;&lt;/code&gt; or a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular type of personally identifiable information (PII)&lt;/code&gt;. These tags indicate that the field contains a business metric or PII.&lt;/p&gt;

&lt;p&gt;We ask the language model to be a column tag generator and to assign the most appropriate tag to each column. Here we showcase an excerpt of the prompt we use:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;You are a database column tag classifier, your job is to assign the most appropriate tag based on table name and column name. The database columns are from a company that provides ride-hailing, delivery, and financial services. Assign one tag per column. However not all columns can be tagged and these columns should be assigned &amp;lt;None&amp;gt;. You are precise, careful and do your best to make sure the tag assigned is the most appropriate.

The following is the list of tags to be assigned to a column. For each line, left hand side of the : is the tag and right hand side is the tag definition

…
&amp;lt;Personal.ID&amp;gt; : refers to government-provided identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar. This tag should absolutely not be assigned to columns named &quot;id&quot;, &quot;merchant id&quot;, &quot;passenger id&quot;, “driver id&quot; or similar since these are not government-provided identification numbers. This tag should be very rarely assigned.

&amp;lt;None&amp;gt; : should be used when none of the above can be assigned to a column.
…

Output Format is a valid json string, for example:

[{
        &quot;column_name&quot;: &quot;&quot;,
        &quot;assigned_tag&quot;: &quot;&quot;
}]

Example question

`These columns belong to the &quot;deliveries&quot; table

        1. merchant_id
        2. status
        3. delivery_time`

Example response

[{
        &quot;column_name&quot;: &quot;merchant_id&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;Personal.ID&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;status&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;delivery_time&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also curated a tag library for LLM to classify. Here is an example:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column-level Tag&lt;/th&gt;
      &lt;th&gt;Definition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.ID&lt;/td&gt;
      &lt;td&gt;Refers to external identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Name &lt;/td&gt;
      &lt;td&gt;Refers to the name or username of a person and should be assigned to columns containing &quot;name&quot;, &quot;username&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Contact_Info&lt;/td&gt;
      &lt;td&gt;Refers to the contact information of a person and should be assigned to columns containing &quot;email&quot;, &quot;phone&quot;, &quot;address&quot;, &quot;social media&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Geo.Geohash&lt;/td&gt;
      &lt;td&gt;Refers to a geohash and should be assigned to columns containing &quot;geohash&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;Should be used when none of the above can be assigned to a column.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The output of the language model is typically in free text format, however, we want the output in a fixed format for downstream processing. Due to this nature, prompt engineering is a crucial component to make sure downstream workflows can process the LLM’s output.&lt;/p&gt;

&lt;p&gt;Here are some of the techniques we found useful during our development:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Articulate the requirements: The requirement of the task should be as clear as possible, LLM is only instructed to do what you ask it to do.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/en-gb/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#few-shot-learning&quot;&gt;Few-shot learning&lt;/a&gt;: By showing the example of interaction, models understand how they should respond.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/TypeChat&quot;&gt;Schema Enforcement&lt;/a&gt;: Leveraging its ability of understanding code, we explicitly provide the DTO (Data Transfer Object) schema to the model so that it understands that its output must conform to it.&lt;/li&gt;
  &lt;li&gt;Allow for confusion: In our prompt we specifically added a default tag – the LLM is instructed to output the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;None&amp;gt;&lt;/code&gt; tag when it cannot make a decision or is confused.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Regarding classification accuracy, we found that it is surprisingly accurate with its great semantic understanding. For acknowledged tables, users on average change less than one tag. Also, during an internal survey done among data owners at Grab in September 2023, 80% reported that this new tagging process helped them in tagging their data entities.&lt;/p&gt;

&lt;h3 id=&quot;publish-and-verification&quot;&gt;Publish and verification&lt;/h3&gt;

&lt;p&gt;The predictions are published to the Kafka queue to downstream data platforms. The platforms inform respective users weekly to verify the classified tags to improve the model’s correctness and to enable iterative prompt improvement. Meanwhile, we plan to remove the verification mandate for users once the accuracy reaches a certain level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/verification_message.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Verification message shown in the data platform for user to verify the tags&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since the new system was rolled out, we have successfully integrated this with Grab’s metadata management platform and production database management platform. Within a month since its rollout, we have scanned more than 20,000 data entities, averaging around 300-400 entities per day.&lt;/p&gt;

&lt;p&gt;Using a quick back-of-the-envelope calculation, we can see the significant time savings achieved through automated tagging. Assuming it takes a data owner approximately 2 minutes to classify each entity, we are saving approximately 360 man-days per year for the company. This allows our engineers and analysts to focus more on their core tasks of engineering and analysis rather than spending excessive time on data governance.&lt;/p&gt;

&lt;p&gt;The classified tags pave the way for more use cases downstream. These tags, in combination with rules provided by data privacy office in Grab, enable us to determine the sensitivity tier of data entities, which in turn will be leveraged for enforcing the Attribute-based Access Control (ABAC) policies and enforcing Dynamic Data Masking for downstream queries. To learn more about the benefits of ABAC, readers can refer to another engineering &lt;a href=&quot;https://engineering.grab.com/migrating-to-abac&quot;&gt;blog&lt;/a&gt; posted earlier.&lt;/p&gt;

&lt;p&gt;Cost wise, for the current load, it is extremely affordable contrary to common intuition. This affordability enables us to scale the solution to cover more data entities in the company.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;h3 id=&quot;prompt-improvement&quot;&gt;Prompt improvement&lt;/h3&gt;

&lt;p&gt;We are currently exploring feeding sample data and user feedback to greatly increase accuracy. Meanwhile, we are experimenting on outputting the confidence level from LLM for its own classification. With confidence level output, we would only trouble users when the LLM is uncertain of its answers. Hopefully this can remove even more manual processes in the current workflow.&lt;/p&gt;

&lt;h3 id=&quot;prompt-evaluation&quot;&gt;Prompt evaluation&lt;/h3&gt;

&lt;p&gt;To track the performance of the prompt given, we are building analytical pipelines to calculate the metrics of each version of the prompt. This will help the team better quantify the effectiveness of prompts and iterate better and faster.&lt;/p&gt;

&lt;h3 id=&quot;scaling-out&quot;&gt;Scaling out&lt;/h3&gt;

&lt;p&gt;We are also planning to scale out this solution to more data platforms to streamline governance-related metadata generation to more teams. The development of downstream applications using our metadata is also on the way. These exciting applications are from various domains such as security, data discovery, etc.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Oct 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-powered-data-classification</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-powered-data-classification</guid>
        
        <category>Data</category>
        
        <category>Machine Learning</category>
        
        <category>Generative AI</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Scaling marketing for merchants with targeted and intelligent promos</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A promotional campaign is a marketing effort that aims to increase sales, customer engagement, or brand awareness for a product, service, or company. The target is to have more orders and sales by assigning promos to consumers within a given budget during the campaign period.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/customer-feedback.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Merchant feedback on marketing&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;From our research, we found that merchants have specific goals for the promos they are willing to offer. They want a simple and cost-effective way to achieve their specific business goals by providing well-designed offers to target the correct customers. From Grab’s perspective, we want to help merchants set up and run campaigns efficiently, and help them achieve their specific business goals.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;
&lt;p&gt;One of Grab’s platform offerings for merchants is the ability to create promotional campaigns. With the emergence of AI technologies, we found that there are opportunities for us to further optimise the platform. The following are the gaps and opportunities we identified:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Globally assigned promos without smart targeting&lt;/strong&gt;: The earlier method targeted every customer, so everyone could redeem until the promo reached the redemption limits. However, this method did not accurately meet business goals or optimise promo spending. The promotional campaign should intelligently target the best promo for each customer to increase sales and better utilise promo spending.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No customised promos for every merchant&lt;/strong&gt;: To better optimise sales for each merchant, merchants should offer customised promos based on their historical consumer trends, not just a general offer set. For example, for a specific merchant, a 27% discount may be the appropriate offer to uplift revenue and sales based on user bookings. However, merchants do not always have the expertise to decide which offer to select to increase profit.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No AI-driven optimisation&lt;/strong&gt;: Without AI models, it was harder for merchants to assign the right promos at scale to each consumer and optimise their business goals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As shown in the following figure, AI-driven promotional campaigns are expected to bring higher sales with more promo spend than heuristic ones. Hence, at Grab we looked to introduce an automated, AI-driven tool that helps merchants intelligently target consumers with appropriate promos, while optimising sales and promo spending. That’s where Bullseye comes in.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/ai-campaign-graph.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Graph showing the sales expectations for AI-driven pomotional campaigns&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Bullseye is an automated, AI-driven promo assignment system that leverages the following capabilities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Automated user segmentation&lt;/strong&gt;: Enables merchants to target new, churned, and active users or all users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automatic promo design&lt;/strong&gt;: Enables a merchant-level promo design framework to customise promos for each merchant or merchant group according to their business goals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assign each user the optimal promo&lt;/strong&gt;: Users will receive promos selected from an array of available promos based on the merchant’s business objective.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Achieve different Grab and merchant objectives&lt;/strong&gt;: Examples of objectives are to increase merchant sales and decrease Grab promo spend.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Flexibility to optimise for an individual merchant brand or group of merchant brands&lt;/strong&gt;: For promotional campaigns, targeting and optimisation can be performed for a single or group of merchants (e.g. enabling GrabFood to run cuisine-oriented promo campaigns).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/architecture.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Bullseye architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The Bullseye architecture consists of a user interface (UI) and a backend service to handle requests. To use Bullseye, our operations team inputs merchant information into the Bullseye UI. The backend service will then interact with APIs to process the information using the AI model. As we work with a large customer population, data is stored in S3 and the API service triggering Chimera Spark job is used to run the prediction model and generate promo assignments. During the assignment, the Spark job parses the input parameters, pre-validates the input, makes some predictions, and then returns the promo assignment results to the backend service.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;The key components in Bullseye are shown in the following figure:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/implementation.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Key components of Bullseye&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Eater Segments Identifier&lt;/strong&gt;: Identifies each user as active, churned, or new based on their historical orders from target merchants.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Promo Designer&lt;/strong&gt;: We constructed a promo variation design framework to adaptively design promo variations for each campaign request as shown in the diagram below.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Offer Content Candidate Generation&lt;/strong&gt;: Generates variant settings of promos based on the promo usage history.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Campaign Impact Simulator&lt;/strong&gt;: Predicts business metrics such as revenue, sales, and cost based on the user and merchant profiles and offer features.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Optimal Promo Selection&lt;/strong&gt;: Selects the optimal offer based on the predicted impact and the given campaign objective. The optimal would be based on how you define optimal. For example, if the goal is to maximise merchant sales, the model selects the top candidate which can bring the highest revenue. Finally, with the promo selection, the service returns the promo set to be used in the target campaign.&lt;/p&gt;

        &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scaling-marketing-for-merchants/promo-designer.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Optimal Promo Selection&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Customer Response Model&lt;/strong&gt;: Predicts customer responses such as order value, redemption, and take-up rate if assigning a specific promo. Bullseye captures various user attributes and compares it with an offer’s attributes. Examples of attributes are cuisine type, food spiciness, and discount amount. When there is a high similarity in the attributes, there is a higher probability that the user will take up the offer.&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/scaling-marketing-for-merchants/customer-response-model.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Customer Response Model&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Hyper-parameter Selection&lt;/strong&gt;: Optimises toward multiple business goals. Tuning of hyper-parameters allows the AI assignment model to learn how to meet success criteria such as cost per merchant sales (cpSales) uplift and sales uplift. The success criteria is the achieving of business goals. For example, the merchant wants the sales uplift after assigning promo, but cpSales uplift cannot be higher than 10%. With tuning, the optimiser can find optimal points to meet business goals and use AI models to search for better settings with high efficiency compared to manual specification. We need to constantly tune and iterate models and hyper-parameters to adapt to ever-evolving business goals and the local landscape.&lt;/p&gt;

    &lt;p&gt;As shown in the image below, AI assignments without hyper-parameter tuning (HPT) leads to a high cpSales uplift but low sales uplift (red dot). So the  hyper-parameters would help to fine-tune the assignment result to be in the optimal space such as the blue dot, which may have lower sales than the red dot but meet the success criteria.&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/scaling-marketing-for-merchants/hpt.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Graph showing the impact of using AI assignments with HPT&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;We started using Bullseye in 2021. From its use we found that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hyper-parameters tuning and auto promo design can increase sales and reduce promo spend for food campaigns.&lt;/li&gt;
  &lt;li&gt;Promo Designer optimises budget utilisation and increases the number of promo redemptions for food campaigns.&lt;/li&gt;
  &lt;li&gt;The Customer Response Model reduced promo spending for Mart promotional campaigns.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have seen positive results with the implementation of Bullseye such as reduced promo spending and maximised budget spending returns. In our efforts to serve our merchants better and help them achieve their business goals, we will continue to improve Bullseye. In the next phase, we plan to implement a more intelligent service, enabling reinforcement learning, and online assignment. We also aim to scale AI adoption by onboarding regional promotional campaigns as much as possible.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to William Wu, Rui Tan, Rahadyan Pramudita, Krishna Murthy, and Jiesin Chia for making this project a success.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Oct 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/scaling-marketing-for-merchants</link>
        <guid isPermaLink="true">https://engineering.grab.com/scaling-marketing-for-merchants</guid>
        
        <category>Data</category>
        
        <category>Advertising</category>
        
        <category>Scalability</category>
        
        <category>Data science</category>
        
        <category>Marketing</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Stepping up marketing for advertisers: Scalable lookalike audience</title>
        <description>&lt;p&gt;The advertising industry is constantly evolving, driven by advancements in technology and changes in consumer behaviour. One of the key challenges in this industry is reaching the right audience, reaching people who are most likely to be interested in your product or service. This is where the concept of a lookalike audience comes into play. By identifying and targeting individuals who share similar characteristics with an existing customer base, businesses can significantly improve the effectiveness of their advertising campaigns.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image3.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However, as the scale of Grab advertisements grows, there are several optimisations needed to maintain the efficacy of creating lookalike audiences such as high service level agreement (SLA), high cost of audience creation, and unstable data ingestion.&lt;/p&gt;

&lt;p&gt;The need for an even more efficient and scalable solution for creating lookalike audiences was the motivation behind the development of the scalable lookalike audience platform. By developing a high-performance in-memory lookalike audience retrieval service and embedding-based lookalike audience creation and updating pipelines, t​his improved platform builds on the existing system and provides an even more effective tool for advertisers to reach their target audience.&lt;/p&gt;

&lt;h2 id=&quot;constant-optimisation-for-greater-precision&quot;&gt;Constant optimisation for greater precision&lt;/h2&gt;

&lt;p&gt;In the dynamic world of digital advertising, the ability to quickly and efficiently reach the right audience is paramount and a key strategy is targeted advertising. As such, we have to constantly find ways to improve our current approach to creating lookalike audiences that impacts both advertisers and users. Some of the gaps we identified included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Long SLA&lt;/strong&gt; for audience creation. Earlier, the platform stored results on Segmentation Platform (SegP) and it took two working days to generate a lookalike audience list. This is because inserting a single audience into SegP took three times longer than generating the audience. Extended creation times impacted the effectiveness of advertising campaigns, as it limited the ability of advertisers to respond quickly to changing market dynamics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low scalability&lt;/strong&gt;. As the number of onboarded merchant-partners increased, the time and cost of generating lookalike audiences also increased proportionally. This limited the availability of lookalike audience generation for all advertisers, particularly those with large customer bases or rapidly changing audience profiles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low updating frequency&lt;/strong&gt; of lookalike audiences. With automated updates only occurring on a weekly basis, this increased the likelihood that audiences may become outdated and ineffective. This meant there was scope to further improve to help advertisers more effectively reach their campaign goals, by targeting individuals who fit the desired audience profile.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High cost of creation&lt;/strong&gt;. The cost of producing one segment can add up quickly for advertisers who need to generate multiple audiences. This could impact scalability for advertisers as they could hesitate to effectively use multiple lookalike audiences in their campaigns.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image4.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To efficiently identify the top N lookalike audiences for each Grab user from our pool of millions of users, we developed a solution that leverages user and audience representations in the form of embeddings. Embeddings are vector representations of data that utilise linear distances to capture structure from the original datasets. With embeddings, large sets of data are compressed and easily processed without affecting data integrity. This approach ensures high accuracy, low latency, and low cost in retrieving the most relevant audiences.&lt;/p&gt;

&lt;p&gt;Our solution takes into account the fact that representation drift varies among entities as data is added. For instance, merchant-partner embeddings are more stable than passenger embeddings. By acknowledging this reality, we optimised our process to minimise cost while maintaining a desirable level of accuracy. Furthermore, we believe that having a strong representation learning strategy in the early stages reduced the need for complex models in the following stages.&lt;/p&gt;

&lt;p&gt;Our solution comprises two main components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Real-time lookalike audience retrieving&lt;/strong&gt;: We developed an in-memory high-performance retrieving service that stores passenger embeddings, audience embeddings, and audience score thresholds. To further reduce cost, we designed a passenger embedding compression algorithm that reduces the memory needs of passenger embeddings by around 90%.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Embedding-based audience creation and updating&lt;/strong&gt;: The output of this part of the project is an online retrieving model that includes passenger embeddings, audience embeddings, and thresholds. To minimise costs, we leverage the passenger embeddings that are also utilised by other projects within Grab, beyond advertising, thus sharing the cost. The audience embeddings and thresholds are produced with a low-cost small neural network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In summary, our approach to creating scalable lookalike audiences is designed to be cost-effective, accurate, and efficient, leveraging the power of embeddings and smart computational strategies to deliver the best possible audiences for our advertisers.&lt;/p&gt;

&lt;h3 id=&quot;solution-architecture&quot;&gt;Solution architecture&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image2.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The advertiser creates a campaign with a custom audience, which triggers the audience creation process. During this process, the audience service stores the audience metadata provided by advertisers in a message queue.&lt;/li&gt;
  &lt;li&gt;A scheduled Data Science (DS) job then retrieves the pending audience metadata, creates the audience, and updates the TensorFlow Serving (TFS) model.&lt;/li&gt;
  &lt;li&gt;During the serving period, the Backend (BE) service calls the DS service to retrieve all audiences that include the target user. Ads that are targeting these audiences are then selected by the Click-Through Rate (CTR) model to be displayed to the user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;To ensure the efficiency of the lookalike audience retrieval model and minimise the costs associated with audience creation and serving, we’ve trained the user embedding model using billions of user actions. This extensive training allows us to employ straightforward methods for audience creation and serving, while still maintaining high levels of accuracy.&lt;/p&gt;

&lt;h4 id=&quot;creating-lookalike-audiences&quot;&gt;Creating lookalike audiences&lt;/h4&gt;

&lt;p&gt;The Audience Creation Job retrieves the audience metadata from the online audience service, pulls the passenger embeddings, and then averages these embeddings to generate the audience embedding.&lt;/p&gt;

&lt;p&gt;We use the cosine score of a user and the audience embedding to identify the audiences the user belongs to. Hence, it’s sufficient to store only the audience embedding and score threshold. Additionally, a global &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target-all-pax&lt;/code&gt; Audience list is stored to return these audiences for each online request.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image1.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;serving-lookalike-audiences&quot;&gt;Serving lookalike audiences&lt;/h4&gt;

&lt;p&gt;The online audience service is also tasked with returning all the audiences to which the current user belongs. This is achieved by utilising the cosine score of the user embedding and audience embeddings, and filtering out all audiences that surpass the audience thresholds.&lt;/p&gt;

&lt;p&gt;To adhere to latency requirements, we avoid querying any external feature stores like Redis and instead, store all the embeddings in memory. However, the embeddings of all users are approximately 20 GB, which could affect model loading. Therefore, we devised an embedding compression method based on hash tricks inspired by &lt;a href=&quot;https://brilliant.org/wiki/bloom-filter/#:~:text=A%20bloom%20filter%20is%20a,is%20added%20to%20the%20set&quot;&gt;Bloom Filter&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-lookalike-audience/image5.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;We utilise hash functions to obtain the hash64 value of the paxID, which is then segmented into four 16-bit values. Each 16-bit value corresponds to a 16-dimensional embedding block, and the compressed embedding is the concatenation of these four 16-dimensional embeddings.&lt;/li&gt;
  &lt;li&gt;For each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;paxID&lt;/code&gt;, we have both the original user embedding and the compressed user embedding. The compressed user embeddings are learned by minimising the Mean Square Error loss.&lt;/li&gt;
  &lt;li&gt;We can balance the storage cost and the accuracy by altering the number of hash functions used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Users can see advertisements targeting a new audience within 15 mins after the advertiser creates a campaign.&lt;/li&gt;
  &lt;li&gt;This new system doubled the impressions and clicks, while also improving the CTR, conversion rate, and return on investment.&lt;/li&gt;
  &lt;li&gt;Costs for generating lookalike audiences decreased by 98%.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;To evaluate the effectiveness of our new scalable system besides addressing these issues, we conducted an A/B test to compare it with the earlier system. The results revealed that this new system effectively doubled the number of impressions and clicks while also enhancing the CTR, conversion rate, and return on investment.&lt;/p&gt;

&lt;p&gt;Over the years, we have amassed over billions of user actions, which have been instrumental in training the model and creating a comprehensive representation of user interests in the form of embeddings.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;While this scalable system has proved its effectiveness and demonstrated impressive results in CTR, conversion rate, and return on investment, there is always room for improvement.  &lt;/p&gt;

&lt;p&gt;In the next phase, we plan to explore more advanced algorithms, refine our feature engineering process, and conduct more extensive hyperparameter tuning. Additionally, we will continue to monitor the system’s performance and make necessary adjustments to ensure it remains robust and effective in serving our advertisers’ needs.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.05022.pdf&quot;&gt;Real-time Attention Based Look-alike Model for Recommender System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://brilliant.org/wiki/bloom-filter/#:~:text=A%20bloom%20filter%20is%20a,is%20added%20to%20the%20set&quot;&gt;Bloom Filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3383313.3418481&quot;&gt;Smart Targeting: A Relevance-driven and Configurable Targeting Framework for Advertising System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.05022.pdf&quot;&gt;GUIM - General User and Item Embedding with Mixture of Representation in E-commerce&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Sep 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/scalable-lookalike-audiences</link>
        <guid isPermaLink="true">https://engineering.grab.com/scalable-lookalike-audiences</guid>
        
        <category>Data</category>
        
        <category>Advertising</category>
        
        <category>Scalability</category>
        
        <category>Data science</category>
        
        <category>Marketing</category>
        
        <category>Lookalike audience</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Building hyperlocal GrabMaps</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Southeast Asia (SEA) is a dynamic market, very different from other parts of the world. When travelling on the road, you may experience fast-changing road restrictions, new roads appearing overnight, and high traffic congestion. To address these challenges, GrabMaps has adapted to the SEA market by leveraging big data solutions. One of the solutions is the integration of hyperlocal data in GrabMaps.&lt;/p&gt;

&lt;p&gt;Hyperlocal information is oriented around very small geographical communities and obtained from the local knowledge that our map team gathers. The map team is spread across SEA, enabling us to define clear specifications (e.g. legal speed limits), and validate that our solutions are viable.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/map-detection.gif&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Map showing detections from images and probe data, and hyperlocal data.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Hyperlocal inputs make our mapping data even more robust, adding to the details collected from our image and probe detection pipelines. Figure 1 shows how data from our detection pipeline is overlaid with hyperlocal data, and then mapped across the SEA region. If you are curious and would like to check out the data yourself, you can download it &lt;a href=&quot;https://dumps.improveosm.org/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;processing-hyperlocal-data&quot;&gt;Processing hyperlocal data&lt;/h2&gt;

&lt;p&gt;Now let’s go through the process of detecting hyperlocal data.&lt;/p&gt;

&lt;h3 id=&quot;download-data&quot;&gt;Download data&lt;/h3&gt;

&lt;p&gt;GrabMaps is based on &lt;a href=&quot;https://www.openstreetmap.org/&quot;&gt;OpenStreetMap&lt;/a&gt; (OSM). The first step in the process is to download the .pbf file for Asia from &lt;a href=&quot;https://www.geofabrik.de/&quot;&gt;geofabrick.de&lt;/a&gt;. This .pbf file contains all the data that is available on OSM, such as details of places, trees, and roads. Take for example a park, the .pbf file would contain data on the park name, wheelchair accessibility, and many more.&lt;/p&gt;

&lt;p&gt;For this article, we will focus on hyperlocal data related to the road network. For each road, you can obtain data such as the type of road (residential or motorway), direction of traffic (one-way or more), and road name.&lt;/p&gt;

&lt;h3 id=&quot;convert-data&quot;&gt;Convert data&lt;/h3&gt;
&lt;p&gt;To take advantage of big data computing, the next step in the process is to convert the .pbf file into Parquet format using a Parquetizer. This will convert the binary data in the .pbf file into a table format. Each road in SEA is now displayed as a row in a table as shown in Figure 2.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/data-parquet.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Road data in Parquet format.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;identify-hyperlocal-data&quot;&gt;Identify hyperlocal data&lt;/h3&gt;

&lt;p&gt;After the data is prepared, GrabMaps then identifies and inputs all of our hyperlocal data, and delivers a consolidated view to our downstream services. Our hyperlocal data is obtained from various sources, either by looking at geometry, or other attributes in OSM such as the direction of travel and speed limit. We also apply customised rules defined by our local map team, all in a fully automated manner. This enhances the map together with data obtained from our rides and deliveries GPS pings and from KartaView, Grab’s product for imagery collection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/architecture.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Architecture diagram showing how hyperlocal data is integrated into GrabMaps.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;benefit-of-our-hyperlocal-grabmaps&quot;&gt;Benefit of our hyperlocal GrabMaps&lt;/h2&gt;

&lt;p&gt;GrabNav, a turn-by-turn navigation tool available on the Grab driver app, is one of our products that benefits from having hyperlocal data. Here are some hyperlocal data that are made available through our approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Localisation of roads: The country, state/county, or city the road is in&lt;/li&gt;
  &lt;li&gt;Language spoken, driving side, and speed limit&lt;/li&gt;
  &lt;li&gt;Region-specific default speed regulations&lt;/li&gt;
  &lt;li&gt;Consistent name usage using language inference&lt;/li&gt;
  &lt;li&gt;Complex attributes like intersection links&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To further explain the benefits of this hyperlocal feature, we will use intersection links as an example. In the next section, we will explain how intersection links data is used and how it impacts our driver-partners and passengers.&lt;/p&gt;

&lt;h3 id=&quot;identifying-hyperlocal-data---intersection-links&quot;&gt;Identifying hyperlocal data - intersection links&lt;/h3&gt;
&lt;p&gt;An intersection link is when two or more roads meet. Figure 4 and 5 illustrates what an intersection link looks like in a GrabMaps mock and in OSM.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/intersection-link-mock.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Mock of an intersection link. 
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/intersection-link-illustration.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Intersection link illustration from a real road network in OSM.  
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To locate intersection links in a road network, there are computations involved. We would first combine big data processing (which we do using Spark) with graphs. We use geohash as the unit of processing, and for each geohash, a bi-directional graph is created.&lt;/p&gt;

&lt;p&gt;From such resulting graphs, we can determine intersection links if:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Road segments are parallel&lt;/li&gt;
  &lt;li&gt;The roads have the same name&lt;/li&gt;
  &lt;li&gt;The roads are one way roads&lt;/li&gt;
  &lt;li&gt;Angles and the shape of the road are in the intervals or requirements we seek&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each intersection link we identify is tagged in the map as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intersection_links&lt;/code&gt;. Our downstream service teams can then identify them by searching for the tag.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;The impact we create with our intersection link can be explained through the following example.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/impact1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Longer route, without GrabMaps intersection link feature. The arrow indicates where the route should have suggested a U-turn.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/impact2.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Shorter route using GrabMaps by taking a closer link between two main roads.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 6 and Figure 7 show two different routes for the same origin and destination. However, you can see that Figure 7 has a shorter route and this is made available by taking an intersection link early on in the route. The highlighted road segment in Figure 7 is an intersection link, tagged by the process we described earlier. The route is now much shorter making GrabNav more efficient in its route suggestion.&lt;/p&gt;

&lt;p&gt;There are numerous factors that can impact a driver-partner’s trip, and intersection links are just one example. There are many more features that GrabMaps offers across Grab’s services that allow us to “outserve” our partners.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;GrabMaps and GrabNav deliver enriched experiences to our driver-partners. By integrating certain hyperlocal data features, we are also able to provide more accurate pricing for both our driver-partners and passengers. In our mission towards sustainable growth, this is an area that we will keep on improving by leveraging scalable tech solutions.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Aug 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/building-hyperlocal-grabmaps</link>
        <guid isPermaLink="true">https://engineering.grab.com/building-hyperlocal-grabmaps</guid>
        
        <category>Maps</category>
        
        <category>Data</category>
        
        <category>Big Data</category>
        
        <category>Data processing</category>
        
        <category>hyperlocalisation</category>
        
        <category>GrabMaps</category>
        
        <category>navigation</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Streamlining Grab's Segmentation Platform with faster creation and lower latency</title>
        <description>&lt;p&gt;Launched in 2019, Segmentation Platform has been Grab’s one-stop platform for user segmentation and audience creation across all business verticals. User segmentation is the process of dividing passengers, driver-partners, or merchant-partners (users) into sub-groups (segments) based on certain attributes. Segmentation Platform empowers Grab’s teams to create segments using attributes available within our data ecosystem and provides APIs for downstream teams to retrieve them.&lt;/p&gt;

&lt;p&gt;Checking whether a user belongs to a segment (Membership Check) influences many critical flows on the Grab app:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;When a passenger launches the Grab app, our in-house experimentation platform will tailor the app experience based on the segments the passenger belongs to.&lt;/li&gt;
  &lt;li&gt;When a driver-partner goes online on the Grab app, the Drivers service calls Segmentation Platform to ensure that the driver-partner is not blacklisted.&lt;/li&gt;
  &lt;li&gt;When launching marketing campaigns, Grab’s communications platform relies on Segmentation Platform to determine which passengers, driver-partners, or merchant-partners to send communication to.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This article peeks into the current design of Segmentation Platform and how the team optimised the way segments are stored to reduce read latency thus unlocking new segmentation use cases.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;Segmentation Platform comprises two major subsystems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Segment creation&lt;/li&gt;
  &lt;li&gt;Segment serving&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image1.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. Segmentation Platform architecture
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;segment-creation&quot;&gt;Segment creation&lt;/h3&gt;

&lt;p&gt;Segment creation is powered by Spark jobs. When a Grab team creates a segment, a Spark job is started to retrieve data from our data lake. After the data is retrieved, cleaned, and validated, the Spark job calls the serving sub-system to populate the segment with users.&lt;/p&gt;

&lt;h3 id=&quot;segment-serving&quot;&gt;Segment serving&lt;/h3&gt;

&lt;p&gt;Segment serving is powered by a set of Go services. For persistence and serving, we use ScyllaDB as our primary storage layer. We chose to use ScyllaDB as our NoSQL store due to its ability to scale horizontally and meet our &amp;lt;80ms p99 SLA. Users in a segment are stored as rows indexed by the user ID. The table is partitioned by the user ID ensuring that segment data is evenly distributed across the ScyllaDB clusters.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;User ID&lt;/th&gt;
    &lt;th&gt;Segment Name&lt;/th&gt;
    &lt;th&gt;Other metadata columns&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td&gt;1221&lt;/td&gt;
    &lt;td&gt;Segment_A&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;3421&lt;/td&gt;
    &lt;td&gt;Segment_A&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;5632&lt;/td&gt;
    &lt;td&gt;Segment_B&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;7889&lt;/td&gt;
    &lt;td&gt;Segment_B&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With this design, Segmentation Platform handles up to 12K read and 36K write QPS, with a p99 latency of 40ms.&lt;/p&gt;

&lt;h2 id=&quot;problems&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;The existing system has supported Grab, empowering internal teams to create rich and personalised experiences. However, with the increased adoption and use, certain challenges began to emerge:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As more and larger segments are being created, the write QPS became a bottleneck leading to longer wait times for segment creation.&lt;/li&gt;
  &lt;li&gt;Grab services requested even lower latency for membership checks.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;long-segment-creation-times&quot;&gt;Long segment creation times&lt;/h3&gt;

&lt;p&gt;As more segments were created by different teams within Grab, the write QPS was no longer able to keep up with the teams’ demands. Teams would have to wait for hours for their segments to be created, reducing their operational velocity.&lt;/p&gt;

&lt;h3 id=&quot;read-latency&quot;&gt;Read latency&lt;/h3&gt;

&lt;p&gt;Further, while the platform already offers sub-40ms p99 latency for reads, this was still too slow for certain services and their use cases. For example, Grab’s communications platform needed to check whether a user belongs to a set of segments before sending out communication and incurring increased latency for every communication request was not acceptable. Another use case was for Experimentation Platform, where checks must have low latency to not impact the user experience. 
Thus, the team explored alternative ways of storing the segment data with the goals of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reducing segment creation time&lt;/li&gt;
  &lt;li&gt;Reducing segment read latency&lt;/li&gt;
  &lt;li&gt;Maintaining or reducing cost&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;h3 id=&quot;segments-as-bitmaps&quot;&gt;Segments as bitmaps&lt;/h3&gt;

&lt;p&gt;One of the main engineering challenges was scaling the write throughput of the system to keep pace with the number of segments being created. As a segment is stored across multiple rows in ScyllaDB, creating a large segment incurs a huge number of writes to the database. What we needed was a better way to store a large set of user IDs. Since user IDs are represented as integers in our system, a natural solution to storing a set of integers was a bitmap.&lt;/p&gt;

&lt;p&gt;For example, a segment containing the following user IDs: 1, 6, 25, 26, 89 could be represented with a bitmap as follows:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. Bitmap representation of a segment
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To perform a membership check, a bitwise operation can be used to check if the bit at the user ID’s index is 0 or 1. As a bitmap, the segment can also be stored as a single Blob in object storage instead of inside ScyllaDB.&lt;/p&gt;

&lt;p&gt;However, as the number of user IDs in the system is large, a small and sparse segment would lead to prohibitively large bitmaps. For example, if a segment contains 2 user IDs 100 and 200,000,000, it will require a bitmap containing 200 million bits (25MB) where all but 2 of the bits are just 0. Thus, the team needed an encoding to handle sparse segments more efficiently.&lt;/p&gt;

&lt;h4 id=&quot;roaring-bitmaps&quot;&gt;Roaring Bitmaps&lt;/h4&gt;

&lt;p&gt;After some research, we landed on Roaring Bitmaps, which are compressed uint32 bitmaps. With roaring bitmaps, we are able to store a segment with 1 million members in a Blob smaller than 1 megabyte, compared to 4 megabytes required by a naive encoding.&lt;/p&gt;

&lt;p&gt;Roaring Bitmaps achieve good compression ratios by splitting the set into fixed-size (216) integer chunks and using three different data structures (containers) based on the data distribution within the chunk. The most significant 16 bits of the integer are used as the index of the chunk, and the least significant 16 bits are stored in the containers.&lt;/p&gt;

&lt;h5 id=&quot;array-containers&quot;&gt;Array containers&lt;/h5&gt;

&lt;p&gt;Array containers are used when data is sparse (&amp;lt;= 4096 values). An array container is a sorted array of 16-bit integers. It is memory-efficient for sparse data and provides logarithmic-time access.&lt;/p&gt;

&lt;h5 id=&quot;bitmap-containers&quot;&gt;Bitmap containers&lt;/h5&gt;

&lt;p&gt;Bitmap containers are used when data is dense. A bitmap container is a 216 bit container where each bit represents the presence or absence of a value. It is memory-efficient for dense data and provides constant-time access.&lt;/p&gt;

&lt;h5 id=&quot;run-containers&quot;&gt;Run containers&lt;/h5&gt;

&lt;p&gt;Finally, run containers are used when a chunk has long consecutive values. Run containers use run-length encoding (RLE) to reduce the storage required for dense bitmaps. Run containers store a pair of values representing the start and the length of the run. It provides good memory efficiency and fast lookups.&lt;/p&gt;

&lt;p&gt;The diagram below shows how a dense bitmap container that would have required 91 bits can be compressed into a run container by storing only the start (0) and the length (90). It should be noted that run containers are used only if it reduces the number of bytes required compared to a bitmap.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. A dense bitmap container compressed into a run container
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;By using different containers, Roaring Bitmaps are able to achieve good compression across various data distributions, while maintaining excellent lookup performance. Additionally, as segments are represented as Roaring Bitmaps, service teams are able to perform set operations (union, interaction, and difference, etc) on the segments on the fly, which previously required re-materialising the combined segment into the database.&lt;/p&gt;

&lt;h3 id=&quot;caching-with-an-sdk&quot;&gt;Caching with an SDK&lt;/h3&gt;

&lt;p&gt;Even though the segments are now compressed, retrieving a segment from the Blob store for each membership check would incur an unacceptable latency penalty. To mitigate the overhead of retrieving a segment, we developed an SDK that handles the retrieval and caching of segments.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image2.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. How the SDK caches segments
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The SDK takes care of the retrieval, decoding, caching, and watching of segments. Users of the  SDK are only required to specify the maximum size of the cache to prevent exhausting the service’s memory. The SDK provides a cache with a least-recently-used eviction policy to ensure that hot segments are kept in the cache. They are also able to watch for updates on a segment and the SDK will automatically refresh the cached segment when it is updated.&lt;/p&gt;

&lt;h2 id=&quot;hero-teams&quot;&gt;Hero teams&lt;/h2&gt;

&lt;h3 id=&quot;communications-platform&quot;&gt;Communications Platform&lt;/h3&gt;

&lt;p&gt;Communications Platform has adopted the SDK to implement a new feature to control the communication frequency based on which segments a user belongs to. Using the SDK, the team is able to perform membership checks on multiple multi-million member segments, achieving peak QPS 15K/s with a p99 latency of &amp;lt;1ms. With the new feature, they have been able to increase communication engagement and reduce the communication unsubscribe rate.&lt;/p&gt;

&lt;h3 id=&quot;experimentation-platform&quot;&gt;Experimentation Platform&lt;/h3&gt;

&lt;p&gt;Experimentation Platform powers experimentation across all Grab services. Segments are used heavily in experiments to determine a user’s experience. Prior to using the SDK, Experimentation Platform limited the maximum size of the segments that could be used to prevent exhausting a service’s memory.&lt;/p&gt;

&lt;p&gt;After migrating to the new SDK, they were able to lift this restriction due to the compression efficiency of Roaring Bitmaps. Users are now able to use any segments as part of their experiment without worrying that it would require too much memory.&lt;/p&gt;

&lt;h2 id=&quot;closing&quot;&gt;Closing&lt;/h2&gt;

&lt;p&gt;This blog post discussed the challenges that Segmentation Platform faced when scaling and how the team explored alternative storage and encoding techniques to improve segment creation time, while also achieving low latency reads. The SDK allows our teams to easily make use of segments without having to handle the details of caching, eviction, and updating of segments.&lt;/p&gt;

&lt;p&gt;Moving forward, there are still existing use cases that are not able to use the Roaring Bitmap segments and thus continue to rely on segments from ScyllaDB. Therefore, the team is also taking steps to optimise and improve the scalability of our service and database.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Axel, the wider Segmentation Platform team, and Data Technology team for reviewing the post. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Aug 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/streamlining-grabs-segmentation-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/streamlining-grabs-segmentation-platform</guid>
        
        <category>Back End</category>
        
        <category>Performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Unsupervised graph anomaly detection - Catching new fraudulent behaviours</title>
        <description>&lt;p&gt;Earlier in this series, we covered the &lt;a href=&quot;/graph-networks&quot;&gt;importance of graph networks&lt;/a&gt;, &lt;a href=&quot;/graph-concepts&quot;&gt;graph concepts&lt;/a&gt;, &lt;a href=&quot;/graph-visualisation&quot;&gt;graph visualisation&lt;/a&gt;, and &lt;a href=&quot;/graph-for-fraud-detection&quot;&gt;graph-based fraud detection methods&lt;/a&gt;. In this article, we will discuss how to automatically detect new types of fraudulent behaviour and swiftly take action on them.&lt;/p&gt;

&lt;p&gt;One of the challenges in fraud detection is that fraudsters are incentivised to always adversarially innovate their way of conducting frauds, i.e., their modus operandi (MO in short). Machine learning models trained using historical data may not be able to pick up new MOs, as they are new patterns that are not available in existing training data. To enhance Grab’s existing security defences and protect our users from these new MOs, we needed a machine learning model that is able to detect them quickly without the need for any label supervision, i.e., an unsupervised learning model rather than the regular supervised learning model.&lt;/p&gt;

&lt;p&gt;To address this, we developed an in-house machine learning model for detecting anomalous patterns in graphs, which has led to the discovery of new fraud MOs. Our focus was initially on GrabFood and GrabMart verticals, where we monitored the interactions between consumers and merchants. We modelled these interactions as a bipartite graph (a type of graph for modelling interactions between two groups) and then performed anomaly detection on the graph. Our in-house anomaly detection model was also presented at the International Joint Conference on Neural Networks (IJCNN) 2023, a premier academic conference in the area of neural networks, machine learning, and artificial intelligence.&lt;/p&gt;

&lt;p&gt;In this blog, we discuss the model and its application within Grab. For avid audiences that want to read the details of our model, you can access our paper below. Note that even though we implemented our model for anomaly detection in GrabFood and GrabMart, the model is designed for general purposes and is applicable to interaction graphs between any two groups.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;/img/graph-anomaly-model/image1.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;/files/Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs.pdf&quot; download=&quot;&quot;&gt;Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs&lt;/a&gt;&lt;br /&gt;By Rizal Fathony, Jenn Ng, Jia Chen&lt;br /&gt;Presented at the International Joint Conference on Neural Networks (IJCNN) 2023&lt;br /&gt;DOI: &lt;a href=&quot;https://ieeexplore.ieee.org/document/10191331&quot; target=&quot;_blank&quot;&gt;10.1109/IJCNN54540.2023.10191331&lt;/a&gt; (&lt;a href=&quot;#citation-info&quot;&gt;citation&lt;/a&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
Before we dive into how our model works, it is important to understand the process of graph construction in our application as the model assumes the availability of the graphs in a standardised format.&lt;/p&gt;

&lt;h2 id=&quot;graph-construction&quot;&gt;Graph construction &lt;/h2&gt;

&lt;p&gt;We modelled the interactions between consumers and merchants in GrabFood and GrabMart platforms as bipartite graphs (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;G&lt;/code&gt;), where the first group of nodes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U&lt;/code&gt;) represents the consumers, the second group of nodes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;) represents the merchants, and the edges (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;E&lt;/code&gt;) connecting them means that the consumers have placed some food/mart orders to the merchants. The graph is also supplied with rich transactional information about the consumers and the merchants in the form of node features (&lt;code&gt;X&lt;sub&gt;u&lt;/sub&gt;&lt;/code&gt; and &lt;code&gt;X&lt;sub&gt;v&lt;/sub&gt;&lt;/code&gt;), as well as order information in the form of edge features (&lt;code&gt;X&lt;sub&gt;e&lt;/sub&gt;&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. Graph construction process&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The goal of our anomaly model is to detect anomalous and suspicious behaviours from the consumers or merchants (node-level anomaly detection), as well as anomalous order interactions (edge-level anomaly detection). As mentioned, this detection needs to be done without any label supervision.&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model architecture&lt;/h2&gt;

&lt;p&gt;We designed our graph anomaly model as a type of autoencoder, with an encoder and two decoders – a feature decoder and a structure decoder. The key feature of our model is that it accepts a bipartite graph with both node and edge attributes as the input. This is important as both node and edge attributes encode essential information for determining if certain behaviours are suspicious. Many previous works on graph anomaly detection only support node attributes. In addition, our model can produce both node and edge level anomaly scores, unlike most of the previous works that produce node-level scores only. We named our model GraphBEAN, which is short for &lt;strong&gt;B&lt;/strong&gt;ipartite Node-and-&lt;strong&gt;E&lt;/strong&gt;dge-&lt;strong&gt;A&lt;/strong&gt;ttributed &lt;strong&gt;N&lt;/strong&gt;etworks.&lt;/p&gt;

&lt;p&gt;From the input, the encoder then processes the attributed bipartite graph into a series of graph convolution layers to produce latent representations for both node groups. Our graph convolution layers produce new representations for each node in both node groups (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;), as well as for each edge in the graph. Note that the last convolution layer in the encoder only produces the latent representations for nodes, without producing edge representations. The reason for this design is that we only put the latent representations for the active actors, the nodes representing consumers and merchants, but not their interactions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. GraphBEAN architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;From the nodes’ latent representations, the feature decoder is tasked to reconstruct the original graph with both node and edge attributes via a series of graph convolution layers. As the graph structure is provided by the feature decoder, we task the structure decoder to learn the graph structure by predicting if there exists an edge connecting two nodes. This edge prediction, as well as the graph reconstructed by the feature decoder, are then compared to the original input graph via a reconstruction loss function.&lt;/p&gt;

&lt;p&gt;The model is then trained using the bipartite graph constructed from GrabFood and GrabMart transactions. We use a reconstruction-based loss function as the training objective of the model. After the training is completed, we compute the anomaly score of each node and edge in the graph using the trained model.&lt;/p&gt;

&lt;h2 id=&quot;anomaly-score-computation&quot;&gt;Anomaly score computation&lt;/h2&gt;
&lt;p&gt;Our anomaly scores are reconstruction-based. The score design assumes that normal behaviours are common in the dataset and thus, can be easily reconstructed by the model. On the other hand, anomalous behaviours are rare. Therefore the model will have a hard time reconstructing them, hence producing high errors.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. Edge-level and node-level anomaly scores computation
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The model produces two types of anomaly scores. First, the edge-level anomaly scores, which are calculated from the edge reconstruction error. Second, the node-level anomaly scores, which are calculated from node reconstruction error plus an aggregate over the edge scores from the edges connected to the node. This aggregate could be a mean or max aggregate.&lt;/p&gt;

&lt;h2 id=&quot;actioning-system&quot;&gt;Actioning system&lt;/h2&gt;

&lt;p&gt;In our implementation of GraphBEAN within Grab, we designed a full pipeline of anomaly detection and actioning systems. It is a fully-automated system for constructing a bipartite graph from GrabFood and GrabMart transactions, training a GraphBEAN model using the graph, and computing anomaly scores. After computing anomaly scores for all consumers and merchants (node-level), as well as all of their interactions (edge-level), it automatically passes the scores to our actioning system. But before that, it also passes them through a system we call &lt;em&gt;fraud type tagger&lt;/em&gt;. This is also a fully-automated heuristic-based system that tags some of the detected anomalies with some fraud tags. The purpose of this tagging is to provide some context in general, like the types of detected anomalies. Some examples of these tags are &lt;em&gt;promo abuse&lt;/em&gt; or &lt;em&gt;possible collusion&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. Pipeline in our actioning system&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Both the anomaly scores and the fraud type tags are then forwarded to our actioning system. The system consists of two subsystems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Human expert actioning system&lt;/strong&gt;: Our fraud experts analyse the detected anomalies and perform certain actioning on them, like suspending certain transaction features from suspicious merchants.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automatic actioning system&lt;/strong&gt;: Combines the anomaly scores and fraud type tags with other external signals to automatically do actioning on the detected anomalies, like preventing promos from being used by fraudsters or preventing fraudulent transactions from occurring. These actions vary depending on the type of fraud and the scores.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;The GraphBEAN model enables the detection of suspicious behaviour on graph data without the need for label supervision. By implementing the model on GrabFood and GrabMart platforms, we learnt that having such a system enables us to quickly identify new types of fraudulent behaviours and then swiftly perform action on them. This also allows us to enhance Grab’s defence against fraudulent activity and actively protect our users.&lt;/p&gt;

&lt;p&gt;We are currently working on extending the model into more generic heterogeneous (multi-entity) graphs. In addition, we are also working on implementing it to more use cases within Grab.&lt;/p&gt;

&lt;h2 id=&quot;citation-info&quot;&gt;Citation information&lt;/h2&gt;
&lt;p&gt;(*) If you use the paper for academic purposes, please cite the following publication:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R. Fathony, J. Ng and J. Chen, “Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs,” 2023 International Joint Conference on Neural Networks (IJCNN), Gold Coast, Australia, 2023, pp. 1-10, doi: 10.1109/IJCNN54540.2023.10191331.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;IEEE copyright notice:&lt;/p&gt;

&lt;p&gt;© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Aug 2023 01:23:05 +0000</pubDate>
        <link>https://engineering.grab.com/graph-anomaly-model</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-anomaly-model</guid>
        
        <category>Data science</category>
        
        <category>Graph networks</category>
        
        <category>Graphs</category>
        
        <category>Graph visualisation</category>
        
        <category>Security</category>
        
        <category>Fraud detection</category>
        
        <category>Anomaly detection</category>
        
        <category>Machine learning</category>
        
        
        <category>Data Science</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Zero traffic cost for Kafka consumers</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Coban, Grab’s real-time data streaming platform team, has been building an ecosystem around &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt;, serving all Grab verticals. Along with stability and performance, one of our priorities is also cost efficiency.&lt;/p&gt;

&lt;p&gt;In this article, we explain how the Coban team has substantially reduced Grab’s annual cost for data streaming by enabling Kafka consumers to fetch from the closest replica.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;The Grab platform is primarily hosted on AWS cloud, located in one region, spanning over three Availability Zones (AZs). When it comes to data streaming, both the Kafka brokers and Kafka clients run across these three AZs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Initial design, consumers fetching from the partition leader&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 1 shows the initial design of our data streaming platform. To ensure high availability and resilience, we configured each Kafka partition to have three replicas. We have also set up our Kafka clusters to be rack-aware (i.e. 1 “rack” = 1 AZ) so that all three replicas reside in three different AZs.&lt;/p&gt;

&lt;p&gt;The problem with this design is that it generates staggering cross-AZ network traffic. This is because, by default, Kafka clients communicate only with the partition leader, which has a 67% probability of residing in a different AZ.&lt;/p&gt;

&lt;p&gt;This is a concern as we are charged for cross-AZ traffic as per &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer_within_the_same_AWS_Region&quot;&gt;AWS’s network traffic pricing model&lt;/a&gt;. With this design, our cross-AZ traffic amounted to half of the total cost of our Kafka platform.&lt;/p&gt;

&lt;p&gt;The Kafka cross-AZ traffic for this design can be broken down into three components as shown in Figure 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Producing&lt;/strong&gt; (step 1): Typically, a single service produces data to a given Kafka topic. Cross-AZ traffic occurs when the producer does not reside in the same AZ as the partition leader it is producing data to. This cross-AZ traffic cost is minimal, because the data is transferred to a different AZ at most once (excluding retries).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Replicating&lt;/strong&gt; (step 2): The ingested data is replicated from the partition leader to the two partition followers, which reside in two other AZs. The cost of this is also relatively small, because the data is only transferred to a different AZ twice.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consuming&lt;/strong&gt; (step 3): Most of the cross-AZ traffic occurs here because there are many consumers for a single Kafka topic. Similar to the producers, the consumers incur cross-AZ traffic when they do not reside in the same AZ as the partition leader. However, on the consuming side, cross-AZ traffic can occur as many times as there are consumers (on average, two-thirds of the number of consumers). The solution described in this article addresses this particular component of the cross-AZ traffic in the initial design.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://archive.apache.org/dist/kafka/2.3.0/RELEASE_NOTES.html&quot;&gt;Kafka 2.3&lt;/a&gt; introduced the ability for consumers to fetch from partition replicas. This opens the door to a more cost-efficient design.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Target design, consumers fetching from the closest replica&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Step 3 of Figure 2 shows how consumers can now consume data from the replica that resides in their own AZ. Implementing this feature requires rack-awareness and extra configurations for both the Kafka brokers and consumers. We will describe this in the following sections.&lt;/p&gt;

&lt;h2 id=&quot;the-coban-journey&quot;&gt;The Coban journey&lt;/h2&gt;

&lt;h3 id=&quot;kafka-upgrade&quot;&gt;Kafka upgrade&lt;/h3&gt;

&lt;p&gt;Our journey started with the upgrade of our legacy Kafka clusters. We decided to upgrade them directly to version 3.1, in favour of capturing bug fixes and optimisations over version 2.3. This was a safe move as version 3.1 was deemed stable for almost a year and we projected no additional operational cost for this upgrade.&lt;/p&gt;

&lt;p&gt;To perform an online upgrade with no disruptions for our users, we broke down the process into three stages.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stage 1&lt;/strong&gt;: Upgrading Zookeeper. All versions of Kafka are tested by the community with a specific version of Zookeeper. To ensure stability, we followed this same process. The upgraded Zookeeper would be backward compatible with the pre-upgrade version of Kafka which was still in use at this early stage of the operation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 2&lt;/strong&gt;: Rolling out the upgrade of Kafka to version 3.1 with an explicit backward-compatible inter-broker protocol version (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inter.broker.protocol.version&lt;/code&gt;). During this progressive rollout, the Kafka cluster is temporarily composed of brokers with heterogeneous Kafka versions, but they can communicate with one another because they are explicitly set up to use the same inter-broker protocol version. At this stage, we also upgraded Cruise Control to a compatible version, and we configured Kafka to import the updated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cruise-control-metrics-reporter&lt;/code&gt; JAR file on startup.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 3&lt;/strong&gt;: Upgrading the inter-broker protocol version. This last stage makes all brokers use the most recent version of the inter-broker protocol. During the progressive rollout of this change, brokers with the new protocol version can still communicate with brokers on the old protocol version.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;/h3&gt;
&lt;p&gt;Enabling Kafka consumers to fetch from the closest replica requires a configuration change on both Kafka brokers and Kafka consumers. They also need to be aware of their AZ, which is done by leveraging Kafka rack-awareness (1 “rack” = 1 AZ).&lt;/p&gt;

&lt;h4 id=&quot;brokers&quot;&gt;Brokers&lt;/h4&gt;
&lt;p&gt;In our Kafka brokers’ configuration, we already had &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.rack&lt;/code&gt; set up to distribute the replicas across different AZs for resiliency. Our Ansible role for Kafka automatically sets it with the AZ ID that is dynamically retrieved from the EC2 instance’s metadata at deployment time.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Get availability zone ID
  uri:
    url: http://169.254.169.254/latest/meta-data/placement/availability-zone-id
    method: GET
    return_content: yes
  register: ec2_instance_az_id
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that we use AWS AZ IDs (suffixed &lt;em&gt;az1, az2, az3&lt;/em&gt;) instead of the typical AWS AZ names (suffixed &lt;em&gt;1a, 1b, 1c&lt;/em&gt;) because the latter’s mapping is &lt;a href=&quot;https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html&quot;&gt;not consistent across AWS accounts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, we added the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replica.selector.class&lt;/code&gt; parameter, set with value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;org.apache.kafka.common.replica.RackAwareReplicaSelector&lt;/code&gt;, to enable the new feature on the server side.&lt;/p&gt;

&lt;h4 id=&quot;consumers&quot;&gt;Consumers&lt;/h4&gt;
&lt;p&gt;On the Kafka consumer side, we mostly rely on Coban’s internal Kafka SDK in Golang, which streamlines how service teams across all Grab verticals utilise Coban Kafka clusters. We have updated the SDK to support fetching from the closest replica.&lt;/p&gt;

&lt;p&gt;Our users only have to export an environment variable to enable this new feature. The SDK then dynamically retrieves the underlying host’s AZ ID from the host’s metadata on startup, and sets a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client.rack&lt;/code&gt; parameter with that information. This is similar to what the Kafka brokers do at deployment time.&lt;/p&gt;

&lt;p&gt;We have also implemented the same logic for our non-SDK consumers, namely &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; pipelines and &lt;a href=&quot;https://developer.confluent.io/learn-kafka/kafka-connect/intro/&quot;&gt;Kafka Connect&lt;/a&gt; connectors.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;
&lt;p&gt;We rolled out fetching from the closest replica at the turn of the year and the feature has been progressively rolled out on more and more Kafka consumers since then.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Variation of our cross-AZ traffic before and after enabling fetching from the closest replica&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 shows the relative impact of this change on our cross-AZ traffic, as reported by AWS Cost Explorer. AWS charges cross-AZ traffic on both ends of the data transfer, thus the two data series. On the Kafka brokers’ side, less cross-AZ traffic is sent out, thereby causing the steep drop in the dark green line. On the Kafka consumers’ side, less cross-AZ traffic is received, causing the steep drop in the light green line. Hence, both ends benefit by fetching from the closest replica.&lt;/p&gt;

&lt;p&gt;Throughout the observeration period, we maintained a relatively stable volume of data consumption. However, after three months, we observed a substantial 25% drop in our cross-AZ traffic compared to December’s average. This reduction had a direct impact on our cross-AZ costs as it directly correlates with the cross-AZ traffic volume in a linear manner.&lt;/p&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;

&lt;h4 id=&quot;increased-end-to-end-latency&quot;&gt;Increased end-to-end latency&lt;/h4&gt;

&lt;p&gt;After enabling fetching from the closest replica, we have observed an increase of up to 500ms in end-to-end latency, that comes from the producer to the consumers. Though this is expected by design, it makes this new feature unsuitable for Grab’s most latency-sensitive use cases. For these use cases, we retained the traditional design whereby consumers fetch directly from the partition leaders, even when they reside in different AZs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - End-to-end latency (99th percentile) of one of our streams, before and after enabling fetching from the closest replica&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;inability-to-gracefully-isolate-a-broker&quot;&gt;Inability to gracefully isolate a broker&lt;/h4&gt;

&lt;p&gt;We have also verified the behaviour of Kafka clients during a broker rotation; a common maintenance operation for Kafka. One of the early steps of our corresponding runbook is to demote the broker that is to be rotated, so that all of its partition leaders are drained and moved to other brokers.&lt;/p&gt;

&lt;p&gt;In the traditional architecture design, Kafka clients only communicate with the partition leaders, so demoting a broker gracefully isolates it from all of the Kafka clients. This ensures that the maintenance is seamless for them. However, by fetching from the closest replica, Kafka consumers still consume from the demoted broker, as it keeps serving partition followers. When the broker effectively goes down for maintenance, those consumers are suddenly disconnected. To work around this, they must handle connection errors properly and implement a retry mechanism.&lt;/p&gt;

&lt;h4 id=&quot;potentially-skewed-load&quot;&gt;Potentially skewed load&lt;/h4&gt;

&lt;p&gt;Another caveat we have observed is that the load on the brokers is directly determined by the location of the consumers. If they are not well balanced across all of the three AZs, then the load on the brokers is similarly skewed. At times, new brokers can be added to support an increasing load on an AZ. However, it is undesirable to remove any brokers from the less loaded AZs as more consumers can suddenly relocate there at any time. Having these additional brokers and underutilisation of existing brokers on other AZs can also impact cost efficiency.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Average CPU utilisation by AZ of one of our critical Kafka clusters&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 shows the CPU utilisation by AZ for one of our critical Kafka clusters. The skewage is visible after 01/03/2023. To better manage this skewage in load across AZs, we have updated our SDK to expose the AZ as a new metric. This allows us to monitor the skewness of the consumers and take measures proactively, for example, moving some of them to different AZs.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;We have implemented the feature to fetch from the closest replica on all our Kafka clusters and all Kafka consumers that we control. This includes internal Coban pipelines as well as the managed pipelines that our users can self-serve as part of our data streaming offering.&lt;/p&gt;

&lt;p&gt;We are now evangelising and advocating for more of our users to adopt this feature.&lt;/p&gt;

&lt;p&gt;Beyond Coban, other teams at Grab are also working to reduce their cross-AZ traffic, notably, Sentry, the team that is in charge of Grab’s service mesh.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 07 Jul 2023 00:23:05 +0000</pubDate>
        <link>https://engineering.grab.com/zero-traffic-cost</link>
        <guid isPermaLink="true">https://engineering.grab.com/zero-traffic-cost</guid>
        
        <category>Engineering</category>
        
        <category>Kafka</category>
        
        <category>Performance</category>
        
        <category>Access control</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
  </channel>
</rss>
