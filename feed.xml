<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 17 Jul 2025 07:23:06 +0000</pubDate>
    <lastBuildDate>Thu, 17 Jul 2025 07:23:06 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>Grab&apos;s service mesh evolution: From Consul to Istio</title>
        <description>&lt;h2 id=&quot;the-challenge-when-good-enough-isnt-good-enough&quot;&gt;The challenge: When good enough isn’t good enough&lt;/h2&gt;

&lt;p&gt;Picture this: It’s 2024, and Grab’s microservices ecosystem is thriving with over 1000 services running in different infrastructure. But behind the scenes, our service mesh setup is showing its age. We’re running Consul with a fallback mechanism called Catcher - a setup that has  served us well but is starting to feel like wearing a winter coat in Singapore’s heat.&lt;/p&gt;

&lt;p&gt;The challenges we faced were becoming increasingly apparent. A single Consul server issue could trigger a fleet-wide impact, affecting critical services like food delivery and ride-hailing. Our fallback solution, while necessary, added complexity and limited our ability to implement advanced features like circuit breaking and retry policies. As we expanded our presence across Southeast Asia, the need for robust multi-cluster support became more critical than ever. The existing setup struggled with modern requirements like advanced traffic management and fine-grained security controls, while the growing complexity of our microservices architecture demanded better traffic management capabilities.&lt;/p&gt;

&lt;h2 id=&quot;the-complexity-of-grabs-infrastructure&quot;&gt;The complexity of Grab’s infrastructure&lt;/h2&gt;

&lt;p&gt;Our infrastructure landscape is as diverse as the Southeast Asian markets we serve. We operate a complex hybrid environment encompassing services on traditional VMs and EKS clusters with diverse infrastructure provisioning and deployment approaches. This diversity isn’t merely about deployment models—it’s about meeting the unique needs of different business units and regulatory requirements across the region.&lt;/p&gt;

&lt;p&gt;The complexity doesn’t stop there. We handle dual traffic protocols (HTTP and &lt;a href=&quot;https://en.wikipedia.org/wiki/GRPC&quot;&gt;gRPC&lt;/a&gt;) across our entire service ecosystem. Our services communicate across cloud providers between AWS and GCP. Within AWS alone, we maintain multiple organizations to segregate different Grab entities, each operating in its own isolated network. This multi-cloud, multi-protocol, multi-organization setup presented unique challenges for our service mesh implementation.&lt;/p&gt;

&lt;h2 id=&quot;the-quest-for-a-better-solution&quot;&gt;The quest for a better solution&lt;/h2&gt;

&lt;p&gt;Like any good tech team, we didn’t just jump to conclusions. We embarked on a thorough evaluation of service mesh solutions, considering various options including Application Load Balancer (&lt;a href=&quot;https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html&quot;&gt;ALB&lt;/a&gt;), &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/vpc/lattice/&quot;&gt;AWS Lattice&lt;/a&gt;, and &lt;a href=&quot;https://linkerd.io/&quot;&gt;Linkerd&lt;/a&gt;. Our evaluation process was comprehensive and focused on real-world needs, examining everything from stability under high load to the performance impact on service-to-service communication.&lt;/p&gt;

&lt;p&gt;We needed a solution that could handle our distributed architecture while maintaining operational simplicity. The ideal service mesh would need to integrate seamlessly with our existing infrastructure landscape while offering the flexibility to scale with our growing needs. After careful consideration, Istio emerged as the clear winner, offering robust multi-cluster support with flexible deployment models and a comprehensive set of features for traffic management, security, and observability.&lt;/p&gt;

&lt;p&gt;What really sealed the deal was Istio’s strong Kubernetes integration and native support, combined with active community backing. The rich ecosystem of tools and integrations meant we wouldn’t be building everything from scratch, while the flexible deployment options could accommodate our unique requirements.&lt;/p&gt;

&lt;h2 id=&quot;designing-our-istio-architecture&quot;&gt;Designing our Istio architecture&lt;/h2&gt;

&lt;p&gt;When it came to designing our Istio implementation, we took a slightly unconventional approach. Instead of following the traditional “one control plane per cluster” pattern, we designed a more resilient architecture that would better suit our needs. We implemented multiple control planes running on dedicated Kubernetes clusters for better isolation and scalability, with active-active pairs ensuring high availability.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/service-mesh-evolution/external-control-planes.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. External control planes and Kubernetes API servers - Endpoints discovery&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/service-mesh-evolution/istio-proxy.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Istio proxy to Istio control plane - xDS flow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our architecture needed to support high-throughput service-to-service communication while enabling complex routing rules for A/B testing and canary deployments. We implemented custom resource definitions for service mesh configuration and integrated with our existing monitoring and alerting systems. The organization-based mesh boundaries we designed would support our multi-tenant architecture, while our solution for cross-cluster endpoint discovery would ensure reliable service communication across our distributed system.&lt;/p&gt;

&lt;p&gt;This design wasn’t just about following best practices - it was about learning from our past experiences with etcd and Consul. We wanted a setup that could handle Grab’s scale while maintaining simplicity and reliability. The architecture needed to support everything from high-throughput service-to-service communication to complex routing rules for A/B testing and canary deployments, all while maintaining fine-grained security policies and comprehensive observability.&lt;/p&gt;

&lt;h2 id=&quot;the-migration-journey-begins&quot;&gt;The migration journey begins&lt;/h2&gt;

&lt;p&gt;In Q4 2024, we kicked off our migration journey with a clear plan. While our initial strategy focused on gRPC traffic migration, real-world priorities led us down a different path. Our first major milestone was the GCP to AWS migration, a cross-cloud initiative that would test our service mesh capabilities in a complex, multi-cloud environment.&lt;/p&gt;

&lt;p&gt;This cross-cloud migration was a significant undertaking, requiring careful coordination between teams and careful consideration of network policies, security requirements, and service dependencies. We had to ensure seamless communication between services running in different cloud providers while maintaining security and performance standards.&lt;/p&gt;

&lt;p&gt;Alongside our ongoing cloud migration efforts, we launched parallel initiatives focused on gRPC and HTTP traffic migration with cross-mesh connectivity requirements. This phase introduced distinct challenges, as it involved migrating business-critical services while implementing gradual traffic shifting capabilities and quick rollback mechanisms to ensure zero-downtime migrations. We also maintained close monitoring of performance metrics throughout the process.&lt;/p&gt;

&lt;p&gt;Additionally, we needed to ensure seamless compatibility between different service mesh implementations and navigate the complexities of cross-mesh communication. The insights and experience gained from our cloud migration phase have proven invaluable in informing our approach and execution strategy for this critical migration effort.&lt;/p&gt;

&lt;p&gt;The journey hasn’t been without its challenges. We’ve had to balance migration speed with stability while coordinating across multiple teams and organizations. Handling both gRPC and HTTP traffic patterns required careful planning and execution. We’ve had to deal with legacy systems and technical debt while training and supporting teams through the transition. Maintaining service continuity during these transitions has been our top priority.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h2&gt;

&lt;p&gt;This journey has taught us several valuable lessons. We’ve learned that sometimes the standard approach isn’t the best fit, and innovation often comes from questioning assumptions. We’ve discovered the importance of balancing innovation with stability, taking calculated risks while building capability for a quick mitigation.&lt;/p&gt;

&lt;p&gt;Keeping the bigger picture in mind has been crucial, considering long-term implications and planning for scale and growth. We’ve learned to document challenges and solutions, sharing knowledge across teams to avoid repeating mistakes. Most importantly, we’ve learned to stay flexible and adapt to changing needs, being ready to pivot when necessary while keeping an eye on emerging technologies.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The service mesh landscape is constantly evolving, and we’re excited to be part of this journey. Our next steps include continuing our migration efforts with a focus on stability while exploring mesh features like advanced traffic management, and enhanced security policies.&lt;/p&gt;

&lt;p&gt;We’re also working on enhancing our operational capabilities through automated testing and validation, improved monitoring and alerting, and better debugging tools. As we progress, we’re committed to sharing our experiences with the community through open source contributions, conference participation, and technical blogs.&lt;/p&gt;

&lt;h2 id=&quot;shape-the-future-with-us&quot;&gt;Shape the future with us&lt;/h2&gt;

&lt;p&gt;We’re not just implementing a service mesh—we’re architecting the backbone of Grab’s microservices future. Every decision prioritizes reliability, scalability, and maintainability, ensuring we build something that will stand the test of time.&lt;/p&gt;

&lt;p&gt;The journey continues, and we’re excited about what lies ahead. Follow our progress for real-world insights that might shape your own service mesh evolution.&lt;/p&gt;

&lt;p&gt;Want to help us build the future? We have exciting &lt;a href=&quot;https://jobs.smartrecruiters.com/Grab/744000061741882-infra-engineer-manager-service-mesh&quot;&gt;opportunities&lt;/a&gt; waiting for you.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Credits to the Service Mesh team: Aashif Bari, Hilman Kurniawan, Hofid Mashudi, Jingshan Pang, Kaitong Guo, Mikko Turpeinen, Sok Ann Yap, Jesse Nguyen, and Xing Yii. &lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://www.grab.careers&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Jul 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/service-mesh-evolution</link>
        <guid isPermaLink="true">https://engineering.grab.com/service-mesh-evolution</guid>
        
        <category>Microservice</category>
        
        <category>Service mesh</category>
        
        <category>Kubernetes</category>
        
        <category>AWS</category>
        
        <category>GCP</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>DispatchGym: Grab’s reinforcement learning research framework</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;DispatchGym is a research framework designed to facilitate Reinforcement Learning (RL) studies and applications for the dispatch system, which matches bookings with drivers. The primary goal is to empower data scientists with a tool that allows them to independently develop and test RL-related concepts for dispatching systems. It accelerates research by providing a suite of modules that include a reinforcement learning algorithm, a dispatching process simulation, and an interface connecting the two through the &lt;a href=&quot;https://gymnasium.farama.org/introduction/basic_usage/&quot;&gt;Gymnasium&lt;/a&gt; API.&lt;/p&gt;

&lt;p&gt;To ensure efficient and cost-effective RL research without compromising on quality, DispatchGym aims to be both comprehensive and accessible. Anyone with basic RL knowledge and Python programming skills can use it to explore new ideas in RL and dispatch system logic.&lt;/p&gt;

&lt;p&gt;This article walks you through the principles behind DispatchGym, how these principles effectively and efficiently empower impactful research, and how it can be applied to solve real world problems.&lt;/p&gt;

&lt;h2 id=&quot;the-challenge-with-rl&quot;&gt;The challenge with RL&lt;/h2&gt;

&lt;p&gt;Although RL methods can be applied to a wide variety of problems that can be formulated as a Markov Decision Process (MDP), designing an effective RL-based solution is not a trivial task. The primary challenges stem from two key components: the reward function and the lever.&lt;/p&gt;

&lt;p&gt;In RL, the reward function represents the objective we aim to maximize. At first glance, it might seem straightforward to plug in any metric, such as the company’s profit or the number of completed bookings per day. However, these metrics are not always sensitive to the lever that RL can manipulate, or the lever itself may not significantly influence the objective. For example, consider a setup where we aim to maximize the daily number of completed bookings by adjusting the maximum number of candidate drivers considered to each booking. Beyond a minimal threshold (e.g., one driver), further increasing this limit provides negligible benefits. As a result, RL struggles to determine whether setting this limit to 11 or 15 would result in higher rewards.&lt;/p&gt;

&lt;p&gt;In summary, when a lever exerts weak influence on a reward function, the RL setup becomes ineffective. Therefore, we should strive to select a lever that strongly influences the reward function and define a reward function that is both sensitive to manipulations of that lever and aligned with our overall goal. Note that the reward function does not have to be identical to our ultimate objective; it merely needs to be highly correlated with it.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dispatchgym/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Illustration of weak lever influence on a reward function.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;empowering-research-with-dispatchgym&quot;&gt;Empowering research with DispatchGym&lt;/h2&gt;

&lt;p&gt;The primary application of DispatchGym is to accelerate and broaden cost-effective research and impactful RL applications for Grab’s dispatching system. A system which is responsible for assigning a driver to each booking. To achieve this, DispatchGym must have the following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reliable&lt;/strong&gt;&lt;br /&gt;
The simulation component should be accurate enough to capture essential behaviors strongly linked to the metrics of interest, without necessarily modeling everything else. While it’s beneficial if the simulation can do more than the specific use case (e.g., simulating both batching and allocation when only allocation is needed), it is not strictly required.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost-effective&lt;/strong&gt;&lt;br /&gt;
Updating all of DispatchGym’s components should require minimal monetary and labor costs to enable rapid iteration. This includes keeping the simulation component aligned with real system behaviors, incorporating the latest technologies in the optimization component, and maintaining seamless integration between the simulation and optimization components.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Empowering&lt;/strong&gt;&lt;br /&gt;
It should be as easy as possible for data scientists and engineers to modify any DispatchGym component and then run experiments. This flexibility is crucial because new research typically requires adjustments to both the simulation and optimization components. By granting users the freedom to adapt DispatchGym, the framework fosters continuous innovation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-friendly-simulated-environment&quot;&gt;Research-friendly simulated environment&lt;/h2&gt;

&lt;p&gt;The simulation component of DispatchGym, or the “simulated environment,” is designed with reliability, cost-effectiveness, and user empowerment in mind. It models the full dispatching process, from booking creation and driver dispatch to driver movement and booking completion. While this environment may not be perfectly accurate in absolute terms (there can be differences between real and simulated metric values), it emphasizes directional accuracy. This means that the metric trends (up or down) in the simulation closely match real-world behavior. This focus on directional accuracy is crucial because most research involves sim-to-sim comparisons, where shifts in metrics are the most important. Verifying directional accuracy is also simpler and more practical for evaluating simulation performance. For instance, we can test various supply-demand imbalance scenarios and check whether a supply-rich situation indeed fulfills more bookings, and vice versa.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dispatchgym/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Simulated processes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The simulated environment’s cost-effectiveness and empowerment features come from a modular architecture and Python, a research-friendly programming language. The modular design offers a gentle learning curve, allowing users to easily navigate and make necessary changes in the codebase. Meanwhile, Python is selected to lower the entry barrier for adopting DispatchGym. To mitigate Python’s runtime overhead, DispatchGym leverages Numba to significantly speed up simulation execution.&lt;/p&gt;

&lt;h2 id=&quot;dispatchgym-in-action&quot;&gt;DispatchGym in action&lt;/h2&gt;

&lt;p&gt;Data scientists use DispatchGym by modifying a local copy of the codebase to implement their ideas. They then upload the updated codebase to an internal infrastructure using a single CLI command, which spawns a Spark job to run the DispatchGym program. This setup grants complete flexibility over the simulation and optimization components without requiring users to manage the underlying infrastructure.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dispatchgym/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Data scientist interactions with DispatchGym.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;applying-rl-approach-for-dispatch&quot;&gt;Applying RL approach for dispatch&lt;/h2&gt;

&lt;p&gt;Amongst its many uses, DispatchGym was applied in building an effective contextual bandit strategy for the auto-adaptive tuning of dispatch-related hyperparameters. Its flexibility allowed us to experiment with various contextual bandit model variants, including linear bandits, neural-linear bandits, and Gaussian-process bandits, as well as multiple action sampling strategies, such as epsilon-greedy, Thompson sampling, SquareCB, and FastCB. These capabilities accelerated our progress in determining the best combination of levers, reward functions, and contextual bandits for improved fulfilment efficiency and reliability.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;DispatchGym provides us a framework that equips data scientists with everything they need to develop and test RL solutions for dispatch systems. By integrating an RL optimization approach and a realistic dispatch simulation using a Gymnasium API, it enables rapid exploration and iteration of RL applications with just basic RL knowledge and Python programming language.&lt;/p&gt;

&lt;p&gt;A major hurdle in applying RL to dispatch problems modeled as MDP is ensuring that the reward function aligns with ultimate business goals and is sensitive to the lever under control. If the lever (e.g., tweaking driver count) does not meaningfully influence the reward, the RL approach falters. DispatchGym addresses this by making it easy for data scientists to determine the most effective combinations of levers, reward functions, and RL approaches, ultimately driving positive business impact.&lt;/p&gt;

&lt;p&gt;DispatchGym’s architecture focuses on reliability, cost-effectiveness, and user empowerment. Its simulation is designed to capture critical metrics and reflect real-world trends (directional accuracy), while its Python-based modular design enhanced by Numba enables easy prototyping. Researchers can adjust the environment locally before deploying changes seamlessly via a command-line interface, avoiding infrastructure overhead. These design decisions and capabilities empower data scientists to refine contextual bandit approaches for optimizing dispatch hyperparameters and explore innovative RL applications in the dispatch process.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to thank Chongyu Zhou, Guowei Wong, and Roman Kotelnikov for their collaboration in developing the RL-based optimizer. &lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdispgym&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Jul 2025 07:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/techblog_-dispatchgym</link>
        <guid isPermaLink="true">https://engineering.grab.com/techblog_-dispatchgym</guid>
        
        <category>Dispatch</category>
        
        <category>Python</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Counter Service: How we rewrote it in Rust</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;The Integrity Data Platform (IDP) team decided to rewrite one of our heavy Queries Per Second (QPS) Golang microservices in Rust. It resulted in 70% infrastructure savings at a similar performance, but was not without its pitfalls. This article will elaborate on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How we picked what to rewrite in Rust.&lt;/li&gt;
  &lt;li&gt;Approach taken to tackle the rewrite.&lt;/li&gt;
  &lt;li&gt;The pitfalls and speed bumps along the way.&lt;/li&gt;
  &lt;li&gt;Was it worthwhile?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Grab is predominantly based on a microservice architecture, with the vast majority of microservices being hosted in a monorepo and written in Golang. It has served the company well so far, as the “simplicity” of Golang allows developers to ramp up and iterate quickly.&lt;/p&gt;

&lt;p&gt;However, Rust has seen some gradual adoption across the company. Starting with a few minor &lt;a href=&quot;/how-we-reduced-our-ci-yaml&quot;&gt;CLIs&lt;/a&gt;, which then progressed to notable success with a Rust-based reverse proxy in &lt;a href=&quot;/catwalk-serving-machine-learning-models-at-scale&quot;&gt;Catwalk&lt;/a&gt; for model serving. Additionally, a growing community of Rust enthusiasts within the organisation has expressed interest in advocating for and expanding the adoption of Rust more proactively.&lt;/p&gt;

&lt;p&gt;After achieving success with several projects on the ML platform and addressing concerns about Rust’s ability to handle traffic at scale, the next logical step was to assess the Return on Investment (ROI) of rewriting a Golang microservice in Rust.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Rust has the reputation of being &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3136014.3136031&quot;&gt;highly efficient&lt;/a&gt; yet poses a steep learning curve. Rust is often touted to perform close to C, doing away with garbage collection while remaining memory safe through strict compile checks and the borrow checker. It is loved by developers for having rich features like being multi-paradigm (supporting both functional and OOP style), having a rich type system, and doing away with nil pointers and errors.&lt;/p&gt;

&lt;p&gt;However, regardless of how well regarded a certain language is in the industry, rewrites of any system should always be considered very carefully. When it comes to “legacy software”, there is a prevalent assumption that rewriting legacy software is a solution to eliminate technical debt and phase out legacy systems. The reality is often more nuanced.&lt;/p&gt;

&lt;p&gt;Legacy code occurs when the developers who originally wrote the code are no longer working on the project. There are often business logic and edge-cases baked into complex legacy codebases of which the context has been lost over time. In practice, rewrites frequently take longer than anticipated and tend to reintroduce bugs and edge cases that must be identified and resolved all over again.&lt;/p&gt;

&lt;p&gt;Rewriting vs refactoring has been written at length across the internet, you can read more about it &lt;a href=&quot;https://herbcaudill.com/words/20190219-rewrite-refactor-reinvent&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The trade-offs of rewriting need to be properly weighed and balanced. It must take into consideration:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How much engineering bandwidth goes into the rewrite?&lt;/li&gt;
  &lt;li&gt;What is the complexity of the rewrite?&lt;/li&gt;
  &lt;li&gt;What tangible benefits are brought about by the rewrite?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rewriting a system solely for the purpose of “rewriting it in Rust” is not a strong enough business justification.&lt;/p&gt;

&lt;p&gt;A legitimate concern was the steep learning curve of Rust, coupled with the risk of having only one team member proficient in the language, which would make its adoption unsustainable.&lt;/p&gt;

&lt;p&gt;Therefore, we established a set of guidelines to follow when identifying a suitable system for a potential rewrite:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The system must be “simple” enough in functionality. For example, it has one or two main functionalities that can be rewritten in a reasonable amount of time and have its complexity constrained.&lt;/li&gt;
  &lt;li&gt;The system targeted should have large enough traffic such that cost savings brought about by adopting Rust is something tangible when balanced against the effort.&lt;/li&gt;
  &lt;li&gt;The members of the team must be comfortable and willing to pick up the language and achieve a certain level of familiarity to make maintaining the service sustainable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;finding-the-right-service&quot;&gt;Finding the right service&lt;/h2&gt;

&lt;p&gt;The ideal service should have a sufficiently large infrastructure footprint to justify the potential cost savings, while also being straightforward in functionality to minimise time spent on handling edge cases and complex business logic.&lt;/p&gt;

&lt;p&gt;Looking across the stack of microservices in Integrity, &lt;a href=&quot;/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;Counter Service&lt;/a&gt; stands out. As its name implies, Counter Service is a service that “counts” and serves the counters for ML models and fraud rules. The original service has two primary functionalities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consuming from streams, counting events and &lt;strong&gt;writing&lt;/strong&gt; to &lt;a href=&quot;/seamless-migration&quot;&gt;Scylla&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Exposing Google Remote Procedure Call (GRPC) endpoints to query from Scylla (and Redis) and return counts of events based on query keys. For example, &lt;strong&gt;BatchRead&lt;/strong&gt;. BatchRead’s functionality of Counter Service serves up to tens of thousands of QPS at peak and is fairly constrained in functionality. Hence, it fulfilled our target criteria of being “simple” in functionality yet serving a large enough amount of traffic that justifies the ROI of a rewrite.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rust-blog/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: BatchRead flow of Counter Service, reading data from Scylla, DynamoDB, Redis, MySQL and serving the counters through GRPC.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;rewrite-approach&quot;&gt;Rewrite approach&lt;/h3&gt;

&lt;p&gt;There are a few ways to approach a rewrite in another language. One popular way is to convert your code line by line. If the languages are close enough, it might even be possible to programmatically convert your code like &lt;a href=&quot;https://github.com/immunant/c2rust&quot;&gt;C2Rust&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We decided not to use such an approach for our rewrite. The major reason is that idiomatic Golang was not necessarily idiomatic Rust. We wanted to approach this rewrite with a fresh perspective and treat this as a true rewrite.&lt;/p&gt;

&lt;p&gt;We treated the application like a black box, with the interfaces well defined, like GRPC endpoints and contracts. Similar to a function, you could call the API and get a deterministic result, and we had the data that was stored in Scylla.&lt;/p&gt;

&lt;p&gt;Based on how we understood the application to work based on its specs and contract, we chose to rewrite the application logic from scratch to meet the API contract and to get as close as identical outputs from the new black box.&lt;/p&gt;

&lt;h3 id=&quot;oss-library-support&quot;&gt;OSS library support&lt;/h3&gt;

&lt;p&gt;We started out by mapping out the key external dependencies and checking how well they were supported in the Rust ecosystem and in open source. 
&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;&quot; align=&quot;middle&quot;&gt;
&lt;b&gt;Table 1: List of libraries and their star ratings&lt;/b&gt;
&lt;/div&gt;
&lt;table class=&quot;table&quot; style=&quot;&quot; align=&quot;middle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Functionality&lt;/th&gt;
      &lt;th&gt;Library&lt;/th&gt;
      &lt;th&gt;Stars (as of Nov 24) &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Datadog (Statsd Client)&lt;/td&gt;
      &lt;td&gt; &lt;a href=&quot;https://github.com/56quarters/cadence&quot;&gt;https://github.com/56quarters/cadence&lt;/a&gt; &lt;/td&gt;
      &lt;td&gt;&amp;lt; 500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Lightstep (OpenTelemetry) &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-rust&quot;&gt;https://github.com/56quarters/cadence&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &amp;gt; 1000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; GRPC Server  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/hyperium/tonic&quot;&gt;https://github.com/hyperium/tonic&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;  &amp;gt; 500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Web Server   &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/actix/actix-web&quot;&gt;https://github.com/actix/actix-web&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &amp;gt; 20,000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Redis Client  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/aembke/fred.rs&quot;&gt;https://github.com/aembke/fred.rs&lt;/a&gt; (Async Redis Library + Client pool)&lt;/td&gt;
      &lt;td&gt; &amp;gt; 5000&lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt; 
    &lt;td&gt; Redis Client  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/redis-rs/redis-rs&quot;&gt;https://github.com/redis-rs/redis-rs&lt;/a&gt; (“Official” redis client, initially picked but discarded)&lt;/td&gt;
      &lt;td&gt; &amp;gt; 3000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; Scylla Client  &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/scylladb/scylla-rust-driver&quot;&gt;https://github.com/scylladb/scylla-rust-driver&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; ~500 &lt;/td&gt;
    &lt;/tr&gt;
   &lt;tr&gt;
    &lt;td&gt; Kafka Client &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kafka-rust/kafka-rust&quot;&gt;https://github.com/kafka-rust/kafka-rust&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &amp;gt;1000 &lt;/td&gt;
    &lt;/tr&gt;
 &lt;/tbody&gt;  
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;All the functionality we need is available through libraries in the Rust ecosystem. However, we found that some libraries are not particularly “popular,” as indicated by their relatively low number of GitHub stars.&lt;/p&gt;

&lt;p&gt;The practical concern with using less “popular” libraries is the risk of limited community support or potential abandonment over time. That said, if an “unpopular” library is officially maintained by the associated open-source project—for instance, the Scylla driver has only about 500 stars but is officially provided by the Scylla project—we would need to ensure confidence that it will continue to receive active support.&lt;/p&gt;

&lt;p&gt;Out of the list of libraries above, the “unpopular” and unofficial libraries can be narrowed down to two libraries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Datadog - Cadence&lt;/li&gt;
  &lt;li&gt;Redis - Fred&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For Datadog, there is no “official” Datadog Rust client. Yet, we picked Cadence as the API looked intuitive and the features we needed were already supported.&lt;/p&gt;

&lt;p&gt;In regards to Redis, after testing it, we discovered that the support was not up to par with our requirements. We then opted for a newer and less popular library, fred.rs that seemed to be actively being developed by the community.&lt;/p&gt;

&lt;h3 id=&quot;company-specific-internal-libraries&quot;&gt;Company specific internal libraries&lt;/h3&gt;

&lt;p&gt;With the vast majority of microservices being written in Golang, most internal libraries are also written in Golang. Opting to rewrite a service in Rust means we are not able to use these internal libraries.&lt;/p&gt;

&lt;p&gt;Examples include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An internal configuration library that utilises Go Templates to template configurations for different environments (staging and production).&lt;/li&gt;
  &lt;li&gt;The internal configuration library has its own wrappers and injectors to pull and render secrets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To overcome this gap and re-use Go Templates and configuration language, we decided to write a simple wrapper and parser using the &lt;a href=&quot;https://github.com/rust-bakery/nom&quot;&gt;nom parser combinator&lt;/a&gt; to parse the templates and render the config.&lt;/p&gt;

&lt;p&gt;Nom poses a steep learning curve. But once familiarised, it is flexible and performant enough to build an equivalent to the internal library. Parser combinators are an interesting subset of tooling that allows you to create some fairly elegant parsers.&lt;/p&gt;

&lt;h2 id=&quot;road-bumps&quot;&gt;Road bumps&lt;/h2&gt;

&lt;h3 id=&quot;the-borrow-checker&quot;&gt;The borrow checker&lt;/h3&gt;

&lt;p&gt;One of the most striking paradigm shifts for developers transitioning to Rust is adapting to the strict rules of the borrow checker, which enforces that variables cannot be reused multiple times unless explicitly cloned or borrowed.&lt;/p&gt;

&lt;p&gt;Interestingly, the borrow checker was not the biggest hurdle for new developers. The key is to avoid introducing lifetimes too early in the development process, as this can lead to premature code optimisation.&lt;/p&gt;

&lt;p&gt;In many cases, adding a few clones (and occasionally Arcs) can help new developers get up to speed and iterate more quickly during development. The resulting code is usually “fast” enough for initial purposes. After that, the code can be revisited to eliminate unnecessary clones for improved performance. An efficient approach to this can be taken by using Flamegraph to profile your code and identify memory allocation bottlenecks.&lt;/p&gt;

&lt;h3 id=&quot;async-gotchas&quot;&gt;Async gotchas&lt;/h3&gt;

&lt;p&gt;When rewriting Golang logic in Rust, there are fundamental differences in how they treat concurrency and parallelism.&lt;/p&gt;

&lt;p&gt;One of Golang’s most remarkable strengths is its ability to deliver high-performance concurrency while preserving simplicity.&lt;/p&gt;

&lt;p&gt;There are two fundamental approaches to concurrency in programming languages, namely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Preemptive scheduling (stackful coroutines).&lt;/li&gt;
  &lt;li&gt;Cooperative scheduling (stackless coroutines).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Preemptive vs cooperative scheduling is an in-depth topic with the gist of it being, Golang uses preemptive scheduling and each “Goroutine” has a stack that needs a runtime. The Golang scheduler has the power to “preempt” and “freeze” functions and switch to another stack like stackful coroutine. This is a gross oversimplification of the nuances. For more details, this is a good &lt;a href=&quot;https://medium.com/a-journey-with-go/go-asynchronous-preemption-b5194227371c&quot;&gt;introduction&lt;/a&gt; to the topic.&lt;/p&gt;

&lt;p&gt;Rust opts for cooperative scheduling whereby it has no runtime and each coroutine does not maintain a stack. Hence, it has no ability to “freeze” a function and swap context. This allows Rust to be more efficient in terms of memory and resources, as it maintains a state machine. However, the consequence is that this moves the complexity up the stack to the programming language itself. Similar to Javascript, functions are “coloured”, and the developer has to explicitly annotate their functions to be async or sync. Await points need to be explicitly called and control needs to be “yielded” (i.e. cooperative and stackless) so the Rust program knows when it is allowed to stop and swap between coroutines. To read more on this, refer to &lt;a href=&quot;https://tokio.rs/blog/2019-10-scheduler&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;https://without.boats/blog/why-async-rust/&quot;&gt;this&lt;/a&gt; article for the history of async Rust.&lt;/p&gt;

&lt;p&gt;Needing to annotate a function is a classic complaint that is addressed in the article “&lt;a href=&quot;https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/&quot;&gt;What Colour is Your Function&lt;/a&gt;” that highlights developers’ responsibility to explicitly colour their function and consciously think about &lt;a href=&quot;https://ryhl.io/blog/async-what-is-blocking/&quot;&gt;blocking vs non-blocking code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Contrast this with Golang, where you simply need to add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go&lt;/code&gt; keyword without thinking about which code might block the execution and use channels to communicate across Goroutines. Golang allows the developer to achieve high performance without much cognitive overhead.&lt;/p&gt;

&lt;p&gt;This is especially important for developers new to Rust. As the lack of experience in async and blocking code can be somewhat of a footgun. In the initial rewrite of Rust, we made an amateur mistake of using a synchronous Redis function to call the Redis cache. It resulted in the application performing poorly until we corrected it with the non-blocking asynchronous version using the Fred redis library.&lt;/p&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;Following the eventful process of rewriting the service from the ground up in Rust, the outcomes proved to be quite intriguing.&lt;/p&gt;

&lt;p&gt;Shadowing traffic to both services as seen in Figure 2, the P99 latency is similar (or perhaps even slightly worse) in the Rust service compared to the original Golang one.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rust-blog/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: P99 latency comparison between the Golang service (purple) and Rust service (blue). 
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Normalising the QPS and resource consumption, we see from Table 2 that Rust consumes ~20% of the resources of the original Golang application, resulting in 5x savings in terms of resource consumption.
&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;&quot; align=&quot;middle&quot;&gt;
&lt;b&gt;Table 2: Comparison of resource consumption between Rust and Golang service.&lt;/b&gt;
&lt;/div&gt;
&lt;table class=&quot;table&quot; style=&quot;&quot; align=&quot;middle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Service&lt;/th&gt;
      &lt;th&gt;Indicative QPS&lt;/th&gt;
      &lt;th&gt;Resources &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Original Golang Service&lt;/td&gt;
      &lt;td&gt;1,000&lt;/td&gt;
      &lt;td&gt;20 Cores&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt; New Rust Service&lt;/td&gt;
      &lt;td&gt;1,000&lt;/td&gt;
      &lt;td&gt;4.5 Cores&lt;/td&gt;
    &lt;/tr&gt;
 &lt;/tbody&gt;   
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;learnings-and-conclusion&quot;&gt;Learnings and conclusion&lt;/h2&gt;

&lt;p&gt;The outcomes and insights from this rewrite have been eye-opening, debunking certain myths while also validating others.&lt;/p&gt;

&lt;h3 id=&quot;myth-1-rust-is-blazingly-fast-faster-than-golang&quot;&gt;Myth 1: Rust is blazingly fast! Faster than Golang!&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: Disproved.
Golang is “fast enough” for most use cases. It’s a mature language built with concurrency at its core, and it performs exceptionally well in its intended domain. While Rust can outperform Golang due to its higher performance ceiling and finer-grained control, rewriting a Golang service in Rust solely for performance improvements is unlikely to yield significant benefits.&lt;/p&gt;

&lt;h3 id=&quot;myth-2-rust-is-more-efficient-than-golang&quot;&gt;Myth 2: Rust is more efficient than Golang&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: True.
Rewriting a Golang service in Rust will probably give you 50% savings in compute. Rust does fulfill its promise of being memory safe without garbage collection, allowing it to be one of the more efficient languages out there. This is in line with &lt;a href=&quot;https://aws.amazon.com/blogs/opensource/sustainability-with-rust/&quot;&gt;other discoveries&lt;/a&gt; in the market.&lt;/p&gt;

&lt;h3 id=&quot;myth-3-the-learning-curve-of-rust-is-too-high&quot;&gt;Myth 3: The learning curve of Rust is too high&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: It depends.
Pure synchronous Rust is fine. As long as you don’t overcomplicate the code and only clone what is needed, it is mostly true. The language is easy enough to pick up for most experienced developers. Even with cloning sprinkled in, the code is usually “fast enough”. The compiler is a good teacher, the compiler error messages are amazing, and if your code compiles, it probably works. Also, the Clippy linter is amazing.&lt;/p&gt;

&lt;p&gt;However, introducing async can be challenging. Async is something quite different from what you would encounter in other languages like Go. Improper use of blocking code in async code can result in nuanced bugs that can catch inexperienced Rust developers off-guard.&lt;/p&gt;

&lt;h2 id=&quot;evaluating-the-worth-of-the-rewrite&quot;&gt;Evaluating the worth of the rewrite&lt;/h2&gt;

&lt;p&gt;Yes, the effort was worth it for this service. The trade-off between development effort spent and the cost savings were justified.&lt;/p&gt;

&lt;p&gt;As a side effect, the service is 80% cheaper and probably more bug free, as Rust eliminates a class of common Golang errors like Null pointers and concurrent map writes by virtue of the design of the language. If your code compiles, you usually have the confidence that it will work as you expect due to the language being more explicit.&lt;/p&gt;

&lt;p&gt;Would we encourage choosing Rust over Golang for new microservices? Absolutely, as the resulting service is likely to be at least 50% more efficient than its Go counterpart. However, this decision presents an important and exciting opportunity for management and leaders to invest in empowering their engineers by equipping them with the skills to master Rust’s unique concepts, such as Async and Lifetimes. While the initial development pace might be slower as the team builds proficiency, this investment can unlock long-term benefits. Once the workforce is skilled in Rust, development speed should align with expectations, and the resulting systems are likely to be more stable and secure, thanks to Rust’s inherent safety features.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebrust&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Jun 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/counter-service-how-we-rewrote-it-in-rust</link>
        <guid isPermaLink="true">https://engineering.grab.com/counter-service-how-we-rewrote-it-in-rust</guid>
        
        <category>Database</category>
        
        <category>Rust</category>
        
        <category>Data</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>The complete stream processing journey on FlinkSQL</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the fast-paced world of data analytics, real-time processing has become a necessity. Modern businesses require insights not just quickly, but in real-time to make informed decisions and stay ahead of the competition. &lt;strong&gt;Apache Flink&lt;/strong&gt; has emerged as a powerful tool in this domain, offering state-of-the-art stream processing capabilities. In this blog, we introduce our FlinkSQL interactive solution in accompanying productionising automation, and enhancing our users’ stream processing development journey.&lt;/p&gt;

&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;Last year, we introduced Zeppelin notebooks for Flink, as detailed in our previous post &lt;a href=&quot;https://engineering.grab.com/rethinking-streaming-processing-data-exploration&quot;&gt;Rethinking Stream Processing: Data Exploration&lt;/a&gt; in an effort to enhance data exploration for downstream data users. However, as our use cases evolved over time, we quickly hit a few technical roadblocks.&lt;/p&gt;

&lt;h3 id=&quot;flink-version-maintenance&quot;&gt;Flink version maintenance&lt;/h3&gt;

&lt;p&gt;Zeppelin notebook source code is maintained by a community separate from Flink’s community. As of writing, the latest Flink version supported is 1.17, whilst the latest Flink is already on version 1.20. This discrepancy in version support hinders our Flink upgrading efforts.&lt;/p&gt;

&lt;h3 id=&quot;cluster-start-up-time&quot;&gt;Cluster start up time&lt;/h3&gt;

&lt;p&gt;Our design to spin up a Zeppelin cluster per user on demand invokes a cold start delay, taking roughly around 5 minutes for the notebook to be ready. This delay is not suitable for use cases that require quick insights from production data. We quickly noticed that the user uptake of this solution was not as high as we expected.&lt;/p&gt;

&lt;h3 id=&quot;integration-challenges&quot;&gt;Integration challenges&lt;/h3&gt;

&lt;p&gt;Whilst Zeppelin notebooks were useful for serving individual developers, we experienced difficulty integrating it with other internal platforms. We designed Zeppelin to empower solo data explorers, but other internal platforms like dashboards or automated pipelines needed a way to aggregate data from Kafka and Zeppelin just couldn’t keep up. The notebook setup was too isolated, requiring a workaround to share insights or plug into existing tools. For instance, if a team wanted to pull aggregated real-time metrics into a monitoring system, they had to export data manually, which is far from seamless access that we aimed for.&lt;/p&gt;

&lt;h2 id=&quot;introducing-flinksql-interactive&quot;&gt;Introducing FlinkSQL interactive&lt;/h2&gt;

&lt;p&gt;With those considerations in mind, we decided to swap out our Zeppelin cluster with a shared &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql-gateway/overview/&quot;&gt;FlinkSQL gateway&lt;/a&gt; cluster. We simplified our solution by removing some features our notebooks offered, focusing only on features that promote data democratisation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Shared FlinkSQL gateway architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We split our solution into 3 layers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compute layer&lt;/li&gt;
  &lt;li&gt;Integration layer&lt;/li&gt;
  &lt;li&gt;Query layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Users first interact with our platform portal to submit queries for data from Kafka online store using SQL (1). Upon submission, our backend orchestrator then creates a session for the user (2) and submits the SQL query to our FlinkSQL gateway using their inbuilt REST API (3). The FlinkSQL gateway then packages the SQL query into a Flink job to be submitted to our Flink session cluster (4) before collating its results. The subsequent results would be polled from the query layer to be displayed back to the user.&lt;/p&gt;

&lt;h3 id=&quot;compute-layer&quot;&gt;Compute layer&lt;/h3&gt;

&lt;p&gt;With FlinkSQL gateway acting as the main compute engine for ad-hoc queries, it is now more straightforward to perform Flink version upgrades along with our solution, since the FlinkSQL gateway is packaged along with the main Flink distribution. We do not need to maintain Flink shims for each version as adapters between the Flink compute cluster and Zeppelin notebook cluster.&lt;/p&gt;

&lt;p&gt;Another advantage of using the shared FlinkSQL gateway was the reduced cold start time for each ad-hoc queries. Since all users share the same FlinkSQL cluster instead of having their own Zeppelin cluster, there was no need to wait for cluster startup during initialisation of their sessions. This brought the lead time to the first results displayed down from 5 minutes to 1 minute. There was still lead time involved as the tool provisions task managers on an ad-hoc basis to balance availability of such developer tools and the associated cost.&lt;/p&gt;

&lt;h3 id=&quot;integration-layer&quot;&gt;Integration layer&lt;/h3&gt;

&lt;p&gt;The Integration layer serves as the glue between the user-facing query layer and the underlying compute layer, ensuring seamless communication and security across our ecosystem. With the shift to a shared FlinkSQL gateway, we recognised the need for an intermediary that could handle authentication, authorisation, orchestration, and integration with internal platforms - all while abstracting the complexities of Flink’s native REST API.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: FlinkSQL gateway&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The FlinkSQL gateway’s built-in REST API gets the job done for basic query submission, but it falls short in areas like session management, requiring multiple POST requests just to fetch results. To address this, we extended a custom control plane with its own set of REST APIs, layered on top of the gateway.&lt;/p&gt;

&lt;p&gt;We then extend these sessions and integrate them to our inhouse authentication and authorisation platform. For each query made, the control plane authenticates the user, spins up lightweight sessions and manages the communication between the caller and the Flink Session Cluster. If you are interested, check out our previous blog post, &lt;a href=&quot;https://engineering.grab.com/an-elegant-platform&quot;&gt;An elegant platform&lt;/a&gt;, for more details on the above mentioned streaming platform and its control plane.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl --location &apos;https://example.com/v1/flink/flinksql/interactive&apos; \
--header &apos;Content-Type: application/json&apos; \
--header &apos;Authorization: Bearer ...&apos; \
--data &apos;{
    &quot;environment&quot; : &quot;prd&quot;,
    &quot;namespace&quot; : &quot;flink&quot;,
    &quot;sql&quot; : &quot;SELECT * FROM titanicstream&quot;}&apos;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Example API request for running a FlinkSQL query&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The integration layer also caters to B2B needs via our Headless APIs. By exposing the endpoints, developers are able to integrate real-time processing into their own tools. To run a query, programs can simply make a POST request with the SQL query and an operation ID would be returned. This operation ID could then be used in subsequent GET requests to fetch the paginated results of the unbounded query. This setup is ideal for internal platforms that need to query Kafka data programmatically. By abstracting these complexities, it ensures that users, whether individual analysts or internal platforms—can tap into Kafka data without wrestling with Flink’s raw interfaces.&lt;/p&gt;

&lt;h3 id=&quot;query-layer&quot;&gt;Query layer&lt;/h3&gt;

&lt;p&gt;We then proceed to pair our APIs developed with an Interactive UI to build a Query layer that serves both human workflows. This is where users meet our platform.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Flink query layer’s user flow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Through our platform portal, users land in a clean SQL editor. We used a Hive Metastore (HMS) catalog that translates Kafka topics into tables. Users don’t need to decode stream internals; they can jump straight into it by simply selecting a table to query on. Once a query is submitted, it is then handled by the integration layer which routes it through the control plane to the gateway. Results are then streamed back, appearing in the UI within one minute, a significant improvement from the five minute Zeppelin cold starts.&lt;/p&gt;

&lt;p&gt;This all crystalises into the user flow demonstrated in Figure 3, where we can easily retrieve Titanic data from a Kafka stream with a short command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT COUNT(*) FROM titanicstream WHERE kafkaEventTime &amp;gt; NOW() - INTERVAL &apos;1&apos; HOUR.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This setup enables a few use cases for our teams, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fraud analysts using the real-time data to debug and spot patterns in fraudulent transactions.&lt;/li&gt;
  &lt;li&gt;Data scientists querying live signals to validate their prediction models.&lt;/li&gt;
  &lt;li&gt;Engineers validating the messages sent from their system to confirm they are properly structured and accurately delivered.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;productionising-flinksql&quot;&gt;Productionising FlinkSQL&lt;/h2&gt;

&lt;p&gt;With data being democratised, we see more users building use cases around our online data store and utilising the above tools to build new stream processing pipelines expressed as SQL queries. To simplify the last step of the software development lifecycle of deployment, we have also developed a tool to create a configuration based stream processing pipeline, with the business logic expressed as a SQL statement.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/flink-sql/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Portal for FlinkSQL pipeline creation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We host connectors for users to connect to other platforms within Grab, such as Kafka and our internal feature stores. Users could simply use them off-the-shelf and configure according to their needs before deploying their stream processing pipeline.&lt;/p&gt;

&lt;p&gt;Users would then proceed to submit their streaming logic as a SQL statement. In the example illustrated in the diagram, the logic expressed is a simple filter on a Kafka stream for sinking the filtered events into a separate Kafka stream.&lt;/p&gt;

&lt;p&gt;Users have the ability to then define the parallelism and associated resources they want to run their Flink jobs with. Upon submission, the associated resources would be provisioned and the Flink pipeline would be automatically deployed. Behind the scenes, we manage the application JAR file that is being used to run the job that dynamically parses these configurations and translates them into a proper Flink job graph to be submitted to the Flink cluster.&lt;/p&gt;

&lt;p&gt;Within 10 minutes, users would have completed deploying their stream processing pipeline to production.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With our full suite of solutions for low code development via FlinkSQL, from exploration and design, to development and then deployment, we have simplified the journey for developing business use cases off online streaming stores. By offering both a user-friendly interface for low-code users and a robust API for developers, these tools empower businesses to harness the full potential of real-time data processing. Whether you are a data analyst looking for quick insights or a developer integrating real-time analytics into your applications, our tools are able to lower the barrier of entry to utilising real-time data.&lt;/p&gt;

&lt;p&gt;After we released these solutions, we quickly saw an uptick in pipelines created as well as the number of interactive queries fired. This result was encouraging and we hope that this would gradually bring upon a paradigm shift, enabling Grab to make data-driven operational decisions on real-time signals, empowering us with the ability to react to ever-changing market conditions in the most efficient manner.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebflinksql&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Jun 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/the-complete-stream-processing-journey-on-flinksql</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-complete-stream-processing-journey-on-flinksql</guid>
        
        <category>Database</category>
        
        <category>FlinkSQL</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Effortless enterprise authentication at Grab: Dex in action</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Grab, Southeast Asia’s leading superapp, has created many internal applications to support its diverse range of internal and external business needs. Authentication&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and authorisation&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; serve as fundamental components of application development, as robust identity and access management are essential for all systems.&lt;/p&gt;

&lt;p&gt;We recognised the need for a centralised internal system to manage access, authentication, and authorisation. This system would streamline access management, ensure compliance with audit requirements, enhance developer velocity, and simplify authentication and authorisation processes for both developers and business operations.&lt;/p&gt;

&lt;p&gt;Grab created Concedo to fulfill this requirement by providing a mechanism for services to configure their access control based on their specific role to permission matrix (R2PM)&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. This allows for quick and easy integration with Concedo, enabling developers to expedite the shipping of their systems without investing excessive time in building the authentication and authorisation module.&lt;/p&gt;

&lt;p&gt;The authentication mechanism, based on Google’s OAuth2.0&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, includes custom features that enhance identity for service integration. However, this customisation isn’t standard, creating integration challenges with external platforms like Databricks and Datadog. These platforms then use their own authentication and authorisation, resulting in a fragmented and undesirable sign-on experience for users.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/undesired-sign-on-experience.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Undesired user sign-on experience due to fragmented authentication approaches.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The inconsistency in user experience also resulted in complications. The lack of standardisation led to difficulties in establishing authentication and authorisation for individual applications. Additionally, it created substantial administrative overhead due to the necessity of managing multiple identities. The absence of standardisation also hindered transparency in access control across all applications.&lt;/p&gt;

&lt;p&gt;This led us to inquire how a standardised protocol could be established to function seamlessly across all applications, regardless of whether they were developed internally or sourced from external platforms.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/desired-state.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Desired state, having something in between the different identity providers (IdP).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;choosing-among-industry-standards&quot;&gt;Choosing among industry standards&lt;/h2&gt;

&lt;p&gt;We wanted to build a platform to serve both authentication and authorisation, providing a seamless integration and user sign-on experience. We then asked ourselves, “What are the current industry standards we can leverage on?”.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Security Assertion Markup Language (SAML)&lt;/strong&gt;: An authentication protocol which leverages heavily on session cookies to manage each authentication session.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Open Authorisation (OAuth)&lt;/strong&gt;: An authorisation protocol which focuses on granting access for particular details rather than providing user identity information.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;OpenID Connect (OIDC)&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot; role=&quot;doc-noteref&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;: An authentication protocol built on OAuth 2.0, enabling single sign-on (SSO). OIDC unifies and standardises user authentication, making it a solution for organisations with numerous applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OIDC enhances user experience by redirecting them to an identity provider (IdP) like Google or Microsoft for authentication when accessing an application. Upon successful verification, the IdP sends a secure token with the user’s identity information back to the application, granting access without the need for additional credentials.&lt;/p&gt;

&lt;p&gt;With OIDC, authentication and authorisation are fully implemented, enabling seamless integration across platforms, including mobile, API, and browser-based applications, while also providing SSO functionality.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/desired-state-protocol.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Desired state with the protocol decided.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;OIDC seemed like an ideal solution, but it came with potential drawbacks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OIDC relies on trusting a third-party authentication service. Any disruption to this service could result in downtime.&lt;/li&gt;
  &lt;li&gt;Compromised credentials could affect access to multiple services.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following section, we will explore our strategies in mitigating these challenges effectively.&lt;/p&gt;

&lt;h2 id=&quot;implementing-the-chosen-standard&quot;&gt;Implementing the chosen standard&lt;/h2&gt;

&lt;p&gt;With OIDC chosen as the standard, the focus shifted to implementation.&lt;/p&gt;

&lt;p&gt;We have always been a supporter of open source projects. Rather than building a platform from the ground up, we leveraged existing solutions while seeking opportunities to contribute back to the open source community.&lt;/p&gt;

&lt;p&gt;The team explored Cloud Native Computing Foundation (CNCF) projects and discovered &lt;a href=&quot;https://dexidp.io/&quot;&gt;Dex&lt;/a&gt; - A federated OpenID connect provider that aims to allow integration of any IdP into an application using OIDC. Dex was selected as our open-source platform of choice due to its alignment with our high-level objectives.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/desired-state-platform.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Desired state with Dex as the platform foundation.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;how-dex-works&quot;&gt;How Dex works&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/dex-work.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. High level architecture of Dex. Source: https://dexidp.io/docs/&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When a user or machine tries to access a protected application or service, they are redirected to Dex for authentication. Dex acts as a middleman (identity aggregator) between the user and various IdPs to establish an authenticated session.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/sequence-diagram-dex-auth.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Simplified sequence diagram of how authentication works for Dex.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Dex’s key features include enabling SSO experiences, allowing users to access multiple applications after authenticating through a single provider. Dex also supports multiple IdP use cases and provides standardised OIDC authentication tokens.&lt;/p&gt;

&lt;p&gt;Dex implementation separated application authentication concerns, established a single source of truth for identity, enabled new IdP additions, ensured adherence to security best practices, and provided scalability for deployments of all sizes.&lt;/p&gt;

&lt;h2 id=&quot;how-dex-is-streamlining-authentication-and-authorisation&quot;&gt;How Dex is streamlining authentication and authorisation&lt;/h2&gt;

&lt;h3 id=&quot;token-delegation&quot;&gt;Token delegation&lt;/h3&gt;

&lt;p&gt;When services communicate with each other, one service often assigns an identity to ensure that authorisation can be carried out on a specific service. For example, in figure 7, a service account or robot account is typically used as an identity so that service B can identify the caller.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/service-identification.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Service identification through service account.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Although service accounts are the recommended approach for enabling Service B to identify the caller, they come with challenges that must be addressed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Service account compromise&lt;/strong&gt;: Service accounts often have high-level privileges and typically broad access to Service B. If compromised, they pose a significant security risk, making careful management essential.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Access control issue&lt;/strong&gt;: The other approach creates unnecessary complexity by requiring Service A to handle user-level permissions for Service B. This violates the principle of separation of concerns.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address this issue, Dex introduced a &lt;a href=&quot;https://dexidp.io/docs/guides/token-exchange/&quot;&gt;token exchange&lt;/a&gt; feature.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/token-exchange.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. Token exchange example with trusted peers established.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The token exchange process involves two main components; token minting and trust relationship.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Token minting&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The user (Alice) logs into Service A.&lt;/li&gt;
  &lt;li&gt;Service A, acting as a trusted peer, is authorised to mint tokens.&lt;/li&gt;
  &lt;li&gt;Service A generates a token valid for both Service A and Service B. This is reflected in the token’s “aud” (audience) field: “aud”: “serviceA serviceB”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Trust relationship&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Service B must be configured to trust Service A as a peer.&lt;/li&gt;
  &lt;li&gt;Service B accepts tokens minted by Service A.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach differs from the service account-based scenario by using a trust-based peer relationship. Service A is authorised to mint tokens for Service B providing a more sophisticated but preferred method. The token is properly scoped for both services, ensuring a clear audit trail of token issuance, while reducing token manipulation risks.&lt;/p&gt;

&lt;h3 id=&quot;kill-switch&quot;&gt;Kill switch&lt;/h3&gt;

&lt;p&gt;As highlighted earlier,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;OIDC relies on trusting a third-party authentication service. Any disruption to this service could result in downtime.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dex’s ability to support multiple IdPs enables traffic to be shifted to a different IdP if one, such as Google, experiences an outage. This “kill switch” mechanism ensures that integrated services are not disrupted and do not require any changes to mitigate the issue. It is only triggered during specific IdP outages.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/dex-in-action/killswitch.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. Trigger kill switch without having other services changing from their end.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;looking-forward&quot;&gt;Looking forward&lt;/h2&gt;

&lt;p&gt;Following the successful implementation of Dex as the unified authentication provider, the next phase in enhancing our identity and access management infrastructure is to leverage this robust identity foundation to establish a unified and simplified authorisation model. This initiative is driven by the recognition that the current authorisation landscape remains fragmented and complex, leading to potential inefficiencies and security vulnerabilities.&lt;/p&gt;

&lt;p&gt;By centralising authorisation and aligning it with the unified identity provided by Dex, we can streamline access control, improve user experience, and strengthen security across our applications and services. This will involve consolidating authorisation policies, standardising access control mechanisms, and simplifying the management of user permissions.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Shoutout to the awesome Concedo team for driving Dex integration and to our leadership for steering the way toward a simpler, unified authentication and authorisation journey! Special recognition to Arun Ravi for his invaluable contributions to the project—his work on Dex within the Data Tech platform laid the foundation for what eventually evolved into the Concedo Dex.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdex&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h3 id=&quot;definition-of-terms&quot;&gt;Definition of terms&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Authentication: Who you are. Making sure you are who you say you are by verifying your identity. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Authorisation: What you can do. Defining the resources or actions you are allowed to access or perform after your identity has been verified. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Role-to-Permission Matrix (R2PM): A structured framework used to map roles within an organisation to the permissions or access rights each role has in a system, application, or process. This matrix serves as a critical component in access control and identity management, ensuring that users have appropriate access based on their roles while minimising the risk of unauthorised access. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Open Authorisation (OAuth 2.0): Protocol for authorisation. For example, Google Login on third-party portals allows your identity to remain with Google, but third-party portals can obtain limited access to specific data such as your profile photo. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;OpenID Connect (OIDC): Identity protocol built on top of OAuth 2.0. On top of authorisation provided by OAuth 2.0, it verifies and provides a trusted identity. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 23 May 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/dex-in-action</link>
        <guid isPermaLink="true">https://engineering.grab.com/dex-in-action</guid>
        
        <category>Access control</category>
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>From failure to success: The birth of GrabGPT, Grab’s internal ChatGPT</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In March 2023, I embarked on a mission to explore the potential of Large Language Models (LLMs) within Grab. What started off as an attempt to solve a specific problem—reducing the burden on our ML Platform team’s support channels, ended up becoming something much bigger. The creation of &lt;strong&gt;GrabGPT&lt;/strong&gt;, an internal ChatGPT-like tool that has transformed how folks in Grab interact with AI. This is the story of how a failed experiment led to one of Grab’s most impactful internal tools.&lt;/p&gt;

&lt;h3 id=&quot;the-problem-overwhelmed-support-channels&quot;&gt;The problem: Overwhelmed support channels&lt;/h3&gt;

&lt;p&gt;As part of Grab’s machine learning platform team, we were drowning in user inquiries. Slack channels were flooded with questions and our on-call engineers were spending more time answering repetitive queries than building innovative solutions. This led me to ponder on this question, &lt;em&gt;“could we use LLMs to build a chatbot that understands our platform’s documentation and answers these questions automatically?”&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-first-attempt-a-chatbot-for-platform-support&quot;&gt;The first attempt: A chatbot for platform support&lt;/h3&gt;

&lt;p&gt;I started by exploring open-source frameworks to build a chatbot. I stumbled upon &lt;a href=&quot;https://github.com/mckaywrigley/chatbot-ui&quot;&gt;chatbot-ui&lt;/a&gt;, a simple yet powerful tool that could be wired up with LLMs. My idea was to feed the chatbot our platform’s Q&amp;amp;A documentation (over 20,000 words) and let it handle user queries.&lt;/p&gt;

&lt;p&gt;But there was a catch: &lt;strong&gt;GPT-3.5-turbo could only handle 8,000 tokens (~2,000 words)&lt;/strong&gt;. I spent days summarising the documentation, reducing it to less than 800 words. While the chatbot worked for a handful of frequently asked questions, it was clear that this approach wasn’t scalable. I tried with embedding search and it didn’t work that well too, so I decided to &lt;strong&gt;give up on this idea&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-pivot-why-not-build-grabs-own-chatgpt&quot;&gt;The pivot: Why not build Grab’s own ChatGPT?&lt;/h3&gt;

&lt;p&gt;As I stepped back, a new thought struck me: &lt;em&gt;Grab doesn’t have its own ChatGPT-like tool yet.&lt;/em&gt; I had the frameworks, the LLM knowledge, and most importantly—access to &lt;a href=&quot;https://engineering.grab.com/catwalk-evolution&quot;&gt;Grab’s model-serving platform, catwalk&lt;/a&gt;. Why not build an internal tool that every Grabber could use?&lt;/p&gt;

&lt;p&gt;Over a weekend, I extended the existing frameworks, added Google login for authentication, and deployed the tool internally. I called it &lt;strong&gt;Grab’s ChatGPT&lt;/strong&gt;. Little did I know, this would become one of the most widely used tools in the company.&lt;/p&gt;

&lt;p&gt;The tool quickly became a staple for Grabbers, especially in regions where ChatGPT was inaccessible (e.g., China). The name evolved too—our PM suggested &lt;strong&gt;GrabGPT&lt;/strong&gt;, and it stuck.&lt;/p&gt;

&lt;h3 id=&quot;the-success-grabgpt-takes-off&quot;&gt;The Success: GrabGPT takes off&lt;/h3&gt;

&lt;p&gt;The response was overwhelming:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Day 1:&lt;/strong&gt; 300 users registered.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Day 2:&lt;/strong&gt; 600 new users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Week 1:&lt;/strong&gt; 900 new users&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Month 3:&lt;/strong&gt; Over 3000 users, with 600 daily active users&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Today:&lt;/strong&gt; Almost all Grabbers are using  GrabGPT.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/birth-of-grabgpt/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Number of GrabGPT users in one month&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;why-grabgpt-works-more-than-just-technology&quot;&gt;Why GrabGPT works: More than just technology&lt;/h3&gt;

&lt;p&gt;The success of GrabGPT isn’t just about the tech,it’s about &lt;strong&gt;timing, security, and accessibility&lt;/strong&gt;. Here’s why it resonated so deeply within Grab:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Data security:&lt;/strong&gt; GrabGPT operates on a private route, ensuring that sensitive company data never leaves our infrastructure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Global accessibility:&lt;/strong&gt; Unlike ChatGPT, which is banned in some regions, GrabGPT is accessible to all Grabbers, regardless of location.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model agnosticism:&lt;/strong&gt; GrabGPT isn’t tied to a single LLM provider. It supports models from OpenAI, Claude, Gemini, and more.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Auditability:&lt;/strong&gt; Every interaction on GrabGPT is auditable, making it a favorite of our data security and governance teams.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-broader-impact-a-catalyst-for-llm-strategy&quot;&gt;&lt;strong&gt;The broader impact: A catalyst for LLM strategy&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;GrabGPT didn’t just solve an immediate problem, it sparked a broader conversation about how LLMs can be leveraged across Grab. It showed that a single engineer, provided with the right tools and timing, can create something transformative. Today, GrabGPT is more than a tool; it’s a testament to the power of experimentation and adaptability.&lt;/p&gt;

&lt;h3 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Failure is a stepping stone:&lt;/strong&gt; My initial failure with the support chatbot which then led me to a much bigger opportunity.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Timing matters:&lt;/strong&gt; GrabGPT succeeded because it addressed a critical need at the right time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Think big, start small:&lt;/strong&gt; What began as a weekend project became a company-wide tool.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collaboration is key:&lt;/strong&gt; The enthusiasm and contributions from other Grabbers were instrumental in scaling GrabGPT.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;GrabGPT is a story of resilience, innovation, and the unexpected rewards from thinking outside the box. It’s a reminder that sometimes, the best solution comes from pivoting away from what doesn’t work and embracing new possibilities. As LLMs continue to evolve, I’m excited to see how GrabGPT will grow and inspire even more innovation within Grab.&lt;/p&gt;

&lt;p&gt;I would like to end this article by letting readers know that if you’re working on a project and feel stuck, don’t be afraid to pivot. You never know, your next failure might just be the beginning of your greatest success. And if you’re at Grab, give GrabGPT a try. It might just change the way you work!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebgrabgpt&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 19 May 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/the-birth-of-grab-gpt</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-birth-of-grab-gpt</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>AI</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Streamlining RiskOps with the SOP agent framework</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the blog our previous introduction to the &lt;a href=&quot;https://engineering.grab.com/introducing-the-sop-drive-llm-agent-framework&quot;&gt;SOP-driven LLM Agent Framework&lt;/a&gt;, we the potential of LLM agent framework to revolutionise business operations was discussed. Now, we’re excited to explore a compelling use case: automating Account Takeover (ATO) investigations in Risk Operations (RiskOps). This framework has significantly reduced manual effort, improved efficiency, and minimised errors in the investigation process, setting a new standard for secure and streamlined operations.&lt;/p&gt;

&lt;h2 id=&quot;the-challenge-in-riskops&quot;&gt;The challenge in RiskOps&lt;/h2&gt;

&lt;p&gt;Traditionally, ATO investigations have been fraught with challenges due to their complexity and the manual effort required. Analysts must sift through vast amounts of data, cross-referencing multiple systems and executing numerous SQL queries to make informed decisions. This process is not only labor-intensive but also susceptible to human error, which can lead to inconsistencies and potential security breaches.&lt;/p&gt;

&lt;p&gt;The manual approach often involves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Time-consuming data analysis:&lt;/strong&gt; Analysts spend significant time gathering and interpreting data from disparate sources, leading to delays and inefficiencies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decision fatigue:&lt;/strong&gt; Continuous decision-making in a high-pressure environment can result in oversight or errors, especially when relying on predefined thresholds without adaptive insights.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource constraints:&lt;/strong&gt; The need for specialised skills to handle SQL queries and interpret complex patterns limits the scalability of the process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These challenges highlight the need for a more efficient, reliable, and scalable solution.&lt;/p&gt;

&lt;h2 id=&quot;leveraging-the-sop-agent-framework&quot;&gt;Leveraging the SOP agent framework&lt;/h2&gt;

&lt;p&gt;Our framework transforms the ATO investigation process by mirroring manual workflows while leveraging advanced automation.&lt;/p&gt;

&lt;p&gt;At its core, a Standard Operating Procedure (SOP) guides the investigation process. This comprehensive SOP, is designed with an intuitive tree structure. It outlines the sequence of investigative actions, required data for each step, necessary SQL queries and external function calls, as well as decision criteria guiding the investigation. &lt;strong&gt;Figure 1&lt;/strong&gt; shows the example of ATO investigation SOP.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/riskops-sop-img/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt; Figure 1: Example of fictional ATO investigation SOP&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The SOP is written in natural language in an indentation format. Users can easily define SOPs using an intuitive editor. This format also clearly denotes the specific functions or queries associated with each step in the SOP. The @function_name notation (eg. @IP_web_login_history) makes it easy to identify where external calls are made within the process, highlighting the integration points between the SOP-driven LLM agent framework and the existing systems or databases.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-execution&quot;&gt;Dynamic execution&lt;/h2&gt;

&lt;p&gt;The dynamic execution engine consists of the SOP planner and the Worker Agent, working in tandem to drive efficient operations. The SOP planner serves as the navigator, guiding the investigation’s path by generating the necessary SOP steps and determining the appropriate APIs to call. It uses a structured execution approach inspired by Depth-First Search (DFS) to ensure thorough and systematic processing. Meanwhile, the Worker Agent acts as the executor, interpreting the JSON-formatted SOPs, invoking required APIs or SQL queries, and storing results. This continuous interplay between the SOP planner and the Worker Agent establishes an efficient feedback loop, propelling the investigation forward with precision and reliability.&lt;/p&gt;

&lt;p&gt;The automated investigation process begins at the root of the SOP tree and methodically progresses through each defined step. At each juncture, the system executes specified SQL queries as needed, retrieving and analysing relevant data. Based on this analysis, the framework evaluates step specific criteria and makes informed decisions that guide subsequent steps. This iterative process allows the investigation to delve as deeply into the data as the SOP dictates, ensuring both thoroughness and efficiency.&lt;/p&gt;

&lt;p&gt;As the investigation concludes, having completed all of the steps, the framework enters its final phase. It compiles a comprehensive summary of the entire process, synthesising all gathered information to generate a final decision. The culmination of this process is a detailed report that encapsulates the investigation’s findings and provides clear, actionable conclusions.&lt;/p&gt;

&lt;p&gt;This automated approach combines the best of human expertise with computational efficiency. It maintains the depth and detail of a human-conducted investigation while leveraging the speed and consistency of automation. The result is a powerful tool that can handle complex investigations with precision and reliability, making it an invaluable asset in various fields requiring thorough and systematic analysis.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/riskops-sop-img/figure-2.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt; Figure 2: Example of dynamic execution&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;efficiency-impact-and-future-potential&quot;&gt;Efficiency, impact and future potential&lt;/h2&gt;

&lt;p&gt;The SOP-driven LLM agent framework has demonstrated remarkable efficiency and impact in automating RiskOps processes. By automating data handling and leveraging AI to adapt to emerging patterns, the framework has significantly reduced manual tasks and streamlined operations. &lt;strong&gt;Figure 3&lt;/strong&gt; shows an example of an automated RiskOps process integrated with Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/riskops-sop-img/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Slack integration&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Key achievements of automating RiskOps process:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reduction in handling time from 22 to 3 minutes per ticket.&lt;/li&gt;
  &lt;li&gt;Automation of 87% of ATO cases since launch.&lt;/li&gt;
  &lt;li&gt;Achievement of a zero-error rate, enhancing both efficiency and security.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These results not only demonstrate the framework’s effectiveness in streamlining RiskOps but also provide stakeholders with increased confidence in the security and reliability of their operations.&lt;/p&gt;

&lt;p&gt;The success of the framework in automating ATO investigations opens the door to a wider range of applications across various sectors. By adapting the framework to different processes, organisations can achieve similar improvements in efficiency and reliability, leading to a more responsive and agile business environment.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The SOP-driven LLM agent framework is more than an automation tool. It’s a catalyst for transforming enterprise operations. By applying it to ATO investigations, we’ve demonstrated its potential to enhance efficiency, reliability, and security. As we continue to explore its capabilities, we anticipate unlocking new levels of productivity and innovation across industries.&lt;/p&gt;

&lt;p&gt;We look forward to sharing more as we explore how this groundbreaking framework can be applied to various challenges, helping organisations navigate the complexities of modern operations with confidence and precision.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebriskops&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 08 May 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/streamlining-riskops-with-sop</link>
        <guid isPermaLink="true">https://engineering.grab.com/streamlining-riskops-with-sop</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Data Analytics</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Introducing the SOP-driven LLM agent frameworks</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We’re excited to introduce an innovative Large Language Model (LLM) agent framework that reimagines how enterprises can harness the power of AI to streamline operations and boost productivity. At its core, this framework leverages Standard Operating Procedures (SOPs) to guide AI-driven execution, ensuring reliability and consistency in complex processes. Initial evaluations have shown remarkable results, with over 99.8% accuracy in real-world use cases. For example, the framework has powered solutions like the Account Takeover Investigations (ATI) bot, which achieved a 0 false rate while reducing investigation time from 23 minutes to just 3, automating 87% of cases. The fraud investigation use case also reduced the average handling time (AHT) by 45%, saving over 300 man-hours monthly with a 0 false rate, demonstrating its potential to transform even the most intricate enterprise operations with a high degree of accuracy.&lt;/p&gt;

&lt;p&gt;The framework’s capabilities extend far beyond just accuracy, it offers a versatile suite of tools that revolutionise automation and app development, enabling AI-powered solutions up to 10 times faster than traditional methods.&lt;/p&gt;

&lt;h2 id=&quot;the-power-of-sops-in-ai-automation&quot;&gt;The power of SOPs in AI automation&lt;/h2&gt;

&lt;p&gt;Traditional agent-based applications often use LLMs as the core controller to navigate through standard operating procedure (SOPs). However, this approach faces several challenges. LLMs may make incorrect decisions or invent non-existent steps due to hallucination. As generative models, they struggle to consistently produce results in a fixed format. Moreover, navigating complex SOPs with multiple branching pathways is particularly challenging for LLMs. These issues can lead to inefficiencies and inaccuracies in implementing business operations, especially when dealing with intricate, multi-step procedures.&lt;/p&gt;

&lt;p&gt;Our framework addresses these challenges head-on by leveraging the structure and reliability of SOPs. We represent SOPs as a tree, with nodes encapsulating individual actions or decision points. This structure supports both sequential and conditional branching operations, mirroring the hierarchical nature of real-world business processes.&lt;/p&gt;

&lt;p&gt;To make this powerful tool accessible to all, we’ve developed an intuitive SOP editor that allows non-technical users to easily define and visualise complex workflows. These visual representations are then converted into a structured, indented format that our system can interpret and execute efficiently.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/sop-llm-agent-framework-img/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: SOP editor in our framework&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The example above demonstrates how our framework transforms the customer support process by mirroring manual workflows while leveraging advanced automation. The SOP is written in natural language using an indentation format, making it easy for users to define and understand. The @function_name (@get_order_detail) notation clearly identifies where external calls are made within the process, highlighting the integration points between the SOP-driven LLM agent framework and existing systems or databases.&lt;/p&gt;

&lt;h2 id=&quot;the-magic-behind-the-scenes&quot;&gt;The magic behind the scenes&lt;/h2&gt;

&lt;p&gt;The framework’s strength lies in the synergy between three key components: the planner module, LLM-powered worker agent, and user agent. This intelligent trio works in harmony to deliver a seamless, efficient, and adaptable automation experience.&lt;/p&gt;

&lt;p&gt;The planner module employs a Depth-First Search (DFS) algorithm to navigate the SOP tree, ensuring thorough execution with step-by-step prompt generation and sophisticated backtracking mechanisms. The LLM-powered worker agent dynamically updates its understanding and makes decisions based on the most current information. Our approach tackles hallucination and improves efficiency through context compression and strategic limitation of available Application Programming Interface tools (APIs). The framework’s dynamic branching capability allows for adaptive navigation based on real-time data and analysis.&lt;/p&gt;

&lt;p&gt;Serving as the primary user interface, the user agent offers multilingual interaction, accurate intent identification, and seamless handling of out-of-order scenarios.&lt;/p&gt;

&lt;p&gt;By combining structured SOPs with flexible LLM-powered agents and advanced algorithmic approaches, our framework adeptly handles complex, real-world scenarios while maintaining reliability and consistency. This innovative architecture effectively mitigates common LLM challenges, resulting in a robust system capable of navigating intricate business processes with high accuracy and adaptability.&lt;/p&gt;

&lt;h2 id=&quot;beyond-sops-a-suite-of-powerful-features&quot;&gt;Beyond SOPs: A suite of powerful features&lt;/h2&gt;

&lt;p&gt;While SOPs form the backbone of our framework, we’ve incorporated several other cutting-edge features to create a truly comprehensive solution. Our Graph Retrieval-Augmented Generation (GRAG) pipeline enhances information retrieval and content generation tasks, allowing for more accurate and context-aware responses. The workflow feature enables chaining multiple plugins together to handle complex processes effortlessly, improving efficiency across various departments.&lt;/p&gt;

&lt;p&gt;Our plugin system seamlessly integrates with various technologies such as API, Python, and SQL, providing the flexibility to meet diverse needs. Whether you’re an engineer coding in Python, a data analyst working with SQL, or a risk operations specialist, our plugin system adapts to your preferred tools. Additionally, our playground feature allows users to develop, test, and refine LLM applications easily in an interactive environment, supporting the latest multi-modal APIs for accelerated innovation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/sop-llm-agent-framework-img/figure-2.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Workflow builder feature in our framework&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;empowering-teams-through-versatility-and-accessibility&quot;&gt;Empowering teams through versatility and accessibility&lt;/h2&gt;

&lt;p&gt;Our framework is designed to empower teams across the organisation. The multilingual capabilities of our user agent ensure that language barriers don’t hinder adoption or efficiency. For scenarios requiring human intervention, we’ve implemented a state stack that allows for pausing and resuming execution seamlessly. This feature ensures that complex processes can be handled with the right balance of automation and human oversight.&lt;/p&gt;

&lt;h2 id=&quot;security-and-transparency-at-the-forefront&quot;&gt;Security and transparency at the forefront&lt;/h2&gt;

&lt;p&gt;In an era where data security and process transparency are paramount, our framework doesn’t fall short. It’s designed with a security-first approach, ensuring granular access control so that users only access information they’re authorised to see. Additionally, we provide detailed logging and visualisation of each execution, offering complete explainability of the automation process. This level of transparency not only aids in troubleshooting but also helps in building trust in the AI-driven processes across the organisation.&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking ahead&lt;/h2&gt;

&lt;p&gt;As we continue to refine and expand this LLM agent framework, we’re excited to explore its potential across different industries. We’ll be sharing more about each of these features in the future and showcase how they can be leveraged to solve specific business challenges and explore real-world applications.&lt;/p&gt;

&lt;p&gt;Look forward to more in-depth explorations of the framework’s capabilities, use cases, and technical innovations. With this revolutionary approach, you’re not just automating tasks – you’re transforming the way your enterprise operates, unleashing the true power of LLM in your organisation.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebllmagent&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 25 Apr 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/introducing-the-sop-drive-llm-agent-framework</link>
        <guid isPermaLink="true">https://engineering.grab.com/introducing-the-sop-drive-llm-agent-framework</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Data Analytics</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Evaluating performance impact of removing Redis-cache from a Scylla-backed service</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we operate a set of services that manage and provide counts of various items. While this may seem straightforward, the scale at which this feature operates—benefiting millions of Grab users daily—introduces complexity. This feature is divided into three microservices: one for “writing” counts, another for handling “read” requests, and a third serving as the backend for a portal used by data scientists and analysts to configure these counters.&lt;/p&gt;

&lt;p&gt;This article focuses on the service responsible for handling “read” requests. This service is backed by Scylla storage and a Redis cache. It also connects to a MySQL RDS to retrieve “counter configurations” that are necessary for processing incoming requests. Written in Rust, the service serves tens of thousands of queries per second (QPS) during peak times, with each request typically being a “batch request” requiring multiple lookups (~10) on Scylla.&lt;/p&gt;

&lt;p&gt;Recently, the service has encountered performance challenges, causing periodic spikes in Scylla QPS. These spikes occur throughout the day but are particularly evident during peak hours. To understand this better, we’ll first walk you through how this service operates, particularly how it serves incoming requests. We will then explain our proposed solution and the outcomes of our experiment.&lt;/p&gt;

&lt;h2 id=&quot;anatomy-of-a-request&quot;&gt;Anatomy of a request&lt;/h2&gt;

&lt;p&gt;Each counter configuration stored in MySQL has a template that dictates the format of incoming queries. For example, this sample counter configuration is used to count the raindrops for a specific city:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;id&quot;: 34,
    &quot;name&quot;: &quot;count_rain_drops&quot;,
    &quot;template&quot;: &quot;rain_drops:city:{city_id}&quot;
    ....
    ....
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;An incoming request using this counter might look like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;key&quot;: &quot;rain_drops:city:111222&quot;,
    &quot;fromTime&quot;: 1727215430, // 24 September 2024 22:03:50
    &quot;toTime&quot;: 1727400000, // 27 September 2024 01:20:00
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This request seeks the number of raindrops in our imaginary city with city ID: 111222, between 1727215430 (24 September 2024 22:03:50) and 1727400000 (27 September 2024 01:20:00).&lt;/p&gt;

&lt;p&gt;Another service keeps track of raindrops by city and writes the minutely (truncated at 15 minutes), hourly, and daily counts to three different Scylla tables:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;minutely_count_table&lt;/li&gt;
  &lt;li&gt;hourly_count_table&lt;/li&gt;
  &lt;li&gt;daily_count_table&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The service processing the request rounds down the time to the nearest 15 minutes. As a result, the request is processed with the following time range:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start time: 24 September 2024 22:00:00&lt;/li&gt;
  &lt;li&gt;End time: 27 September 2024 01:15:00&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s assume we have the following data in these three tables for “rain_drops:city:111222”. The datapoints used in the above example request are highlighted in &lt;strong&gt;bold&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;minutely_count_table&lt;/strong&gt;:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;key&lt;/th&gt;
      &lt;th&gt;minutely_timestamp&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-24T22:00:00Z&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-24T22:15:00Z&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-24T22:30:00Z&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-24T22:45:00Z&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;2024-09-27T01:00:00Z&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-27T01:15:00Z&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;hourly_count_table&lt;/strong&gt;:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;key&lt;/th&gt;
      &lt;th&gt;hourly_timestamp&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;2024-09-24T22:00:00Z&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;2024-09-24T23:00:00Z&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-25T00:00:00Z&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;2024-09-27T00:00:00Z&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-27T01:00:00Z&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;daily_count_table&lt;/strong&gt;:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;key&lt;/th&gt;
      &lt;th&gt;daily_timestamp&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-24T00:00:00Z&lt;/td&gt;
      &lt;td&gt;214&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;2024-09-25T00:00:00Z&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;189&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;2024-09-26T00:00:00Z&lt;/td&gt;
      &lt;td style=&quot;font-weight:bold&quot;&gt;245&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rain_drops:city:111222&lt;/td&gt;
      &lt;td&gt;2024-09-27T00:00:00Z&lt;/td&gt;
      &lt;td&gt;78&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
Now, let’s see how the service calculates the total count for the incoming request with “rain_drops:city:111222” based on the provided data:&lt;/p&gt;

&lt;p&gt;Time range:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From: 24 September 2024 22:03:50&lt;/li&gt;
  &lt;li&gt;To:  27 September 2024 01:20:00&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the full days within the range, specifically 25th and 26th September, we can use data from the &lt;strong&gt;daily_count_table&lt;/strong&gt;. However, for the start (24th September) and end (27th September) date of the range, we cannot use data from the &lt;strong&gt;daily_count_table&lt;/strong&gt; as the range only includes portions of these dates. Instead, we will use a combination of data from the &lt;strong&gt;hourly_count_table&lt;/strong&gt; and &lt;strong&gt;minutely_count_table&lt;/strong&gt; to accurately capture the counts for these days.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Query the &lt;strong&gt;daily_count_table&lt;/strong&gt;:&lt;/p&gt;

    &lt;p&gt;Sum (full day: 25 and 26th Sep): 189 + 245 = 434&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Query the &lt;strong&gt;hourly_count_table&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;For 24th September (from 22:00:00 to 23:59:59):&lt;/p&gt;

        &lt;p&gt;Hourly count: 18 + 22 = 40&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For 27th September (from 00:00:00 to 01:00:00):&lt;/p&gt;

        &lt;p&gt;Hourly count: 11&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Query the &lt;strong&gt;minutely_count_table&lt;/strong&gt;:&lt;/p&gt;

    &lt;p&gt;For 27th September (from 01:00:00 to 01:15:00):&lt;/p&gt;

    &lt;p&gt;Minutely count: 2&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Total count:&lt;/p&gt;

    &lt;p&gt;Total = Daily count (25th and 26th) + Hourly count (24th) + Hourly count (27th) + Minutely count (27th)&lt;/p&gt;

    &lt;p&gt;= 434 + 40 + 11 + 2&lt;/p&gt;

    &lt;p&gt;= 487&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/example-request.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: The example request for “rain_drops:city:111222” is handled using data from three different Scylla tables.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As shown in the calculation, when the service receives the request, it comes up with the total count of raindrops by querying three Scylla tables and summing them up using some specific rules within the service itself.&lt;/p&gt;

&lt;h2 id=&quot;querying-the-cache&quot;&gt;Querying the cache&lt;/h2&gt;

&lt;p&gt;In the previous section, we explained how Scylla handles a query. If we cached the response for the same request earlier, retrieval from the cache follows a simpler logic. For instance, for the example request, the total count is stored using the floored start and end times (rounded to the nearest 15-minute window within an hour), which was used for the Scylla query instead of the original time in the request. The cache key-value pair would look like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;key: id:34:rain_drops:city:111222:1727215200:1727399700&lt;/li&gt;
  &lt;li&gt;value: 487&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Timestamps 1727215200 and 1727399700 represent the adjusted start and end times of 24 September 2024 22:00:00 and 27 September 2024 01:15:00, respectively. It has a Time-To-Live (TTL) of 5 minutes. During this TTL window, any request for the key “rain_drops:city:111222” having the same start and end times (after rounding to the nearest 15 minutes) will be read from the cache instead of querying Scylla.&lt;/p&gt;

&lt;p&gt;For example, for the following three start times, although they are different, after flooring the request to the nearest 15 minutes, the start time becomes 24 September 2024 22:00:00 for all of them, which is the same start time as the one in the cache.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;24 September 2024 22:01:00&lt;/li&gt;
  &lt;li&gt;24 September 2024 22:02:00&lt;/li&gt;
  &lt;li&gt;24 September 2024 22:06:00&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In day-to-day operations, this caching setup allows roughly half of our total production requests to be served by the Redis cache.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/visualise-cache-hits.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. The graph visualises the relative quantity of cache hits vs Scylla-bound requests.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;The setup consisting of Scylla and Redis cache works well. Particularly because Scylla-bound queries need to look up 1-3 tables (minutely, hourly, daily, depending on the time range) and perform the summation as explained earlier, whereas a single cache lookup gets the final value for the same query. However, as our cache key pattern follows the 15-minute truncation strategy, along with a 5-minute cache TTL, it leads to an interesting phenomenon - our cache hits plummet and Scylla QPS spikes at the end of every 15 minutes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/15-mins-spikes-scylla-request.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Graph showing 15-minute spikes in Scylla-bound requests accompanied by a decline in cache hit rates.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This occurs primarily due to the fact that almost all requests to our service are for recent data. Due to this, at the end of every 15-minute block within an hour (i.e., 00, 15, 30, 45), most of the requests require creating new cache keys for the latest 15-minute block. At this point in time, there may be many unexpired (i.e., have not reached 5 min TTL) cache keys from the previous 15-minutes block, but they become less relevant as most requests are asking for recent data.&lt;/p&gt;

&lt;p&gt;The table in Figure 4 shows example data for configurations “rain_drops:city:111222” and “bird_sighting:city:333444”. For these two configurations, new cache keys are created due to TTL expiry at random times. However, at the end of the 15-minute block, which, in this case is at the end of 22:00-22:15 block, both configurations need new cache keys for the new 15-minute time block that has just started (i.e., start of 22:15-22:30), even though some of their cache keys from the previous 15-minute block are still valid. This requirement of creating new cache keys for most of the requests at the end of a 15-minute block causes spikes in Scylla QPS and a sharp decline in cache hits.&lt;/p&gt;

&lt;p&gt;One question that arises is - “Why don’t we see a spike every 5 minutes for cache key TTL expiry?” This is because, within the 15 minutes block, new cache keys are continuously created when a key reaches TTL and a new request for that is received. Since this happens all the time as shown in Figure 4, we do not see a sharp spike. In other words, although Scylla does receive more queries due to cache TTL expiry, it does not lead to a spike in Scylla queries or a sharp drop in cache hits. This is because the cache keys are always being created and invalidated due to TTL expiry instead of following a fixed 5-minute block similar to the 15-minute block we use for our truncation strategy.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/scenario-new-cache-keys.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. This table visualises scenarios when new cache keys are required due to TTL expiry vs due to 15-minute truncation strategy.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These Scylla QPS spikes at the end of every 15-minute block lead to a highly imbalanced Scylla QPS. This often causes high latency in our service during the 15-minute blocks that fall within the peak traffic hours. This further causes more requests to time out, eventually increasing the number of failed requests.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h2&gt;

&lt;p&gt;We propose mitigating this issue by completely removing the Redis-backed caching mechanism from the service. Our observations indicate that the Scylla spikes at the end of 15-minute blocks occur due to cache hit misses. Therefore, removing the caching should eliminate the spikes and provide for a more balanced load.&lt;/p&gt;

&lt;p&gt;We acknowledge that this may seem counterintuitive from an overall performance standpoint as removing caching means all queries will be Scylla-bound, potentially impacting the overall performance since caching usually speeds up processes. In addition, caching also comes with an advantage where for cache hits, the service does not need to do the summation on Scylla results from minutely, hourly, and the daily table. Despite these shortcomings, we hypothesise that removing caching should not have an adverse impact on the overall performance. This is based on the fact the Scylla has its own sophisticated caching mechanism. However, our existing setup uses Redis for caching, underutilising Scylla’s cache as the most subsequent queries hit the Redis cache instead.&lt;/p&gt;

&lt;p&gt;In summary, we propose eliminating the Redis caching component from our current architecture. This change is expected to resolve the Scylla query spikes observed at the end of every 15-minute block. By relying on Scylla’s native caching mechanism, we anticipate maintaining the service’s overall performance more effectively. The removal of Redis is counterbalanced by the optimised utilisation of Scylla’s built-in caching capabilities.&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;h3 id=&quot;procedure&quot;&gt;Procedure&lt;/h3&gt;

&lt;p&gt;The experiment was done on an important live service serving thousands of QPS. To avoid disruptions, we followed a gradual approach. We first turned off caching for a few configurations. If there were no adverse impacts observed, we incrementally disabled cache for more configurations. We controlled the rollout increment by using a mathematical operator on the configuration IDs. This approach is simple and allows us to deterministically disable the cache for specific configurations across all requests, as opposed to using a percentage rollout which randomly disables the cache for different configurations across different requests. This is also due to the fact that the number of configurations is relatively steady and small (less than a thousand). Since these configurations are already fully cached in the service memory from RDS, there will be no performance impact of having a condition that operates on these configurations.&lt;/p&gt;

&lt;p&gt;To make sense of the graphs and metrics reported in this section, it is important to understand the traffic pattern of this service. The service usually sees two peaks every day: noon and another around 6-7 PM. On a weekly basis, we usually see the highest traffic on Friday, with the busiest period being from 6-8 PM.&lt;/p&gt;

&lt;p&gt;In addition, the timeline of when and how we made various changes to our setup is important to accurately interpret our results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment timeline: Nov 5 - Nov 13, 2024:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Redis cache disabled for ~5% of the counter configurations - Nov 5, 2024, 10.26 AM (Canary started: 10.00 AM)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Redis cache disabled for ~25% of the counter configurations - Nov 5, 2024, 12.44 PM (Canary started: 12.20 PM)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Redis cache disabled for ~35% of the counter configurations - Nov 6, 2024, 10.50 AM (Canary started: 10.21 AM)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Redis cache disabled for ~75% of the counter configurations - Nov 7, 2024, 10.53 AM (Canary started: 10.26 AM) &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Experimenting with running a major compaction job during the day time: Tue, Nov 12, 2024, between 2-5 PM (on all nodes)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Day time scheduled major compaction job starts from: Tue, Nov 13, 2024, between 2-5 PM (on all nodes)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Redis cache disabled for 100% of the counter configs - Wed, 13 Nov 2024, 10:56 AM (Canary started: 10:32 AM)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unless otherwise specified, the graphs and metrics we report in this article uses this fixed time window: Oct 31 (Thu) 12.00 AM  - Nov 15 (Friday) 11.59 PM SGT. This time window covers the entire experiment period with some buffer to observe the experiment’s impact.&lt;/p&gt;

&lt;h3 id=&quot;observations&quot;&gt;Observations&lt;/h3&gt;

&lt;p&gt;As we progressively disabled read from external Redis cache over the span of 8 days (Nov 5 - Nov 13), we made interesting observations and experimented with some Scylla configuration changes on our end. We describe them in the following sections.&lt;/p&gt;

&lt;h4 id=&quot;scylla-hit-vs-cache-hit&quot;&gt;Scylla hit vs. cache hit&lt;/h4&gt;

&lt;p&gt;As we progressively disabled Redis cache for most of the counters, one obvious impact was the gradual increase in Scylla-bound QPS and similar decrease in Redis-cache hit. When Redis-cache was enabled for 100% of the configurations, 50% of the requests were bound for Scylla and the other 50% were for Redis. At the end of the experiment, after fully disabling Redis cache, 100% of the requests were Scylla-bound.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/gradual-increase-scylla-qps.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Gradual increase in Scylla QPS and simultaneous decrease in Redis cache hit.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;15-minutes-and-hourly-spikes&quot;&gt;15-minutes and hourly spikes&lt;/h4&gt;

&lt;p&gt;We noticed that the 15-minute spikes in Scylla QPS as well as the associated latency slowly became less prominent and eventually disappeared from the graph after we completely disabled the Redis cache. However, we noticed that the hourly spike still remained. This is attributed to the higher QPS from the clients calling this service at the turn of every hour. As a result, limited optimisation can be done to reduce the hourly spike on this service’s end.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/scyalla-vs-cache.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. The 15-minute spikes in Scylla QPS disappeared after the external Redis cache was fully disabled. This graph uses a smaller time window to show the earlier spikes. It also shows the persistence of hourly spikes after the experiment which is attributed to the clients of this service sending more requests at the start of every hour.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/avg-latency.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. The graph shows that the 15-minute spikes in Scylla’s latency disappeared after the external Redis cache was fully disabled. This graph uses a smaller time window to show the earlier spikes. It also shows the persistence of hourly spikes in latency after the experiment which is attributed to the clients of this service sending more requests at the start of every hour.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;service-latency-and-additional-scylla-compaction-job&quot;&gt;Service latency and additional Scylla compaction job&lt;/h3&gt;

&lt;p&gt;When we disabled Redis cache for about 75% of the counters configurations on Nov 7 (which accounts for about 85% of the overall QPS), we noticed an increase in the overall average service latency, from between 6-8 ms to 7-12 ms (P99 went from ~30-50ms to ~30-70ms). This caused a spike in open circuit breaker (CB) events on &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-1&quot;&gt;Hystrix&lt;/a&gt;. At this point, before disabling cache for more counters, on Nov 12, we experimented with running an additional major compaction job on Scylla between 2-5 PM on all our Scylla nodes, progressively on each availability zone (AZ). It is noteworthy that we already have a scheduled major compaction job that runs around 3 AM every day. The outcome of this experiment was quite positive. It brought back the average and P99 latency almost to the prior level when we had Redis cache enabled for 100% of the counters. This also had a similar effect on the Hystrix CB open events. Based on this observation, we made this additional day time major compaction job as a daily scheduled job. We disabled Redis cache for 100% of the counters the next day (Nov 13). This expectedly increased the Scylla QPS, with no noticeable adverse effect on the service latency or Hystrix CB open events.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/avg-latency-change.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. This graph shows how the average latency changed as a result of the experiment. The higher spikes correspond to the time when Redis cache was being progressively disabled before introducing the day time Scylla compaction job. The spikes lessened after the compaction job was introduced on Nov 12 (Note: Friday spike was due to higher traffic in general).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/p99-latency.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. This graph shows how the P99 latency changed as a result of the experiment. The higher spikes correspond to the time when Redis cache was being progressively disabled before introducing the day time Scylla compaction job. The spikes lessened after the compaction job was introduced on Nov 12 (Note: Friday spike was due to higher traffic in general).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;scyllas-own-cache&quot;&gt;Scylla’s own cache&lt;/h3&gt;

&lt;p&gt;One of our hypotheses was that we were not using Scylla cache due to our system’s design, along with all the service specific characteristics discussed earlier. Our experimental results show that this is indeed the case. We observed a significant increase in Scylla reads with Scylla’s own cache hits, while Scylla reads with Scylla’s own cache misses remained about the same despite our Scylla cluster receiving double the traffic. Percentage-wise, before disabling the external Redis cache, Scylla hit its own cache for ~30% of the total reads, and after we have completely disabled the external Redis cache, Scylla hit its cache for about 70% of the reads. We believe that this largely contributes to the overall performance of the service despite fully decommissioning the expensive Redis cache component from our system architecture.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/scylla-reads-increase.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10. Significant increase in Scylla reads after disable Redis cache.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/reads-with-cache-miss.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 11. No change in Scylla cache miss despite the doubling of Scylla traffic.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;scylla-cpu-and-memory-usage&quot;&gt;Scylla CPU and memory usage&lt;/h3&gt;

&lt;p&gt;Contrary to our assumption, although the Scylla QPS doubled due to the change done as part of this experiment, there was marginal increase in Scylla CPU usage (from ~50% to ~52% at peak). In terms of memory, Scylla log-structured allocator (LSA) memory usage remains consistent. For Non-LSA memory, the maximum utilisation did not increase. However, we noticed two daily spikes instead of one existed before the experiment. The second spike results from the newly added daily major compaction job. Notably,the overall non-LSA peak has slightly decreased after the introduction of the new compaction job.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/load.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 12. Relatively steady Scylla CPU utilisation.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/evaluate-performance-remove-redis-from-scylla-service/non-lsa-used-memory.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 13. Non-LSA memory usage spikes twice a day after the experiment. The new spike corresponds to the newly added day time compaction job.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In summary, we were able to maintain the same service performance while removing an expensive Redis cache component from our system architecture, which accounted for about 25% of the overall service cost. This has been made possible primarily by significant increase in the utilisation of Scylla’s own cache and adding a daily major compaction job on all our Scylla nodes.&lt;/p&gt;

&lt;p&gt;In the future, we plan to further experiment with different Scylla configurations for potential performance gain, specifically to improve the latency.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebscylla&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Apr 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/evaluate-performance-remove-redis-from-scylla-service</link>
        <guid isPermaLink="true">https://engineering.grab.com/evaluate-performance-remove-redis-from-scylla-service</guid>
        
        <category>Database</category>
        
        <category>Engineering</category>
        
        <category>Event processing</category>
        
        <category>Optimisation</category>
        
        <category>Redis</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Facilitating Docs-as-Code implementation for users unfamiliar with Markdown</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Although Grab is a tech company, not everyone is an engineer. Many team members don’t use GitLab daily, and Markdown’s quirks can be challenging for them. This made adopting the Docs-as-Code culture a hurdle, particularly for non-engineering teams responsible for key engineering-facing documents. In this article, we’ll discuss how we’ve streamlined the Docs-as-Code process for technical contributors, specifically non-engineers, who are not very familiar with GitLab and might face challenges with Markdown. For more on the benefits of the Docs-as-Code approach, check out &lt;a href=&quot;https://engineering.grab.com/doc-as-code&quot;&gt;this blog&lt;/a&gt; on the subject.&lt;/p&gt;

&lt;p&gt;As part of our ongoing efforts to enhance the TechDocs experience, we’ve introduced a rich text editor for those who prefer a &lt;a href=&quot;https://en.wikipedia.org/wiki/WYSIWYG&quot;&gt;WYSIWYG (What You See Is What You Get)&lt;/a&gt; interface on top of a Git workflow, helping to simplify authoring. We’ll also cover how we plan to improve the workflow for non-engineering teams contributing to service and standalone documentation.&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-a-rich-text-editor&quot;&gt;The need for a rich text editor&lt;/h2&gt;

&lt;p&gt;Ask any developer today, and they’ll likely tell you that Markdown is the go-to format for documentation. Due to its simplicity, whether it’s GitHub, GitLab, Bitbucket, or other platforms, Markdown has become the default choice, even for issue tracking. It’s also integrated into most text editors, like IntelliJ, VS Code, Vim, and Emacs, with handy plugins for syntax highlighting and previewing.&lt;/p&gt;

&lt;p&gt;Engineers are gradually embracing the Docs-as-Code approach and enjoying the benefits of writing the documentation in Markdown format directly in their IDEs and pushing them out as merge requests (MR). However, non-engineers face the nuance of writing in Markdown and going through the Git workflow. This is when the call for a &lt;strong&gt;WYSIWYG (What You See Is What You Get)&lt;/strong&gt; editor aka TechDocs editor came about. This solution brought about several benefits to non-engineers. It provides a familiar, UI-based experience for editing, but it still aligns with the Docs-as-Code model. This tool allows users to edit documentation via a simple UI in the Backstage portal without having to deal with the complexities of MkDocs, entity catalogs, or Markdown syntax. In the context Backstage, “entities” refer to services, platforms, tools, or libraries, and documentation is often tied to these entities to provide context sensitivity. The goal was to make it easy for people to focus on content, not the tools, and enable quick updates without the technical overhead.&lt;/p&gt;

&lt;p&gt;We’ve kept GitLab as the central storage system, but now, with the TechDocs editor, non-engineers can contribute with ease. Figure 1 highlights our editor’s features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reordering&lt;/li&gt;
  &lt;li&gt;Renaming&lt;/li&gt;
  &lt;li&gt;Deleting pages&lt;/li&gt;
  &lt;li&gt;Switching between normal and Markdown views&lt;/li&gt;
  &lt;li&gt;Formatting text with titles, bullets, numbering&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/Facilitating-Docs-as-Code/figure-1.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: TechDocs editor in Helix TechDocs portal&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our goal for our editor is to make it more flexible, performant, and user-friendly. Based on user feedback, key priorities include customisation, extensibility for non-standard Markdown elements, and long-term maintainability.&lt;/p&gt;

&lt;p&gt;To achieve this, we selected the &lt;strong&gt;Lexical framework&lt;/strong&gt;. Compared to other Markdown-based tools like Toast UI, Lexical offers greater extensibility, allowing us to implement advanced features such as autocomplete and support for non-standard Markdown elements like Kroki diagrams.&lt;/p&gt;

&lt;p&gt;The following flowchart illustrates how Markdown content is imported and exported within the Lexical editor, ensuring seamless integration with TechDocs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/Facilitating-Docs-as-Code/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Lexical Markdown transformer flow chart &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;By continuously iterating based on user needs, we aim to make Docs-as-Code accessible not just for engineers but for anyone contributing to documentation at Grab.&lt;/p&gt;

&lt;h1 id=&quot;user-journeys&quot;&gt;User journeys&lt;/h1&gt;

&lt;p&gt;We explored various workflows to streamline the documentation lifecycle, focusing on both creation and editing processes. By integrating these workflows into the developer portal, we ensured that users can easily create and edit documentation, enhancing overall efficiency and collaboration.&lt;/p&gt;

&lt;p&gt;Here are the three key user journeys we focused on addressing:&lt;/p&gt;

&lt;h3 id=&quot;journey-1-edit-existing-techdocs&quot;&gt;Journey 1: Edit existing TechDocs&lt;/h3&gt;

&lt;h4 id=&quot;high-level-workflow-definition&quot;&gt;High level workflow definition:&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Toggle to ‘edit’ mode&lt;/strong&gt;: The user switches to the edit mode to start making changes to the TechDocs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;User starts editing TechDocs&lt;/strong&gt;: The user begins the process of editing the documentation and clicks save.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;User gets redirected to GitLab&lt;/strong&gt;: If not authenticated, they are redirected to GitLab for authentication. Once authenticated, a merge request is created to update the entity YAML file and add the new TechDocs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Access check&lt;/strong&gt;: The system checks if the user has access to the TechDocs file repository. If not, they are prompted to request access.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/Facilitating-Docs-as-Code/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: User journey 1&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;journey-2-create-stand-alone-techdocs-from-documentation-page&quot;&gt;Journey 2: Create stand-alone TechDocs from “Documentation” page&lt;/h3&gt;

&lt;h4 id=&quot;high-level-workflow-definition-1&quot;&gt;High level workflow definition:&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;User authentication&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;If the user is not authenticated, they are redirected to GitLab for authentication.&lt;/li&gt;
      &lt;li&gt;If the user is already authenticated, the process skips to the next step.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Registering merge requests&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;The MR is registered to a scheduler job to automatically register a new entity catalog when it detects that the MR has been merged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This workflow ensures that users are authenticated via GitLab before proceeding and that new entity catalogs are automatically registered upon the merging of MRs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/Facilitating-Docs-as-Code/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: User journey 2&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;journey-3-create-techdocs-from-docs-tab-on-entity-page&quot;&gt;Journey 3: Create TechDocs from “Docs” tab on entity page&lt;/h3&gt;

&lt;h4 id=&quot;high-level-workflow-definition-2&quot;&gt;High level workflow definition:&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Start creating TechDocs&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;User selects ‘create TechDocs’ on the ‘Docs’ tab in the Helix TechDocs portal UI.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Save and redirect&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;User clicks ‘save’ and is redirected to GitLab with a merge request (MR) created to update the entity YAML file and add new TechDocs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Access check and MR registration&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;If the user has access to the entity YAML file repository, proceed with the MR. If not, prompt the user to get access.&lt;/li&gt;
      &lt;li&gt;Register the MR to a scheduler job to automatically refresh the entity catalog when it detects the MR as merged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/Facilitating-Docs-as-Code/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: User journey 3&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;phased-rollout&quot;&gt;Phased rollout&lt;/h2&gt;

&lt;p&gt;We phased the rollout of our Markdown editor to ensure a smooth transition, allowing users to gradually adapt while we gathered feedback and iterated on features. This approach helped us address challenges early, refine usability, and deliver meaningful improvements with each phase.&lt;/p&gt;

&lt;h3 id=&quot;phase-1-initial-markdown-editor-for-developer-portal&quot;&gt;Phase 1: Initial Markdown editor for developer portal&lt;/h3&gt;

&lt;p&gt;In Phase 1, we built a basic editor aligned with our documentation standards. Users can create and edit TechDocs for different entity catalogs, with support for basic Markdown and image previews for both absolute and relative paths. The editor tracks concurrent editing sessions and shows pending merge requests. It also includes Markdown configuration options to add, rename, reorganise, or delete pages. Additionally, our GitLab integration consolidates changes into a single commit and opens a merge request.&lt;/p&gt;

&lt;h3 id=&quot;phase-2-independent-documentation-creation&quot;&gt;Phase 2: Independent documentation creation&lt;/h3&gt;

&lt;p&gt;Phase 2 includes expanded functionality to support independent documentation creation and related features, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HTML preview and image uploads (relative paths).&lt;/li&gt;
  &lt;li&gt;Save drafts locally in the browser.&lt;/li&gt;
  &lt;li&gt;Pending MRs listed in the editor.&lt;/li&gt;
  &lt;li&gt;Draw.io and Excalidraw integration for diagrams.&lt;/li&gt;
  &lt;li&gt;MkDocs updates: change site name.&lt;/li&gt;
  &lt;li&gt;Auto-registeration of new entity catalogs when MRs are merged.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-3-advanced-editor-capabilities&quot;&gt;Phase 3: Advanced editor capabilities&lt;/h3&gt;

&lt;p&gt;Phase 3 introduced additional features, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for Kroki / Mermaid diagrams.&lt;/li&gt;
  &lt;li&gt;Display concurrent edit sessions for better collaboration.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each phase improved the editor, enhancing TechDocs at Grab with seamless GitLab integration and user-friendly features.&lt;/p&gt;

&lt;h2 id=&quot;integrating-the-ability-to-do-a-live-preview&quot;&gt;Integrating the ability to do a live preview&lt;/h2&gt;

&lt;p&gt;While syntax highlighting in the TechDocs editor is helpful, it can’t fully predict how the final Markdown document will appear once rendered due to Markdown flavour inconsistencies. This is especially true for elements like images, tables, and diagrams, where visual verification is crucial. To minimise these risks, the TechDocs editor includes a live preview feature, allowing users to see the fully rendered document alongside the editor in a split-screen view. This lets users verify their work as they go, preventing the need to switch back and forth between the editor and the final document, saving time and reducing potential formatting errors.&lt;/p&gt;

&lt;p&gt;However, like most live preview features, performance challenges can arise. For larger documents, the process of continuously converting Markdown to HTML can slow down editing. External resources such as images that need to be re-rendered, can cause visual glitches or delays in the preview. Running scripts or using plugins with extended grammar also adds to the performance load, requiring frequent re-execution and potentially slowing down the experience.&lt;/p&gt;

&lt;p&gt;To mitigate these issues, the TechDocs editor uses an inbuilt preview feature that shows users exactly how their changes are going to appear on the portal once their changes are merged. This ensures that users can confidently make adjustments and understand the final presentation before committing their updates. Additionally, the live preview feature enables more efficient collaboration by providing real-time feedback on content and formatting, further enhancing the overall documentation workflow.&lt;/p&gt;

&lt;h2 id=&quot;gitlab-integration-strategy&quot;&gt;GitLab integration strategy&lt;/h2&gt;

&lt;p&gt;The TechDocs editor integrates seamlessly with GitLab, allowing users to make changes effortlessly through OAuth2 authentication. When users log into the editor, they simply click the “Connect with GitLab” button, which provides access via the OAuth 2.0 protocol. Once connected, all modifications made within the editor are executed using the user’s GitLab credentials, streamlining the documentation process and ensuring a smooth experience for users as they update their documentation directly within the TechDocs framework.&lt;/p&gt;

&lt;p&gt;To minimise Git conflicts, we considered and implemented some of these approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Display pending merge requests at the top of the editor to alert users of existing changes.&lt;/li&gt;
  &lt;li&gt;Show who else is editing the same TechDocs to help users coordinate and avoid conflicts.&lt;/li&gt;
  &lt;li&gt;Include tools to automatically or semi-automatically resolve Git conflicts.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Bringing Docs-as-Code to a broader audience at Grab meant addressing the challenges faced by non-engineering contributors. With the introduction of a WYSIWYG editor, seamless GitLab integration, and a live preview feature, we’ve made it easier for everyone to contribute without needing deep Markdown expertise.&lt;/p&gt;

&lt;p&gt;As we continue to improve the TechDocs editor, our focus remains on removing barriers to documentation, enhancing collaboration, and ensuring that our docs evolve alongside our fast-moving engineering teams.&lt;/p&gt;

&lt;p&gt;Docs-as-Code isn’t just about engineers writing documentation—it’s about making documentation a natural and frictionless part of the development process for everyone.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmarkdown&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 04 Apr 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/facilitating-docs-as-code-with-markdown</link>
        <guid isPermaLink="true">https://engineering.grab.com/facilitating-docs-as-code-with-markdown</guid>
        
        <category>Blog</category>
        
        <category>TechDocs</category>
        
        <category>Helix</category>
        
        <category>Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
