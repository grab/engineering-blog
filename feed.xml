<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 10 Sep 2024 07:45:44 +0000</pubDate>
    <lastBuildDate>Tue, 10 Sep 2024 07:45:44 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Unveiling the process: The creation of our powerful campaign builder</title>
        <description>&lt;p&gt;In a previous &lt;a href=&quot;https://engineering.grab.com/trident-real-time-event-processing-at-scale&quot;&gt;blog&lt;/a&gt;, we introduced Trident, Grab’s internal marketing campaign platform. Trident empowers our marketing team to configure If This, Then That (IFTTT) logic and processes real-time events based on that.&lt;/p&gt;

&lt;p&gt;While we mainly covered how we scaled up the system to handle large volumes of real-time events, we did not explain the implementation of the event processing mechanism. This blog will fill up this missing piece. We will walk you through the various processing mechanisms supported in Trident and how they were built.&lt;/p&gt;

&lt;h2 id=&quot;base-building-block-treatment&quot;&gt;Base building block: Treatment&lt;/h2&gt;

&lt;p&gt;In our system, we use the term “treatment” to refer to the core unit of a full IFTTT data structure. A treatment is an amalgamation of three key elements - an event, conditions (which are optional), and actions. For example, consider a promotional campaign that offers “100 GrabPoints for completing a ride paid with GrabPay Credit”. This campaign can be transformed into a treatment in which the event is “ride completion”, the condition is “payment made using GrabPay Credit”, and the action is “awarding 100 GrabPoints”.&lt;/p&gt;

&lt;p&gt;Data generated across various Kafka streams by multiple services within Grab forms the crux of events and conditions for a treatment. Trident processes these Kafka streams, treating each data object as an event for the treatments. It evaluates the set conditions against the data received from these events. If all conditions are met, Trident then executes the actions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/processing-treatments.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Trident processes Kafka streams as events for treatments.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When the Trident user interface (UI) was first established, campaign creators had to grasp the treatment concept and configure the treatments accordingly. As we improved the UI, it became more user-friendly.&lt;/p&gt;

&lt;h2 id=&quot;building-on-top-of-treatment&quot;&gt;Building on top of treatment&lt;/h2&gt;

&lt;p&gt;Campaigns can be more complex than the example we provided earlier. In such scenarios, a single campaign may need transformation into several treatments. All these individual treatments are categorised under what we refer to as a “treatment group”. In this section, we discuss features that we have developed to manage such intricate campaigns.&lt;/p&gt;

&lt;h3 id=&quot;counter&quot;&gt;Counter&lt;/h3&gt;

&lt;p&gt;Let’s say we have a marketing campaign that “rewards users after they complete 4 rides”. For this requirement, it’s necessary for us to keep track of the number of rides each user has completed. To make this possible, we developed a capability known as &lt;strong&gt;counter&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;On the backend, a single counter setup translates into two treatments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treatment 1&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: N/A&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStats&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Treatment 2&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ride Count == 4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this feature, we introduce a new event, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStats&lt;/code&gt; action in &lt;strong&gt;Treatment 1&lt;/strong&gt; triggers the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; event following the update of the user counter. This allows &lt;strong&gt;Treatment 2&lt;/strong&gt; to consume the event and perform subsequent evaluations.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. The end-to-end evaluation process when using the Counter feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt; event is consumed, &lt;strong&gt;Treatment 1&lt;/strong&gt; is evaluated which then executes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStat&lt;/code&gt; action. This action increments the user’s ride counter in the database, gets the latest counter value, and publishes an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; event to Kafka.&lt;/p&gt;

&lt;p&gt;There are also other consumers that listen to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; events. When this event is consumed, &lt;strong&gt;Treatment 2&lt;/strong&gt; is evaluated. This process involves verifying whether the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ride Count&lt;/code&gt; equals to 4. If the condition is satisfied, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt; action is triggered.&lt;/p&gt;

&lt;p&gt;This feature is not limited to counting the number of event occurrences only. It’s also capable of tallying the total amount of transactions, among other things.&lt;/p&gt;

&lt;h3 id=&quot;delay&quot;&gt;Delay&lt;/h3&gt;

&lt;p&gt;Another feature available on Trident is a delay function. This feature is particularly beneficial in situations where we want to time our actions based on user behaviour. For example, we might want to give a ride voucher to a user three hours after they’ve ordered a ride to a theme park. The intention for this is to offer them a voucher they can use for their return trip.&lt;/p&gt;

&lt;p&gt;On the backend, a delay setup translates into two treatments. Given the above scenario, the treatments are as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treatment 1&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dropoff Location == Universal Studio&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduleDelayedEvent&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Treatment 2&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onDelayedEvent&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: N/A&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We introduce a new event, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onDelayedEvent&lt;/code&gt;, which &lt;strong&gt;Treatment 1&lt;/strong&gt; triggers during the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduleDelayedEvent&lt;/code&gt; action. This is made possible by using Simple Queue Service (SQS), given its built-in capability to publish an event with a delay.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process-delay-feature.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. The end-to-end evaluation process when using the Delay feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The maximum delay that SQS supports is 15 minutes; meanwhile, our platform allows for a delay of up to x hours. To address this limitation, we publish the event multiple times upon receiving the message, extending the delay by another 15 minutes each time, until it reaches the desired delay of x hours.&lt;/p&gt;

&lt;h3 id=&quot;limit&quot;&gt;Limit&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;Limit&lt;/strong&gt; feature is used to restrict the number of actions for a specific campaign or user within that campaign. This feature can be applied on a daily basis or for the full duration of the campaign.&lt;/p&gt;

&lt;p&gt;For instance, we can use the &lt;strong&gt;Limit&lt;/strong&gt; feature to distribute 1000 vouchers to users who have completed a ride and restrict it to only one voucher for one user per day. This ensures a controlled distribution of rewards and prevents a user from excessively using the benefits of a campaign.&lt;/p&gt;

&lt;p&gt;In the backend, a limit setup translates into conditions within a single treatment. Given the above scenario, the treatment would be as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TotalUsageCount &amp;lt;= 1000 AND DailyUserUsageCount &amp;lt;= 1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similar to the &lt;strong&gt;Counter&lt;/strong&gt; feature, it’s necessary for us to keep track of the number of completed rides for each user in the database.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process-limit-feature.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. The end-to-end evaluation process when using the Limit feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;a-better-campaign-builder&quot;&gt;A better campaign builder&lt;/h2&gt;

&lt;p&gt;As our campaigns grew more and more complex, the treatment creation quickly became overwhelming. A complex logic flow often required the creation of many treatments, which was cumbersome and error-prone. The need for a more visual and simpler campaign builder UI became evident.&lt;/p&gt;

&lt;p&gt;Our design team came up with a flow-chart-like UI. Figure 5, 6, and 7 show examples of how certain imaginary campaign setup would look like in the new UI.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. When users complete a food order, if they are a gold user, award them with A. However, if they are a silver user, award them with  B.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. When users complete a food or mart order, increment a counter. When the counter reaches 5, send them a message. Once the counter reaches 10, award them with points.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. When a user confirms a ride booking, wait for 1 minute, and then conduct A/B testing by sending a message 50% of the time.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The campaign setup in the new UI can be naturally stored as a node tree structure. The following is how the example in figure 5 would look like in JSON format. We assign each node a unique number ID, and store a map of the ID to node content.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;1&quot;: {
    &quot;type&quot;: &quot;scenario&quot;,
    &quot;data&quot;: { &quot;eventType&quot;: &quot;foodOrderComplete&quot;  },
    &quot;children&quot;: [&quot;2&quot;, &quot;3&quot;]
  },
  &quot;2&quot;: {
    &quot;type&quot;: &quot;condition&quot;,
    &quot;data&quot;: { &quot;lhs&quot;: &quot;var.user.tier&quot;, &quot;operator&quot;: &quot;eq&quot;, &quot;rhs&quot;: &quot;gold&quot; },
    &quot;children&quot;: [&quot;4&quot;]
  },
  &quot;3&quot;: {
    &quot;type&quot;: &quot;condition&quot;,
    &quot;data&quot;: { &quot;lhs&quot;: &quot;var.user.tier&quot;, &quot;operator&quot;: &quot;eq&quot;, &quot;rhs&quot;: &quot;silver&quot; },
    &quot;children&quot;: [&quot;5&quot;]
  },
  &quot;4&quot;: {
    &quot;type&quot;: &quot;action&quot;,
    &quot;data&quot;: {
      &quot;type&quot;: &quot;awardReward&quot;,
      &quot;payload&quot;: { &quot;rewardID&quot;: &quot;ID-of-A&quot;  }
    }
  },
  &quot;5&quot;: {
    &quot;type&quot;: &quot;action&quot;,
    &quot;data&quot;: {
      &quot;type&quot;: &quot;awardReward&quot;,
      &quot;payload&quot;: { &quot;rewardID&quot;: &quot;ID-of-B&quot;  }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conversion-to-treatments&quot;&gt;Conversion to treatments&lt;/h3&gt;

&lt;p&gt;The question then arises, how do we execute this node tree as treatments? This requires a conversion process. We then developed the following algorithm for converting the node tree into equivalent treatments:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// convertToTreatments is the main function
func convertToTreatments(rootNode) -&amp;gt; []Treatment:
  output = []

  for each scenario in rootNode.scenarios:
    // traverse down each branch
    context = createConversionContext(scenario)
    for child in rootNode.children:
      treatments = convertHelper(context, child)
      output.append(treatments)

  return output

// convertHelper is a recursive helper function
func convertHelper(context, node) -&amp;gt; []Treatment:
  output = []
  f = getNodeConverterFunc(node.type)
  treatments, updatedContext = f(context, node)

  output.append(treatments)

  for child in rootNode.children:
    treatments = convertHelper(updatedContext, child)
    output.append(treatments)

  return output
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getNodeConverterFunc&lt;/code&gt; will return different handler functions according to the node type. Each handler function will either update the conversion context, create treatments, or both.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;caption style=&quot;caption-side:bottom&quot;&gt;Table 1. The handler logic mapping for each node type.&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;width:20%&quot;&gt;Node type&lt;/th&gt;
      &lt;th&gt;Logic&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;condition&lt;/td&gt;
      &lt;td&gt;Add conditions into the context and return the updated context.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;action&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and the action itself.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;delay&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and a scheduleDelayedEvent action.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;count&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and an incrementUserStats action.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;count condition&lt;/td&gt;
      &lt;td&gt;Form a condition with the count key from the context, and return an updated context with the condition.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It is important to note that treatments cannot always be reverted to their original node tree structure. This is because different node trees might be converted into the same set of treatments.&lt;/p&gt;

&lt;p&gt;The following is an example where two different node trees setups correspond to the same set of treatments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Food order complete -&amp;gt; if gold user -&amp;gt; then award A&lt;/li&gt;
  &lt;li&gt;Food order complete -&amp;gt; if silver user -&amp;gt; then award B&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/two-node-tree-setup.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. An example of two node tree setups corresponding to the the same set of treatments.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Therefore, we need to store both the campaign node tree JSON and treatments, along with the mapping between the nodes and the treatments. Campaigns are executed using treatments, but displayed using the node tree JSON.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/storing-campaigns.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. For each campaign, we store both the node tree JSON and treatments, along with their mapping.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;how-we-handle-campaign-updates&quot;&gt;How we handle campaign updates&lt;/h3&gt;

&lt;p&gt;There are instances where a marketing user updates a campaign after its creation. For such cases we need to identify:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Which existing treatments should be removed.&lt;/li&gt;
  &lt;li&gt;Which existing treatments should be updated.&lt;/li&gt;
  &lt;li&gt;What new treatments should be added.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can do this by using the node-treatment mapping information we stored. The following is the pseudocode for this process:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func howToUpdateTreatments(oldTreatments []Treatment, newTreatments []Treatment):
  treatmentsUpdate = map[int]Treatment // treatment ID -&amp;gt; updated treatment
  treatmentsRemove = []int // list of treatment IDs
  treatmentsAdd = []Treatment // list of new treatments to be created

  matchedOldTreamentIDs = set()

  for newTreatment in newTreatments:
    matched = false

    // see whether the nodes match any old treatment
    for oldTreatment in oldTreatments:
      // two treatments are considered matched if their linked node IDs are identical
      if isSame(oldTreatment.nodeIDs, newTreatment.nodeIDs):
        matched = true
        treatmentsUpdate[oldTreament.ID] = newTreatment
        matchedOldTreamentIDs.Add(oldTreatment.ID)
        break

    // if no match, that means it is a new treatment we need to create
    if not matched:
      treatmentsAdd.Append(newTreatment)

  // all the non-matched old treatments should be deleted
  for oldTreatment in oldTreatments:
    if not matchedOldTreamentIDs.contains(oldTreatment.ID):
      treatmentsRemove.Append(oldTreatment.ID)

  return treatmentsAdd, treatmentsUpdate, treatmentsRemove
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For a visual illustration, let’s consider a campaign that initially resembles the one shown in figure 10. The node IDs are highlighted in red.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/campaign-node-tree-structure.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10. A campaign in node tree structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This campaign will generate two treatments.&lt;/p&gt;

&lt;table style=&quot;width: 70%&quot; class=&quot;table&quot;&gt;
  &lt;caption style=&quot;caption-side:bottom&quot;&gt;Table 2. The campaign shown in the figure 10 will generated two treatments.&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;width:10%&quot;&gt;ID&lt;/th&gt;
      &lt;th style=&quot;width:50%&quot;&gt;Treatment&lt;/th&gt;
      &lt;th&gt;Linked node IDs&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Event: food order complete &lt;br /&gt; Condition: gold user &lt;br /&gt; Action: award A&lt;/td&gt;
      &lt;td&gt;1, 2, 3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Event: food order complete &lt;br /&gt; Condition: silver user &lt;br /&gt; Action: award B&lt;/td&gt;
      &lt;td&gt;1, 4, 5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;After creation, the campaign creator updates the upper condition branch, deletes the lower branch, and creates a new branch. Note that after node deletion, the deleted node ID will not be reused.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/updated-campaign-node-tree-structure.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 11. An updated campaign in node tree structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;According to our logic in figure 11, the following update will be performed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Update action for treatment 1 to “award C”.&lt;/li&gt;
  &lt;li&gt;Delete treatment 2&lt;/li&gt;
  &lt;li&gt;Create a new treatment: food -&amp;gt; is promo used -&amp;gt; send push&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article reveals the workings of Trident, our bespoke marketing campaign platform. By exploring the core concept of a “treatment” and additional features like Counter, Delay and Limit, we illustrated the flexibility and sophistication of our system.&lt;/p&gt;

&lt;p&gt;We’ve explained changes to the Trident UI that have made campaign creation more intuitive. Transforming campaign setups into executable treatments while preserving the visual representation ensures seamless campaign execution and adaptation.&lt;/p&gt;

&lt;p&gt;Our devotion to improving Trident aims to empower our marketing team to design engaging and dynamic campaigns, ultimately providing excellent experiences to our users.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Sep 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/the-creation-of-our-powerful-campaign-builder</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-creation-of-our-powerful-campaign-builder</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Chimera Sandbox: A scalable experimentation and development platform for Notebook services</title>
        <description>&lt;p&gt;Key to innovation and improvement in machine learning (ML) models is the ability for rapid iteration. Our team, Chimera, part of the Artificial Intelligence (AI) Platform team, provides the essential compute infrastructure, ML pipeline components, and backend services. This support enables our ML engineers, data scientists, and data analysts to efficiently experiment and develop ML solutions at scale.&lt;/p&gt;

&lt;p&gt;With a commitment to leveraging the latest Generative AI (GenAI) technologies, Grab is enhancing productivity tools for all Grabbers. Our Chimera Sandbox, a scalable Notebook platform, facilitates swift experimentation and development of ML solutions, offering deep integration with our AI Gateway. This enables easy access to various Large Language Models (LLMs) (both proprietary and open source), ensuring scalability, compliance, and access control are managed seamlessly.&lt;/p&gt;

&lt;h2 id=&quot;what-is-chimera-sandbox&quot;&gt;What is Chimera Sandbox?&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox is a Notebook service platform. It allows users to launch multiple notebook and visualisation services for experimentation and development. The platform offers an extremely quick onboarding process enabling any Grabber to start learning, exploring and experimenting in just a few minutes. This inclusivity and ease of use have been key in driving the adoption of the platform across different teams within Grab and empowering all Grabbers to be GenAI-ready.&lt;/p&gt;

&lt;p&gt;One significant challenge in harnessing ML for innovation, whether for technical experts or non-technical enthusiasts, has been the accessibility of resources. This includes GPU instances and specialised services for developing LLM-powered applications. Chimera Sandbox addresses this head-on by offering an extensive array of compute instances, both with and without GPU support, thus removing barriers to experimentation. Its deep integration with Grab’s suite of internal ML tools transforms the way users approach ML projects. Users benefit from features like hyperparameter tuning, tracking ML training metadata, accessing diverse LLMs through Grab’s AI Gateway, and experimenting with rich datasets from Grab’s data lake. Chimera Sandbox ensures that users have everything they need at their fingertips. This ecosystem not only accelerates the development process but also encourages innovative approaches to solving complex problems.&lt;/p&gt;

&lt;p&gt;The underlying compute infrastructure of the Chimera Sandbox platform is Grab’s very own battle-tested, highly scalable ML compute infrastructure running on multiple Kubernetes clusters. Each cluster can scale up to thousands of nodes at peak times gracefully. This scalability ensures that the platform can handle the high computational demands of ML tasks. The robustness of Kubernetes ensures that the platform remains stable, reliable, and highly available even under heavy load. At any point in time, there can be hundreds of data scientists, ML engineers and developers experimenting and developing on the Chimera Sandbox platform.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/chimera-sandbox-platform.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Chimera Sandbox Platform.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/ui-starting-chimera.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. UI for Starting Chimera Sandbox.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;best-of-both-worlds&quot;&gt;Best of both worlds&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox is suitable for both new users who want to explore and experiment ML solutions and advanced users who want to have full control over the Notebook services they run. Users can launch Notebook services using default Docker images provided by the Chimera Sandbox platform. These images come pre-loaded with popular data science and ML libraries and various Grab internal systems integrations. Chimera also provides basic Docker images from which the users can use as base images to build their own customised Notebook service Docker images. Once the images are built, the users can configure their Notebook services to use their custom Docker images. This ensures their Notebook environment can be exactly the way they want them to be.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/customise-notebook-packages.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Users are able to customise their Notebook service with additional packages.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;real-time-collaboration&quot;&gt;Real-time collaboration&lt;/h2&gt;
&lt;p&gt;The Chimera Sandbox platform also features a real-time collaboration feature. This feature fosters a collaborative environment where users can exchange ideas and work together on projects.&lt;/p&gt;

&lt;h2 id=&quot;cpu-and-gpu-choices&quot;&gt;CPU and GPU choices&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox offers a wide variety of CPU and GPU choices to cater to specific needs, whether it is a CPU, memory, or GPU intensive experimentation. This flexibility allows users to choose the most suitable computational resources for their tasks, ensuring optimal performance and efficiency.&lt;/p&gt;

&lt;h2 id=&quot;deep-integration-with-spark&quot;&gt;Deep integration with Spark&lt;/h2&gt;

&lt;p&gt;The platform is deeply integrated with internal Spark engines, enabling users to experiment building  extract, transform, and load (ETL) jobs with data from Grab’s data lake. Integrated helpers such as SparkConnect Kernel and %%spark_sql magic cell, provide a faster developer experience, which can execute Spark SQL queries without needing to write additional code to start a Spark session and query.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/magic-cell.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. %%spark_sql magic cell enables users to quickly explore data with Spark.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to Magic Cell, the Chimera Sandbox offers advanced Spark functionalities. Users can write PySpark code using pre-configured and configurable Spark clients in the runtime environment. The underlying computation engine leverages Grab’s custom Spark-on-Kubernetes operator, enabling support for large-scale Spark workloads. This high-code capability complements the low-code Magic Cell feature, providing users with a versatile data processing environment.&lt;/p&gt;

&lt;h2 id=&quot;ai-gallery&quot;&gt;AI Gallery&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox features an AI Gallery to guide and accelerate users to start experimenting with ML solutions or building GenAI-powered applications. This is especially useful for new or novice users who are keen to explore what they can do on the Chimera Sandbox platform. With Chimera Sandbox, users are not just presented with a bare bones compute solution but rather are provided with ways to do ML tasks right from Chimera Sandbox Notebooks. This approach saves users from the hassle of having to piece together the examples from the public internet, which may not work on the platform. These ready-to-run and comprehensive notebooks in the AI Gallery assure users that they can run end-to-end examples without a hitch. Based on these examples, the users can only extend their experimentations and development for their specific needs. Not only that, these tutorials and notebooks exhibit the platform capabilities and integrations available on the platform in an interactive manner rather than having the users refer to a separate documentation.&lt;/p&gt;

&lt;p&gt;Lastly, the AI Gallery encourages contributions from other Grabbers, fostering a collaborative environment. Users who are enthusiastic about creating educational contents on Chimera Sandbox can effectively share their work with other Grabbers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/ai-gallery.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Including AI Gallery in user specified sandbox images.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;integration-with-various-llm-services&quot;&gt;Integration with various LLM services&lt;/h2&gt;

&lt;p&gt;Notebook users on Chimera Sandbox can easily tap into a plethora of LLMs, both open source and proprietary models, without any additional setup via our AI Gateway. The platform takes care of access mechanisms and endpoints for various LLM services so that the users can easily use their favourite libraries to create LLM-powered applications and conduct experimentations. This seamless integration with LLMs enables users to focus on their GAI-powered ideas rather than having to worry about underlying logistics and technicalities of using different LLMs.&lt;/p&gt;

&lt;h2 id=&quot;more-than-a-notebook-service&quot;&gt;More than a notebook service&lt;/h2&gt;

&lt;p&gt;While Notebook is the most popular service on the platform, Chimera Sandbox offers much more than just notebook capabilities. It serves as a comprehensive namespace workspace equipped with a suite of ML/AI tools. Alongside notebooks, users can access essential ML tools such as Optuna for hyperparameter tuning, MLflow for experiment tracking, and other tools including Zeppelin, RStudio, Spark history, Polynote, and LabelStudio. All these services use a shared storage system, creating a tailored workspace for ML and AI tasks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/sandbox-namespace.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. A Sandbox namespace with its out-of-the-box services.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, the Sandbox framework allows for the seamless integration of more services into personal workspaces. This high level of flexibility significantly enhances the capabilities of the Sandbox platform, making it an ideal environment for diverse ML and AI applications.&lt;/p&gt;

&lt;h2 id=&quot;cost-attribution&quot;&gt;Cost attribution&lt;/h2&gt;

&lt;p&gt;For a multi-tenanted platform such as Chimera Sandbox, it is crucial to provide users information on how much they have spent with their experimentations. Cost showback and chargeback capabilities are of utmost importance for a platform on which users can launch Notebook services that use accelerated instances with GPUs. The platform provides cost attribution to individual users, so each user knows exactly how much they are spending on their experimentations and can make budget-conscious decisions. This transparency in cost attribution encourages responsible usage of resources and helps users manage their budgets effectively.&lt;/p&gt;

&lt;h2 id=&quot;growth-and-future-plans&quot;&gt;Growth and future plans&lt;/h2&gt;

&lt;p&gt;In essence, Chimera Sandbox is more than just a tool; it’s a catalyst for innovation and growth, empowering Grabbers to explore the frontiers of ML and AI. By providing an inclusive, flexible, and powerful platform, Chimera Sandbox is helping shape the future of Grab, making every Grabber not just ready but excited to contribute to the AI-driven transformation of our products and services.&lt;/p&gt;

&lt;p&gt;In July and August of this year, teams were given the opportunity to intensively learn and experiment with AI. Since then, we have observed hockey stick growth on the Chimera Sandbox platform. We are enabling massive experimentation across different teams at Grab to experiment and work on different GAI-powered applications.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/daily-active-users.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Chimera Sandbox daily active users.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our future plans include mechanisms for better notebook discovery, collaboration and usability, and the ability to enable users to schedule their notebooks right from Chimera Sandbox. These enhancements aim to improve the user experience and make the platform even more versatile and powerful.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Aug 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/chimera-sandbox</link>
        <guid isPermaLink="true">https://engineering.grab.com/chimera-sandbox</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>How we improved translation experience with cost efficiency</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As COVID restrictions were fully lifted in 2023, the number of tourists grew dramatically. People began to explore the world again, frequently using the Grab app to make bookings outside of their home country. However, we noticed that communication posed a challenge for some users. Despite our efforts to integrate an auto-translation feature in the booking chat, we received feedback about occasional missed or inaccurate translations. You can refer to this &lt;a href=&quot;https://engineering.grab.com/message-center&quot;&gt;blog&lt;/a&gt; for a better understanding of Grab’s chat system.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/improved-translation-experience-with-cost-efficiency/translation-example.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;An example of a bad translation. The correct translation is: &apos;ok sir&apos;.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In an effort to enhance the user experience for travellers using the Grab app, we formed an engineering squad to tackle this problem. The objectives are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure translation is provided when it’s needed.&lt;/li&gt;
  &lt;li&gt;Improve the quality of translation.&lt;/li&gt;
  &lt;li&gt;Maintain the cost of this service within a reasonable range.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ensure-translation-is-provided-when-its-needed&quot;&gt;Ensure translation is provided when it’s needed&lt;/h2&gt;

&lt;p&gt;Originally, we relied on users’ device language settings to determine if translation is needed. For example, if both the passenger and driver’s language setting is set to English, translation is not needed. Interestingly, it turned out that the device language setting did not reliably indicate the language in which a user would send their messages. There were numerous cases where despite having their device language set to English, drivers sent messages in another language.&lt;/p&gt;

&lt;p&gt;Therefore, we needed to detect the language of user messages on the fly to make sure we trigger translation when it’s needed.&lt;/p&gt;

&lt;h3 id=&quot;language-detection&quot;&gt;Language detection&lt;/h3&gt;

&lt;p&gt;Simple as it may seem, language detection is not that straightforward a task. We were unable to find an open-source language detector library that covered all Southeast Asian languages. We looked for Golang libraries as our service was written in Golang. The closest we could find were the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Whatlang: unable to detect Malay&lt;/li&gt;
  &lt;li&gt;Lingua: unable to detect Burmese and Khmer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We decided to choose Lingua over Whatlang as the base detector due to the following factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overall higher accuracy.&lt;/li&gt;
  &lt;li&gt;Capability to provide detection confidence level.&lt;/li&gt;
  &lt;li&gt;We have more users using Malay than those using Burmese or Khmer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a translation request comes in, our first step is to use Lingua for language detection. If the detection confidence level falls below a predefined threshold, we fall back to call the third-party translation service as it can detect all Southeast Asian languages.&lt;/p&gt;

&lt;p&gt;You may ask, why don’t we simply use the third-party service in the first place. It’s because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The third-party service only has a translate API that also does language detection, but it does not provide a standalone language detection API.&lt;/li&gt;
  &lt;li&gt;Using the translate API is costly, so we need to avoid calling it when it’s unnecessary. We will cover more on this in a later section.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another challenge we’ve encountered is the difficulty of distinguishing between Malay and Indonesian languages due to their strong similarities and shared vocabulary. The identical text might convey different meanings in these two languages, which the third-party translation service struggles to accurately detect and translate.&lt;/p&gt;

&lt;p&gt;Differentiating Malay and Indonesian is a tough problem in general. However, in our case, the detection has a very specific context, and we can make use of the context to enhance our detection accuracy.&lt;/p&gt;

&lt;h3 id=&quot;making-use-of-translation-context&quot;&gt;Making use of translation context&lt;/h3&gt;

&lt;p&gt;All our translations are for the messages sent in the context of a booking or order, predominantly between passenger and driver. There are two simple facts that can aid in our language detection:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Booking/order happens in one single country.&lt;/li&gt;
  &lt;li&gt;Drivers are almost always local to that country.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, for a booking that happens in an Indonesian city, if the driver’s message is detected as Malay, it’s highly likely that the message is actually in Bahasa Indonesia.&lt;/p&gt;

&lt;h2 id=&quot;improve-quality-of-translation&quot;&gt;Improve quality of translation&lt;/h2&gt;

&lt;p&gt;Initially, we were entirely dependent on a third-party service for translating our chat messages. While overall powerful, the third-party service is not perfect, and it does generate weird translations from time to time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/improved-translation-experience-with-cost-efficiency/weird-translation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;An example of a weird translation from a third-party service recorded on 19 Dec 2023.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Then, it came to us that we might be able to build an in-house translation model that could translate chat messages better than the third-party service. The reasons being:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The scope of our chat content is highly specific. All the chats are related to bookings or orders. There would not be conversations about life or work in the chat. Maybe a small Machine Learning (ML) model would suffice to do the job.&lt;/li&gt;
  &lt;li&gt;The third-party service is a general translation service. It doesn’t know the context of our messages. We, however, know the whole context. Having the right context gives us a great edge on generating the right translation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training-steps&quot;&gt;Training steps&lt;/h3&gt;

&lt;p&gt;To create our own translation model, we took the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perform topic modelling on Grab chat conversations.&lt;/li&gt;
  &lt;li&gt;Worked with the localisation team to create a benchmark set of translations.&lt;/li&gt;
  &lt;li&gt;Measured existing translation solutions against benchmarks.&lt;/li&gt;
  &lt;li&gt;Used an open source Large Language Model (LLM) to produce synthetic training data.&lt;/li&gt;
  &lt;li&gt;Used synthetic data to train our lightweight translation model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;topic-modelling&quot;&gt;Topic modelling&lt;/h4&gt;

&lt;p&gt;In this step, our aim was to generate a dataset which is both representative of the chat messages sent by our users and diverse enough to capture all of the nuances of the conversations. To achieve this, we took a stratified sampling approach. This involved a random sample of past chat conversation messages stratified by various topics to ensure a comprehensive and balanced representation.&lt;/p&gt;

&lt;h4 id=&quot;developing-a-benchmark&quot;&gt;Developing a benchmark&lt;/h4&gt;

&lt;p&gt;For this step we engaged Grab’s localisation team to create a benchmark for translations. The intention behind this step wasn’t to create enough translation examples to fully train or even finetune a model, but rather, it was to act as a benchmark for translation quality, and also as a set of few-shot learning examples for when we generate our synthetic data.&lt;/p&gt;

&lt;p&gt;This second point was critical! Although LLMs can generate good quality translations, LLMs are highly susceptible to their training examples. Thus, by using a set of handcrafted translation examples, we hoped to produce a set of examples that would teach the model the exact style, level of formality, and correct tone for the context in which we plan to deploy the final model.&lt;/p&gt;

&lt;h4 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h4&gt;

&lt;p&gt;From a theoretical perspective there are two ways that one can measure the performance of a machine translation system. The first is through the computation of some sort of translation quality score such as a BLEU or CHRF++ score. The second method is via subjective evaluation. For example, you could give each translation a score from 1 to 5 or pit two translations against each other and ask someone to assess which they prefer.&lt;/p&gt;

&lt;p&gt;Both methods have their relative strengths and weaknesses. The advantage of a subjective method is that it corresponds better with what we want, a high quality translation experience for our users. The disadvantage of this method is that it is quite laborious. The opposite is true for the computed translation quality scores, that is to say that they correspond less well to a human’s subjective experience of our translation quality, but that they are easier and faster to compute.&lt;/p&gt;

&lt;p&gt;To overcome the inherent limitations of each method, we decided to do the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Set a benchmark score for the translation quality of various translation services using a CHRF++ score.&lt;/li&gt;
  &lt;li&gt;Train our model until its CHRF++ score is significantly better than the benchmark score.&lt;/li&gt;
  &lt;li&gt;Perform a manual A/B test between the newly trained model and the existing translation service.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;synthetic-data-generation&quot;&gt;Synthetic data generation&lt;/h4&gt;

&lt;p&gt;To generate the training data needed to create our model, we had to rely on an open source LLM to generate the synthetic translation data. For this task, we spent considerable effort looking for a model which had both a large enough parameter count to ensure high quality outputs, but also a model which had the correct tokenizer to handle the diverse sets of languages which Grab’s customers speak. This is particularly important for languages which use non-standard character sets such as Vietnamese and Thai. We settled on using a public model from &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; for this task.&lt;/p&gt;

&lt;p&gt;We then used a subset of the previously mentioned benchmark translations to input as few-shot learning examples to our prompt. After many rounds of iteration, we were able to generate translations which were superior to the benchmark CHRF++ scores which we had attained in the previous section.&lt;/p&gt;

&lt;h4 id=&quot;model-fine-tuning&quot;&gt;Model fine tuning&lt;/h4&gt;
&lt;p&gt;We now had one last step before we had something that was production ready! Although we had successfully engineered a prompt capable of generating high quality translations from the public Hugging Face model, there was no way we’d be able to deploy such a model. The model was far too big for us to deploy it in a cost efficient manner and within an acceptable latency. Our solution to this was to fine-tune a smaller bespoke model using the synthetic training data which was derived from the larger model.&lt;/p&gt;

&lt;p&gt;These models were language specific (e.g. English to Indonesian) and built solely for the purpose of language translation. They are 99% smaller than the public model. With approximately 10 Million synthetic training examples, we were able to achieve performance which was 98% as effective as our larger model.&lt;/p&gt;

&lt;p&gt;We deployed our model and ran several A/B tests with it. Our model performed pretty well overall, but we noticed a critical problem: sometimes, numbers got mutated in the translation. These numbers can be part of an address, phone number, price etc. Showing the wrong number in a translation can cause great confusion to the users. Unfortunately, an ML model’s output can never be fully controlled; therefore, we added an additional layer of programmatic check to mitigate this issue.&lt;/p&gt;

&lt;h3 id=&quot;post-translation-quality-check&quot;&gt;Post-translation quality check&lt;/h3&gt;
&lt;p&gt;Our goal is to ensure non-translatable content such as numbers, special symbols, and emojis  in the original message doesn’t get mutated in the translation produced by our in-house model. We extract all the non-translatable content from the original message, count the occurrences of each, and then try to match the same in the translation. If it fails to match, we discard the in-house translation and fall back to using the third-party translation service.&lt;/p&gt;

&lt;h2 id=&quot;keep-cost-low&quot;&gt;Keep cost low&lt;/h2&gt;

&lt;p&gt;At Grab, we try to be as cost efficient as possible in all aspects. In the case of translation, we tried to minimise cost by avoiding unnecessary on-the-fly translations.&lt;/p&gt;

&lt;p&gt;As you would have guessed, the first thing we did was to implement caching. A cache layer is placed before both the in-house translation model and the third-party translation. We try to serve translation from the cache first before hitting the underlying translation service. However, given that translation requests are in free text and can be quite dynamic, the impact of caching is limited. There’s more we need to do.&lt;/p&gt;

&lt;p&gt;For context, in a booking chat, other than the users, Grab’s internal services can also send messages to the chat room. These messages are called system messages. For example,our food service always sends a message with information on the food order when an order is confirmed.&lt;/p&gt;

&lt;p&gt;System messages are all fairly static in nature, however, we saw a very high amount of translation cost attributed to system messages. Taking a deeper look, we noticed the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Many system messages were not sent in the recipient’s language, thus requiring on-the-fly translation.&lt;/li&gt;
  &lt;li&gt;Many system messages, though having the same static structure, contain quite a few variants such as passenger’s name and  food order item name. This makes it challenging to utilise our translation cache effectively as each message is different.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since all system messages are manually prepared, we should be able to get them all manually translated into all the required languages, and avoid on-the-fly translations altogether.&lt;/p&gt;

&lt;p&gt;Therefore, we launched an internal campaign, mandating all internal services that send system messages to chat rooms to get manual translations prepared, and pass in the translated contents. This alone helped us save roughly US$255K a year!&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;At Grab, we firmly believe that our proprietary in-house translation models are not only more cost-effective but cater more accurately to our unique use cases compared to third-party services. We will focus on expanding these models to more languages and countries across our operating regions.&lt;/p&gt;

&lt;p&gt;Additionally, we are exploring opportunities to apply learnings of our chat translations to other Grab content. This strategy aims to guarantee a seamless language experience for our rapidly expanding user base, especially travellers. We are enthusiastically looking forward to the opportunities this journey brings!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Aug 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/improved-translation-experience-with-cost-efficiency</link>
        <guid isPermaLink="true">https://engineering.grab.com/improved-translation-experience-with-cost-efficiency</guid>
        
        <category>Chat</category>
        
        <category>Chat support</category>
        
        <category>Engineering</category>
        
        <category>GrabChat</category>
        
        <category>Messaging</category>
        
        <category>Translation</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>LLM-powered data classification for data entities at scale</title>
        <description>&lt;p&gt;&lt;small class=&quot;credits&quot;&gt; Editor’s note: This post was originally published in October 2023 and has been updated to reflect Grab’s partnership with the Infocomm Media Development Authority as part of its Privacy Enhancing Technology Sandbox that concluded in March 2024.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we deal with PetaByte-level data and manage countless data entities ranging from database tables to Kafka message schemas. Understanding the data inside is crucial for us, as it not only streamlines the data access management to safeguard the data of our users, drivers and merchant-partners, but also improves the data discovery process for data analysts and scientists to easily find what they need.&lt;/p&gt;

&lt;p&gt;The Caspian team (Data Engineering team) collaborated closely with the Data Governance team on automating governance-related metadata generation. We started with Personal Identifiable Information (PII) detection and built an orchestration service using a third-party classification service. With the advent of the Large Language Model (LLM), new possibilities dawned for metadata generation and sensitive data identification at Grab. This prompted the inception of the project, which aimed to integrate LLM classification into our existing service. In this blog, we share insights into the transformation from what used to be a tedious and painstaking process to a highly efficient system, and how it has empowered the teams across the organisation.&lt;/p&gt;

&lt;p&gt;For ease of reference, here’s a list of terms we’ve used and their definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Entity&lt;/strong&gt;: An entity representing a schema that contains rows/streams of data, for example, database tables, stream messages, data lake tables.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Refers to the model’s output given a data entity, unverified manually.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Classification&lt;/strong&gt;: The process of classifying a given data entity, which in the context of this blog, involves generating tags that represent sensitive data or Grab-specific types of data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metadata Generation&lt;/strong&gt;: The process of generating the metadata for a given data entity. In this blog, since we limit the metadata to the form of tags, we often use this term and data classification interchangeably.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;: Refers to the level of confidentiality of data. High sensitivity means that the data is highly confidential. The lowest level of sensitivity often refers to public-facing or publicly-available data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;When we first approached the data classification problem, we aimed to solve something more specific - Personal Identifiable Information (PII) detection. Initially, to protect sensitive data from accidental leaks or misuse, Grab implemented manual processes and campaigns targeting data producers to tag schemas with sensitivity tiers. These tiers ranged from Tier 1, representing schemas with highly sensitive information, to Tier 4, indicating no sensitive information at all. As a result, half of all schemas were marked as Tier 1, enforcing the strictest access control measures.&lt;/p&gt;

&lt;p&gt;The presence of a single Tier 1 table in a schema with hundreds of tables justifies classifying the entire schema as Tier 1. However, since Tier 1 data is rare, this implies that a large volume of non-Tier 1 tables, which ideally should be more accessible, have strict access controls.&lt;/p&gt;

&lt;p&gt;Shifting access controls from the schema-level to the table-level could not be done safely due to the lack of table classification in the data lake. We could have conducted more manual classification campaigns for tables, however this was not feasible for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The volume, velocity, and variety of data had skyrocketed within the organisation, so it took significantly more time to classify at table level compared to schema level. Hence, a programmatic solution was needed to streamline the classification process, reducing the need for manual effort.&lt;/li&gt;
  &lt;li&gt;App developers, despite being familiar with the business scope of their data, interpreted internal data classification policies and external data regulations differently, leading to inconsistencies in understanding.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A service called Gemini &lt;em&gt;(named before Google announced the Gemini model!)&lt;/em&gt; was built internally to automate the tag generation process using a third party data classification service. Its purpose was to scan the data entities in batches and generate column/field level tags. These tags would then go through a review process by the data producers. The data governance team provided classification rules and used regex classifiers, alongside the third-party tool’s own machine learning classifiers, to discover sensitive information.&lt;/p&gt;

&lt;p&gt;After the implementation of the initial version of Gemini, a few challenges remained.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The third-party tool did not allow customisations of its machine learning classifiers, and the regex patterns produced too many false positives during our evaluation.&lt;/li&gt;
  &lt;li&gt;Building in-house classifiers would require a dedicated data science team to train a customised model. They would need to invest a significant amount of time to understand data governance rules thoroughly and prepare datasets with manually labelled training data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;LLM came up on our radar following its recent &lt;em&gt;“iPhone moment”&lt;/em&gt; with ChatGPT’s explosion onto the scene. It is trained using an extremely large corpus of text and contains trillions of parameters. It is capable of conducting natural language understanding tasks, writing code, and even analysing data based on requirements. The LLM naturally solves the mentioned pain points as it provides a natural language interface for data governance personnel. They can express governance requirements through text prompts, and the LLM can be customised effortlessly without code or model training.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;In this section, we dive into the implementation details of the data classification workflow. Please refer to the diagram below for a high-level overview:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/data_classification_workflow.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Overview of data classification workflow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This diagram illustrates how data platforms, the metadata generation service (Gemini), and data owners work together to classify and verify metadata. Data platforms trigger scan requests to the Gemini service to initiate the tag classification process. After the tags are predicted, data platforms consume the predictions, and the data owners are notified to verify these tags.&lt;/p&gt;

&lt;h3 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/arch_diagram_orchestration.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Architecture diagram of the orchestration service Gemini&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our orchestration service, Gemini, manages the data classification requests from data platforms. From the diagram, the architecture contains the following components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data platforms: These platforms are responsible for managing data entities and initiating data classification requests.&lt;/li&gt;
  &lt;li&gt;Gemini: This orchestration service communicates with data platforms, schedules and groups data classification requests.&lt;/li&gt;
  &lt;li&gt;Classification engines: There are two available engines (a third-party classification service and GPT3.5) for executing the classification jobs and return results. Since we are still in the process of evaluating two engines, both of the engines are working concurrently.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the orchestration service receives requests, it helps aggregate the requests into reasonable mini-batches. Aggregation is achievable through the message queue at fixed intervals. In addition, a rate limiter is attached at the workflow level. It allows the service to call the Cloud Provider APIs with respective rates to prevent the potential throttling from the service providers.&lt;/p&gt;

&lt;p&gt;Specific to LLM orchestration, there are two limits to be mindful of. The first one is the context length. The input length cannot surpass the context length, which was 4000 tokens for GPT3.5 at the time of development (or around 3000 words). The second one is the overall token limit (since both the input and output share the same token limit for a single request). Currently, all Azure OpenAI model deployments share the same quota under one account, which is set at 240K tokens per minute.&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section, we focus on LLM-powered column-level tag classification. The tag classification process is defined as follows:&lt;/p&gt;

&lt;p&gt;Given a data entity with a defined schema, we want to tag each field of the schema with metadata classifications that follow an internal classification scheme from the data governance team. For example, the field can be tagged as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular kind of business metric&amp;gt;&lt;/code&gt; or a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular type of personally identifiable information (PII)&lt;/code&gt;. These tags indicate that the field contains a business metric or PII.&lt;/p&gt;

&lt;p&gt;We ask the language model to be a column tag generator and to assign the most appropriate tag to each column. Here we showcase an excerpt of the prompt we use:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;You are a database column tag classifier, your job is to assign the most appropriate tag based on table name and column name. The database columns are from a company that provides ride-hailing, delivery, and financial services. Assign one tag per column. However not all columns can be tagged and these columns should be assigned &amp;lt;None&amp;gt;. You are precise, careful and do your best to make sure the tag assigned is the most appropriate.

The following is the list of tags to be assigned to a column. For each line, left hand side of the : is the tag and right hand side is the tag definition

…
&amp;lt;Personal.ID&amp;gt; : refers to government-provided identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar. This tag should absolutely not be assigned to columns named &quot;id&quot;, &quot;merchant id&quot;, &quot;passenger id&quot;, “driver id&quot; or similar since these are not government-provided identification numbers. This tag should be very rarely assigned.

&amp;lt;None&amp;gt; : should be used when none of the above can be assigned to a column.
…

Output Format is a valid json string, for example:

[{
        &quot;column_name&quot;: &quot;&quot;,
        &quot;assigned_tag&quot;: &quot;&quot;
}]

Example question

`These columns belong to the &quot;deliveries&quot; table

        1. merchant_id
        2. status
        3. delivery_time`

Example response

[{
        &quot;column_name&quot;: &quot;merchant_id&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;Personal.ID&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;status&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;delivery_time&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also curated a tag library for LLM to classify. Here is an example:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column-level Tag&lt;/th&gt;
      &lt;th&gt;Definition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.ID&lt;/td&gt;
      &lt;td&gt;Refers to external identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Name &lt;/td&gt;
      &lt;td&gt;Refers to the name or username of a person and should be assigned to columns containing &quot;name&quot;, &quot;username&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Contact_Info&lt;/td&gt;
      &lt;td&gt;Refers to the contact information of a person and should be assigned to columns containing &quot;email&quot;, &quot;phone&quot;, &quot;address&quot;, &quot;social media&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Geo.Geohash&lt;/td&gt;
      &lt;td&gt;Refers to a geohash and should be assigned to columns containing &quot;geohash&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;Should be used when none of the above can be assigned to a column.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The output of the language model is typically in free text format, however, we want the output in a fixed format for downstream processing. Due to this nature, prompt engineering is a crucial component to make sure downstream workflows can process the LLM’s output.&lt;/p&gt;

&lt;p&gt;Here are some of the techniques we found useful during our development:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Articulate the requirements: The requirement of the task should be as clear as possible, LLM is only instructed to do what you ask it to do.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/en-gb/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#few-shot-learning&quot;&gt;Few-shot learning&lt;/a&gt;: By showing the example of interaction, models understand how they should respond.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/TypeChat&quot;&gt;Schema Enforcement&lt;/a&gt;: Leveraging its ability of understanding code, we explicitly provide the DTO (Data Transfer Object) schema to the model so that it understands that its output must conform to it.&lt;/li&gt;
  &lt;li&gt;Allow for confusion: In our prompt we specifically added a default tag – the LLM is instructed to output the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;None&amp;gt;&lt;/code&gt; tag when it cannot make a decision or is confused.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Regarding classification accuracy, we found that it is surprisingly accurate with its great semantic understanding. For acknowledged tables, users on average change less than one tag. Also, during an internal survey done among data owners at Grab in September 2023, 80% reported that this new tagging process helped them in tagging their data entities.&lt;/p&gt;

&lt;h3 id=&quot;publish-and-verification&quot;&gt;Publish and verification&lt;/h3&gt;

&lt;p&gt;The predictions are published to the Kafka queue to downstream data platforms. The platforms inform respective users weekly to verify the classified tags to improve the model’s correctness and to enable iterative prompt improvement. Meanwhile, we plan to remove the verification mandate for users once the accuracy reaches a certain level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/verification_message.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Verification message shown in the data platform for user to verify the tags&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since the new system was rolled out, we have successfully integrated this with Grab’s metadata management platform and production database management platform. Within a month since its rollout, we have scanned more than 20,000 data entities, averaging around 300-400 entities per day.&lt;/p&gt;

&lt;p&gt;Using a quick back-of-the-envelope calculation, we can see the significant time savings achieved through automated tagging. Assuming it takes a data owner approximately 2 minutes to classify each entity, we are saving approximately 360 man-days per year for the company. This allows our engineers and analysts to focus more on their core tasks of engineering and analysis rather than spending excessive time on data governance.&lt;/p&gt;

&lt;p&gt;The classified tags pave the way for more use cases downstream. These tags, in combination with rules provided by data privacy office in Grab, enable us to determine the sensitivity tier of data entities, which in turn will be leveraged for enforcing the Attribute-based Access Control (ABAC) policies and enforcing Dynamic Data Masking for downstream queries. To learn more about the benefits of ABAC, readers can refer to another engineering &lt;a href=&quot;https://engineering.grab.com/migrating-to-abac&quot;&gt;blog&lt;/a&gt; posted earlier.&lt;/p&gt;

&lt;p&gt;Cost wise, for the current load, it is extremely affordable contrary to common intuition. This affordability enables us to scale the solution to cover more data entities in the company.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;h3 id=&quot;prompt-improvement&quot;&gt;Prompt improvement&lt;/h3&gt;

&lt;p&gt;We are currently exploring feeding sample data and user feedback to greatly increase accuracy. Meanwhile, we are experimenting on outputting the confidence level from LLM for its own classification. With confidence level output, we would only trouble users when the LLM is uncertain of its answers. Hopefully this can remove even more manual processes in the current workflow.&lt;/p&gt;

&lt;h3 id=&quot;prompt-evaluation&quot;&gt;Prompt evaluation&lt;/h3&gt;

&lt;p&gt;To track the performance of the prompt given, we are building analytical pipelines to calculate the metrics of each version of the prompt. This will help the team better quantify the effectiveness of prompts and iterate better and faster.&lt;/p&gt;

&lt;h3 id=&quot;scaling-out&quot;&gt;Scaling out&lt;/h3&gt;

&lt;p&gt;We are also planning to scale out this solution to more data platforms to streamline governance-related metadata generation to more teams. The development of downstream applications using our metadata is also on the way. These exciting applications are from various domains such as security, data discovery, etc.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Grab recently participated in the Singapore government’s regulatory &lt;a href=&quot;https://www.imda.gov.sg/how-we-can-help/data-innovation/privacy-enhancing-technology-sandboxes&quot;&gt;sandbox&lt;/a&gt;, where we successfully demonstrated how LLMs can efficiently and effectively perform data classification, allowing Grab to compound the value of its data for innovative use cases while safeguarding sensitive information such as PII.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Jul 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-powered-data-classification</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-powered-data-classification</guid>
        
        <category>Data</category>
        
        <category>Machine Learning</category>
        
        <category>Generative AI</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Profile-guided optimisation (PGO) on Grab services</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://go.dev/doc/pgo&quot;&gt;Profile-guided optimisation (PGO)&lt;/a&gt; is a technique where CPU profile data for an application is collected and fed back into the next compiler build of Go application. The compiler then uses this CPU profile data to optimise the performance of that build by around &lt;a href=&quot;https://tip.golang.org/doc/pgo#overview&quot;&gt;2-14%&lt;/a&gt; currently (future releases could likely improve this figure further).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/high-level-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;High level view of how PGO works&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;PGO is a &lt;a href=&quot;https://docs.oracle.com/en/graalvm/jdk/22/docs/reference-manual/native-image/optimizations-and-performance/PGO/&quot;&gt;widely used technique&lt;/a&gt; that can be implemented with many programming languages. When it was released in May 2023, PGO was introduced as a preview in Go 1.20.&lt;/p&gt;

&lt;h2 id=&quot;enabling-pgoon-a-service&quot;&gt;Enabling PGO on a service&lt;/h2&gt;

&lt;h3 id=&quot;profile-the-service-to-get-pprof-file&quot;&gt;Profile the service to get pprof file&lt;/h3&gt;

&lt;p&gt;First, make sure that your service is built using Golang version v1.20 or higher, as only these versions support PGO.&lt;/p&gt;

&lt;p&gt;Next, &lt;a href=&quot;https://pkg.go.dev/net/http/pprof&quot;&gt;enable pprof&lt;/a&gt; in your service.&lt;/p&gt;

&lt;p&gt;If it’s already enabled, you can use the following command to capture a 6-minute profile and save it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp/pprof&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl &apos;http://localhost:6060/debug/pprof/profile?seconds=360&apos; -o /tmp/pprof
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;enabled-pgo-on-the-service&quot;&gt;Enabled PGO on the service&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/big-data-real-time-presto-talariadb&quot;&gt;TalariaDB&lt;/a&gt;: TalariaDB is a distributed, highly available, and low latency time-series database for Presto open sourced by Grab.&lt;/p&gt;

&lt;p&gt;It is a service that runs on an EKS cluster and is entirely managed by our team, we will use it as an example here.&lt;/p&gt;

&lt;p&gt;Since the cluster deployment relies on a Docker image, we only need to update the Docker image’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go build&lt;/code&gt; command to include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-PGO=./talaria.PGO&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;talaria.PGO&lt;/code&gt; file is a pprof profile collected from production services over a span of 360 seconds.&lt;/p&gt;

&lt;p&gt;If you’re utilising a &lt;a href=&quot;https://pkg.go.dev/plugin&quot;&gt;go plugin&lt;/a&gt;, &lt;a href=&quot;/faster-using-the-go-plugin-to-replace-Lua-VM&quot;&gt;as we do in TalariaDB&lt;/a&gt;, it’s crucial to ensure that the PGO is also applied to the plugin.&lt;/p&gt;

&lt;p&gt;Here’s our Dockerfile, with the additions to support PGO.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM arm64v8/golang:1.21 AS builder

ARG GO111MODULE=&quot;on&quot;
ARG GOOS=&quot;linux&quot;
ARG GOARCH=&quot;arm64&quot;
ENV GO111MODULE=${GO111MODULE}
ENV GOOS=${GOOS}
ENV GOARCH=${GOARCH}

RUN mkdir -p /go/src/talaria
COPY . src/talaria
#RUN cd src/talaria &amp;amp;&amp;amp; go mod download  &amp;amp;&amp;amp; go build &amp;amp;&amp;amp; test -x talaria
RUN cd src/talaria &amp;amp;&amp;amp; go mod download  &amp;amp;&amp;amp; go build -PGO=./talaria.PGO &amp;amp;&amp;amp; test -x talaria

RUN mkdir -p /go/src/talaria-plugin
COPY ./talaria-plugin  src/talaria-plugin
RUN cd src/talaria-plugin &amp;amp;&amp;amp; make plugin &amp;amp;&amp;amp; test -f talaria-plugin.so
FROM arm64v8/debian:latest AS base

RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ca-certificates &amp;amp;&amp;amp; rm -rf /var/cache/apk/*

WORKDIR /root/ 
ARG GO_BINARY=talaria
COPY  --from=builder /go/src/talaria/${GO_BINARY} .
COPY  --from=builder /go/src/talaria-plugin/talaria-plugin.so .

ADD entrypoint.sh . 
RUN mkdir /etc/talaria/ &amp;amp;&amp;amp; chmod +x /root/${GO_BINARY} /root/entrypoint.sh
ENV TALARIA_RC=/etc/talaria/talaria.rc 
EXPOSE 8027
ENTRYPOINT [&quot;/root/entrypoint.sh&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;result-on-enabling-pgo-on-one-grabx-service&quot;&gt;Result on enabling PGO on one GrabX service&lt;/h3&gt;

&lt;p&gt;It’s important to mention that the pprof utilised for PGO was not captured during peak hours and was limited to a duration of 360 seconds.&lt;/p&gt;

&lt;p&gt;Service &lt;a href=&quot;/big-data-real-time-presto-talariadb&quot;&gt;TalariaDB&lt;/a&gt; has three clusters and the time we enabled PGO for these clusters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We enabled PGO on cluster 0, and deployed on 4 Sep 11.16 AM.&lt;/li&gt;
  &lt;li&gt;We enabled PGO on cluster 1, and deployed on 5 Sep 15:00 PM.&lt;/li&gt;
  &lt;li&gt;We enabled PGO on cluster 2, and deployed on 6 Sep 16:00 PM.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The size of the instances, their quantity, and all other dependencies remained unchanged.&lt;/p&gt;

&lt;h4 id=&quot;cpu-metrics-on-cluster&quot;&gt;CPU metrics on cluster&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/cpu-before-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Cluster CPU usage before enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/cpu-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Cluster CPU usage after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It’s evident that enabling PGO resulted in at least a 10% reduction in CPU usage.&lt;/p&gt;

&lt;h4 id=&quot;memory-metrics-on-cluster&quot;&gt;Memory metrics on cluster&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/mem-before-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Memory usage of the cluster before enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/percentage-free-mem-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Percentage of free memory after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It’s clear that enabling PGO led to a reduction of at least 10GB (30%) in memory usage.&lt;/p&gt;

&lt;h4 id=&quot;volume-metrics-on-cluster&quot;&gt;Volume metrics on cluster&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/persistent-volume-usage-before-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Persistent volume usage on cluster before enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/volume-usage-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Volume usage after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Enabling PGO resulted in a reduction of at least 7GB (38%) in volume usage. This volume is utilised for storing events that are queued for ingestion.&lt;/p&gt;

&lt;h4 id=&quot;ingested-event-countcpumetrics-on-cluster&quot;&gt;Ingested event count/CPU metrics on cluster&lt;/h4&gt;

&lt;p&gt;To gauge the enhancements, I employed the metric of ingested event count per CPU unit (event count / CPU). This approach was adopted to account for the variable influx of events, which complicates direct observation of performance gains.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/count-ingested-event-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Count of ingested events on cluster after enabling PGO&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Upon activating PGO, there was a noticeable increase in the ingested event count per CPU, rising from 1.1 million to 1.7 million, as depicted by the blue line in the cluster screenshot.&lt;/p&gt;

&lt;h2 id=&quot;how-we-enabled-pgo-on-a-catwalk-service&quot;&gt;How we enabled PGO on a Catwalk service&lt;/h2&gt;

&lt;p&gt;We also experimented with enabling PGO on certain orchestrators in a Catwalk service. This section covers our findings.&lt;/p&gt;

&lt;h3 id=&quot;enabling-pgo-on-the-test-golang-orch-tfsorchestrator&quot;&gt;Enabling PGO on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test-golang-orch-tfs&lt;/code&gt; orchestrator&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Attempt 1&lt;/em&gt;&lt;/strong&gt;: Take pprof for 59 seconds&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Just 1 pod running with a constant throughput of 420 QPS.&lt;/li&gt;
  &lt;li&gt;Load test started with a non-PGO image at 5:39 PM SGT.&lt;/li&gt;
  &lt;li&gt;Take pprof for 59 seconds.&lt;/li&gt;
  &lt;li&gt;Image with PGO enabled deployed at 5:49 PM SGT.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observation: &lt;strong&gt;CPU usage increased&lt;/strong&gt; after enabling PGO with pprof for 59 seconds.&lt;/p&gt;

&lt;p&gt;We suspected that taking pprof for just 59 seconds may not be sufficient to collect accurate metrics. Hence, we extended the duration to 6 minutes in our second attempt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Attempt 2&lt;/em&gt;&lt;/strong&gt; : Take pprof for 6 minutes&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Just 1 pod running with a constant throughput of 420 QPS.&lt;/li&gt;
  &lt;li&gt;Deployed non PGO image with custom pprof server at 6:13 PM SGT.&lt;/li&gt;
  &lt;li&gt;pprof taken at 6:19 PM SGT for 6 minutes.&lt;/li&gt;
  &lt;li&gt;Image with PGO enabled deployed at 6:29 PM SGT.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observation: &lt;strong&gt;CPU usage decreased&lt;/strong&gt; after enabling PGO with pprof for 6 minutes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/cpu-usage-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;CPU usage after enabling PGO on Catwalk&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/profile-guided-optimisation/container-mem-after-pgo.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Container memory utilisation after enabling PGO on Catwalk&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Based on this experiment, we found that the impact of PGO is around 5% but the effort involved to enable PGO outweighs the impact. To enable PGO on Catwalk, we would need to create Docker images for each application through CI pipelines.&lt;/p&gt;

&lt;p&gt;Additionally, the Catwalk team would require a workaround to pass the pprof dump, which is not a straightforward task. Hence, we decided to put off the PGO application for Catwalk services.&lt;/p&gt;

&lt;h2 id=&quot;looking-into-pgo-for-monorepo-services&quot;&gt;Looking into PGO for monorepo services&lt;/h2&gt;

&lt;p&gt;From the information provided above, enabling PGO for a service requires the following support mechanisms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A pprof service, which is currently facilitated through Jenkins.&lt;/li&gt;
  &lt;li&gt;A build process that supports PGO arguments and can attach or retrieve the pprof file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For services that are hosted outside the monorepo and are self-managed, the effort required to experiment is minimal. However, for those within the monorepo, we will require support from the build process, which is currently unable to support this.&lt;/p&gt;

&lt;h2 id=&quot;conclusionlearnings&quot;&gt;Conclusion/Learnings&lt;/h2&gt;

&lt;p&gt;Enabling PGO has proven to be highly beneficial for some of our services, particularly TalariaDB. By using PGO, we’ve observed a clear reduction in both CPU usage and memory usage to the tune of approximately 10% and 30% respectively. Furthermore, the volume used for storing queued ingestion events has been reduced by a significant 38%. These improvements definitely underline the benefits and potential of utilising PGO on services.&lt;/p&gt;

&lt;p&gt;Interestingly, applying PGO resulted in an increased rate of ingested event count per CPU unit on TalariaDB, which demonstrates an improvement in the service’s efficiency.&lt;/p&gt;

&lt;p&gt;Experiments with the Catwalk service have however shown that the effort involved to enable PGO might not always justify the improvements gained. In our case, a mere 5% improvement did not appear to be worth the work required to generate Docker images for each application via CI pipelines and create a solution to pass the pprof dump.&lt;/p&gt;

&lt;p&gt;On the whole, it is evident that the applicability and benefits of enabling PGO can &lt;strong&gt;&lt;em&gt;vary across different services&lt;/em&gt;&lt;/strong&gt;. Factors such as application characteristics, current architecture, and available support mechanisms can influence when and where PGO optimisation is feasible and beneficial.&lt;/p&gt;

&lt;p&gt;Moving forward, further improvements to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go-build&lt;/code&gt; and the introduction of PGO support for monorepo services may drive greater adoption of PGO. In turn, this has the potential to deliver powerful system-wide gains that translate to faster response times, lower resource consumption, and improved user experiences. As always, the relevance and impact of adopting new technologies or techniques should be considered on a case-by-case basis against operational realities and strategic objectives.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 05 Jun 2024 00:10:10 +0000</pubDate>
        <link>https://engineering.grab.com/profile-guided-optimisation</link>
        <guid isPermaLink="true">https://engineering.grab.com/profile-guided-optimisation</guid>
        
        <category>Go</category>
        
        <category>optimisation</category>
        
        <category>experiments</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we evaluated the business impact of marketing campaigns</title>
        <description>&lt;p&gt;In a previous &lt;a href=&quot;/supporting-large-campaigns-at-scale&quot;&gt;post&lt;/a&gt;, we introduced our systems for running marketing campaigns. Although we sent millions of messages daily, we had little insight into their effectiveness. Did they engage our users with our promotions? Did they encourage more transactions and bookings?&lt;/p&gt;

&lt;p&gt;As Grab’s business expanded and the number of marketing campaigns increased, understanding the impact of these campaigns became crucial. This knowledge enables campaign managers to design more effective campaigns and avoid wasteful ones that degrade user experience.&lt;/p&gt;

&lt;p&gt;Initially, campaign managers had to consult marketing analysts to gauge the impact of campaigns. However, this approach soon proved unsustainable:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Manual analysis doesn’t scale with an increasing number of campaigns.&lt;/li&gt;
  &lt;li&gt;Different analysts might assess the business impact in slightly different ways, leading to inconsistent results over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, we recognised the need for a centralised solution allowing campaign managers to view their campaign impact analyses.&lt;/p&gt;

&lt;h2 id=&quot;marketing-attribution-model&quot;&gt;Marketing attribution model&lt;/h2&gt;

&lt;p&gt;The marketing analyst team designed a Marketing attribution model (MAM) for estimating the business impact of any campaign that sends messages to users. It quantifies business impact in terms of generated gross merchandise value (GMV), revenue, etc.&lt;/p&gt;

&lt;p&gt;Unlike traditional models that only credit the last touchpoint (i.e. the last message user reads before making a transaction), MAM offers a more nuanced view. It recognises that users are exposed to various marketing messages (emails, pushes, feeds, etc.) throughout their decision-making process. As shown in Fig 1, MAM assigns credit to each touchpoint that influences a conversion (e.g., Grab usage) based on two key factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;: Content directly related to the conversion receives a higher weightage. Imagine a user opening a GrabFood push notification before placing a food order. This push would be considered highly relevant and receive significant credit.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recency&lt;/strong&gt;: Touchpoints closer in time to the conversion hold more weight. For instance, a brand awareness email sent weeks before the purchase would be less impactful than a targeted GrabFood promotion right before the order.
By factoring in both relevance and recency, MAM avoids crediting the same touchpoint twice and provides a more accurate picture of which marketing campaigns are driving higher conversions.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/mam-business-attribution.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. How MAM does business attribution&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;While MAM is effective for comparing the impacts of different campaigns, it struggles with the assessment of a single campaign because it does not account for negative impacts. For example, consider a message stating, “Hey, don’t use Grab.” Clearly, not all messages positively impact business.&lt;/p&gt;

&lt;h2 id=&quot;hold-out-group&quot;&gt;Hold-out group&lt;/h2&gt;

&lt;p&gt;To better evaluate the impact of a single campaign, we divide targeted users into two groups:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hold-out (control): do not send any message&lt;/li&gt;
  &lt;li&gt;Treatment: send the message&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/campaign-hold-out-group.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. Campaign setup with hold-out group&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We then compare the business performance of sending versus not sending messages. For the treatment group, we ideally count only the user transactions potentially linked to the message (i.e., transactions occurring within X days of message receipt). However, since the hold-out group receives no messages, there are no equivalent metrics for comparison.&lt;/p&gt;

&lt;p&gt;The only business metrics available for the hold-out group are the aggregated totals of GMV, revenue, etc., over a given time, divided by the number of users. We must calculate the same for the treatment group to ensure a fair comparison.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/metrics-calculation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. Metrics calculation for both hold-out and treatment group&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The comparison might seem unreliable due to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The metrics are raw aggregations, lacking attribution logic.&lt;/li&gt;
  &lt;li&gt;The aggregated GMV and revenue might be skewed by other simultaneous campaigns involving the same users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, we have to admit that figuring out true business impact is difficult. All we can do is try our best to get as close to the truth as possible. To make the comparison more precise, we employed the following strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stratify the two groups, so that both groups contain roughly the same distribution of users.&lt;/li&gt;
  &lt;li&gt;Calculate statistical significance to rule out the difference caused by random factors.&lt;/li&gt;
  &lt;li&gt;Allow users to narrow down the business metrics to compare according to campaign set-up. For example, we don’t compare ride bookings if the campaign is promoting food.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Statistical significance is a common, yet important technique for evaluating the result of controlled experiments. Let’s see how it’s used in our case.&lt;/p&gt;

&lt;h3 id=&quot;statistical-significance&quot;&gt;Statistical significance&lt;/h3&gt;

&lt;p&gt;When we do an A/B testing, we cannot simply conclude that A is better than B when A’s result is better than B. The difference could be due to other random factors. If you did an A/A test, you will still see differences in the results even without doing anything different to the two groups.&lt;/p&gt;

&lt;p&gt;Statistical significance is a method to calculate the probability that the difference between two groups is really due to randomness. The lower the probability, the more confidently we can say our action is truly making some impact.&lt;/p&gt;

&lt;p&gt;In our case, to derive statistical significance, we assume:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Our hold-out and treatment group are two sets of samples drawn from two populations, A and B.&lt;/li&gt;
  &lt;li&gt;A and B are the same except that B received our message. We can’t 100% prove this, but can reasonably guess this is close to true, since we split with stratification.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Assuming the business metrics we are comparing is food GMV, the base numbers can be formulated as shown in Fig 4.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/calculate-statistical-significance.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. Formulation for calculating statistical significance&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To calculate the probability, we then use a formula derived from the central limit theorem (CLT). The mathematical derivation of the formula is beyond the scope of this post. Programmatically, we use the popular jStat library for the calculation.&lt;/p&gt;

&lt;p&gt;The calculation result of statistical significance as a special notice to the campaign owners is shown in Fig 5.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/evaluate-business-impact-of-marketing-campaigns/business-impact-analysis.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 5. Display of business impact analysis with statistical significance&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;Evaluating the true business impact remains challenging. We continue to refine our methodology and address potential biases, such as the assumption that both groups are of the same distribution, which might not hold true, especially in smaller group sizes. Furthermore, consistently reserving a 10% hold-out in each campaign is impractical for some campaigns, as sometimes campaign owners require messages to reach all targeted users.&lt;/p&gt;

&lt;p&gt;We are committed to advancing our business impact evaluation solutions and will continue improving our existing solutions. We look forward to sharing more insights in future blogs.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 23 May 2024 00:10:10 +0000</pubDate>
        <link>https://engineering.grab.com/evaluate-business-impact-of-marketing-campaigns</link>
        <guid isPermaLink="true">https://engineering.grab.com/evaluate-business-impact-of-marketing-campaigns</guid>
        
        <category>Marketing</category>
        
        <category>Metrics</category>
        
        <category>Optimisation</category>
        
        <category>Statistic</category>
        
        <category>A/B Testing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>No version left behind: Our epic journey of GitLab upgrades</title>
        <description>&lt;p&gt;In a tech-driven field, staying updated isn’t an option—it’s essential. At Grab, we’re committed to providing top-notch technology services. However, keeping pace can be demanding. At one point in time, our GitLab instance was trailing by roughly 14 months of releases. This blog post recounts our experience updating and formulating a consistent upgrade routine.&lt;/p&gt;

&lt;h2 id=&quot;recognising-the-need-to-upgrade&quot;&gt;Recognising the need to upgrade&lt;/h2&gt;

&lt;p&gt;Our team, while skilled, was still learning GitLab’s complexities. Regular stability issues left us little time for necessary upgrades. Understanding the importance of upgrades for our operations to get latest patches for important security fixes and vulnerabilities, we started preparing for GitLab updates while managing system stability. This meant a quick learning and careful approach to updates.&lt;/p&gt;

&lt;p&gt;The following image illustrates the version discrepancy between our self-hosted GitLab instance and the official most recent release of GitLab as of July 2022. GitLab follows a set &lt;a href=&quot;https://about.gitlab.com/releases&quot;&gt;release schedule&lt;/a&gt;, issuing one minor update monthly and rolling out a major upgrade annually.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/no-version-left-behind-our-epic-journey-of-gitlab-upgrades/version-diff.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. The difference between our hosted version and the latest available GitLab version by 22 July 2022&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;addressingfears-and-concerns&quot;&gt;Addressing fears and concerns&lt;/h2&gt;

&lt;p&gt;We were concerned about potential downtime, data integrity, and the threat of encountering unforeseen issues. GitLab is critical for the daily activities of Grab engineers. It serves a critical user base of thousands of engineers actively using it, hosting multiple mono repositories with code bases ranging in size from 1GB to a sizable &lt;strong&gt;15GB&lt;/strong&gt;. When taking into account all its artefacts, the overall imprint of a monorepo can extend to an impressive &lt;strong&gt;39TB&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Our self-hosted GitLab firmly intertwines with multiple critical components. We’ve aligned our systems with GitLab’s official &lt;a href=&quot;https://docs.gitlab.com/ee/administration/reference_architectures/5k_users.html&quot;&gt;reference architecture for 5,000 users&lt;/a&gt;. We use Terraform to configure complete infrastructure with immutable Amazon Machine Images (&lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html&quot;&gt;AMIs&lt;/a&gt;) built using Packer and Ansible. Our efficient GitLab setup is designed for reliable performance to serve our wide user base. However, any fault leading to outages can disrupt our engineers, resulting in a loss of productivity for hundreds of teams.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/no-version-left-behind-our-epic-journey-of-gitlab-upgrades/architecture.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;High-level GitLab Architecture Diagram&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The above is the top level architecture diagram of our GitLab infrastructure. Here are the major components of the GitLab architecture and their functions: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Gitaly&lt;/strong&gt;: Handles low-level Git operations for GitLab, such as interacting directly with the code repository present on disk. It’s important to mention that these code repositories are also stored on the same Gitaly nodes, using the attached Amazon Elastic Block Store (Amazon EBS) disks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Praefect&lt;/strong&gt;: Praefect in GitLab acts as a manager, coordinating Gitaly nodes to maintain data consistency and high availability.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sidekiq&lt;/strong&gt;: The background processing framework for GitLab written in Ruby. It handles asynchronous tasks in GitLab, ensuring smooth operation without blocking the main application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;App Server&lt;/strong&gt;: The core web application server that serves the GitLab user interface and interacts with other components.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-importance-of-preparation&quot;&gt;The importance of preparation&lt;/h2&gt;

&lt;p&gt;Recognising the complexity of our task, we prioritised careful planning for a successful upgrade. We studied GitLab’s documentation, shared insights within the team, and planned to prevent data losses.&lt;/p&gt;

&lt;p&gt;To minimise disruptions from major upgrades or database migrations, we scheduled these during weekends. We also developed a checklist and a systematic approach for each upgrade, which include the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Diligently go through the release notes for each version of GitLab that falls within the scope of our upgrade.&lt;/li&gt;
  &lt;li&gt;Read through all dependencies like RDS, Redis, and Elasticsearch to ensure version compatibility.&lt;/li&gt;
  &lt;li&gt;Create documentation outlining new features, any deprecated elements, and changes that could potentially impact our operations.&lt;/li&gt;
  &lt;li&gt;Generate immutable AMIs for various components reflecting the new version of GitLab.&lt;/li&gt;
  &lt;li&gt;Revisit and validate all the backup plans.&lt;/li&gt;
  &lt;li&gt;Refresh staging environment with production data for accurate, realistic testing and performance checks, and validation of migration scripts under conditions similar to the actual setup.&lt;/li&gt;
  &lt;li&gt;Upgrade the staging environment.&lt;/li&gt;
  &lt;li&gt;Conduct extensive testing, incorporating both automated and manual functional testing, as well as load testing.&lt;/li&gt;
  &lt;li&gt;Conduct rollback tests on the staging environment to the previous version to confirm the rollback procedure’s reliability.&lt;/li&gt;
  &lt;li&gt;Inform all impacted stakeholders, and provide a defined timeline for upcoming upgrades.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We systematically follow GitLab’s official documentation for each &lt;a href=&quot;https://docs.gitlab.com/ee/update/index.html#upgrade-paths&quot;&gt;upgrade&lt;/a&gt;, ensuring compatibility across software versions and reviewing &lt;a href=&quot;https://docs.gitlab.com/ee/update/index.html#version-specific-upgrading-instructions&quot;&gt;specific instructions&lt;/a&gt; and changes, including any &lt;a href=&quot;https://docs.gitlab.com/ee/update/deprecations.html&quot;&gt;deprecations or removals&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-first-upgrade&quot;&gt;The first upgrade&lt;/h2&gt;

&lt;p&gt;Equipped with knowledge, backup plans, and a robust support system, we embarked on our first GitLab upgrade two years ago. We carefully followed our checklist, handling each important part systematically. GitLab comprises both stateful (Gitaly) and stateless (Praefect, Sidekiq, and App Server) components, all managed through auto-scaling groups. We use a &lt;strong&gt;‘create before destroy’&lt;/strong&gt; strategy for deploying stateless components and an &lt;strong&gt;‘in-place node rotation’&lt;/strong&gt; method via Terraform for stateful ones.&lt;/p&gt;

&lt;p&gt;We deployed key parts like Gitaly, Praefect, Sidekiq, App Servers, Network File System (NFS) server, and Elasticsearch in a specific sequence. Starting with Gitaly, followed by Praefect, then Sidekiq and App Servers, and finally NFS and Elasticsearch. Our thorough testing showed this order to be the most dependable and safe.&lt;/p&gt;

&lt;p&gt;However, the journey was full of challenges. For instance, we encountered issues such as the Gitaly cluster falling out of sync for monorepo and the Praefect server failing to distribute the load effectively. Praefect assigns a primary Gitaly node for each repository to host it. All write operations are sent to the repository’s primary node, while read requests are spread across all synced nodes in the Gitaly cluster. If the Gitaly nodes aren’t synced, Praefect will redirect all write and read operations to the repository’s primary node.&lt;/p&gt;

&lt;p&gt;Gitaly is a stateful application, we upgraded each Gitaly node with the latest AMI using an &lt;strong&gt;in-place node rotation&lt;/strong&gt; strategy. In older versions of GitLab (up to v14.0), if a Gitaly node is unhealthy, Praefect would immediately update the primary node for the repository to any healthy Gitaly node. After the rolling upgrade for a 3-node Gitaly cluster, repositories were mainly concentrated on only one Gitaly node.&lt;/p&gt;

&lt;p&gt;In our situation, a very busy monorepo was assigned to a Gitaly node that was also the main node for many other repositories. When real traffic began after deployment, the Gitaly node had trouble syncing the monorepo with the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;Because the Gitaly node was out of sync, Praefect started sending all changes and access requests for monorepo to this struggling Gitaly node. This increased the load on the Gitaly server, causing it to fail. We found this to be the main issue and decided to manually move our monorepo to a Gitaly node that was less crowded. We also added a step to validate primary node distribution to our deployment checklist.&lt;/p&gt;

&lt;p&gt;This immediate failover behaviour changed in &lt;a href=&quot;=https://gitlab.com/gitlab-org/gitaly/-/issues/3207&quot;&gt;GitLab version 14.1&lt;/a&gt;. Now, a primary is only elected lazily when a write request arrives for any repository. However, since we enabled maintenance mode before the Gitaly deployment, we didn’t receive any write requests. As a result, we did not see a shift in the primary node of the monorepo with new GitLab versions.&lt;/p&gt;

&lt;h2 id=&quot;regular-upgrades-our-new-normal&quot;&gt;Regular upgrades: Our new normal&lt;/h2&gt;

&lt;p&gt;Embracing the practice of consistent upgrades dramatically transformed the way we operate. We initiated frequent upgrades and implemented measures to reduce the actual deployment time.  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perform all major testing in one day before deployment.&lt;/li&gt;
  &lt;li&gt;Prepare a detailed checklist to follow during the deployment activity.&lt;/li&gt;
  &lt;li&gt;Reduce the minimum number of App Server and Sidekiq Servers required just after we start the deployment.&lt;/li&gt;
  &lt;li&gt;Upgrade components like App Server and Sidekiq in parallel.&lt;/li&gt;
  &lt;li&gt;Automate smoke testing to examine all major workflows after deployment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Leveraging the lessons learned and the experience gained with each upgrade, we successfully cut the time spent on the entire operation by 50%. The image-3 shows how we reduced our deployment time for major upgrades from 6 hours to 3 hours and our deployment time for minor upgrades from 4 to 1.5 hours.&lt;/p&gt;

&lt;p&gt;Each upgrade enriched our comprehensive knowledge base, equipping us with insights into the possible behaviours of each component under varying circumstances. Our growing experience and enhanced knowledge helped us achieve successful upgrades with less downtime with each deployment.&lt;/p&gt;

&lt;p&gt;Rather than moving up one minor version at a time, we learned about the feasibility of skipping versions. We began using the &lt;a href=&quot;https://gitlab-com.gitlab.io/support/toolbox/upgrade-path/&quot;&gt;GitLab Upgrade Path&lt;/a&gt;. This method allowed us to skip several versions, closing the distance to the latest version with fewer deployments. This approach enabled us to catch up on 24 months’ worth of upgrades in just 11 months, even though we started 14 months behind. &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/no-version-left-behind-our-epic-journey-of-gitlab-upgrades/upgrade-hours.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Time taken in hrs for each upgrade. The blue line depicts major and the red line is for minor upgrades&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;overcoming-challenges&quot;&gt;Overcoming challenges&lt;/h2&gt;

&lt;p&gt;Our journey was not without hurdles. We faced challenges in maintaining system stability during upgrades, navigating unexpected changes in functionality post upgrades, and ensuring data integrity.&lt;/p&gt;

&lt;p&gt;However, these challenges served as an opportunity for our team to innovate and create robust workarounds. Here are a few highlights:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Unexpected project distribution&lt;/strong&gt;: During upgrades and Gitaly server restarts, we observed unexpected migration of the monorepo to a crowded Gitaly server, resulting in higher rate limiting. We manually updated primary nodes for the monorepo and made this validation as a part of our deployment checklist.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NFS deprecation&lt;/strong&gt;: We migrated all required data to S3 buckets and deprecated NFS to become more resilient and independent of Availability Zone (AZ).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Handling unexpected Continuous Integration (CI) operations&lt;/strong&gt;: A sudden surge in CI operations sometimes resulted in rate limiting and interrupted more essential Git operations for developers. This is because GitLab uses different RPC calls and their concurrency for SSH and HTTP operations. We encouraged using HTTPS links for GitLab CI and automation script and SSH links for regular Git operations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Right-sizing resources&lt;/strong&gt;: We countered resource limitations by right-sizing our infrastructure, ensuring each component had optimal resources to function efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Performance testing&lt;/strong&gt;: We conducted performance testing of our GitLab using the &lt;a href=&quot;https://handbook.gitlab.com/handbook/support/workflows/gpt_quick_start&quot;&gt;GitLab Performance Tool (GPT)&lt;/a&gt;. In addition, we used our custom scripts to load test Grab specific use cases and mono repositories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Limiting maintenance windows&lt;/strong&gt;: Each deployment required a maintenance window or downtime. To minimise this, we structured our deployment processes more efficiently, reducing potential downtime and ensuring uninterrupted service for users.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dependency on GitLab.com image registry&lt;/strong&gt;: We introduced measures to host necessary images internally, which increased our resilience and allowed us to cut ties with external dependencies.&lt;/p&gt;

&lt;h2 id=&quot;the-results&quot;&gt;The results&lt;/h2&gt;

&lt;p&gt;Through careful planning, we’ve improved our upgrade process, ensuring system stability and timely updates. We’ve also reduced the delay in aligning with official GitLab releases. The image below displays how the time delay between release date and deployment has been reduced with each upgrade. It sharply brought down from &lt;strong&gt;396 days (around 14 months)&lt;/strong&gt; to &lt;strong&gt;35 days&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;At the time of this article, we’re just two minor versions behind the latest GitLab release, with a strong focus on security and resilience. We are also seeing a reduced number of reported issues after each upgrade.&lt;/p&gt;

&lt;p&gt;Our refined process has allowed us to perform regular updates without any service disruptions. We aim to leverage these learnings to automate our upgrade deployments, painting a positive picture for our future updates, marked by efficiency and stability.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/no-version-left-behind-our-epic-journey-of-gitlab-upgrades/days-since-deployed.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Time delay between official release date and date of deployment&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking ahead&lt;/h2&gt;

&lt;p&gt;Our dedication extends beyond staying current with the most recent GitLab versions. With stabilised deployment, we are now focusing on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Automated upgrades&lt;/strong&gt;: Our efforts extend towards bringing in more automation to enhance efficiency. We’re already employing &lt;strong&gt;zero-downtime&lt;/strong&gt; automated upgrades for patch versions involving no database migrations, utilising GitLab pipelines. Looking forward, we plan to automate minor version deployments as well, ensuring minimal human intervention during the upgrade process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automated runner onboarding for service teams&lt;/strong&gt;: We’ve developed a &lt;strong&gt;‘Runner as a Service’&lt;/strong&gt; solution for our service teams. Service teams can create their dedicated runners by providing minimal details, while we manage these runners centrally. This setup allows the service team to stay focused on development, ensuring smooth operations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improved communication and data safety&lt;/strong&gt;: We’re regularly communicating new features and potential issues to our service teams. We also ensure targeted solutions for any disruptions. Additionally, we’re focusing on developing automated data validation via our data restoration process. &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Focus on development&lt;/strong&gt;: With stabilised updates, we’ve created an environment where our development teams can focus more on crafting new features and supporting ongoing work, rather than handling upgrade issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;

&lt;p&gt;The upgrade process taught us the importance of adaptability, thorough preparation, effective communication, and continuous learning. Our ‘No Version Left Behind’ motto underscores the critical role of regular tech updates in boosting productivity, refining processes, and strengthening security. These insights will guide us as we navigate ongoing technological advancements.&lt;/p&gt;

&lt;p&gt;Below are the key areas in which we improved:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Enhanced testing procedures&lt;/strong&gt;: We’ve fine-tuned our testing strategies, using both automated and manual testing for GitLab, and regularly conducting performance tests before upgrades.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approvals&lt;/strong&gt;: We’ve designed approval workflows that allow us to obtain necessary clearances or approvals before each upgrade efficiently, further ensuring the smooth execution of our processes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improved communication&lt;/strong&gt;: We’ve improved stakeholder communication, regularly sharing updates and detailed documents about new features, deprecated items, and significant changes with each upgrade.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streamlined planning&lt;/strong&gt;: We’ve improved our upgrade planning, strictly following our checklist and rotating the role of Upgrade Ownership among team members.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimised activity time&lt;/strong&gt;: We’ve significantly reduced the time for production upgrade activity through advanced planning, automation, and eliminating unnecessary steps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Efficient issue management&lt;/strong&gt;: We’ve improved our ability to handle potential GitLab upgrade issues, with minimal to no issues occurring. We’re prepared to handle any incidents that could cause an outage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Knowledge base creation and automation&lt;/strong&gt;: We’ve created a GitLab knowledge base and continuously enhanced it with rich content, making it even more invaluable for training new team members and for reference during unexpected situations. We’ve also automated routine tasks to improve efficiency and reduce manual errors.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 03 May 2024 00:10:10 +0000</pubDate>
        <link>https://engineering.grab.com/no-version-left-behind-our-epic-journey-of-gitlab-upgrades</link>
        <guid isPermaLink="true">https://engineering.grab.com/no-version-left-behind-our-epic-journey-of-gitlab-upgrades</guid>
        
        <category>stability</category>
        
        <category>automation</category>
        
        <category>optimisation</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Ensuring data reliability and observability in risk systems</title>
        <description>&lt;p&gt;Grab has an in-house Risk Management platform called &lt;a href=&quot;https://www.grab.com/sg/business/defence/&quot;&gt;GrabDefence&lt;/a&gt; which relies on ingesting large amounts of data gathered from upstream services to power our heuristic risk rules and data science models in real time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/data-observability/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. GrabDefence aggregates data from different upstream services&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As Grab’s business grows, so does the amount of data. It becomes imperative that the data which fuels our risk systems is of reliable quality as any data discrepancy or missing data could impact fraud detection and prevention capabilities.&lt;/p&gt;

&lt;p&gt;We need to quickly detect any data anomalies, which is where data observability comes in.&lt;/p&gt;

&lt;h2 id=&quot;data-observability-as-a-solution&quot;&gt;Data observability as a solution&lt;/h2&gt;

&lt;p&gt;Data observability is a type of data operation (DataOps; similar to DevOps) where teams build visibility over the health and quality of their data pipelines. This enables teams to be notified of data quality issues, and allows teams to investigate and resolve these issues faster.&lt;/p&gt;

&lt;p&gt;We needed a solution that addresses the following issues:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Alerts for any data quality issues as soon as possible - so this means the observability tool had to work in real time.&lt;/li&gt;
  &lt;li&gt;With hundreds of data points to observe, we needed a neat and scalable solution which allows users to quickly pinpoint which data points were having issues.&lt;/li&gt;
  &lt;li&gt;A consistent way to compare, analyse, and compute data that might have different formats.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hence, we decided to use Flink to standardise data transformations, compute, and observe data trends quickly (in real time) and scalably.&lt;/p&gt;

&lt;h2 id=&quot;utilising-flink-for-real-time-computations-at-scale&quot;&gt;Utilising Flink for real-time computations at scale&lt;/h2&gt;

&lt;h3 id=&quot;what-is-flink&quot;&gt;What is Flink?&lt;/h3&gt;

&lt;p&gt;Flink SQL is a powerful, flexible tool for performing real-time analytics on streaming data. It allows users to query continuous data streams using standard SQL syntax, enabling complex event processing and data transformation within the Apache Flink ecosystem, which is particularly useful for scenarios requiring low-latency insights and decisions.&lt;/p&gt;

&lt;h3 id=&quot;how-we-used-flink-to-compute-data-output&quot;&gt;How we used Flink to compute data output&lt;/h3&gt;

&lt;p&gt;In Grab, data comes from multiple sources and while most of the data is in JSON format, the actual JSON structure differs between services. Because of JSON’s nested and dynamic data structure, it is difficult to consistently analyse the data – posing a significant challenge for real-time analysis.&lt;/p&gt;

&lt;p&gt;To help address this issue, Apache Flink SQL has the capability to manage such intricacies with ease. It offers specialised functions tailored for parsing and querying JSON data, ensuring efficient processing.&lt;/p&gt;

&lt;p&gt;Another standout feature of Flink SQL is the use of custom table functions, such as JSONEXPLOAD, which serves to deconstruct and flatten nested JSON structures into tabular rows. This transformation is crucial as it enables subsequent aggregation operations. By implementing a 5-minute tumbling window, Flink SQL can easily aggregate these now-flattened data streams. This technique is pivotal for monitoring, observing, and analysing data patterns and metrics in near real-time.&lt;/p&gt;

&lt;p&gt;Now that data is aggregated by Flink for easy analysis, we still needed a way to incorporate comprehensive monitoring so that teams could be notified of any data anomalies or discrepancies in real time.&lt;/p&gt;

&lt;h3 id=&quot;how-we-interfaced-the-output-with-datadog&quot;&gt;How we interfaced the output with Datadog &lt;/h3&gt;

&lt;p&gt;Datadog is the observability tool of choice in Grab, with many teams using Datadog for their service reliability observations and alerts. By aggregating data from Apache Flink and integrating it with Datadog, we can harness the synergy of real-time analytics and comprehensive monitoring. Flink excels in processing and aggregating data streams, which, when pushed to Datadog, can be further analysed and visualised. Datadog also provides seamless integration with collaboration tools like Slack, which enables teams to receive instant notifications and alerts.&lt;/p&gt;

&lt;p&gt;With Datadog’s out-of-the-box features such as anomaly detection, teams can identify and be alerted to unusual patterns or outliers in their data streams. Taking a proactive approach to monitoring is crucial in maintaining system health and performance as teams can be alerted, then collaborate quickly to diagnose and address anomalies.&lt;/p&gt;

&lt;p&gt;This integrated pipeline—from Flink’s real-time data aggregation to Datadog’s monitoring and Slack’s communication capabilities—creates a robust framework for real-time data operations. It ensures that any potential issues are quickly traced and brought to the team’s attention, facilitating a rapid response. Such an ecosystem empowers organisations to maintain high levels of system reliability and performance, ultimately enhancing the overall user experience.&lt;/p&gt;

&lt;h2 id=&quot;organising-monitors-and-alerts-using-out-of-the-box-solutions-from-datadog&quot;&gt;Organising monitors and alerts using out-of-the-box solutions from Datadog&lt;/h2&gt;

&lt;p&gt;Once we integrated Flink data into Datadog, we realised that it could become unwieldy to try to identify the data point with issues from hundreds of other counters.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/data-observability/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. Hundreds of data points on a graph make it hard to decipher which ones have issues&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We decided to organise the counters according to the service stream it was coming from, and create individual monitors for each service stream. We used Datadog’s Monitor Summary tool to help visualise the total number of service streams we are reading from and the number of underlying data points within each stream.  &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/data-observability/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. Data is grouped according to their source stream&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Within each individual stream, we used Datadog’s &lt;a href=&quot;https://docs.datadoghq.com/monitors/types/anomaly/&quot;&gt;Anomaly Detection&lt;/a&gt; feature to create an alert whenever a data point from the stream exceeds a predefined threshold. This can be configured by the service teams on Datadog.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/data-observability/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. Datadog’s built-in Anomaly Detection function triggers alerts whenever a data point exceeds a threshold&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These alerts are then sent to a Slack channel where the Data team is informed when a data point of interest starts throwing anomalous values.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/data-observability/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 5. Datadog integration with Slack to help alert users&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;Since the deployment of this data observability tool, we have seen significant improvement in the detection of anomalous values. If there are any anomalies or issues, we now get alerts within the same day (or hour) instead of days to weeks later.&lt;/p&gt;

&lt;p&gt;Organising the alerts according to source streams have also helped simplify the monitoring load and allows users to quickly narrow down and identify which pipeline has failed.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;At the moment, this data observability tool is only implemented on selected checkpoints in GrabDefence. We plan to expand the observability tool’s coverage to include more checkpoints, and continue to refine the workflows to detect and resolve these data issues.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Apr 2024 00:15:10 +0000</pubDate>
        <link>https://engineering.grab.com/data-observability</link>
        <guid isPermaLink="true">https://engineering.grab.com/data-observability</guid>
        
        <category>Data Science</category>
        
        <category>Security</category>
        
        <category>Risk</category>
        
        <category>Data observability</category>
        
        <category>Data reliability</category>
        
        
        <category>Data Science</category>
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Grab Experiment Decision Engine - a Unified Toolkit for Experimentation</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This article introduces the GrabX Decision Engine, an internal open-source package that offers a comprehensive framework for designing and analysing experiments conducted on online experiment platforms. The package encompasses a wide range of functionalities, including a pre-experiment advisor, a post-experiment analysis toolbox, and other advanced tools. In this article, we explore the motivation behind the development of these functionalities, their integration into the unique ecosystem of Grab’s multi-sided marketplace, and how these solutions strengthen the culture and calibre of experimentation at Grab.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Today, &lt;a href=&quot;/building-grab-s-experimentation-platform&quot;&gt;Grab’s Experimentation (GrabX) platform&lt;/a&gt; orchestrates the testing of thousands of experimental variants each week. As the platform continues to expand and manage a growing volume of experiments, the need for dependable, scalable, and trustworthy experimentation tools becomes increasingly critical for data-driven and evidence-based 
decision-making.&lt;/p&gt;

&lt;p&gt;In our previous article, we presented the &lt;a href=&quot;https://engineering.grab.com/automated-experiment-analysis&quot;&gt;Automated Experiment Analysis&lt;/a&gt; application, a tool designed to automate data pipelines for analyses. However, during the development of this application for Grab’s experimenter community, we noticed a prevailing trend: experiments were predominantly analysed on a one-by-one, manual basis. While such a federated approach may be needed in a few cases, it presents numerous challenges at 
the organisational level:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Lack of a contextual toolkit&lt;/strong&gt;: GrabX facilitates executing a diverse range of experimentation designs, catering to the varied needs and contexts of different tech teams across the organisation. However, experimenters may often rely on generic online tools for experiment configurations (e.g. sample size calculations), which were not specifically designed to cater to the nuances of GrabX experiments or the recommended evaluation method, given the design. This is exacerbated by the fact 
that most online tutorials or courses on experimental design do not typically address the nuances of multi-sided marketplaces, and cannot consider the nature or constraints of specific experiments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lack of standards&lt;/strong&gt;: In this federated model, the absence of standardised and vetted practices can lead to reliability issues. In some cases, these can include poorly designed experiments, inappropriate evaluation methods, suboptimal testing choices, and unreliable inferences, all of which are difficult to monitor and rectify.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lack of scalability and efficiency&lt;/strong&gt;: Experimenters, coming from varied backgrounds and possessing distinct skill sets, may adopt significantly different approaches to experimentation and inference. This diversity, while valuable, often impedes the transferability and sharing of methods, hindering a cohesive and scalable experimentation framework. Additionally, this variance in methods can extend the lifecycle of experiment analysis, as disagreements over approaches may give rise to 
repeated requests for review or modification.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;To address these challenges, we developed the GrabX Decision Engine, a Python package open-sourced internally across all of Grab’s development platforms. Its central objective is to institutionalise best practices in experiment efficiency and analytics, thereby ensuring the derivation of precise and reliable conclusions from each experiment.&lt;/p&gt;

&lt;p&gt;In particular, this unified toolkit significantly enhances our end-to-end experimentation processes by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Ensuring compatibility with GrabX and Automated Experiment Analysis&lt;/strong&gt;: The package is fully integrated with the &lt;a href=&quot;https://engineering.grab.com/automated-experiment-analysis&quot;&gt;Automated Experiment Analysis&lt;/a&gt; app, and provides analytics and test results tailored to the designs supported by GrabX. The outcomes can be further used for other downstream jobs, e.g. market modelling, simulation-based calibrations, or auto-adaptive configuration tuning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Standardising experiment analytics&lt;/strong&gt;: By providing a unified framework, the package ensures that the rationale behind experiment design and the interpretation of analysis results adhere to a company-wide standard, promoting consistency and ease of review across different teams.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhancing collaboration and quality&lt;/strong&gt;: As an open-source package, it not only fosters a collaborative culture but also upholds quality through peer reviews. It invites users to tap into a rich pool of features while encouraging contributions that refine and expand the toolkit’s capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The package is designed for everyone involved in the experimentation process, with data scientists and product analysts being the primary users. Referred to as experimenters in this article, these key stakeholders can not only leverage the existing capabilities of the package to support their projects, but can also contribute their own innovations. Eventually, the experiment results and insights generated from the package via the &lt;a href=&quot;https://engineering.grab.com/automated-experiment-analysis&quot;&gt;Automated Experiment Analysis&lt;/a&gt; app have an even wider reach to stakeholders across all functions.&lt;/p&gt;

&lt;p&gt;In the following section, we go deeper into the key functionalities of the package.&lt;/p&gt;

&lt;h2 id=&quot;feature-details&quot;&gt;Feature details&lt;/h2&gt;

&lt;p&gt;The package comprises three key components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An experimentation trusted advisor&lt;/li&gt;
  &lt;li&gt;A comprehensive post-experiment analysis toolbox&lt;/li&gt;
  &lt;li&gt;Advanced tools&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These have been built taking into account the type of experiments we typically run at Grab. To understand their functionality, it’s useful to first discuss the key experimental designs supported by GrabX.&lt;/p&gt;

&lt;h3 id=&quot;a-note-on-experimental-designs&quot;&gt;A note on experimental designs&lt;/h3&gt;

&lt;p&gt;While there is a wide variety of specific experimental designs implemented, they can be bucketed into two main categories: a &lt;strong&gt;between-subject&lt;/strong&gt; design and a &lt;strong&gt;within-subject&lt;/strong&gt; design.&lt;/p&gt;

&lt;p&gt;In a between-subject design, participants — like our app users, driver-partners, and merchant-partners — are split into experimental groups, and each group gets exposed to a distinct condition throughout the experiment. One challenge in this design is that each participant may provide multiple observations to our experimental analysis sample, causing a high within-subject correlation among observations and deviations between the randomisation and session unit. This can affect the accuracy of 
pre-experiment power analysis, and post-experiment inference, since it necessitates adjustments, e.g. clustering of standard errors when conducting hypothesis testing.&lt;/p&gt;

&lt;p&gt;Conversely, a within-subject design involves every participant experiencing all conditions. Marketplace-level switchback experiments are a common GrabX use case, where a timeslice becomes the experimental unit. This design not only faces the aforementioned challenges, but also creates other complications that need to be accounted for, such as spillover effects across timeslices.&lt;/p&gt;

&lt;p&gt;Designing and analysing the results of both experimental approaches requires careful nuanced statistical tools. Ensuring proper duration, sample size, controlling for confounders, and addressing potential biases are important considerations to enhance the validity of the results.&lt;/p&gt;

&lt;h3 id=&quot;trusted-advisor&quot;&gt;Trusted Advisor&lt;/h3&gt;

&lt;p&gt;The first key component of the Decision Engine is the Trusted Advisor, which provides a recommendation to the experimenter on key experiment attributes to be considered when preparing the experiment. This is dependent on the design; at a minimum, the experimenter needs to define whether the experiment design is between- or within-subject.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The between-subject design&lt;/strong&gt;: We strongly recommend that experimenters utilise the “Trusted Advisor” feature in the Decision Engine for estimating their required sample size. This is designed to account for the multiple observations per user the experiment is expected to generate and adjusts for the presence of clustered errors (Moffatt, 2020; List, Sadoff, &amp;amp; Wagner, 2011). This feature allows users to input their data, either as a PySpark or Pandas dataframe. Alternatively, a function is 
provided to extract summary statistics from their data, which can then be inputted into the Trusted Advisor. Obtaining the data beforehand is actually not mandatory; users have the option to directly query the recommended sample size based on common metrics derived from a regular data pipeline job. These functionalities are illustrated in the flowchart below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabx-decision-engine/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Trusted Advisor functionalities&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Furthermore, the Trusted Advisor feature can identify the underlying characteristics of the data, whether it’s passed directly, or queried from our common metrics database. This enables it to determine the appropriate power analysis for the experiment, without further guidance. For instance, it can detect if the target metric is a binary decision variable, and will adapt the power analysis to the correct context.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The within-subject design&lt;/strong&gt;: In this case, we instead provide a best practices guideline to follow. Through our experience supporting various Tech Families running switchback experiments, we have observed various challenges highly dependent on the use case. This makes it difficult to create a one-size-fits-all solution.&lt;/p&gt;

&lt;p&gt;For instance, an important factor affecting the final sample size requirement is how frequently treatments switch, which is also tied to what data granularity is appropriate to use in the post-experiment analysis. These considerations are dependent on, among other factors, how quickly a given treatment is expected to cause an effect. Some treatments may take effect relatively quickly (near-instantly, e.g. if applied to price checks), while others may take significantly longer (e.g. 15-30 minutes because they may require a trip to be completed). This has further consequences, e.g. autocorrelation between observations within a treatment window, spillover effects between different treatment windows, requirements for cool-down windows when treatments switch, etc.&lt;/p&gt;

&lt;p&gt;Another issue we have identified from analysing the history of experiments on our platform is that a significant portion is prone to issues related to sample ratio mismatch (SRM). We therefore also heavily emphasise the post-experiment analysis corrections and robustness checks that are needed in switchback experiments, and do not simply rely on pre-experiment guidance such as power analysis.&lt;/p&gt;

&lt;h3 id=&quot;post-experiment-analysis&quot;&gt;Post-experiment analysis&lt;/h3&gt;

&lt;p&gt;Upon completion of the experiment, a comprehensive toolbox for post-experiment analysis is available. This toolbox consists of a wide range of statistical tests, ranging from normality tests to non-parametric and parametric tests. Here is an overview of the different types of tests included in the toolbox for different experiment setups:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabx-decision-engine/image2.png&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Tests supported by the post-experiment analysis component&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Though we make all the relevant tests available, the package sets a default list of output. With just two lines of code specifying the desired experiment design, experimenters can easily retrieve the recommended results, as summarised in the following table.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;Types&lt;/th&gt;
    &lt;th&gt;Details&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td&gt;Basic statistics&lt;/td&gt;
    &lt;td&gt;The mean, variance, and sample size of Treatment and Control &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Uplift tests&lt;/td&gt;
    &lt;td&gt;Welch&apos;s t-test;&lt;br /&gt;Non-parametric tests, such as Wilcoxon signed-rank test and Mann-Whitney U Test&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Misc tests&lt;/td&gt;
    &lt;td&gt;Normality tests such as the Shapiro-Wilk test, Anderson-Darling test, and Kolmogorov-Smirnov test;&lt;br /&gt;Levene test which assesses the equality of variances between groups&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Regression models&lt;/td&gt;
    &lt;td&gt;A standard OLS/Logit model to estimate the treatment uplift;&lt;br /&gt;&lt;b&gt;Recommended regression models&lt;/b&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Warning&lt;/td&gt;
    &lt;td&gt;Provides a warning or notification related to the statistical analysis or results, for example:&lt;br /&gt;- Lack of variation in the variables&lt;br /&gt;- Sample size is too small&lt;br /&gt;- Too few randomisation units which will lead to under-estimated standard errors&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;recommended-regression-models&quot;&gt;Recommended regression models&lt;/h3&gt;

&lt;p&gt;Besides reporting relevant statistical test results, we adopt regression models to leverage their flexibility in controlling for confounders, fixed effects and heteroskedasticity, as is commonly observed in our experiments. As mentioned in the section “A note on experimental design”, each approach has different implications on the achieved randomisation, and hence requires its own customised regression models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Between-subject design&lt;/strong&gt;: the observations are not independent and identically distributed (i.i.d) but clustered due to repeated observations of the same experimental units. Therefore, we set the default clustering level at the participant level in our regression models, considering that most of our between-subject experiments only take a small portion of the population (Abadie et al., 2022).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Within-subject design&lt;/strong&gt;: this has further challenges, including spillover effects and randomisation imbalances. As a result, they often require better control of confounding factors. We adopt panel data methods and impose time fixed effects, with no option to remove them. Though users have the flexibility to define these themselves, we use hourly fixed effects as our default as we have found that these match the typical seasonality we observe in marketplace metrics. Similar to between-subject 
designs, we use standard error corrections for clustered errors, and small number of clusters, as the default. Our API is flexible for users to include further controls, as well as further fixed effects to adapt the estimator to geo-timeslice designs.&lt;/p&gt;

&lt;h3 id=&quot;advanced-tools&quot;&gt;Advanced tools&lt;/h3&gt;

&lt;p&gt;Apart from the pre-experiment Trusted Advisor and the post-experiment Analysis Toolbox, we have enriched this package by providing more advanced tools. Some of them are set as a default feature in the previous two components, while others are ad-hoc capabilities which the users can utilise via calling the functions directly.&lt;/p&gt;

&lt;h4 id=&quot;variance-reduction&quot;&gt;Variance reduction&lt;/h4&gt;

&lt;p&gt;We bring in multiple methods to reduce variance and improve the power and sensitivity of experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stratified sampling: recognised for reducing variance during assignment&lt;/li&gt;
  &lt;li&gt;Post stratification: a post-assignment variance reduction technique&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf&quot;&gt;CUPED&lt;/a&gt;: utilises ANCOVA to decrease variances&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.07263.pdf&quot;&gt;MLRATE&lt;/a&gt;: an extension of CUPED that allows for the use of non-linear / machine learning models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These approaches offer valuable ways to mitigate variance and improve the overall effectiveness of experiments. The experimenters can directly access these ad hoc capabilities via the package.&lt;/p&gt;

&lt;h4 id=&quot;multiple-comparisons-problem&quot;&gt;Multiple comparisons problem&lt;/h4&gt;

&lt;p&gt;A multiple comparisons problem occurs when multiple hypotheses are simultaneously tested, leading to a higher likelihood of false positives. To address this, we implement various statistical correction techniques in this package, as illustrated below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabx-decision-engine/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Statistical correction techniques&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Experimenters can specify if they have concerns about the dependency of the tests and whether the test results are expected to be negatively related. This capability will adopt the following procedures and choose the relevant tests to mitigate the risk of false positives accordingly:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;False Discovery Rate (FDR) procedures, which control the expected rate of false discoveries.&lt;/li&gt;
  &lt;li&gt;Family-wise Error Rate (FWER) procedures, which control the probability of making at least one false discovery within a set of related tests referred to as a family.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;multiple-treatments-and-unequal-treatment-sizes&quot;&gt;Multiple treatments and unequal treatment sizes&lt;/h4&gt;

&lt;p&gt;We developed a capability to deal with experiments where there are multiple treatments. This capability employs a conservative approach to ensure that the size reaches a minimum level where any pairwise comparison between the control and treatment groups has a sufficient sample size.&lt;/p&gt;

&lt;h4 id=&quot;heterogeneous-treatment-effects&quot;&gt;Heterogeneous treatment effects&lt;/h4&gt;

&lt;p&gt;Heterogeneous treatment effects refer to a situation where the treatment effect varies across different groups or subpopulations within a larger population. For instance, it may be of interest to examine treatment effects specifically on rainy days compared to non-rainy days. We have incorporated this functionality into the tests for both experiment designs. By enabling this feature, we facilitate a more nuanced analysis that accounts for potential variations in treatment effects based on different factors or contexts.&lt;/p&gt;

&lt;h2 id=&quot;maintenance-and-support&quot;&gt;Maintenance and support&lt;/h2&gt;

&lt;p&gt;The package is available across all internal DS/Machine Learning platforms and individual local development environments within Grab. Its source code is openly accessible to all developers within Grab and its release adheres to a semantic release standard.&lt;/p&gt;

&lt;p&gt;In addition to the technical maintenance efforts, we have introduced a dedicated committee and a workspace to address issues that may extend beyond the scope of the package’s current capabilities.&lt;/p&gt;

&lt;h3 id=&quot;experiment-council&quot;&gt;Experiment Council&lt;/h3&gt;

&lt;p&gt;Within Grab, there is a dedicated committee known as the ‘Experiment Council’. This committee includes data scientists, analysts, and economists from various functions. One of their responsibilities is to collaborate to enhance and maintain the package, as well as guide users in effectively utilising its functionalities. The Experiment Council plays a crucial role in enhancing the overall operational excellence of conducting experiments and deriving meaningful insights from them.&lt;/p&gt;

&lt;h3 id=&quot;grabcausal-methodology-bank&quot;&gt;GrabCausal Methodology Bank&lt;/h3&gt;

&lt;p&gt;Experimenters frequently encounter challenges regarding the feasibility of conducting experiments for causal problems. To address this concern, we have introduced an alternative workspace called GrabCausal Methodology Bank. Similar to the internal open-source nature of this project, the GrabCausal Methodology bank is open to contributions from all users within Grab. It provides a collaborative space where users can readily share their code, case studies, guidelines, and suggestions related to 
causal methodologies. By fostering an open and inclusive environment, this workspace encourages knowledge sharing and promotes the advancement of causal research methods.&lt;/p&gt;

&lt;p&gt;The workspace functions as a platform, which now exhibits a wide range of commonly used methods, including Diff-in-Diff, Event studies, Regression Discontinuity Designs (RDD), Instrumental Variables (IV), Bayesian structural time series, and Bunching. Additionally, we are dedicated to incorporating more, such as Synthetic control, Double ML (Chernozhukov et al. 2018), DAG discovery/validation, etc., to further enhance our offerings in this space.&lt;/p&gt;

&lt;h2 id=&quot;learnings&quot;&gt;Learnings&lt;/h2&gt;

&lt;p&gt;Over the past few years, we have invested in developing and expanding this package. Our initial motivation was humble yet motivating - to contribute to improving the quality of experimentation at Grab, helping it develop from its initial start-up modus operandi to a more consolidated, rigorous, and guided approach.&lt;/p&gt;

&lt;p&gt;Throughout this journey, we have learned that prioritisation holds the utmost significance in open-source projects of this nature; the majority of user demands can be met through relatively small yet pivotal efforts. By focusing on these core capabilities, we avoid spreading resources too thinly across all areas at the initial stage of planning and development.&lt;/p&gt;

&lt;p&gt;Meanwhile, we acknowledge that there is still a significant journey ahead. While the package now focuses solely on individual experiments, an inherent challenge in online-controlled experimentation platforms is the interference between experiments (Gupta, et al, 2019). A recent development in the field is to embrace simultaneous tests (&lt;a href=&quot;https://exp-platform.com/Documents/2013%2520controlledExperimentsAtScale.pdf&quot;&gt;Microsoft&lt;/a&gt;, &lt;a href=&quot;https://medium.datadriveninvestor.com/how-google-conducts-more-better-faster-experiments-3b91446cd3b5&quot;&gt;Google&lt;/a&gt;, &lt;a href=&quot;https://www.infoq.com/news/2016/12/large-experimentation-spotify/&quot;&gt;Spotify&lt;/a&gt; and &lt;a href=&quot;https://cxl.com/blog/can-you-run-multiple-ab-tests-at-the-same-time/&quot;&gt;booking.com and Optimizely&lt;/a&gt;), and to carefully consider the tradeoff between accuracy and velocity.&lt;/p&gt;

&lt;p&gt;The key to overcoming this challenge will be a close collaboration between the community of experimenters, the teams developing this unified toolkit, and the GrabX platform engineers. In particular, the platform developers will continue to enrich the experimentation SDK by providing diverse assignment strategies, sampling mechanisms, and user interfaces to manage potential inference risks better. Simultaneously, the community of experimenters can coordinate among themselves effectively to 
avoid severe interference, which will also be monitored by GrabX. Last but not least, the development of this unified toolkit will also focus on monitoring, evaluating, and managing inter-experiment interference.&lt;/p&gt;

&lt;p&gt;In addition, we are committed to keeping this package in sync with industry advancements. Many existing tools in this package, despite being labelled as “advanced” in the earlier discussions, are still relatively simplified. For instance,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Incorporating standard errors clustering based on the diverse assignment and sampling strategies requires attention (Abadie, et al, 2023).&lt;/li&gt;
  &lt;li&gt;Sequential testing will play a vital role in detecting uplifts earlier and safely, avoiding p-hacking. One recent innovation is the “always valid inference” (Johari, et al., 2022)&lt;/li&gt;
  &lt;li&gt;The advancements in investigating heterogeneous effects, such as Causal Forest (Athey and Wager, 2019), have extended beyond linear approaches, now incorporating nonlinear and more granular analyses.&lt;/li&gt;
  &lt;li&gt;Estimating the long-term treatment effects observed from short-term follow-ups is also a long-term objective, and one approach is using a Surrogate Index (Athey, et al 2019).&lt;/li&gt;
  &lt;li&gt;Continuous effort is required to stay updated and informed about the latest advancements in statistical testing methodologies, to ensure accuracy and effectiveness.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article marks the beginning of our journey towards automating the experimentation and product decision-making process among the data scientist community. We are excited about the prospect of expanding the toolkit further in these directions. Stay tuned for more updates and posts.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Abadie, Alberto, et al. “When should you adjust standard errors for clustering?.” The Quarterly Journal of Economics 138.1 (2023): 1-35.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Athey, Susan, et al. “The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely.” No. w26463. National Bureau of Economic Research, 2019.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Athey, Susan, and Stefan Wager. “Estimating treatment effects with causal forests: An application.” Observational studies 5.2 (2019): 37-51.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Chernozhukov, Victor, et al. “Double/debiased machine learning for treatment and structural parameters.” (2018): C1-C68.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Facure, Matheus. Causal Inference in Python. O’Reilly Media, Inc., 2023.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gupta, Somit, et al. “Top challenges from the first practical online controlled experiments summit.” ACM SIGKDD Explorations Newsletter 21.1 (2019): 20-35.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Huntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. CRC Press, 2021.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Imbens, Guido W. and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press, 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Johari, Ramesh, et al. “Always valid inference: Continuous monitoring of a/b tests.” Operations Research 70.3 (2022): 1806-1821.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;List, John A., Sally Sadoff, and Mathis Wagner. “So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design.” Experimental Economics 14 (2011): 439-457.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Moffatt, Peter. Experimetrics: Econometrics for Experimental Economics. Bloomsbury Publishing, 2020.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Apr 2024 02:22:10 +0000</pubDate>
        <link>https://engineering.grab.com/grabx-decision-engine</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabx-decision-engine</guid>
        
        <category>Data Science</category>
        
        <category>Experiment</category>
        
        <category>Statistics</category>
        
        <category>Econometrics</category>
        
        <category>Python Package</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Iris - Turning observations into actionable insights for enhanced decision making</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Iris&lt;/strong&gt; (/ˈaɪrɪs/), a name inspired by the Olympian mythological figure who personified the rainbow and served as the messenger of the gods, is a comprehensive observability platform for Extract, Transform, Load (ETL) jobs. Just as the mythological Iris connected the gods to humanity, our Iris platform bridges the gap between raw data and meaningful insights, serving the needs of data-driven organisations. Specialising in meticulous monitoring and tracking of &lt;strong&gt;Spark&lt;/strong&gt; and &lt;strong&gt;Presto&lt;/strong&gt; jobs, Iris stands as a transformative tool for peak observability and effective decision-making.&lt;/td&gt;
&lt;td&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image14.png&quot; width=&quot;100%&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Iris captures critical job metrics &lt;strong&gt;right at the Java Virtual Machine (JVM) level&lt;/strong&gt;, including but not limited to runtime, CPU and memory utilisation rates, garbage collection statistics, &lt;strong&gt;stage and task execution details&lt;/strong&gt;, and much more.&lt;/li&gt;
  &lt;li&gt;Iris not only regularly records these metrics but also supports &lt;strong&gt;real-time monitoring&lt;/strong&gt; and &lt;strong&gt;offline analytics&lt;/strong&gt; of metrics in the data lake. This gives you multi-faceted control and insights into the operational aspects of your workloads.&lt;/li&gt;
  &lt;li&gt;Iris gives you an overview of your jobs, &lt;strong&gt;predicts&lt;/strong&gt; if your jobs are over or under-provisioned, and provides suggestions on how to optimise resource usage and save costs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;understanding-the-needs&quot;&gt;Understanding the needs&lt;/h2&gt;

&lt;p&gt;When examining ETL job monitoring across various platforms, a common deficiency became apparent. Existing tools could only provide CPU and memory usage data at the instance level, where an instance could refer to an EC2 unit or a Kubernetes pod with resources bound to the container level.&lt;/p&gt;

&lt;p&gt;However, this CPU and memory usage data included usage from the operating system and other background tasks, making it difficult to &lt;strong&gt;isolate usage specific to Spark jobs&lt;/strong&gt; (JVM level). A sizeable fraction of resource consumption, thus, could not be attributed directly to our ETL jobs. This lack of granularity posed significant challenges when trying to perform effective resource optimisation for individual jobs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image8.png&quot; width=&quot;70%&quot; alt=&quot;&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Gap between total instance and JVM provisioned resources&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The situation was further complicated when compute instances were shared among various jobs. In such cases, determining the precise resource consumption for a specific job became nearly impossible. This made in-depth analysis and performance optimisation of specific jobs a complex and often ineffective process.&lt;/p&gt;

&lt;p&gt;In the initial stages of my career in Spark, I took the reins of handling SEGP ETL jobs deployed in Chimera. Then, Chimera did not possess any tool for observing and understanding SEGP jobs. The lack of an efficient tool for &lt;strong&gt;close-to-real-time visualisation&lt;/strong&gt; of Spark cluster/job metrics, profiling code &lt;strong&gt;class/function runtime durations&lt;/strong&gt;, and investigating deep-level job metrics to assess CPU and memory usage, posed a significant challenge even back then.&lt;/p&gt;

&lt;p&gt;In the quest for solutions within Grab, I found no tool that could fulfill all these needs. This prompted me to extend my search beyond the organisation, leading me to discover that Uber had an exceptional tool known as the &lt;strong&gt;JVM Profiler&lt;/strong&gt;. This tool could collect JVM metrics and profile the job. Further research also led me to &lt;strong&gt;sparkMeasure&lt;/strong&gt;, a standalone tool known for its ability to measure Spark metrics on-the-fly without any code changes.&lt;/p&gt;

&lt;p&gt;This personal research and journey highlights the importance of a comprehensive, in-depth observability tool - emphasising the need that Iris aims to fulfill in the world of ETL job monitoring. Through this journey, Iris was ideated, named after the Greek deity, encapsulating the mission to bridge the gap between the realm of raw ETL job metrics and the world of actionable insights.&lt;/p&gt;

&lt;h2 id=&quot;observability-with-iris&quot;&gt;Observability with Iris&lt;/h2&gt;

&lt;h3 id=&quot;platform-architecture&quot;&gt;Platform architecture&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image9.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Platform architecture of Iris&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Iris’s robust architecture is designed to smartly deliver observability into Spark jobs with high reliability. It consists of three main modules: Metrics Collector, Kafka Queue, and Telegraf, InfluxDB, and Grafana (TIG) Stack.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metrics Collector&lt;/strong&gt;: This module listens to Spark jobs, collects metrics, and funnels them to the Kafka queue. What sets this apart is its unobstructive nature - there is no need for end-users to update their application code or notebook.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kafka Queue&lt;/strong&gt;: Serving as an asynchronous deliverer of metrics messages, Kafka is leveraged to prevent Iris from becoming another bottleneck slowing down user jobs. By functioning as a message queue, it enables the efficient processing of metric data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TIG Stack&lt;/strong&gt;: This component is utilised for real-time monitoring, making visualising performance metrics a cinch. The TIG stack proves to be an effective solution for real-time data visualisation.&lt;/p&gt;

&lt;p&gt;For offline analytics, Iris pushes metrics data from Kafka into our data lake. This creates a wealth of historical data that can be utilised for future research, analysis, and predictions. The strategic combination of real-time monitoring and offline analysis forms the basis of Iris’s ability to provide valuable insights.&lt;/p&gt;

&lt;p&gt;Next, we will delve into how Iris collects the metrics.&lt;/p&gt;

&lt;h3 id=&quot;data-collection&quot;&gt;Data collection&lt;/h3&gt;

&lt;p&gt;Iris’s metrics is now primarily driven by two tools that operate under the Metrics Collector module: JVM Profiler and sparkMeasure.&lt;/p&gt;

&lt;h4 id=&quot;jvm-profiler&quot;&gt;JVM Profiler&lt;/h4&gt;

&lt;p&gt;As mentioned earlier, JVM Profiler is an exceptional tool that helps to collect and profile metrics at JVM level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image10.png&quot; width=&quot;70%&quot; alt=&quot;&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Java process for the JVM Profiler tool&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Uber JVM Profiler supports the following features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Debug memory usage for all your Spark application executors, including java heap memory, non-heap memory, native memory (VmRSS, VmHWM), memory pool, and buffer pool (directed/mapped buffer).&lt;/li&gt;
  &lt;li&gt;Debug CPU usage, garbage collection time for all Spark executors.&lt;/li&gt;
  &lt;li&gt;Debug arbitrary Java class methods (how many times they run, how long they take), also called Duration Profiling.&lt;/li&gt;
  &lt;li&gt;Debug arbitrary Java class method call and trace its argument value, also known as Argument Profiling.&lt;/li&gt;
  &lt;li&gt;Do Stacktrack Profiling and generate flamegraph to visualise CPU time spent for the Spark application.&lt;/li&gt;
  &lt;li&gt;Debug I/O metrics (disk read/write bytes for the application, CPU iowait for the machine).&lt;/li&gt;
  &lt;li&gt;Debug JVM Thread Metrics like Count of Total Threads, Peak Threads, Live/Active Threads, and newThreads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example metrics (&lt;a href=&quot;https://gitlab.myteksi.net/olympus/iris/jvm-profiler&quot;&gt;Source code&lt;/a&gt;)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
        &quot;nonHeapMemoryTotalUsed&quot;: 11890584.0,
        &quot;bufferPools&quot;: [
                {
                        &quot;totalCapacity&quot;: 0,
                        &quot;name&quot;: &quot;direct&quot;,
                        &quot;count&quot;: 0,
                        &quot;memoryUsed&quot;: 0
                },
                {
                        &quot;totalCapacity&quot;: 0,
                        &quot;name&quot;: &quot;mapped&quot;,
                        &quot;count&quot;: 0,
                        &quot;memoryUsed&quot;: 0
                }
        ],
        &quot;heapMemoryTotalUsed&quot;: 24330736.0,
        &quot;epochMillis&quot;: 1515627003374,
        &quot;nonHeapMemoryCommitted&quot;: 13565952.0,
        &quot;heapMemoryCommitted&quot;: 257425408.0,
        &quot;memoryPools&quot;: [
                {
                        &quot;peakUsageMax&quot;: 251658240,
                        &quot;usageMax&quot;: 251658240,
                        &quot;peakUsageUsed&quot;: 1194496,
                        &quot;name&quot;: &quot;Code Cache&quot;,
                        &quot;peakUsageCommitted&quot;: 2555904,
                        &quot;usageUsed&quot;: 1173504,
                        &quot;type&quot;: &quot;Non-heap memory&quot;,
                        &quot;usageCommitted&quot;: 2555904
                },
                {
                        &quot;peakUsageMax&quot;: -1,
                        &quot;usageMax&quot;: -1,
                        &quot;peakUsageUsed&quot;: 9622920,
                        &quot;name&quot;: &quot;Metaspace&quot;,
                        &quot;peakUsageCommitted&quot;: 9830400,
                        &quot;usageUsed&quot;: 9622920,
                        &quot;type&quot;: &quot;Non-heap memory&quot;,
                        &quot;usageCommitted&quot;: 9830400
                },
                {
                        &quot;peakUsageMax&quot;: 1073741824,
                        &quot;usageMax&quot;: 1073741824,
                        &quot;peakUsageUsed&quot;: 1094160,
                        &quot;name&quot;: &quot;Compressed Class Space&quot;,
                        &quot;peakUsageCommitted&quot;: 1179648,
                        &quot;usageUsed&quot;: 1094160,
                        &quot;type&quot;: &quot;Non-heap memory&quot;,
                        &quot;usageCommitted&quot;: 1179648
                },
                {
                        &quot;peakUsageMax&quot;: 1409286144,
                        &quot;usageMax&quot;: 1409286144,
                        &quot;peakUsageUsed&quot;: 24330736,
                        &quot;name&quot;: &quot;PS Eden Space&quot;,
                        &quot;peakUsageCommitted&quot;: 67108864,
                        &quot;usageUsed&quot;: 24330736,
                        &quot;type&quot;: &quot;Heap memory&quot;,
                        &quot;usageCommitted&quot;: 67108864
                },
                {
                        &quot;peakUsageMax&quot;: 11010048,
                        &quot;usageMax&quot;: 11010048,
                        &quot;peakUsageUsed&quot;: 0,
                        &quot;name&quot;: &quot;PS Survivor Space&quot;,
                        &quot;peakUsageCommitted&quot;: 11010048,
                        &quot;usageUsed&quot;: 0,
                        &quot;type&quot;: &quot;Heap memory&quot;,
                        &quot;usageCommitted&quot;: 11010048
                },
                {
                        &quot;peakUsageMax&quot;: 2863661056,
                        &quot;usageMax&quot;: 2863661056,
                        &quot;peakUsageUsed&quot;: 0,
                        &quot;name&quot;: &quot;PS Old Gen&quot;,
                        &quot;peakUsageCommitted&quot;: 179306496,
                        &quot;usageUsed&quot;: 0,
                        &quot;type&quot;: &quot;Heap memory&quot;,
                        &quot;usageCommitted&quot;: 179306496
                }
        ],
        &quot;processCpuLoad&quot;: 0.0008024004394748531,
        &quot;systemCpuLoad&quot;: 0.23138430784607697,
        &quot;processCpuTime&quot;: 496918000,
        &quot;appId&quot;: null,
        &quot;name&quot;: &quot;24103@machine01&quot;,
        &quot;host&quot;: &quot;machine01&quot;,
        &quot;processUuid&quot;: &quot;3c2ec835-749d-45ea-a7ec-e4b9fe17c23a&quot;,
        &quot;tag&quot;: &quot;mytag&quot;,
        &quot;gc&quot;: [
                {
                        &quot;collectionTime&quot;: 0,
                        &quot;name&quot;: &quot;PS Scavenge&quot;,
                        &quot;collectionCount&quot;: 0
                },
                {
                        &quot;collectionTime&quot;: 0,
                        &quot;name&quot;: &quot;PS MarkSweep&quot;,
                        &quot;collectionCount&quot;: 0
                }
        ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A list of all metrics and information corresponding to them can be found &lt;a href=&quot;https://gitlab.myteksi.net/olympus/iris/jvm-profiler/-/blob/master/metricDetails.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;sparkmeasure&quot;&gt;sparkMeasure&lt;/h4&gt;

&lt;p&gt;Complementing the JVM Profiler is sparkMeasure, a standalone tool that was built to robustly capture Spark job-specific metrics.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image7.png&quot; width=&quot;80%&quot; alt=&quot;&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt; Architecture of Spark Task Metrics, Listener Bus, and sparkMeasure (&lt;a href=&quot;https://github.com/LucaCanali/sparkMeasure&quot;&gt;Source&lt;/a&gt;)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It is registered as a custom listener and operates by collection built-in metrics that Spark exchanges between the driver node and executor nodes. Its standout feature is the ability to collect all metrics supported by Spark, as defined in Spark’s official documentation &lt;a href=&quot;https://spark.apache.org/docs/latest/monitoring.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Example stage metrics collected by sparkMeasure (&lt;a href=&quot;https://gitlab.myteksi.net/olympus/iris/sparkmeasure&quot;&gt;Source code&lt;/a&gt;)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Scheduling mode = FIFO

Spark Context default degree of parallelism = 8

Aggregated Spark stage metrics:

numStages =&amp;gt; 3
numTasks =&amp;gt; 17
elapsedTime =&amp;gt; 1291 (1 s)
stageDuration =&amp;gt; 1058 (1 s)
executorRunTime =&amp;gt; 2774 (3 s)
executorCpuTime =&amp;gt; 2004 (2 s)
executorDeserializeTime =&amp;gt; 2868 (3 s)
executorDeserializeCpuTime =&amp;gt; 1051 (1 s)
resultSerializationTime =&amp;gt; 5 (5 ms)
jvmGCTime =&amp;gt; 88 (88 ms)
shuffleFetchWaitTime =&amp;gt; 0 (0 ms)
shuffleWriteTime =&amp;gt; 16 (16 ms)
resultSize =&amp;gt; 16091 (15.0 KB)
diskBytesSpilled =&amp;gt; 0 (0 Bytes)
memoryBytesSpilled =&amp;gt; 0 (0 Bytes)
peakExecutionMemory =&amp;gt; 0
recordsRead =&amp;gt; 2000
bytesRead =&amp;gt; 0 (0 Bytes)
recordsWritten =&amp;gt; 0
bytesWritten =&amp;gt; 0 (0 Bytes)
shuffleRecordsRead =&amp;gt; 8
shuffleTotalBlocksFetched =&amp;gt; 8
shuffleLocalBlocksFetched =&amp;gt; 8
shuffleRemoteBlocksFetched =&amp;gt; 0
shuffleTotalBytesRead =&amp;gt; 472 (472 Bytes)
shuffleLocalBytesRead =&amp;gt; 472 (472 Bytes)
shuffleRemoteBytesRead =&amp;gt; 0 (0 Bytes)
shuffleRemoteBytesReadToDisk =&amp;gt; 0 (0 Bytes)
shuffleBytesWritten =&amp;gt; 472 (472 Bytes)
shuffleRecordsWritten =&amp;gt; 8

Stages and their duration:
Stage 0 duration =&amp;gt; 593 (0.6 s)
Stage 1 duration =&amp;gt; 416 (0.4 s)
Stage 3 duration =&amp;gt; 49 (49 ms)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;data-organisation&quot;&gt;Data organisation&lt;/h3&gt;

&lt;p&gt;The architecture of Iris is designed to efficiently route metrics to two key destinations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Real-time datasets: InfluxDB&lt;/li&gt;
  &lt;li&gt;Offline datasets: GrabTech Datalake in AWS&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;real-time-dataset&quot;&gt;Real-time dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Freshness/latency&lt;/strong&gt;: 5 to 10 seconds&lt;/p&gt;

&lt;p&gt;All metrics flowing in through Kafka topics are instantly wired into InfluxDB. A crucial part of this process is accomplished by Telegraf, a plugin-driven server agent used for collecting and sending metrics. Acting as a Kafka consumer, Telegraf listens to each Kafka topic according to its corresponding metrics profiling. It parses the incoming JSON messages and extracts crucial data points (such as role, hostname, jobname, etc.). Once the data is processed, Telegraf writes it into the InfluxDB.&lt;/p&gt;

&lt;p&gt;InfluxDB organises the stored data in what we call ‘measurements’, which could analogously be considered as tables in traditional relational databases.&lt;/p&gt;

&lt;p&gt;In Iris’s context, we have structured our real-time data into the following crucial measurements:&lt;/p&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
  &lt;td&gt;
    &lt;ol&gt;
    &lt;li&gt;&lt;code&gt;CpuAndMemory&lt;/code&gt;: This measures CPU and memory-related metrics, giving us insights into resource utilisation by Spark jobs.&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;I/O&lt;/code&gt;: This records input/output metrics, providing data on the reading and writing operations happening during the execution of jobs.&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;ThreadInfo&lt;/code&gt;: This measurement holds data related to job threading, allowing us to monitor concurrency and synchronisation aspects.&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;application_started&lt;/code&gt; and &lt;code&gt;application_ended&lt;/code&gt;: These measurements allow us to track Spark application lifecycles, from initiation to completion.&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;executors_started&lt;/code&gt; and &lt;code&gt;executors_removed&lt;/code&gt;: These measurements give us a look at the executor dynamics during Spark application execution.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/td&gt;
  &lt;td width=&quot;35%&quot;&gt;
  &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/iris/image3.png&quot; width=&quot;100%&quot; alt=&quot;&quot; /&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;ol start=&quot;6&quot;&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jobs_started&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jobs_ended&lt;/code&gt;: These provide vital data points relating to the lifecycle of individual Spark jobs within applications.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queries_started&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queries_ended&lt;/code&gt;: These measurements are designed to track the lifecycle of individual Spark SQL queries.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_metrics&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stages_started&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stages_ended&lt;/code&gt;: These measurements help monitor individual stages within Spark jobs, a valuable resource for tracking the job progress and identifying potential bottlenecks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The real-time data collected in these measurements form the backbone of the monitoring capabilities of Iris, providing an accurate and current picture of Spark job performances.&lt;/p&gt;

&lt;h4 id=&quot;offline-dataset&quot;&gt;Offline dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Freshness/latency&lt;/strong&gt;: 1 hour&lt;/p&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
  &lt;td&gt;
  In addition to real-time data management with InfluxDB, Iris is also responsible for routing metrics to our offline data storage in the Grab Tech Datalake for &lt;strong&gt;long-term trend&lt;/strong&gt; studies, &lt;strong&gt;pattern analysis&lt;/strong&gt;, and &lt;strong&gt;anomaly detection&lt;/strong&gt;.&lt;br /&gt;&lt;br /&gt;
  The metrics from Kafka are periodically synchronised to the Amazon S3 tables under the &lt;code&gt;iris&lt;/code&gt; schema in the Grab Tech AWS catalogue. This valuable historical data from Kafka is meticulously organised with a one-to-one mapping between the platform or Kafka topic to the table in the iris schema. For example: &lt;code&gt;iris.chimera_jvmprofiler_cpuandmemory&lt;/code&gt; map with &lt;code&gt;prd-iris-chimera-jvmprofiler-cpuandmemory&lt;/code&gt; Kafka topic.
  &lt;/td&gt;
  &lt;td width=&quot;35%&quot;&gt;
  &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/iris/image1.png&quot; width=&quot;100%&quot; alt=&quot;&quot; /&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This streamlined organisation means you can write queries to retrieve information from the AWS dataset very similarly to how you would do it from InfluxDB. Whether it’s CPU and memory usage, I/O, thread info, or spark metrics, you can conveniently fetch historical data for your analysis.&lt;/p&gt;

&lt;h3 id=&quot;data-visualisation&quot;&gt;Data visualisation&lt;/h3&gt;

&lt;p&gt;A well-designed visual representation makes it easier to see patterns, trends, and outliers in groups of data. Iris employs different visualisation tools based on whether the data is real-time or historical.&lt;/p&gt;

&lt;h4 id=&quot;real-time-data-visualisation---grafana&quot;&gt;Real-Time data visualisation - Grafana&lt;/h4&gt;

&lt;p&gt;Iris uses Grafana for showcasing real-time data. For each platform, two primary dashboards have been set up: JVM metrics and Spark metrics.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image11.png&quot; width=&quot;90%&quot; alt=&quot;&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;strong&gt;JVM metrics dashboard&lt;/strong&gt;: This dashboard is designed to display information related to the JVM.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image5.png&quot; width=&quot;90%&quot; alt=&quot;&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;strong&gt;Spark metrics dashboard&lt;/strong&gt;: This dashboard primarily focuses on visualising Spark-specific elements.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;offline-data-visualisation&quot;&gt;Offline data visualisation&lt;/h4&gt;

&lt;p&gt;While real-time visualisation is crucial for immediate awareness and decision-making, visualising historical data provides invaluable insights about long-term trends, patterns, and anomalies. Developers can query the raw or aggregated data from the Iris tables for their specific analyses.&lt;/p&gt;

&lt;p&gt;Moreover, to assist platform owners and end-users in obtaining a quick summary of their job data, we provide built-in dashboards with pre-aggregated visuals. These dashboards contain a wealth of information expressed in an easy-to-understand format. Key metrics include:&lt;/p&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
  &lt;td&gt;
    &lt;li&gt;Total instances&lt;/li&gt;
    &lt;li&gt;Total CPU cores&lt;/li&gt;
    &lt;li&gt;Total memory&lt;/li&gt;
    &lt;li&gt;CPU and memory utilisation&lt;/li&gt;
    &lt;li&gt;Total machine runtimes&lt;/li&gt;
  &lt;/td&gt;
  &lt;td width=&quot;80%&quot;&gt;
  &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/iris/image13.png&quot; width=&quot;70%&quot; alt=&quot;&quot; /&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Besides visualisations for individual jobs, we have designed an overview dashboard providing a comprehensive summary of all resources consumed by all ETL jobs. This is particularly useful for platform owners and tech leads, allowing them to have an all-encompassing visibility of the performance and resource usage across the ETL jobs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/iris/image15.png&quot; width=&quot;80%&quot; alt=&quot;&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Dashboard for monitoring ETL jobs&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These dashboards’ visuals effectively turn the historical metrics data into clear, comprehensible, and insightful information, guiding users towards objective-driven decision-making.&lt;/p&gt;

&lt;h2 id=&quot;transforming-observations-into-insights&quot;&gt;Transforming observations into insights&lt;/h2&gt;

&lt;p&gt;While our journey with Iris is just in the early stages, we’ve already begun harnessing its ability to transform raw data into concrete insights. The strength of Iris lies not just in its data collection capabilities but also in its potential to analyse and infer patterns from the collated data.&lt;/p&gt;

&lt;p&gt;Currently, we’re experimenting with a &lt;strong&gt;job classification&lt;/strong&gt; model that aims to predict resource allocation efficiency (i.e. identifying jobs as over or under-provisioned). This information, once accurately predicted, can help optimise the usage of resources by fine-tuning the provisions for each job. While this model is still in its early stages of testing and lacks sufficient validation data, it exemplifies the direction we’re heading - integrating advanced analytics with operational observability.&lt;/p&gt;

&lt;p&gt;As we continue to refine Iris and develop more models, our aim is to empower users with deep insights into their Spark applications. These insights can potentially identify bottlenecks, optimise resource allocation and ultimately, enhance overall performance. In the long run, we see Iris evolving from being a data collection tool to a platform that can &lt;strong&gt;provide actionable recommendations&lt;/strong&gt; and &lt;strong&gt;enable data-driven decision-making&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;job-classification-feature-set&quot;&gt;Job classification feature set&lt;/h3&gt;

&lt;p&gt;At the core of our job classification model, there are two carefully selected metrics:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CPU cores per hour&lt;/strong&gt;: This represents the number of tasks a job can handle concurrently in a given hour. A higher number would mean more tasks being processed simultaneously.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Total Terabytes of data input per core&lt;/strong&gt;: This considers only the input from the underlying HDFS/S3 input, excluding shuffle data. It represents the volume of data one CPU core needs to process. A larger input would mean more CPUs are required to complete the job in a reasonable timeframe.&lt;img src=&quot;images/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The choice of these two metrics for building feature sets is based on a nuanced understanding of Spark job dynamics:&lt;/p&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
  &lt;td&gt;
    &lt;li&gt;Allocating the right CPU cores is crucial as a higher number of cores means more tasks being processed concurrently. This is especially important for jobs with larger input data and more partitioned files, as they often require more concurrent processing capacity, hence, more CPU cores.&lt;/li&gt;&lt;br /&gt;
    &lt;li&gt;The total data input helps to estimate the data processing load of a job. A job tasked with processing a high volume of input data but assigned low CPU cores might be under-provisioned and result in an extended runtime.&lt;/li&gt;
  &lt;/td&gt;
  &lt;td width=&quot;30%&quot;&gt;
  &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/iris/image2.png&quot; width=&quot;70%&quot; alt=&quot;&quot; /&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As for CPU and memory utilisation, while it could offer useful insights, we’ve found it may not always contribute to predicting if a job is over or under-provisioned because utilisation can vary run-to-run. Thus, to keep our feature set robust and consistent, we primarily focus on CPU cores per hour and total terabytes of input data.&lt;/p&gt;

&lt;p&gt;With these metrics as our foundation, we are developing models that can classify jobs into over-provisioned or under-provisioned, helping us optimise resource allocation and improve job performance in the long run.&lt;/p&gt;

&lt;p&gt;As always, treat any information related to our job classification feature set and the insights derived from it with utmost care for data confidentiality and integrity.&lt;/p&gt;

&lt;p&gt;We’d like to reiterate that these models are still in the early stages of testing and we are constantly working to enhance their predictive accuracy. The true value of this model will be unlocked as it is refined and as we gather more validation data.&lt;/p&gt;

&lt;h3 id=&quot;model-training-and-optimisation&quot;&gt;Model training and optimisation&lt;/h3&gt;

&lt;p&gt;Choosing the right model is crucial for deriving meaningful insights from datasets. We decided to start with a simple, yet powerful algorithm - &lt;strong&gt;K-means clustering&lt;/strong&gt;, for job classification. K-means is a type of unsupervised machine learning algorithm used to classify items into groups (or clusters) based on their features.&lt;/p&gt;

&lt;p&gt;Here is our process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Model exploration&lt;/strong&gt;: We began by exploring the K-means algorithm using a small dataset for validation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Platform-specific cluster numbers&lt;/strong&gt;: To account for the uniqueness of every platform, we ran a Score Test (an evaluation method to determine the optimal number of clusters) for each platform. The derived optimal number of clusters is then used in the monthly job for that respective platform’s data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Set up a scheduled job&lt;/strong&gt;: After ensuring the code was functioning correctly, we set up a job to run the model on a monthly schedule. Monthly re-training was chosen to encapsulate possible changes in the data patterns over time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model saving and utilisation&lt;/strong&gt;: The trained model is saved to our S3 bucket and used to classify jobs as over-provisioned or under-provisioned based on the daily job runs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This iterative learning approach, through which our model learns from an ever-increasing pool of historical data, helps maintain its relevance and improve its accuracy over time.&lt;/p&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
  &lt;td&gt;
    Here is an example output from Databricks train run: &lt;br /&gt;
    &lt;li&gt;&lt;strong&gt;Blue green group&lt;/strong&gt;: Input per core is too large but the CPU per hour is small, so the job may take a lot of time to complete.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Purple group&lt;/strong&gt;: Input per core is too small but the CPU per hour is too high. There may be a lot of wasted CPU here.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Yellow group&lt;/strong&gt;: I think this is the ideal group where input per core and CPU per hour is not high.&lt;/li&gt;
  &lt;/td&gt;
  &lt;td width=&quot;45%&quot;&gt;
  &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/iris/image12.png&quot; width=&quot;70%&quot; alt=&quot;&quot; /&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Keep in mind that classification insights provided by our K-means model are still in the experimental stage. As we continue to refine the approach, the reliability of these insights is expected to grow, providing increasingly valuable direction for resource allocation optimisation.&lt;/p&gt;

&lt;h2 id=&quot;seeing-iris-in-action&quot;&gt;Seeing Iris in action&lt;/h2&gt;

&lt;p&gt;This section provides practical examples and real-case scenarios that demonstrate Iris’s capacity for delivering insights from ETL job observations.&lt;/p&gt;

&lt;h3 id=&quot;case-study-1-spark-benchmarking&quot;&gt;Case study 1: Spark benchmarking&lt;/h3&gt;

&lt;p&gt;From August to September 2023, we carried out a Spark benchmarking exercise to measure and compare the cost and performance of Grab’s Spark platforms: Open Source Spark on Kubernetes (Chimera), Databricks and AWS EMR. Since each platform has its own way to measure a job’s performance and cost, Iris was used to collect the necessary Spark metrics in order to calculate the cost for each job. Furthermore, many other metrics were collected by Iris in order to compare the platforms’ performances like CPU and memory utilisation, runtime, etc.&lt;/p&gt;

&lt;h3 id=&quot;case-study-2-improving-databricks-infra-cost-unit-dbiu-accuracy-with-iris&quot;&gt;Case study 2: Improving Databricks Infra Cost Unit (DBIU) Accuracy with Iris&lt;/h3&gt;

&lt;p&gt;Being able to accurately calculate and fairly distribute Databricks infrastructure costs has always been a challenge, primarily due to difficulties in distinguishing between on-demand and Spot instance usage. This was further complicated by two conditions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Fallback to on-demand instances&lt;/strong&gt;: Databricks has a feature that automatically falls back to on-demand instances when Spot instances are not readily available. While beneficial for job execution, this feature has traditionally made it difficult to accurately track per-job Spot vs. on-demand usage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;User configurable hybrid policy&lt;/strong&gt;: Users can specify a mix of on-demand and Spot instances for their jobs. This flexible, hybrid approach often results in complex, non-uniform usage patterns, further complicating cost categorisation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Iris has made a key difference in resolving these dilemmas. By providing granular, instance-level metrics including whether each instance is on-demand or Spot, Iris has greatly &lt;strong&gt;improved our visibility into per-job instance usage&lt;/strong&gt;.&lt;/p&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
  &lt;td&gt;
    This precise data enables us to isolate the on-demand instance usage, which was previously bundled in the total cost calculation. Similarly, it allows us to accurately gauge and consider the usage ratio of on-demand instances in hybrid policy scenarios.
  &lt;/td&gt;
  &lt;td width=&quot;60%&quot;&gt;
  &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/iris/image6.png&quot; width=&quot;70%&quot; alt=&quot;&quot; /&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The enhanced transparency provided by Iris metrics allows us to standardise DBIU cost calculations, making them fairer for users who majorly or only use Spot instances. In other words, users need to pay more if they intentionally choose or fall back to on-demand instances for their jobs.&lt;/p&gt;

&lt;p&gt;The practical application of Iris in enhancing DBIU accuracy illustrates its potential in driving data-informed decisions and fostering fairness in resource usage and cost distribution.&lt;/p&gt;

&lt;h3 id=&quot;case-study-3-optimising-job-configuration-for-better-performance-and-cost-efficiency&quot;&gt;Case study 3: Optimising job configuration for better performance and cost efficiency&lt;/h3&gt;

&lt;p&gt;One of the key utilities of iris is its potential to assist with job optimisation. For instance, we have been able to pinpoint jobs that were consistently over-provisioned and work with end-users to tune their job configurations.&lt;/p&gt;

&lt;p&gt;Through this exercise and continuous monitoring, we’ve seen substantial results from the job optimisations:&lt;/p&gt;

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
  &lt;td&gt;
    &lt;li&gt;Cost reductions ranging from &lt;strong&gt;20%&lt;/strong&gt; to &lt;strong&gt;50%&lt;/strong&gt; for most jobs.&lt;/li&gt;
    &lt;li&gt;Positive feedback from users about improvements in job performance and cost efficiency.&lt;/li&gt;
  &lt;/td&gt;
  &lt;td width=&quot;50%&quot;&gt;
  &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
    &lt;img src=&quot;/img/iris/image4.png&quot; width=&quot;70%&quot; alt=&quot;&quot; /&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;By the way, interestingly, our analysis led us to identify certain the following patterns. These patterns could be leveraged to widen the impact of our optimisation efforts across multiple use-cases in our platforms:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
   &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Pattern&lt;/strong&gt;
      &lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Recommendation&lt;/strong&gt;
      &lt;/th&gt;
   &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
      &lt;td width=&quot;50%&quot;&gt;
      &lt;li&gt;Job duration &amp;lt; 20 minutes&lt;/li&gt;
      &lt;li&gt;Input per core &amp;lt; 1GB&lt;/li&gt;
      &lt;li&gt;Total used instance is 2x/3x of max worker nodes&lt;/li&gt;
      &lt;/td&gt;
      &lt;td&gt;&lt;li&gt;Use fixed number of workers nodes potentially speeding up performance and certainly reducing costs.&lt;/li&gt;
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;&lt;li&gt;CPU utilisation &amp;lt; 25% &lt;/li&gt;
      &lt;/td&gt;
      &lt;td&gt;
      &lt;li&gt;Cut max worker in half. E.g: 10 to 5 workers&lt;/li&gt;
      &lt;li&gt;Downgrade instance size a half. E.g: 4xlarge -&amp;gt; 2xlarge&lt;/li&gt;
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;&lt;li&gt;Job has much shuffle&lt;/li&gt;
      &lt;/td&gt;
      &lt;td&gt;&lt;li&gt;Bump the instance size and reduce the number of workers. E.g. bump 2xlarge -&amp;gt; 4xlarge and reduce number of workers from 100 -&amp;gt; 50&lt;/li&gt;
      &lt;/td&gt;
   &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;However, we acknowledge that these findings may not apply uniformly to every instance. The optimisation recommendations derived from these patterns might not yield the desired outcomes in all cases.&lt;/p&gt;

&lt;h2 id=&quot;the-future-of-iris&quot;&gt;The future of Iris&lt;/h2&gt;

&lt;p&gt;Building upon its firm foundation as a robust Spark observability tool, we envision a future for Iris wherein it not only monitors metrics but provides actionable insights, discerns usage patterns, and drives predictions.&lt;/p&gt;

&lt;p&gt;Our plans to make Iris more accessible include developing &lt;strong&gt;APIs endpoint&lt;/strong&gt; for platform teams to query performance by job names. Another addition we’re aiming for is the ability for Iris to provide resource tuning recommendations. By making platform-specific and job-specific recommendations easily accessible, we hope to assist platform teams in making informed, data-driven decisions on resource allocation and cost efficiency.&lt;/p&gt;

&lt;p&gt;We’re also looking to expand Iris’s capabilities with the development of a &lt;strong&gt;listener&lt;/strong&gt; for &lt;strong&gt;Presto jobs&lt;/strong&gt;, similar to the sparkMeasure tool currently used for Spark jobs. The listener would provide valuable metrics and insights into the performance of Presto jobs, opening up new avenues for optimisation and cost management.&lt;/p&gt;

&lt;p&gt;Another major focus will be &lt;strong&gt;building a feedback loop&lt;/strong&gt; for Iris to further enhance accuracy, continually refine its models, and improve insights provided. This effort would greatly benefit from the close collaboration and inputs from platform teams and other tech leads, as their expertise aids in interpreting Iris’s metrics and predictions and validating its meaningfulness.&lt;/p&gt;

&lt;p&gt;In conclusion, as Iris continues to develop and mature, we foresee it evolving into a crucial tool for data-driven decision-making and proactive management of Spark applications, playing a significant role in the efficient usage of cloud computing resources.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The role of Iris as an observability tool for Spark jobs in the world of Big Data is rapidly evolving. Iris has proven to be more than a simple data collection tool; it is a platform that integrates advanced analytics with operational observability.&lt;/p&gt;

&lt;p&gt;Even though Iris is in its early stages, it’s already been instrumental in creating detailed visualisations of both real-time and historical data from varied platforms. Besides that, Iris has started making strides in its journey towards using machine learning models like K-means clustering to classify jobs, demonstrating its potential in helping operators fine-tune resource allocation.&lt;/p&gt;

&lt;p&gt;Using instance-level metrics, Iris is helping improve cost distribution fairness and accuracy, making it a potent tool for resource optimisation. Furthermore, the successful case study of reducing job costs and enhancing performance through resource reallocation provides a promising outlook into Iris’s future applicability.&lt;/p&gt;

&lt;p&gt;With ongoing development plans, such as the Presto listener and the creation of endpoints for broader accessibility, Iris is poised to become an integral tool for data-informed decision-making. As we strive to enhance Iris, we will continue to collaborate with platform teams and tech leads whose feedback is invaluable in fulfilling Iris’s potential.&lt;/p&gt;

&lt;p&gt;Our journey with Iris is a testament to Grab’s commitment to creating a data-informed and efficient cloud computing environment. Iris, with its observed and planned capabilities, is on its way to revolutionising the way resource allocation is managed and optimised.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Apr 2024 01:13:10 +0000</pubDate>
        <link>https://engineering.grab.com/iris</link>
        <guid isPermaLink="true">https://engineering.grab.com/iris</guid>
        
        <category>Data insights</category>
        
        <category>Metrics</category>
        
        <category>Decision making</category>
        
        <category>Analytics</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
  </channel>
</rss>
