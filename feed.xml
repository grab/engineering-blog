<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 30 Nov 2022 03:19:51 +0000</pubDate>
    <lastBuildDate>Wed, 30 Nov 2022 03:19:51 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Graph for fraud detection</title>
        <description>&lt;p&gt;In earlier articles of this series, we’ve covered the &lt;a href=&quot;/graph-networks&quot;&gt;importance of graph networks&lt;/a&gt;, &lt;a href=&quot;/graph-concepts&quot;&gt;graph concepts&lt;/a&gt; and &lt;a href=&quot;/graph-visualisation&quot;&gt;how graph visualisation makes fraud investigations easier and more effective&lt;/a&gt;. In this article, we will explore how we use graph-based models to tackle fraud detection as fraud patterns increase and diversify.&lt;/p&gt;

&lt;p&gt;Grab has grown rapidly in the past few years. It has expanded its business from ride hailing to food and grocery delivery, financial services, and more. Fraud detection is challenging in Grab, because new fraud patterns always arise whenever we introduce a new business product. We cannot afford to develop a new model whenever a new fraud pattern appears as it is time consuming and introduces a cold start problem, that is no protection at the early stage. We need a general fraud detection framework to better protect Grab from various unknown fraud risks.&lt;/p&gt;

&lt;p&gt;Our key observation is that although Grab has many different business verticals, the entities within those businesses are connected to each other (Figure 1. Left), for example, two passengers may be connected by a Wi-Fi router or phone device, a merchant may be connected to a passenger by a food order, and so on. A graph provides an elegant way to capture the spatial correlation among different entities in the Grab ecosystem. A common fraud shows clear patterns on a graph, for example, a fraud syndicate tends to share physical devices, and collusion happens between a merchant and an isolated set of passengers (Figure 1. Right).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-for-fraud-detection/fig1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Left: The graph captures different correlations in the Grab ecosystem. &lt;br /&gt; Right: The graph shows that common fraud has clear patterns.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We believe graphs can help us discover subtle traces and complicated fraud patterns more effectively. Graph-based solutions will be a sustainable foundation for us to fight against known and unknown fraud risks.&lt;/p&gt;

&lt;h2 id=&quot;why-graph&quot;&gt;Why graph?&lt;/h2&gt;

&lt;p&gt;The most common fraud detection methods include the rule engine and the decision tree-based models, for example, boosted tree, random forest, and so on. Rules are a set of simple logical expressions designed by human experts to target a particular fraud problem. They are good for simple fraud detection, but they usually do not work well in complicated fraud or unknown fraud cases.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Fraud detection methods &lt;br /&gt;&lt;br /&gt;&lt;/th&gt;
      &lt;th&gt;Utilises correlations &lt;br /&gt; (Higher is better)&lt;/th&gt;
      &lt;th&gt;Detects unknown fraud &lt;br /&gt; (Higher is better)&lt;/th&gt;
      &lt;th&gt;Requires feature engineering &lt;br /&gt; (Lower is better)&lt;/th&gt;
      &lt;th&gt;Depends on labels &lt;br /&gt; (Lower is better)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Rule engine&lt;/td&gt;
      &lt;td&gt;Low&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;Low&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Decision tree&lt;/td&gt;
      &lt;td&gt;Low&lt;/td&gt;
      &lt;td&gt;Low&lt;/td&gt;
      &lt;td&gt;High&lt;/td&gt;
      &lt;td&gt;High&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Graph model&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;High&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;High&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Low&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Low&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
Table 1. Graph vs. common fraud detection methods.&lt;/p&gt;

&lt;p&gt;Decision tree-based models have been dominating fraud detection and Kaggle competitions for structured or tabular data in the past few years. With that said, the performance of a tree-based model is highly dependent on the quality of labels and feature engineering, which is often hard to obtain in real life. In addition, it usually does not work well in unknown fraud which has not been seen in the labels.&lt;/p&gt;

&lt;p&gt;On the other hand, a graph-based model requires little amount of feature engineering and it is applicable to unknown fraud detection with less dependence on labels, because it utilises the structural correlations on the graph.&lt;/p&gt;

&lt;p&gt;In particular, fraudsters tend to show strong correlations on a graph, because they have to share physical properties such as personal identities, phone devices, Wi-Fi routers, delivery addresses, and so on, to reduce cost and maximise revenue as shown in Figure 2 (left). An example of such strong correlations is shown in Figure 2 (right), where the entities on the graph are densely connected, and the known fraudsters are highlighted in red. Those strong correlations on the graph are the key reasons that make the graph based approach a sustainable foundation for various fraud detection tasks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-for-fraud-detection/fig2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Fraudsters tend to share physical properties to reduce cost (left), and they are densely connected as shown on a graph (right).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h1 id=&quot;semi-supervised-graph-learning&quot;&gt;Semi-supervised graph learning&lt;/h1&gt;

&lt;p&gt;Unlike traditional decision tree-based models, the graph-based machine learning model can utilise the graph’s correlations and achieve great performance even with few labels. The semi-supervised Graph Convolutional Network model has been extremely popular in recent years &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. It has proven its success in many fraud detection tasks across industries, for example, e-commerce fraud, financial fraud, internet traffic fraud, etc.
We apply the Relational Graph Convolutional Network (RGCN) &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; for fraud detection in Grab’s ecosystem. Figure 3 shows the overall architecture of RGCN. It takes a graph as input, and the graph passes through several graph convolutional layers to get node embeddings. The final layer outputs a fraud probability for each node. At each graph convolutional layer, the information is propagated along the neighbourhood nodes within the graph, that is nodes that are close on the graph are similar to each other.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-for-fraud-detection/fig3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. A semi-supervised Relational Graph Convolutional Network model.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We train the RGCN model on a graph with millions of nodes and edges, where only a few percentages of the nodes on the graph have labels. The semi-supervised graph model has little dependency on the labels, which makes it a robust model for tackling various types of unknown fraud.&lt;/p&gt;

&lt;p&gt;Figure 4 shows the overall performance of the RGCN model. On the left is the Receiver Operating Characteristic (ROC) curve on the label dataset, in particular, the Area Under the Receiver Operating Characteristic (AUROC) value is close to 1, which means the RGCN model can fit the label data quite well. The right column shows the low dimensional projections of the node embeddings on the label dataset. It is clear that the embeddings of the genuine passenger are well separated from the embeddings of the fraud passenger. The model can distinguish between a fraud and a genuine passenger quite well.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-for-fraud-detection/fig4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. Left: ROC curve of the RGCN model on the label dataset. &lt;br /&gt; Right: Low dimensional projections of the graph node embeddings.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we would like to share a few tips that will make the RGCN model work well in practice.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Use less than three convolutional layers&lt;/strong&gt;: The node feature will be over-smoothed if there are many convolutional layers, that is all the nodes on the graph look similar.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Node features are important&lt;/strong&gt;: Domain knowledge of the node can be formulated as node features for the graph model, and rich node features are likely to boost the model performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;graph-explainability&quot;&gt;Graph explainability&lt;/h1&gt;

&lt;p&gt;Unlike other deep network models, graph neural network models usually come with great explainability, that is why a user is classified as fraudulent. For example, fraudulent accounts are likely to share hardware devices and form dense clusters on the graph, and those fraud clusters can be easily spotted on a graph visualiser &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Figure 5 shows an example where graph visualisation helps to explain the model prediction scores. The genuine passenger with a low RGCN score does not share devices with other passengers, while the fraudulent passenger with a high RGCN score shares devices with many other passengers, that is, dense clusters.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-for-fraud-detection/fig5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Upper left: A genuine passenger with a low RGCN score has no device sharing with other passengers. Bottom right: A fraudulent user with a high RGCN score shares devices with many other passengers.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h1 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h1&gt;

&lt;p&gt;Graphs provide a sustainable foundation for combating many different types of fraud risks. Fraudsters are evolving very fast these days, and the best traditional rules or models can do is to chase after those fraudsters given that a fraud pattern has already been discovered. This is suboptimal as the damage has already been done on the platform. With the help of graph models, we can potentially detect those fraudsters before any fraudulent activity has been conducted, thus reducing the fraud cost.&lt;/p&gt;

&lt;p&gt;The graph structural information can significantly boost the model performance without much dependence on labels, which is often hard to get and might have a large bias in fraud detection tasks. We have shown that with only a small percentage of labelled nodes on the graph, our model can already achieve great performance.&lt;/p&gt;

&lt;p&gt;With that said, there are also many challenges to making a graph model work well in practice. We are working towards solving the following challenges we are facing.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Feature initialisation&lt;/strong&gt;: Sometimes, it is hard to initialise the node feature, for example, a device node does not carry many semantic meanings. We have explored self-supervised pre-training &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; to help the feature initialisation, and the preliminary results are promising.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time model prediction&lt;/strong&gt;: Realtime graph model prediction is challenging because real-time graph updating is a heavy operation in most cases. One possible solution is to do batch real-time prediction to reduce the overhead.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noisy connections&lt;/strong&gt;: Some connections on the graph are inherently noisy on the graph, for example, two users sharing the same IP address does not necessarily mean they are physically connected. The IP might come from a mobile network. One possible solution is to use the attention mechanism in the graph convolutional kernel and control the message passing based on the type of connection and node profiles.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;T. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in ICLR, 2017 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Schlichtkrull, Michael, et al. “Modeling relational data with graph convolutional networks.” European semantic web conference. Springer, Cham, 2018. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Fujiao Liu, Shuqi Wang, et al.. “&lt;a href=&quot;https://engineering.grab.com/graph-visualisation&quot;&gt;Graph Networks - 10X investigation with Graph Visualisations&lt;/a&gt;”. Grab Tech Blog. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wang, Chen, et al.. “Deep Fraud Detection on Non-attributed Graph.” IEEE Big Data conference, PSBD, 2021. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 24 Nov 2022 00:13:40 +0000</pubDate>
        <link>https://engineering.grab.com/graph-for-fraud-detection</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-for-fraud-detection</guid>
        
        <category>Analytics</category>
        
        <category>Data Science</category>
        
        <category>Security</category>
        
        <category>Graphs</category>
        
        <category>Graph visualisation</category>
        
        <category>Graph networks</category>
        
        <category>Fraud detection</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Query expansion based on user behaviour</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Our consumers used to face a few common pain points while searching for food with the Grab app. Sometimes, the results would include merchants that were not yet operational or locations that were out of the delivery radius. Other times, no alternatives were provided. The search system would also have difficulties handling typos, keywords in different languages, synonyms, and even word spacing issues, resulting in a suboptimal user experience.&lt;/p&gt;

&lt;p&gt;Over the past few months, our search team has been building a query expansion framework that can solve these issues. When a user query comes in, it expands the query to a few related keywords based on semantic relevance and user intention. These expanded words are then searched with the original query to recall more results that are high-quality and diversified. Now let’s take a deeper look at how it works.&lt;/p&gt;

&lt;h1 id=&quot;query-expansion-framework&quot;&gt;Query expansion framework&lt;/h1&gt;

&lt;h2 id=&quot;building-the-query-expansion-corpus&quot;&gt;Building the query expansion corpus&lt;/h2&gt;

&lt;p&gt;We used two different approaches to produce query expansion candidates: manual annotation for top keywords and data mining based on user rewrites.&lt;/p&gt;

&lt;h3 id=&quot;manual-annotation-for-top-keywords&quot;&gt;Manual annotation for top keywords&lt;/h3&gt;

&lt;p&gt;Search has a pronounced fat head phenomenon. The most frequent thousand of keywords account for more than 70% of the total search traffic. Therefore, handling these keywords well can improve the overall search quality a lot. We manually annotated the possible expansion candidates for these common keywords to cover the most popular merchants, items and alternatives. For instance, “McDonald’s” is annotated with {“burger”, “western”}.&lt;/p&gt;

&lt;h3 id=&quot;data-mining-based-on-user-rewrites&quot;&gt;Data mining based on user rewrites&lt;/h3&gt;

&lt;p&gt;We observed that sometimes users tend to rewrite their queries if they are not satisfied with the search result. As a pilot study, we checked the user rewrite records within some user search sessions and found several interesting samples:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{Ya Kun Kaya Toast,Starbucks}
{healthy,Subway}
{Muni,Muji}
{奶茶,koi}
{Roti,Indian}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see that besides spelling corrections, users’ rewrite behaviour also reveals deep semantic relations between these pairs that cannot be easily captured by lexical similarity, such as similar merchants, merchant attributes, language differences, cuisine types, and so on. We can leverage the user’s knowledge to build a query expansion corpus to improve the diversity of the search result and user experience. Furthermore, we can use the wisdom of the crowd to find some common patterns with higher confidence.&lt;/p&gt;

&lt;p&gt;Based on this intuition, we leveraged the high volume of search click data available in Grab to generate high-quality expansion pairs at the user session level. To augment the original queries, we collected rewrite pairs that happened to multiple users and multiple times in a time period. Specifically, we used the heuristic rules below to collect the rewrite pairs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Select the sessions where there are at least two distinct queries (rewrite session)&lt;/li&gt;
  &lt;li&gt;Collect adjacent query pairs in the search session where the second query leads to a click but the first does not (effective rewrite)&lt;/li&gt;
  &lt;li&gt;Filter out the sample pairs with time interval longer than 30 seconds in between, as users are more likely to change their mind on what to look for in these pairs (single intention)&lt;/li&gt;
  &lt;li&gt;Count the occurrences and filter out the low-frequency pairs (confidence management)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After we have the mining pairs, we categorised and annotated the rewrite types to gain a deeper understanding of the user’s rewrite behaviour. A few samples mined from the Singapore area data are shown in the table below.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;Original query&lt;/th&gt;
    &lt;th&gt;Rewrite query&lt;/th&gt;
    &lt;th&gt;Frequency in a month&lt;/th&gt;
    &lt;th&gt;Distinct user count&lt;/th&gt;
    &lt;th&gt;Type&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td&gt;playmade by 丸作&lt;/td&gt;
    &lt;td&gt;playmade&lt;/td&gt;
    &lt;td&gt;697&lt;/td&gt;
    &lt;td&gt;666&lt;/td&gt;
    &lt;td&gt;Drop keywords&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;mcdonald's&lt;/td&gt;
    &lt;td&gt;burger&lt;/td&gt;
    &lt;td&gt;573&lt;/td&gt;
    &lt;td&gt;535&lt;/td&gt;
    &lt;td&gt;Merchant -&amp;gt; Food&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Bubble tea&lt;/td&gt;
    &lt;td&gt;koi&lt;/td&gt;
    &lt;td&gt;293&lt;/td&gt;
    &lt;td&gt;287&lt;/td&gt;
    &lt;td&gt;Food -&amp;gt; Merchant&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Kfc&lt;/td&gt;
    &lt;td&gt;McDonald's&lt;/td&gt;
    &lt;td&gt;238&lt;/td&gt;
    &lt;td&gt;234&lt;/td&gt;
    &lt;td&gt;Merchant -&amp;gt; Merchant&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;cake&lt;/td&gt;
    &lt;td&gt;birthday cake&lt;/td&gt;
    &lt;td&gt;206&lt;/td&gt;
    &lt;td&gt;205&lt;/td&gt;
    &lt;td&gt;Add words&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;麦当劳&lt;/td&gt;
    &lt;td&gt;mcdonald's&lt;/td&gt;
    &lt;td&gt;205&lt;/td&gt;
    &lt;td&gt;199&lt;/td&gt;
    &lt;td&gt;Locale change&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;4 fingers&lt;/td&gt;
    &lt;td&gt;4fingers&lt;/td&gt;
    &lt;td&gt;165&lt;/td&gt;
    &lt;td&gt;162&lt;/td&gt;
    &lt;td&gt;Space correction&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;krc&lt;/td&gt;
    &lt;td&gt;kfc&lt;/td&gt;
    &lt;td&gt;126&lt;/td&gt;
    &lt;td&gt;124&lt;/td&gt;
    &lt;td&gt;Spelling correction&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;5 guys&lt;/td&gt;
    &lt;td&gt;five guys&lt;/td&gt;
    &lt;td&gt;120&lt;/td&gt;
    &lt;td&gt;120&lt;/td&gt;
    &lt;td&gt;Number synonym&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;koi the&lt;/td&gt;
    &lt;td&gt;koi thé&lt;/td&gt;
    &lt;td&gt;45&lt;/td&gt;
    &lt;td&gt;44&lt;/td&gt;
    &lt;td&gt;Tone change&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We further computed the percentages of some categories, as shown in the figure below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/query-expansion-based-on-user-behaviour/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;Figure 1. The donut chart illustrates the percentages of the distinct user counts for different types of rewrites.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Apart from adding words, dropping words and spelling corrections, a significant portion of the rewrites are in the category of Other. It is more semantic driven, such as merchant to merchant, or merchant to cuisine. Those rewrites are useful for capturing deeper connections between queries and can be a powerful diversifier to query expansion.&lt;/p&gt;

&lt;h3 id=&quot;grouping&quot;&gt;Grouping&lt;/h3&gt;

&lt;p&gt;After all the rewrite pairs were discovered offline through data mining, we grouped the query pairs by the original query to get the expansion candidates of each query. For serving efficiency, we limited the max number of expansion candidates to three.&lt;/p&gt;

&lt;h2 id=&quot;query-expansion-serving&quot;&gt;Query expansion serving&lt;/h2&gt;

&lt;h3 id=&quot;expansion-matching-architecture&quot;&gt;Expansion matching architecture&lt;/h3&gt;

&lt;p&gt;The expansion matching architecture benefits from the recent search architecture upgrade, where the system flow is changed to a query understanding, multi-recall and result fusion flow. In particular, a query goes through the query understanding module and gets augmented with additional information. In this case, the query understanding module takes in the keyword and expands it to multiple synonyms, for example, KFC will be expanded to fried chicken. The original query together with its expansions are sent together to the search engine under the multi-recall framework. After that, results from multiple recallers with different keywords are fused together.&lt;/p&gt;

&lt;h3 id=&quot;continuous-monitoring-and-feedback-loop&quot;&gt;Continuous monitoring and feedback loop&lt;/h3&gt;

&lt;p&gt;It’s important to make sure the expansion pairs are relevant and up-to-date. We run the data mining pipeline periodically to capture the new user rewrite behaviours. Meanwhile, we also monitor the expansion pairs’ contribution to the search result by measuring the net contribution of recall or user interaction that the particular query brings, and eliminate the obsolete pairs in an automatic way. This reflects our effort to build an adaptive system.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;We conducted online A/B experiments across 6 countries in Southeast Asia to evaluate the expanded queries generated by our system. We set up 3 groups:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Control group, where no query is expanded.&lt;/li&gt;
  &lt;li&gt;Treatment group 1, where we expanded the queries based on manual annotations only.&lt;/li&gt;
  &lt;li&gt;Treatment group 2, where we expanded the queries using the data mining approach.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We observed decent uplift in click-through rate and conversion rate from both treatment groups. Furthermore, in treatment group 2, the data mining approach produced even better results.&lt;/p&gt;

&lt;h1 id=&quot;future-work&quot;&gt;Future work&lt;/h1&gt;

&lt;h2 id=&quot;data-mining-enhancement&quot;&gt;Data mining enhancement&lt;/h2&gt;

&lt;p&gt;Currently, the data mining approach can only identify the pairs from the same search session by one user. This limits the number of linked pairs. Some potential enhancements include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Augment expansion pairs by associating queries from different users who click on the same merchant/item, for example, using a click graph. This can capture relevant queries across user sessions.&lt;/li&gt;
  &lt;li&gt;Build a probabilistic model on top of the current transition pairs. Currently, all the transition pairs are equally weighted but apparently, the transitions that happen more often should carry higher probability/weights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ads-application&quot;&gt;Ads application&lt;/h2&gt;

&lt;p&gt;Query expansion can be applied to advertising and would increase ads fill rate. With “KFC” expanded to “fried chicken”, the sponsored merchants who buy the keyword “fried chicken” would be eligible to show up when the user searches “KFC”. This would enable Grab to provide more relevant sponsored content to our users, which helps not only the consumers but also the merchants.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Zhengmin Xu and Daniel Ng for proofreading this article.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Nov 2022 00:29:00 +0000</pubDate>
        <link>https://engineering.grab.com/query-expansion-based-on-user-behaviour</link>
        <guid isPermaLink="true">https://engineering.grab.com/query-expansion-based-on-user-behaviour</guid>
        
        <category>Analytics</category>
        
        <category>Data Science</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Using mobile sensor data to encourage safer driving</title>
        <description>&lt;p&gt;“Telematics”, a cross between the words &lt;em&gt;telecommunications&lt;/em&gt; and &lt;em&gt;informatics&lt;/em&gt;, was coined in the late 1970s to refer to the use of communication technologies in facilitating exchange of information. In the modern day, such technologies may include cloud platforms, mobile networks, and wireless transmissions (e.g., Bluetooth). Although the initial intention is for a more general scope, telematics is now specifically used to refer to vehicle telematics where details of vehicle movements are tracked for use cases such as driving safety, driver profiling, fleet optimisation, and productivity improvements.&lt;/p&gt;

&lt;p&gt;We’ve previously published &lt;a href=&quot;https://engineering.grab.com/telematics-at-grab&quot;&gt;this article&lt;/a&gt; to share how Grab uses telematics to improve driver safety. In this blog post, we dive deeper into how telematics technology is used at Grab to encourage safer driving for our driver and delivery partners.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;At Grab, the safety of our users and their experience on our platform is our highest priority. By encouraging safer driving habits from our driver and delivery partners, road traffic accidents can be minimised, potentially reducing property damage, injuries, and even fatalities. Safe driving also helps ensure smoother rides and a more pleasant experience for consumers using our platform.&lt;/p&gt;

&lt;p&gt;To encourage safer driving, we should:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Have a data-driven approach to understand how our driver and delivery partners are driving.&lt;/li&gt;
  &lt;li&gt;Help partners better understand how to improve their driving by summarising key driving history into a personalised &lt;strong&gt;Driving Safety Report&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;understanding-driving-behaviour&quot;&gt;Understanding driving behaviour&lt;/h1&gt;

&lt;p&gt;One of the most direct forms of driving assessment is consumer feedback or complaints. However, the frequency and coverage of this feedback is not very high as they are only applicable to transport verticals like JustGrab or GrabBike and not delivery verticals like GrabFood or GrabExpress. Plus, most driver partners tend not to receive any driving-related feedback (whether positive or negative), even for the transport verticals.&lt;/p&gt;

&lt;p&gt;A more comprehensive method of assessing driving behaviour is to use the driving data collected during Grab bookings. To make sense of these data, we focus on selected driving manoeuvres (e.g., braking, acceleration, cornering, speeding) and detect the number of instances where our data shows unsafe driving in each of these areas.&lt;/p&gt;

&lt;p&gt;We acknowledge that the detected instances may be subjected to errors and may not provide the complete picture of what’s happening on the ground (e.g., partners may be forced to do an emergency brake due to someone swerving into their lane).&lt;/p&gt;

&lt;p&gt;To address this, we have incorporated several fail-safe checks into our detection logic to minimise erroneous detection. Also, any assessment of driving behaviour will be based on an aggregation of these unsafe driving instances over a large amount of driving data. For example, individual harsh braking instances may be inconclusive but if a driver partner displays multiple counts consistently across many bookings, it is likely that the partner may be used to unsafe driving practices like tailgating or is distracted while driving.&lt;/p&gt;

&lt;h1 id=&quot;telematics-for-detecting-unsafe-driving&quot;&gt;Telematics for detecting unsafe driving&lt;/h1&gt;

&lt;p&gt;For Grab to consistently ensure our consumers’ safety, we need to proactively detect unsafe driving behaviour before an accident occurs. However, it is not feasible for someone to be with our driver and delivery partners all the time to observe their driving behaviour. We should leverage sensor data to monitor these driving behaviour at scale.&lt;/p&gt;

&lt;p&gt;Traditionally, a specialised “black box” inertial measurement unit (IMU) equipped with sensors such as accelerometers, gyroscopes, and GPS needs to be installed in alignment with the vehicle to directly measure vehicular acceleration and speed. In this manner, it would be straightforward to detect unsafe driving instances using this data. Unfortunately, the cost of purchasing and installing such devices for all our partners is prohibitively high and it would be hard to scale.&lt;/p&gt;

&lt;p&gt;Instead, we can leverage a device that all partners already have: their mobile phone. Modern smartphones already contain similar sensors to those in IMUs and data can be collected through the telematics SDK. More details on telematics data collection can be found in a recently published Grab tech blog article&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;It’s important to note that telematics data are collected at a sufficiently high sampling frequency (much more than 1 Hz) to minimise inaccuracies in detecting unsafe driving instances characterised by sharp acceleration impulses.&lt;/p&gt;

&lt;h2 id=&quot;processing-mobile-sensor-data-to-detect-unsafe-driving&quot;&gt;Processing mobile sensor data to detect unsafe driving&lt;/h2&gt;

&lt;p&gt;Unlike specialised IMUs installed in vehicles, mobile sensor data have added challenges to detecting unsafe driving.&lt;/p&gt;

&lt;h3 id=&quot;accounting-for-orientation-phone-vs-vehicle&quot;&gt;Accounting for orientation: Phone vs. vehicle&lt;/h3&gt;

&lt;p&gt;The phone is usually in a different orientation compared to the vehicle. Strictly speaking, the phone accelerometer sensor measures the accelerations of the phone and not the vehicle acceleration. To infer vehicle acceleration from phone sensor data, we developed a customised processing algorithm optimised specifically for Grab’s data.&lt;/p&gt;

&lt;p&gt;First, the orientation offset of the phone with respect to the vehicle is defined using Euler angles: roll, pitch and yaw. In data windows with no net acceleration of the vehicle (e.g., no braking, turning motion), the only acceleration measured by the accelerometer is gravitational acceleration. Roll and pitch angles can then be determined through trigonometric manipulation. The complete triaxial accelerations of the phone are then rotated to the horizontal plane and the yaw angle is determined by principal component analysis (PCA).&lt;/p&gt;

&lt;p&gt;An assumption here is that there will be sufficient braking and acceleration manoeuvring for PCA to determine the correct forward direction. This Euler angles determination is done periodically to account for any movement of phones during the trip. Finally, the raw phone accelerations are rotated to the vehicle orientation through a matrix multiplication with the rotation matrix derived from the Euler angles (see Figure 1).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-mobile-sensor-data-to-encourage-safer-driving/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Inference of vehicle acceleration from the phone sensor data. &lt;a href=&quot;https://www.freepik.com/free-psd/isometric-minimal-3d-phone-with-rrss-interface-mockup-levitating_13195223.htm&quot;&gt;Smartphone&lt;/a&gt; and &lt;a href=&quot;https://www.freepik.com/free-vector/white-sports-car-isolated-white-vector_3529812.htm&quot;&gt;car&lt;/a&gt; images modified from designs found in Freepik.com.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;handling-variations-in-data-quality&quot;&gt;Handling variations in data quality&lt;/h3&gt;

&lt;p&gt;Our processing algorithm is optimised to be highly robust and handle large variations in data quality that is expected from bookings on the Grab platform. There are many reported methods for processing mobile data to reorientate telematics data for four wheel vehicles&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;However, with the prevalent use of motorcycles on our platform, especially for delivery verticals, we observed that data collected from two wheel vehicles tend to be noisier due to differences in phone stability and vehicular vibrations. Data noise can be exacerbated if partners hold the phone in their hand or place it in their pockets while driving.&lt;/p&gt;

&lt;p&gt;In addition, we also expect a wide variation in data quality and sensor availability from different phone models, such as older, low-end models to the newest, flagship models. A good example to illustrate the robustness of our algorithm is having different strategies to handle different degrees of data noise. For example, a simple low-pass filter is used for low noise data, while more complex variational decomposition and Kalman filter approaches are used for high noise data.&lt;/p&gt;

&lt;h3 id=&quot;detecting-behaviour-anomalies-with-thresholds&quot;&gt;Detecting behaviour anomalies with thresholds&lt;/h3&gt;

&lt;p&gt;Once the vehicular accelerations are inferred, we can use a thresholding approach (see Figure 2) to detect unsafe driving instances.&lt;/p&gt;

&lt;p&gt;For unsafe acceleration and braking, a peak finding algorithm is used to detect acceleration peaks beyond a threshold in the longitudinal (forward/backward) direction. For unsafe cornering, older and lower end phones are usually not equipped with gyroscope sensors, so we should look for peaks of lateral (sidewards) acceleration (which constitutes the centripetal acceleration during the turn) beyond a threshold. GPS bearing data that coarsely measures the orientation of the vehicle is then used to confirm that a cornering and not lane change instance is being detected. The thresholds selected are fine-tuned on Grab’s data using initial values based on published literature&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; and other sources.&lt;/p&gt;

&lt;p&gt;To reduce false positive detection, no unsafe driving instances will be flagged when:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Large discrepancies are observed between speeds derived from integrating the longitudinal (forward/backward) acceleration and speeds directly measured by the GPS sensor.&lt;/li&gt;
  &lt;li&gt;Large phone motions are detected. For example, when the phone falls to the seat from the dashboard, accelerations recorded on the phone sensor will deviate significantly from the vehicle accelerations.&lt;/li&gt;
  &lt;li&gt;GPS speed is very low before and after the unsafe driving instance is detected. This is limited to data collected from motorcycles which is usually used by delivery partners. It implies that the partner is walking and not in a vehicle. For example, a GrabFood delivery partner may be collecting the food from the merchant partner on foot, so no unsafe driving instances should be detected.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-mobile-sensor-data-to-encourage-safer-driving/image2.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Animation showing unsafe driving detection by thresholding. Dotted lines in acceleration charts indicate selected thresholds. Map tiles by stamen design.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;detecting-speeding-instances-from-gps-speeds-and-map-data&quot;&gt;Detecting speeding instances from GPS speeds and map data&lt;/h2&gt;

&lt;p&gt;To define speeding along a stretch of road, we used a rule-based method by comparing raw speeds from GPS pings with speeding thresholds for that road. Although GPS speeds are generally accurate (subjected to minimal GPS errors), we need to take more precautions to ensure the right speeding thresholds are determined.&lt;/p&gt;

&lt;p&gt;These thresholds are set using known speed limits from available map data or hourly aggregated speed statistics where speed limits are not available. The coverage and accuracy of known speed limits is continuously being improved by our in-house mapping initiatives and  validated comprehensively by the respective local ground teams in selected cities.&lt;/p&gt;

&lt;p&gt;Aggregating GPS pings from Grab driver and delivery partners can be a helpful proxy to actual speed limits by defining speeding violations as outliers from socially acceptable speeds derived from partners collectively. To reliably compute aggregated speed statistics, a representative speed profile for each stretch of road must first be inferred from raw GPS pings (see Figure 3).&lt;/p&gt;

&lt;p&gt;As ping sampling intervals are fixed, more pings tend to be recorded for slower speeds. To correct the bias in the speed profile, we reweigh ping counts by using speed values as weights. Furthermore, to minimise distortions in the speed profile from vehicles driving at lower-than-expected speeds due to high traffic volumes, only pings from free-flowing traffic are used when inferring the speed profile.&lt;/p&gt;

&lt;p&gt;Free-flowing traffic is defined by speeds higher than the median speed on each defined road category (e.g., small residential roads, normal primary roads, large expressways). To ensure extremely high speeds are flagged regardless of the speed of other drivers, maximum threshold values for aggregated speeds are set for each road category using heuristics based on the maximum known speed limit of that road category.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-mobile-sensor-data-to-encourage-safer-driving/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Steps to infer a representative speed profile for computing aggregated speed statistics.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Besides a representative speed profile, hourly aggregation should also include data from a sufficient number of unique drivers depending on speed variability. To obtain enough data, hourly aggregations are performed on the same day of the week over multiple weeks. This way, we have a comprehensive time-specific speed profile that accounts for traffic quality (e.g., peak hour traffic, traffic differences between weekdays/weekends) and driving conditions (e.g., visibility difference between day/night).&lt;/p&gt;

&lt;p&gt;When detecting speeding violations, the GPS pings used are snapped-to-road and stationary pings, pings with unrealistic speeds, while pings with low GPS accuracy (e.g., when the vehicle is in a tunnel) are excluded. A speeding violation is defined as a sequence of consecutive GPS pings that exceed the speeding threshold. The following checks were put in place to minimise erroneous flagging of speeding violations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Removal of duplicated (or stale) GPS pings.&lt;/li&gt;
  &lt;li&gt;Sufficient speed buffer given to take into account GPS errors.&lt;/li&gt;
  &lt;li&gt;Sustained speeding for a prolonged period of time is required to exclude transient speeding events (e.g., during lane change).&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;driving-safety-report&quot;&gt;Driving safety report&lt;/h1&gt;

&lt;p&gt;The driving safety report is a platform safety product that driver and delivery partners can access via their driver profile page on the Grab Driver Application (see Figure 4). It is updated daily and aims to create awareness regarding driving habits by summarising key information from the processed data into a personalised report that can be easily consumed.&lt;/p&gt;

&lt;p&gt;Individual reports of each driving manoeuvre (e.g., braking, acceleration, cornering and speeding) are available for daily and weekly views. Partners can also get more detailed information of each individual instance such as when these unsafe driving instances were detected.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-mobile-sensor-data-to-encourage-safer-driving/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Driving safety report for driver and delivery partners using four wheel vehicles. a) Actionable insights feature circled by red dotted lines. b) Daily view of various unsafe driving instances where more details of each instance can be viewed by tapping on “See details”.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;actionable-insights&quot;&gt;Actionable insights&lt;/h2&gt;

&lt;p&gt;Besides compiling the instances of unsafe driving in a report to create awareness, we are also using these data to provide some actionable recommendations for our partners to improve their driving.&lt;/p&gt;

&lt;p&gt;With unsafe driving feedback from consumers and reported road traffic accident data from our platform, we also train machine learning models to identify patterns in the detected unsafe driving instances and estimate the likelihood of partners receiving unsafe driving feedback or getting into accidents. One use case is to compute a safe driving score that equates a four-wheel partner’s driving behaviour to a numerical value where a higher score indicates a safer driver.&lt;/p&gt;

&lt;p&gt;Additionally, we use Shapley additive explanation (SHAP) approaches to determine which driving manoeuvre contributes the most to increasing the likelihood of partners receiving unsafe driving feedback or getting into accidents. This information is included as an actionable insight in the driving safety report and helps partners to identify the key area to improve their driving.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h1&gt;

&lt;p&gt;At the moment, Grab performs telematics processing and unsafe driving detections after the trip and updates the report the next day. One of the biggest improvements would be to share this information with partners faster. We are actively working on developing a real-time processing algorithm that addresses this and also, satisfies the robustness requirements such that partners are immediately aware after an unsafe driving instance is detected.&lt;/p&gt;

&lt;p&gt;Besides detecting typical unsafe driving manoeuvres, we are also exploring other use cases for mobile sensor data in road safety such as detection of poor road conditions, counterflow driving against traffic, and phone usage leading to distracted driving.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Burhan, W. (2022). How telematics helps Grab to improve safety. Grab Tech Blog. https://engineering.grab.com/telematics-at-grab &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Mohan, P., Padmanabhan, V.N. and Ramjee, R. (2008).Nericell: rich monitoring of road and traffic conditions using mobile smartphones. SenSys ‘08: Proceedings of the 6th ACM conference on Embedded network sensor systems, 312-336. https://doi.org/10.1145/1460412.1460444 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sentiance (2016). Driving behavior modeling using smart phone sensor data. Sentiance Blog. https://sentiance.com/2016/02/11/driving-behavior-modeling-using-smart-phone-sensor-data/ &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yarlagadda, J. and Pawar, D.S. (2022). Heterogeneity in the Driver Behavior: An Exploratory Study Using Real-Time Driving Data. Journal of Advanced Transportation. vol. 2022, Article ID 4509071. https://doi.org/10.1155/2022/4509071 &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 25 Oct 2022 11:30:00 +0000</pubDate>
        <link>https://engineering.grab.com/using-mobile-sensor-data-to-encourage-safer-driving</link>
        <guid isPermaLink="true">https://engineering.grab.com/using-mobile-sensor-data-to-encourage-safer-driving</guid>
        
        <category>Analytics</category>
        
        <category>Driving patterns</category>
        
        <category>Data Science</category>
        
        <category>GPS</category>
        
        <category>Security</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Automatic rule backtesting with large quantities of data</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Analysts need to analyse and simulate a rule on historical data to check the performance and accuracy of the rule. Backtesting enables analysts to run simulations of the rules and manage the results from the rule engine UI.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Backtesting&lt;/strong&gt; helps analysts to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Define the desired impact of the rule for our business and users.&lt;/li&gt;
  &lt;li&gt;Evaluate the accuracy of the rule based on historical data.&lt;/li&gt;
  &lt;li&gt;Compare and analyse results with data points, such as known false positives, user segments, risk profile of a user or transaction, and so on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Currently, the analytics process to test performance of a rule is not standardised, and is inaccurate and inefficient. Analysts from different teams have different approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Offline process using Presto tables. This process is lengthy and inaccurate.&lt;/li&gt;
  &lt;li&gt;Offline process based on the rule engine payload. The setup takes time, and the process is not streamlined.&lt;/li&gt;
  &lt;li&gt;Running rules in shadow mode. This process takes days to get the desired result.&lt;/li&gt;
  &lt;li&gt;A team in Grab uses different rule engines to manage rules and do backtesting. This doubles the effort for analysts and engineers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In our vision for backtesting, it should allow analysts to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Efficiently run and manage their jobs.&lt;/li&gt;
  &lt;li&gt;Create custom metrics, reports and dimensions for backtesting.&lt;/li&gt;
  &lt;li&gt;Add external data points and metrics to do a deep dive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the purpose of establishing a minimum viable product (MVP), backtesting will support basic capabilities and enable analysts to access required metrics and data points. Thus, analysts can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Run backtesting jobs from the rule engine UI.&lt;/li&gt;
  &lt;li&gt;Get fixed reports and dimensions for every checkpoint.&lt;/li&gt;
  &lt;li&gt;Get access to relevant data to analyse backtesting results.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Assume a simple use case: &lt;strong&gt;A rule to detect the transaction risk.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Each transaction has a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transaction_id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;currency&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;amount&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;timestamp&lt;/code&gt;. The rule engine also provides a &lt;em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;treatment&lt;/code&gt;&lt;/em&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Approve&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Decline&lt;/code&gt;) based on the rule logic for the transaction.&lt;/p&gt;

&lt;p&gt;In this specific use case, we would like to see what will be the aggregation number of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;total transactions&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;total distinct users&lt;/code&gt;, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum of the amount&lt;/code&gt;, based on the dimensions of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;date&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;treatment&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;and currency&lt;/code&gt; in the last couple of weeks.&lt;/p&gt;

&lt;p&gt;The result may look like the following data:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dimension    &lt;/th&gt;
      &lt;th&gt;Dimension    &lt;/th&gt;
      &lt;th&gt;Dimension    &lt;/th&gt;
      &lt;th&gt;metric      &lt;/th&gt;
      &lt;th&gt;metric          &lt;/th&gt;
      &lt;th&gt;metric     &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Date&lt;/td&gt;
      &lt;td&gt;Treatment&lt;/td&gt;
      &lt;td&gt;Currency&lt;/td&gt;
      &lt;td&gt;Total tx&lt;/td&gt;
      &lt;td&gt;Distinct user    &lt;/td&gt;
      &lt;td&gt;Total amount&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2020-05-1&lt;/td&gt;
      &lt;td&gt;Approve&lt;/td&gt;
      &lt;td&gt;SGD&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;10020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2020-05-1&lt;/td&gt;
      &lt;td&gt;Decline&lt;/td&gt;
      &lt;td&gt;SGD&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;450&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2020-05-1&lt;/td&gt;
      &lt;td&gt;Approve&lt;/td&gt;
      &lt;td&gt;MYR&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;1200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2020-05-1&lt;/td&gt;
      &lt;td&gt;Decline&lt;/td&gt;
      &lt;td&gt;MYR&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;400&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;* This data does not reflect actual Grab data and is for illustrative purposes only.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use a cloud-agnostic Spark-based data pipeline to replay any existing or proposed rule to check performance.&lt;/li&gt;
  &lt;li&gt;Use a Web Portal to:
    &lt;ul&gt;
      &lt;li&gt;Create or select a rule to replay, with replay time range.&lt;/li&gt;
      &lt;li&gt;Display and download the result, such as total events and hit counts.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Replay any existing or proposed rule for checking performance.&lt;/li&gt;
  &lt;li&gt;Allow users to create or select a rule to replay in the rule engine UI, with provided replay time range.&lt;/li&gt;
  &lt;li&gt;Display the replay result in the rule engine UI, such as total events and hit counts.&lt;/li&gt;
  &lt;li&gt;Provide a way to download all testing results in the rule engine UI (for example, all rule responses).&lt;/li&gt;
  &lt;li&gt;Remove dependency on the specific cloud provider stack, so other teams in Grab can use it instead of Google Cloud Platform (GCP).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-rule-testing/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The rule editor UI reacts to the user input. Its engine sends a job command to the Amazon Simple Queue Service (SQS) to initialise the job. After that, the rule editor also performs the following processes in the background:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lambda listens to the request SQS queue and invokes a job via the Spark jobs API.&lt;/li&gt;
  &lt;li&gt;The job fetches the executable artifacts, data source. After the job is completed, the job script saves the result sheet as required to S3.&lt;/li&gt;
  &lt;li&gt;The Spark script pushes the job final status (success, failure, timeout) through the shutdown hook to respond to the SQS queue.&lt;/li&gt;
  &lt;li&gt;The rule editor engine listens to response callback messages, and processes the job metadata to the database, or sends notifications.&lt;/li&gt;
  &lt;li&gt;The rule editor displays the job metadata on the UI.&lt;/li&gt;
  &lt;li&gt;The package pipeline builds and deploys the executable artifacts to S3 as a manageable structure.&lt;/li&gt;
  &lt;li&gt;The Spark script takes the filter logic as its input parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;workflow&quot;&gt;Workflow&lt;/h3&gt;

&lt;h4 id=&quot;historical-data-preparation&quot;&gt;Historical data preparation&lt;/h4&gt;

&lt;p&gt;The historical events are published by the rule engine through Kafka, and stored into the S3 bucket based on time. The Backtesting system then fetches these data for testing based on the time range requested.&lt;/p&gt;

&lt;p&gt;By using a Kubernetes stream pipeline, we also save the trust inference stream to &lt;em&gt;Trust AWS subaccount&lt;/em&gt;. With the customer bucket and file format, we can improve the efficiency of the data processing, and also avoid any delay from the data lake.&lt;/p&gt;

&lt;h4 id=&quot;engineering-specifications&quot;&gt;Engineering specifications&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Target location:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    s3a://stg-trust-inference-event/&amp;lt;engine-name&amp;gt;/&amp;lt;predict-name&amp;gt;/&amp;lt;YYYY&amp;gt;/MM/DD/hh/mm/ss/&amp;lt;000001&amp;gt;.snappy.parquet
    s3a://prd-trust-inference-event/&amp;lt;engine-name&amp;gt;/&amp;lt;predict-name&amp;gt;/&amp;lt;YYYY&amp;gt;/MM/DD/hh/mm/ss/&amp;lt;000001&amp;gt;.snappy.parquet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Following the fields of steam definition, the engine name would be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ruleengine&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;catwalk&lt;/code&gt;. The predict-name would be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preride&lt;/code&gt; (checkpoint name), or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cnpu&lt;/code&gt; (model name).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;File Format: avro&lt;/li&gt;
  &lt;li&gt;File Compression: Snappy&lt;/li&gt;
  &lt;li&gt;There is no auto retention on sub-account S3. We will implement the archive process in the future. &lt;/li&gt;
  &lt;li&gt;The default pipeline and the new pipeline will run in parallel until the Data Engineering team is ready to retire the default pipeline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;backtesting&quot;&gt;Backtesting&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Upon scheduling, the Backtesting Portal sends a message to SQS, which is then captured by the listening Lambda.&lt;/li&gt;
  &lt;li&gt;Lambda invokes a Spark job over the AWS elastic mapreduce engine (EMR).&lt;/li&gt;
  &lt;li&gt;The EMR engine fetches the executable artifacts containing the rule script and historical data from S3, and starts a Spark job to apply the rule script over historical data. Depending on the size of data, the Spark cluster will scale automatically to ensure timely completion.&lt;/li&gt;
  &lt;li&gt;Once completed, a report file is generated and available on Backtesting UI.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ui&quot;&gt;UI&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-rule-testing/image1.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;learnings-and-conclusions&quot;&gt;Learnings and conclusions&lt;/h2&gt;

&lt;p&gt;After the release, here’s what our data analysers had to say:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For trust analysts, testing a rule on historical data happens outside the rule engine UI and is not user-friendly, leading to analysts wasting significant time.&lt;/li&gt;
  &lt;li&gt;For financial analysts, as analysts migrate to the rule engine UI, the existing solution will be deprecated with no other solution.&lt;/li&gt;
  &lt;li&gt;An alternative to simulate a rule;  we no longer need to run a rule in shadow mode because we can use historical data to determine the outcome. This new approach saves us weeks of effort on the rule onboarding process.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The underlying Spark jobs in this tool were developed by knowledgeable data engineers, which is a disadvantage because it requires a high level of expertise to modify the analytics. To mitigate this restriction, we are looking into using domain-specific language (DSL) to allow users to input desired attributes and dimensions, and provide the job release pipeline for self-serving jobs.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Thanks to Jia Long Loh for the support on the offline infrastructure engineering.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 08 Sep 2022 00:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/automatic-rule-backtesting</link>
        <guid isPermaLink="true">https://engineering.grab.com/automatic-rule-backtesting</guid>
        
        <category>Testing</category>
        
        <category>Automation</category>
        
        <category>Backtesting</category>
        
        <category>Data science</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we store and process millions of orders daily</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In the real world, after a passenger places a GrabFood order from the Grab App, the merchant-partner will prepare the order. A driver-partner will then collect the food and deliver it to the passenger. Have you ever wondered what happens in the backend system? The Grab Order Platform is a distributed system that processes millions of GrabFood or GrabMart orders every day. This post aims to share the journey of how we designed the database solution that powers the order platform.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;What are the design goals when building the database solution? We collected the requirements by analysing query patterns and traffic patterns.&lt;/p&gt;

&lt;h3 id=&quot;query-patterns&quot;&gt;Query patterns&lt;/h3&gt;

&lt;p&gt;Here are some important query examples that the Order Platform supports:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Write queries:&lt;/p&gt;

    &lt;p&gt;a.  Create an order.&lt;/p&gt;

    &lt;p&gt;b.  Update an order.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Read queries:&lt;/p&gt;

    &lt;p&gt;a.  Get order by id.&lt;/p&gt;

    &lt;p&gt;b.  Get ongoing orders by passenger id.&lt;/p&gt;

    &lt;p&gt;c.  Get historical orders by various conditions.&lt;/p&gt;

    &lt;p&gt;d.  Get order statistics (for example, get the number of orders)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can break down queries into two categories: transactional queries and analytical queries. Transactional queries are critical to online order creation and completion, including the write queries and read queries such as 2a or 2b. Analytical queries like 2c and 2d retrieves historical orders or order statistics on demand. Analytical queries are not essential to the oncall order processing.&lt;/p&gt;

&lt;h3 id=&quot;traffic-patterns&quot;&gt;Traffic patterns&lt;/h3&gt;

&lt;p&gt;Grab’s Order Platform processes a significant amount of transaction data every month.&lt;/p&gt;

&lt;p&gt;During peak hours, the write Queries per Second (QPS) is three times of primary key reads; whilst the range Queries per Second are four times of the primary key reads.&lt;/p&gt;

&lt;h3 id=&quot;design-goals&quot;&gt;Design goals&lt;/h3&gt;

&lt;p&gt;From the query and traffic patterns, we arrived at the following three design goals:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt; - the database solution must be able to handle high read and write QPS. Online order processing queries must have high availability. Even when some part of the system is down, we must be able to provide a degraded experience to the end users allowing them to still be able to create and complete an order.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability and cost&lt;/strong&gt; - the database solution must be able to support fast evolution of business requirements, given now we handle up to a million orders per month. The solution must also be cost effective at a large scale.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt; - strong consistency for transactional queries, and eventually consistency for analytical queries.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;solution&quot;&gt;Solution&lt;/h1&gt;

&lt;p&gt;The first design principle towards a stable and scalable database solution is to use different databases to serve transactional and analytical queries, also known as OLTP and OLAP queries. An OLTP database serves queries critical to online order processing. This table keeps data for only a short period of time. Meanwhile, an OLAP database has the same set of data, but serves our historical and statistical queries. This database keeps data for a longer time.&lt;/p&gt;

&lt;p&gt;What are the benefits from this design principle? From a stability point of view, we can choose different databases which can better fulfil our different query patterns and QPS requirements. An OLTP database is the single source of truth for online order processing; any failure in the OLAP database will not affect online transactions. From a scalability and cost point of view, we can choose a flexible database for OLAP to support our fast evolution of business requirements. We can maintain less data in our OLTP database while keeping some older data in our OLAP database.&lt;/p&gt;

&lt;p&gt;To ensure that the data in both databases are consistent, we introduced the second design principle - data ingestion pipeline. In Figure 1, Order Platform writes data to the OLTP database to process online orders and asynchronously pushes the data into the data ingestion pipeline. The data ingestion pipeline ensures that the OLAP database data is eventually consistent.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-store-millions-orders/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Order Platform database solution overview&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h2&gt;

&lt;h3 id=&quot;oltp-database&quot;&gt;OLTP database&lt;/h3&gt;

&lt;p&gt;There are two categories of OLTP queries, the key-value queries (for example, load by order id) and the batch queries (for example, Get ongoing orders by passenger id). We use DynamoDB as the database to support these OLTP queries.&lt;/p&gt;

&lt;p&gt;Why DynamoDB?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Scalable and highly available: the tables of DynamoDB are partitioned and each partition is three-way replicated.&lt;/li&gt;
  &lt;li&gt;Support for strong consistent reads by primary key.&lt;/li&gt;
  &lt;li&gt;DynamoDB has a mechanism called adaptive capacity to handle hotkey traffic. Internally, DynamoDB will distribute higher capacity to high-traffic partitions, and isolate frequently accessed items to a dedicated partition. This way, the hotkey can utilise the full capacity of an entire partition, which is up to 3000 read capacity units and 1000 write capacity units.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-store-millions-orders/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: DynamoDB table structure overview. Source:  &lt;a href=&quot;#aws&quot;&gt;Amazon Web Services (2019, 28 April)&lt;/a&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In each DynamoDB table, it has many items with attributes. In each item, it has a partition key and sort key. The partition key is used for key-value queries, and the sort key is used for range queries. In our case, the table contains multiple order items. The partition key is order ID. We can easily support key-value queries by the partition key.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;order_id (PK)&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;pax_id&lt;/th&gt;
      &lt;th&gt;created_at&lt;/th&gt;
      &lt;th&gt;pax_id_gsi&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;order1&lt;/td&gt;
      &lt;td&gt;Ongoing&lt;/td&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;9:00am&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;order2&lt;/td&gt;
      &lt;td&gt;Ongoing&lt;/td&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;9:30am&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;order3&lt;/td&gt;
      &lt;td&gt;Completed&lt;/td&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;8:30am&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Batch queries like ‘Get ongoing orders by passenger id’ are supported by DynamoDB Global Secondary Index (GSI). A GSI is like a normal DynamoDB table, which also has keys and attributes.&lt;/p&gt;

&lt;p&gt;In our case, we have a GSI table where the partition key is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pax_id_gsi&lt;/code&gt;. The attribute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pax_id_gsi&lt;/code&gt; is linked to the main table. It is eventually consistent with the main table that is maintained by DynamoDB. If the Order Platform queries ongoing orders for Alice, two items will be returned from the GSI table.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;pax_id_gsi (PK)&lt;/th&gt;
      &lt;th&gt;created_at (SK)&lt;/th&gt;
      &lt;th&gt;order_id&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;9:00am&lt;/td&gt;
      &lt;td&gt;order1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;9:30am&lt;/td&gt;
      &lt;td&gt;order2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We also make use of an advanced feature of GSI named sparse index to support ongoing order queries. When we update order status from ongoing to completed, at the same time, we set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pax_id_gsi&lt;/code&gt; to empty, so that the linked item in the GSI will be automatically deleted by DynamoDB. At any time, the GSI table only stores the ongoing orders. We use a sparse index mechanism to control our table size for better performance and to be more cost effective.&lt;/p&gt;

&lt;p&gt;The next problem is data retention. This is achieved with the DynamoDB Time To Live (TTL) feature. DynamoDB will auto-scan expired items and delete them. But the challenge is when we add TTL to big tables, it will bring a heavy load to the background scanner and might result in an outage. Our solution is to only add a TTL attribute to the new items in the table. Then, we manually delete the items without TTL attributes, and run a script to delete items with TTL attributes that are too old. After this process, the table size will be quite small, so we can enable the TTL feature on the TTL attribute that we previously added without any concern. The retention period of our DynamoDB data is three months.&lt;/p&gt;

&lt;p&gt;Costwise, DynamoDB is charged by storage size and the provision of the read write capability. The provision capability is actually auto scalable. The cost is on-demand. So it’s generally cheaper than RDS.&lt;/p&gt;

&lt;h3 id=&quot;olap-database&quot;&gt;OLAP database&lt;/h3&gt;

&lt;p&gt;We use MySQL RDS as the database to support historical and statistical OLAP queries.&lt;/p&gt;

&lt;p&gt;Why not Aurora? We choose RDS mainly because it is a mature database solution. Even if Aurora can provide better high-availability, RDS is enough to support our less critical use cases. Costwise, Aurora charges by data storage and the number of requested Input/Output Operations per Second (IOPS). RDS charges only by data storage. As we are using General Purpose (SSD) storage, IOPS is free and supports up to 16k IOPS.&lt;/p&gt;

&lt;p&gt;We use MySQL partitioning for data retention. The order table is partitioned by creation time monthly. Since the data access pattern is mostly by month, the partition key can reduce cross-partition queries. Partitions older than six months are dropped at the beginning of each month.&lt;/p&gt;

&lt;h3 id=&quot;data-ingestion-pipeline&quot;&gt;Data ingestion pipeline&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-store-millions-orders/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Data Ingestion Pipeline Architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;A Kafka stream is used to process data in the data ingestion pipeline. We choose the Kafka stream, because it has 99.95% SLA. It is not restricted by the OLTP and OLAP database types.&lt;/p&gt;

&lt;p&gt;Even if Kafka can provide 99.95% SLA, there is still the chance of stream producer failures. When the producer fails, we will store the message in an Amazon Simple Queue Service (SQS) and retry. If the retry also fails, it will be moved to the SQS dead letter queue (DLQ), to be consumed at a later time.&lt;/p&gt;

&lt;p&gt;On the stream consumer side, we use back-off retry at both stream and database levels to ensure consistency. In a worst-case scenario, we can rewind the stream events from Kafka.&lt;/p&gt;

&lt;p&gt;It is important for the data ingestion pipeline to handle duplicate messages and out-of-order messages.&lt;/p&gt;

&lt;p&gt;Duplicate messages are handled by the database level unique key (for example, order ID + creation time).&lt;/p&gt;

&lt;p&gt;For the out-of-order messages, we implemented the following two mechanisms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Version update: we only update the most recently updated data. The precision of the update time is in microseconds, which is enough for most of the use cases.&lt;/li&gt;
  &lt;li&gt;Upsert: if the update events occur before the create events, we simulate an upsert operation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;After launching our solution this year, we have saved significantly on cloud costs. In the earlier solution, Order Platform synchronously writes to DynamoDB and Aurora and the data is kept forever.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In terms of stability, we use DynamoDB as the critical OLTP database to ensure high availability for online order processing. Scalability wise, we use RDS as the OLAP database to support our quickly evolving business requirements by using a rich, multiple index. Cost efficiency is achieved by data retention in both databases. For consistency, we built a single source of truth OLTP database and an OLAP database that is eventually consistent with the help of the data ingestion pipeline.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h1&gt;

&lt;p&gt;Currently, the database solution is running on the production environment. Even though the database solution is proven to be stable, scalable and consistent, we still see some potential areas of improvement.&lt;/p&gt;

&lt;p&gt;We use MySQL RDS for OLAP data storage. Even though MySQL is stable and cost effective, it is difficult to serve more complicated queries like free text search. Hence, we plan to explore other NoSQL databases like ElasticSearch.&lt;/p&gt;

&lt;p&gt;We hope this post helps you understand how we store Grab orders and fulfil the queries from the Grab Order Platform.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;!-- Figure 2, DynamoDB overview is taken from [Getting Started with Amazon DynamoDB](https://www.slideshare.net/AmazonWebServices/getting-started-with-amazon-dynamodb-63948367), authored by Amazon Web Services. --&gt;

&lt;p&gt;&lt;a id=&quot;aws&quot;&gt;Amazon Web Services. (2019, 28 April) &lt;em&gt;Build with DynamoDB: S1 E1 – Intro to Amazon DynamoDB&lt;/em&gt; [Video]. &lt;a href=&quot;https://youtu.be/W3S1OnDqWl4?t=455&quot;&gt;YouTube&lt;/a&gt;.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Aug 2022 00:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/how-we-store-millions-orders</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-we-store-millions-orders</guid>
        
        <category>Database</category>
        
        <category>Storage</category>
        
        <category>Distributed Systems</category>
        
        <category>Platform</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we automated FAQ responses at Grab</title>
        <description>&lt;h2 id=&quot;overview-and-initial-analysis&quot;&gt;Overview and initial analysis&lt;/h2&gt;

&lt;p&gt;Knowledge management is often one of the biggest challenges most companies face internally. Teams spend several working hours trying to either inefficiently look for information or constantly asking colleagues about information already documented somewhere. A lot of time is spent on the internal employee communication channels (in our case, Slack) simply trying to figure out answers to repetitive questions. On our journey to automate the responses to these repetitive questions, we needed first to figure out exactly how much time and effort is spent by on-call engineers answering such repetitive questions.&lt;/p&gt;

&lt;p&gt;We soon identified that many of the internal engineering tools’ on-call activities involve answering users’ (internal users) questions on various Slack channels. Many of these questions have already been asked or documented on the wiki. These inquiries hinder on-call engineers’ productivity and affect their ability to focus on operational tasks. Once we figured out that on-call employees spend a lot of time answering Slack queries, we decided on a journey to determine the top questions.&lt;/p&gt;

&lt;p&gt;We considered smaller groups of teams for this study and found out that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The topmost user queries are “How do I do ABC?” or “Is XYZ broken?”.&lt;/li&gt;
  &lt;li&gt;The second most commonly asked questions revolve around access requests, approvals, or other permissions. The answer to such questions is often URLs to existing documentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These findings informed us that we didn’t just need an artificial intelligence (AI) based autoresponder to repetitive questions. We must, in fact, also leverage these channels’ chat histories to identify patterns.&lt;/p&gt;

&lt;h2 id=&quot;gathering-user-votes-for-shortlisted-vendors&quot;&gt;Gathering user votes for shortlisted vendors&lt;/h2&gt;

&lt;p&gt;In light of saving costs and time and considering the quality of existing solutions already available in the market, we decided not to reinvent the wheel and instead purchase an existing product. And to figure out which product to purchase, we needed to do a comparative analysis. And thus began our vendor comparison journey!&lt;/p&gt;

&lt;p&gt;While comparing the feature sets offered by different vendors, we understood that our users need to play a part in this decision-making process. However, sharing our vendor analysis with our users and allowing them to choose the bot of their choice posed several challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Users could be biased towards known bots (from previous experiences).&lt;/li&gt;
  &lt;li&gt;Users could be biased towards big brands with a preconceived notion that big brands mean better features and better user support.&lt;/li&gt;
  &lt;li&gt;Users may likely pick the most expensive vendor, assuming that a higher cost means higher efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To ensure that we receive unbiased feedback, here’s how we opened users up to voting. We highlighted the top features of each vendor’s bot compared to other shortlisted bots. We hid the names of the bots to avoid brand attraction. At a high level, here’s what the categorisation looked like:&lt;/p&gt;

&lt;table border=&quot;1&quot; style=&quot;text-align:center;&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Vendor 1 (name  hidden)&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Vendor 2 (name  hidden)&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Vendor 3 (name  hidden)&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Enables crowdsourcing, everyone is incentivised to participate. &lt;br /&gt; Participants/SME names are visible. &lt;br /&gt; Everyone can access the web UI and see how the responses configured on the bot.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Lowers discussions on channels by providing easy ways to raise tickets to the team instead of discussing on Slack.&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Only a specific set of admins (or oncall engineers) feed and maintain the bot thus ensuring information authenticity and reliability.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Easy bot feeding mechanism/web UI to update FAQs.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Superior natural language processing capabilities.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;    
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Please vote&lt;/td&gt;
    &lt;td&gt;Vendor 1&lt;/td&gt;
    &lt;td&gt;Vendor 2&lt;/td&gt;
    &lt;td&gt;Vendor 3&lt;/td&gt;    
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Although none of the options had all the features our users wanted, &lt;strong&gt;&lt;em&gt;about 60% chose Vendor 1 (&lt;a href=&quot;https://onebar.io/&quot;&gt;OneBar&lt;/a&gt;)&lt;/em&gt;&lt;/strong&gt;. From this, we discovered the core features that our users needed while keeping them involved in the decision-making process.&lt;/p&gt;

&lt;h3 id=&quot;matching-our-requirements-with-available-vendors-feature-sets&quot;&gt;Matching our requirements with available vendors’ feature sets&lt;/h3&gt;

&lt;p&gt;Although our users made their preferences clear, we still needed to ensure that the feature sets available in the market suited our internal requirements in terms of the setup and the features available in portals that we envisioned replacing. As part of our requirements gathering process, here are some of the critical conditions that became more and more prominent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An ability to crowdsource Slack discussions/conclusions and save them directly from Slack (preferably with a single command).&lt;/li&gt;
  &lt;li&gt;An ability to auto-respond to Slack queries without calling the bot manually.&lt;/li&gt;
  &lt;li&gt;The bot must be able to respond to queries only on the preconfigured Slack channel (not a Slack-wide auto-responder that is already available).&lt;/li&gt;
  &lt;li&gt;Ability to auto-detect frequently asked questions on the channels would mean less work for platform engineers to feed the bot manually and periodically.&lt;/li&gt;
  &lt;li&gt;A trusted and secured data storage setup and a responsive customer support team.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proof-of-concept&quot;&gt;Proof of concept&lt;/h2&gt;

&lt;p&gt;We considered several tools (including some of the tools used by our HR for auto-answering employee questions). We then decided to do a complete proof of concept (POC) with OneBar to check if it fulfils our internal requirements.&lt;/p&gt;

&lt;p&gt;These were the phases in which we conducted the POC for the shortlisted vendor (OneBar):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 1&lt;/strong&gt;: Study the traffic, see what insights OneBar shows and what it could/should potentially show. Then think about how an ideal oncall or support should behave in such an environment. i.e. we could identify specific messages in history and describe what should’ve happened to each one of them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 2&lt;/strong&gt;: Create required records in OneBar and configure it to match the desired behaviour as closely as possible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 3&lt;/strong&gt;: Let the tool run for a couple of weeks and then evaluate how well it responds to questions, how often people search directly, how much information they add, etc. Onebar adds all these metrics in the app making it easier to monitor activity.&lt;/p&gt;

&lt;p&gt;In addition to the Onebar POC, we investigated other solutions and did a thorough vendor comparison and analysis. After running the POC and investigating other vendors, we decided to use OneBar as its features best meet our needs.&lt;/p&gt;

&lt;h3 id=&quot;prioritising-slack-channels&quot;&gt;Prioritising Slack channels&lt;/h3&gt;

&lt;p&gt;While we had multiple Slack channels that we’d love to have enabled the shortlisted bot on, our initial contract limited our use of the bot to only 20 channels. We could not use OneBar to auto-scan more than 20 Slack channels.&lt;/p&gt;

&lt;p&gt;Users could still chat directly with the bot to get answers to FAQs based on what was fed to the bot’s knowledge base (KB). They could also access the web login, which displays its KB, other valuable features, and additional features for admins/experts.&lt;/p&gt;

&lt;p&gt;Slack channels that we enabled the licensed features on were prioritised based on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Most messages sent on the channel per month, i.e. most active channels.&lt;/li&gt;
  &lt;li&gt;Most members impacted, i.e. channels with a large member count.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To do this, we used Slack analytics reports and identified the channels that fit our prioritisation criteria.&lt;/p&gt;

&lt;h3 id=&quot;change-is-difficult-but-often-essential&quot;&gt;Change is difficult but often essential&lt;/h3&gt;

&lt;p&gt;Once we’d onboarded the vendor, we began training and educating employees on using this new Knowledge Management system for all their FAQs. It was a challenge as change is always complex but essential for growth.&lt;/p&gt;

&lt;p&gt;A series of tech talks and training conducted across the company and at more minor scales also helped guide users about the bot’s features and capabilities.&lt;/p&gt;

&lt;p&gt;At the start, we suffered from a lack of data resulting in incorrect responses from the bot. But as the team became increasingly aware of the features and learned more about its capabilities, the bot’s number of KB items grew, resulting in a much more efficient experience. It took us around one quarter to feed the bot consistently to see accurate and frequent responses from it.&lt;/p&gt;

&lt;h2 id=&quot;crowdsourcing-our-internal-glossary&quot;&gt;Crowdsourcing our internal glossary&lt;/h2&gt;

&lt;p&gt;With an increasing number of acronyms and company-specific words emerging each year, the number of acronyms and company-specific abbreviations that new joiners face is immense.&lt;/p&gt;

&lt;p&gt;We solved this issue by using the bot’s channel-specific KB feature. We created a specific Slack channel dedicated to storing and retrieving definitions of acronyms and other words. This solution turned out to be a big hit with our users.&lt;/p&gt;

&lt;p&gt;And who fed the bot with the terms and glossary items? Who better than our onboarding employees to train the bot to help other onboarders. A targeted campaign dedicated to feeding the bot excited many of our onboarders. They began to play around with the bot’s features and provide it with as many glossary items as possible, thus winning swags!&lt;/p&gt;

&lt;p&gt;In a matter of weeks, the user base grew from a couple of hundred to around 3000. This effort was also called out in one of our company-wide All Hands meetings, a big win for our team!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jul 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/automated-faq</link>
        <guid isPermaLink="true">https://engineering.grab.com/automated-faq</guid>
        
        <category>Automation</category>
        
        <category>Knowledge management</category>
        
        <category>Productivity</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Graph Networks - 10X investigation with Graph Visualisations</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Detecting fraud schemes used to require investigations using large amounts and varying types of data that come from many different anti-fraud systems. Investigators then need to combine the different types of data and use statistical methods to uncover suspicious claims, which is time consuming and inefficient in most cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We are always looking for ways to improve fraud investigation methods and stay one step ahead of our ever-growing fraudsters. In the &lt;a href=&quot;/graph-networks&quot;&gt;introductory blog&lt;/a&gt; and &lt;a href=&quot;/graph-concepts&quot;&gt;graph concepts&lt;/a&gt; articles of this series, we’ve covered experimenting with a set of Graph Network technologies, including Graph Visualisation, and the basics of graph concepts.&lt;/p&gt;

&lt;p&gt;In this post, we will introduce our Graph Visualisation Platform and briefly illustrate how it makes fraud investigations easier and more effective.&lt;/p&gt;

&lt;h2 id=&quot;why-visualise-a-graph&quot;&gt;Why visualise a graph?&lt;/h2&gt;

&lt;p&gt;If you’re a fan of crime shows, you would have come across scenes like a detective putting together evidence, such as pictures, notes and articles, on a board and connecting them with thumb tacks and yarn. When you look at the board, it’s easy to see the relationships between the different pieces of evidence. That’s what graphs do, especially in fraud detection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In the same way, while graph data is the raw material of an investigation, some of the most interesting relationships are often inferred rather than modelled directly in the data. Visualising these relationships can give a unique “big picture” of the data that is difficult or impossible to obtain with traditional relational tables and business intelligence tools.&lt;/p&gt;

&lt;p&gt;On the other hand, graph visualisation enhances the quick identification of relationships and significant structures because it is an intuitive way to help detect patterns. Plus, the human brain processes visual information much faster; that’s where our Graph Visualisation platform comes in.&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-graph-visualisation-platform&quot;&gt;What is the Graph Visualisation platform?&lt;/h2&gt;

&lt;p&gt;Graph Visualisation platform is a full-featured investigation platform that can reveal hidden connections and context in data by transforming raw records into highly visual and interactive maps. From there, investigators can grab any data point and quickly see relationships, patterns, and anomalies, and if necessary, drill down to investigate further.&lt;/p&gt;

&lt;p&gt;This is all done without writing a manual query, switching between anti-fraud systems, or having to think about data science! These are some of the interactions on the platform that easily make anomalies or relevant patterns stand out.&lt;/p&gt;

&lt;h3 id=&quot;expanding-the-data&quot;&gt;Expanding the data&lt;/h3&gt;

&lt;p&gt;To date, we have over three billion nodes and edges in our storage system. It is not possible (nor necessary) to show all of the data at once. The platform allows the user to grab any data point and easily expand to view the relationships.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/expand-data.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;timeline-tracking-and-history-replay&quot;&gt;Timeline tracking and history replay&lt;/h3&gt;

&lt;p&gt;The Graph Visualisation platform’s interactive time filter lets you see temporal relationships within your data and clearly reveals the chronological progression of events. You can start with a specific time of interest, track everything that happens after, then quickly focus on the time and relationships that matter most.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/data-replay.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;10x-investigations&quot;&gt;10X investigations&lt;/h2&gt;

&lt;p&gt;Here are a few examples of how the Graph Visualisation platform facilitates fraud investigations.&lt;/p&gt;

&lt;h3 id=&quot;appeal-confirmation&quot;&gt;Appeal confirmation&lt;/h3&gt;

&lt;p&gt;The following image shows the difference between a true fraudster and a falsely identified one. On the left, we have a Grab rental corporate account that was falsely detected by a fraud rule. Upon review, we discovered that there is no suspicious connection to this account, thus the account got unblocked.&lt;/p&gt;

&lt;p&gt;On the right, we have a passenger that was blocked by the system and they appealed. Investigations showed that the passenger is, in fact, part of an extremely dense device-sharing network, so we maintained our decision to block.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;modus-operandi-discovery&quot;&gt;Modus operandi discovery&lt;/h3&gt;

&lt;h4 id=&quot;passenger-sharing-device&quot;&gt;Passenger sharing device&lt;/h4&gt;

&lt;p&gt;Fraudsters tend to share physical resources to maximise their revenue. With our Graph Visualisation platform, you can see exactly how this pattern looks like. The image below shows a device that is shared by a lot of fraudsters.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;anti-money-laundering-aml&quot;&gt;Anti-money laundering (AML)&lt;/h4&gt;

&lt;p&gt;On the left, we see a pattern of healthy spending on Grab. However, on the right, we can see that passengers are highly connected, and it has frequent large amount transfers to other payment providers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;Graph Visualisation is an intuitive way to investigate suspicious connections and potential patterns of crime. Investigators can directly interact with any data point to get the details they need and literally view the relationships in the data to make fast, accurate, and defensible decisions.&lt;/p&gt;

&lt;p&gt;While fraud detection is a good use case for Graph Visualisation, it’s not the only possibility. Graph Visualisation can help make anything more efficient and intelligent, especially if you have highly connected data.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Jun 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/graph-visualisation</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-visualisation</guid>
        
        <category>Security</category>
        
        <category>Graphs concepts</category>
        
        <category>Graph technology</category>
        
        <category>Graph visualisation</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How facial recognition technology keeps you safe</title>
        <description>&lt;p&gt;Facial recognition technology is one of the many modern technologies that previously only appeared in science fiction movies. The roots of this technology can be traced back to the 1960s and have since grown dramatically due to the rise of deep learning techniques and accelerated digital transformation in recent years.&lt;/p&gt;

&lt;p&gt;In this blog post, we will talk about the various applications of facial recognition technology in Grab, as well as provide details of the technical components that build up this technology.&lt;/p&gt;

&lt;h2 id=&quot;application-of-facial-recognition-technology-&quot;&gt;Application of facial recognition technology  &lt;/h2&gt;

&lt;p&gt;At Grab, we believe in &lt;strong&gt;prevention&lt;/strong&gt;, &lt;strong&gt;protection&lt;/strong&gt;, and &lt;strong&gt;action&lt;/strong&gt; to create a safer every day for our consumers, partners, and the community as a whole. All selfies collected by Grab are handled according to Grab’s Privacy Policy and securely protected under privacy legislation in the countries in which we operate. We will elaborate in detail in a section further below.&lt;/p&gt;

&lt;p&gt;One key incident prevention method is to &lt;strong&gt;verify&lt;/strong&gt; the &lt;strong&gt;identity&lt;/strong&gt; of both our consumers and partners:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From the perspective of protecting the safety of &lt;strong&gt;passengers&lt;/strong&gt;, having a reliable driver authentication process can avoid unauthorized people from delivering a ride. This ensures that trips on Grab are only completed by registered licensed driver-partners that have passed our comprehensive background checks.&lt;/li&gt;
  &lt;li&gt;From the perspective of protecting the safety of &lt;strong&gt;driver-partners&lt;/strong&gt;, verifying the identity of new passengers using facial recognition technology helps to deter crimes targeting our driver-partners and make incident investigations easier.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image7.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image9.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Safety incidents that arise from lack of identity verification&lt;/i&gt;&lt;/figcaption&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Facial recognition technology is also leveraged to improve Grab digital financial services, particularly in facilitating the “electronic Know Your Customer” (&lt;strong&gt;e-KYC&lt;/strong&gt;) process. KYC is a standard regulatory requirement in the financial services industry to verify the identity of customers, which commonly serves to deter financial crime, such as money laundering.&lt;/p&gt;

&lt;p&gt;Traditionally, customers are required to visit a physical counter to verify their government-issued ID as proof of identity. Today, with the widespread use of mobile devices, coupled with the maturity of facial recognition technologies, the process has become much more seamless and can be done entirely digitally.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1: GrabPay wallet e-KYC regulatory requirements in the Philippines&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;overview-of-facial-recognition-technology&quot;&gt;Overview of facial recognition technology&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2: Face recognition flow&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The typical facial recognition pipeline involves multiple stages, which starts with &lt;strong&gt;image preprocessing&lt;/strong&gt;, &lt;strong&gt;face anti-spoof&lt;/strong&gt;, followed by &lt;strong&gt;feature extraction&lt;/strong&gt;, and finally the downstream applications - &lt;strong&gt;face verification&lt;/strong&gt; or &lt;strong&gt;face search&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The most common image preprocessing techniques for face recognition tasks are &lt;strong&gt;face detection&lt;/strong&gt; and &lt;strong&gt;face alignment&lt;/strong&gt;. The face detection algorithm locates the face region in an image, and is usually followed by face alignment, which identifies the key facial landmarks (e.g. left eye, right eye, nose, etc.) and transforms them into a standardised coordinate space. Both of these preprocessing steps aim to ensure a consistent quality of input data for downstream applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Face anti-spoof&lt;/strong&gt; refers to the process of ensuring that the user-submitted facial image is legitimate. This is to prevent fraudulent users from &lt;strong&gt;stealing identities&lt;/strong&gt; (impersonating someone else by using a printed photo or replaying videos from mobile screens) or &lt;strong&gt;hiding identities&lt;/strong&gt; (e.g. wearing a mask). The main approach here is to extract low-level spoofing cues, such as the &lt;em&gt;moiré&lt;/em&gt; pattern, using various machine learning techniques to determine whether the image is spoofed.&lt;/p&gt;

&lt;p&gt;After passing the anti-spoof checks, the user-submitted images are sent for &lt;strong&gt;face feature extraction&lt;/strong&gt;, where important features that can be used to distinguish one person from another are extracted. Ideally, we want the feature extraction model to produce embeddings (i.e. high-dimensional vectors) with &lt;strong&gt;small intra-class distance&lt;/strong&gt; (i.e. faces of the same person) and &lt;strong&gt;large inter-class distance&lt;/strong&gt; (i.e. faces of different people), so that the aforementioned downstream applications (i.e. face verification and face search) become a straightforward task - thresholding the distance between embeddings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Face verification&lt;/strong&gt; is one of the key applications of facial recognition and it answers the question, “&lt;em&gt;Is this the same person?&lt;/em&gt;”. As previously alluded to, this can be achieved by comparing the distance between embeddings generated from a template image (e.g. government-issued ID or profile picture) and a query image submitted by the user. A short distance indicates that both images belong to the same person, whereas a large distance indicates that these images are taken from different people.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Face search&lt;/strong&gt;, on the other hand, tackles the question, “&lt;em&gt;Who is this person?&lt;/em&gt;”, which can be framed as a vector/embedding similarity search problem. Image embeddings belonging to the same person would be highly similar, thus ranked higher, in search results. This is particularly useful for deterring criminals from re-onboarding to our platform by blocking new selfies that match a criminal profile in our criminal denylist database.&lt;/p&gt;

&lt;h4 id=&quot;face-anti-spoof&quot;&gt;Face anti-spoof&lt;/h4&gt;

&lt;p&gt;For face anti-spoof, the most common methods used to attack the facial recognition system are screen replay and printed paper. To distinguish these spoof attacks from genuine faces, we need to solve two main challenges.&lt;/p&gt;

&lt;p&gt;The first challenge is to obtain enough data of spoof attacks to enable the training of models. The second challenge is to carefully train the model to focus on the subtle differences between spoofed and genuine cases instead of overfitting to other background information.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image10.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3: Original face (left), screen replay attack (middle), synthetic data with a moiré pattern (right)&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2110.10444.pdf&quot;&gt;Source&lt;/a&gt; &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Collecting large volumes of spoof data is naturally hard since spoof cases in product flows are very rare. To overcome this problem, one option is to synthesise large volumes of spoof data instead of collecting the real spoof data. More specifically, we synthesise &lt;em&gt;moiré&lt;/em&gt; patterns on genuine face images that we have, and use the synthetic data as the screen replay attack data. This allows our model to use small amounts of real spoof data and sufficiently identify spoofing, while collecting more data to train the model.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4: Data preparation with patch data&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;On the other hand, a spoofed face image contains lots of information with subtle spoof cues such as &lt;em&gt;moiré&lt;/em&gt; patterns that cannot be detected by the naked eye. As such, it’s important to train the model to identify spoof cues instead of focusing on the possible domain bias between the spoof data and genuine data. To achieve this, we need to change the way we prepare the training data.&lt;/p&gt;

&lt;p&gt;Instead of using the entire selfie image as the model input, we firstly detect and crop the face area, then evenly split the cropped face area into several patches. These patches are used as input to train the model. During inference, images are also split into patches the same way and the final result will be the average of outputs from all patches. After this data preprocessing, the patches will contain less global semantic information and more local structure features, making it easier for the model to learn and distinguish spoofed and genuine images.&lt;/p&gt;

&lt;h4 id=&quot;face-verification&quot;&gt;Face verification&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“Data is food for AI.” - Andrew Ng, founder of Google Brain&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The key success factors of artificial intelligence (AI) models are undoubtedly driven by the volume and quality of data we hold. At Grab, we have one of the largest and most comprehensive face datasets, covering a wide range of demographic groups in Southeast Asia. This gives us a strong advantage to build a highly robust and unbiased facial recognition model that serves the region better.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, all selfies collected by Grab are securely protected under privacy legislation in the countries in which we operate. We take reasonable legal, organisational and technical measures to ensure that your Personal Data is protected, which includes measures to prevent Personal Data from getting lost, or used or accessed in an unauthorised way. We limit access to these Personal Data to our employees on a need to know basis. Those processing any Personal Data will only do so in an authorised manner and are required to treat the information with confidentiality.&lt;/p&gt;

&lt;p&gt;Also, selfie data will not be shared with any other parties, including our driver, delivery partners or any other third parties without proper authorisation from the account holder. They are strictly used to improve and enhance our products and services, and not used as a means to collect personal identifiable data. Any disclosure of personal data will be handled in accordance with &lt;a href=&quot;https://www.grab.com/my/terms-policies/privacy-notice/&quot;&gt;Grab Privacy Policy&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5: Semi-Siamese architecture (&lt;a src=&quot;https://arxiv.org/pdf/2007.08398.pdf&quot;&gt;source&lt;/a&gt;)&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Other than data, model architecture also plays an important role, especially when handling less common face verification scenarios, such as ”&lt;strong&gt;&lt;em&gt;selfie to ID photo&lt;/em&gt;&lt;/strong&gt;” and “&lt;strong&gt;&lt;em&gt;selfie to masked selfie&lt;/em&gt;&lt;/strong&gt;” verifications.  &lt;/p&gt;

&lt;p&gt;The main challenge of “&lt;strong&gt;selfie to ID photo&lt;/strong&gt;” verification is the shallow nature of the dataset, i.e. a large number of unique identities, but a low number of image samples per identity. This type of dataset lacks representation in intra-class diversity, which would commonly lead to model collapse during model training. Besides, “selfie to ID photo” verification also poses numerous challenges that are different from general facial recognition, such as aging (old ID photo), attrited ID card (normal wear and tear), and domain difference between printed ID photo and real-life selfie photo.&lt;/p&gt;

&lt;p&gt;To address these issues, we leveraged a novel training method named semi-Siamese training (SST) &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, which is proposed by Du et al. (2020). The key idea is to enlarge intra-class diversity by ensuring that the backbone Siamese networks have similar parameters, but are not entirely identical, hence the name “semi-Siamese”.&lt;/p&gt;

&lt;p&gt;Just like typical Siamese network architecture, feature vectors generated by the subnetworks are compared to compute the loss functions, such as Arc-softmax, Triplet loss, and Large margin cosine loss, all of which aim to reduce intra-class distance while increasing the inter-class distances. With the usage of the semi-Siamese backbone network, intra-class diversity is further promoted as it is guaranteed by the difference between the subnetworks, making the training convergence more stable.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6: Masked face verification&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Another type of face verification problem we need to solve these days is the “&lt;strong&gt;&lt;em&gt;selfie to masked selfie&lt;/em&gt;&lt;/strong&gt;” verification. To pass this type of face verification, users are required to take off their masks as previous face verification models are unable to verify people with masks on. However, removing face masks to do face verification is inconvenient and risky in a crowded environment, which is a pain for many of our driver-partners who need to do verification from time to time.&lt;/p&gt;

&lt;p&gt;To help ease this issue, we developed a face verification model that can verify people even while they are wearing masks. This is done by adding masked selfies into the training data and training the model with both masked and unmasked selfies. This not only enables the model to perform verification for people with masks on, but also helps to increase the accuracy of verifying those without masks. On top of that, masked selfies act as data augmentation and help to train the model with stronger ability of extracting features from the face.&lt;/p&gt;

&lt;h4 id=&quot;face-search&quot;&gt;Face search&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image5.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As previously mentioned, once embeddings are produced by the facial recognition models, face search is fundamentally no different from face verification. Both processes use the distance between embeddings to decide whether the faces belong to the same person. The only difference here is that face search is more computationally expensive, since face verification is a 1-to-1 comparison, whereas face search is a 1-to-N comparison (N=size of the database).&lt;/p&gt;

&lt;p&gt;In practice, there are many ways to significantly reduce the complexity of the search algorithm from O(N), such as using Inverted File Index (IVF) and Hierarchical Navigable Small World (HNSW) graphs. Besides, there are also various methods to increase the query speed, such as accelerating the distance computation using GPU, or approximating the distances using compressed vectors. This problem is also commonly known as Approximate Nearest Neighbor (ANN). Some of the great open-sourced vector similarity search libraries that can help to solve this problem are &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/scann&quot;&gt;ScaNN&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; (by Google), &lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;FAISS&lt;/a&gt;&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;(by Facebook), and &lt;a href=&quot;https://github.com/spotify/annoy&quot;&gt;Annoy&lt;/a&gt; (by Spotify).&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;In summary, facial recognition technology is an effective crime prevention and reduction tool to strengthen the safety of our platform and users. While the enforcement of selfie collection by itself is already a strong deterrent against fraudsters misusing our platform, leveraging facial recognition technology raises the bar by helping us to quickly and accurately identify these offenders.&lt;/p&gt;

&lt;p&gt;As technologies advance, face spoofing patterns also evolve. We need to continuously monitor spoofing trends and actively improve our face anti-spoof algorithms to proactively ensure our users’ safety.&lt;/p&gt;

&lt;p&gt;With the rapid growth of facial recognition technology, there is also a growing concern regarding data privacy issues. At Grab, consumer privacy and safety remain our top priorities and we continuously look for ways to improve our existing safeguards.&lt;/p&gt;

&lt;p&gt;In May 2022, Grab was recognised by the Infocomm Media Development Authority in Singapore for its stringent data protection policies and processes through the &lt;a href=&quot;https://www.grab.com/sg/press/others/grab-singapore-is-the-first-superapp-to-secure-data-protection-trustmark-certification-by-imda/&quot;&gt;award of Data Protection Trustmark (DPTM) certification&lt;/a&gt;. This recognition reinforces our belief that we can continue to draw the benefits from facial recognition technology, while avoiding any misuse of it. As the saying goes, &lt;em&gt;“Technology is not inherently good or evil. It’s all about how people choose to use it”&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Niu, D., Guo R., and Wang, Y. (2021). &lt;em&gt;Moiré&lt;/em&gt; Attack (MA): A New Potential Risk of Screen Photos. Advances in Neural Information Processing Systems. https://papers.nips.cc/paper/2021/hash/db9eeb7e678863649bce209842e0d164-Abstract.html &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Du, H., Shi, H., Liu, Y., Wang, J., Lei, Z., Zeng, D., &amp;amp; Mei, T. (2020). Semi-Siamese Training for Shallow Face Learning. European Conference on Computer Vision, 36–53. Springer. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., &amp;amp; Kumar, S. (2020). Accelerating Large-Scale Inference with Anisotropic Vector Quantization. International Conference on Machine Learning. https://arxiv.org/abs/1908.10396 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Johnson, J., Douze, M., &amp;amp; Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3), 535–547. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 09 Jun 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/facial-recognition</link>
        <guid isPermaLink="true">https://engineering.grab.com/facial-recognition</guid>
        
        <category>Security</category>
        
        <category>Facial recognition</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Graph concepts and applications</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In an &lt;a href=&quot;https://engineering.grab.com/graph-networks&quot;&gt;introductory article&lt;/a&gt;, we talked about the importance of Graph Networks in fraud detection. In this article, we will be adding some further context on graphs, graph technology and some common use cases.&lt;/p&gt;

&lt;p&gt;Connectivity is the most prominent feature of today’s networks and systems. From molecular interactions, social networks and communication systems to power grids, shopping experiences or even supply chains, networks relating to real-world systems are not random. This means that these connections are not static and can be displayed differently at different times. Simple statistical analysis is insufficient to effectively characterise, let alone forecast, networked system behaviour.&lt;/p&gt;

&lt;p&gt;As the world becomes more interconnected and systems become more complex, it is more important to employ technologies that are built to take advantage of relationships and their dynamic properties. There is no doubt that graphs have sparked a lot of attention because they are seen as a means to get insights from related data. Graph theory-based approaches show the concepts underlying the behaviour of massively complex systems and networks.&lt;/p&gt;

&lt;h2 id=&quot;what-are-graphs&quot;&gt;What are graphs?&lt;/h2&gt;

&lt;p&gt;Graphs are mathematical models frequently used in network science, which is a set of technological tools that may be applied to almost any subject. To put it simply, graphs are mathematical representations of complex systems.&lt;/p&gt;

&lt;h3 id=&quot;origin-of-graphs&quot;&gt;Origin of graphs&lt;/h3&gt;

&lt;p&gt;The first graph was produced in 1736 in the city of Königsberg, now known as Kaliningrad, Russia. In this city, there were two islands with two mainland sections that were connected by seven different bridges.&lt;/p&gt;

&lt;p&gt;Famed mathematician Euler wanted to plot a journey through the entire city by crossing each bridge only once. Euler proceeded to abstract the four regions of the city and the seven bridges into edges but he demonstrated that the problem was unsolvable. A simplified abstract graph is shown in Fig 1.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image9.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 1 Abstraction graph&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The graph’s four dots represent Königsberg’s four zones, while the lines represent the seven bridges that connect them. Zones connected by an even number of bridges is clearly navigable because several paths to enter and exit are available. Zones connected by an odd number of bridges can only be used as starting or terminating locations because the same route can only be taken once.&lt;/p&gt;

&lt;p&gt;The number of edges associated with a node is known as the node degree. If two nodes have odd degrees and the rest have even degrees, the Königsberg problem could be solved. For example, exactly two regions must have an even number of bridges while the rest have an odd number of bridges. However, as illustrated in Fig 1, no Königsberg location has an even number of bridges, rendering this problem unsolvable.&lt;/p&gt;

&lt;h3 id=&quot;definition-of-graphs&quot;&gt;Definition of graphs&lt;/h3&gt;

&lt;p&gt;A graph is a structure that consists of vertices and edges. Vertices, or nodes, are the objects in a problem, while edges are the links that connect vertices in a graph.  &lt;/p&gt;

&lt;p&gt;Vertices are the fundamental elements that a graph requires to function; there should be at least one in a graph. Vertices are mathematical abstractions that refer to objects that are linked by a condition.&lt;/p&gt;

&lt;p&gt;On the other hand, edges are optional as graphs can still be defined without any edges. An edge is a link or connection between any two vertices in a graph, including a connection between a vertex and itself. The idea is that if two vertices are present, there is a relationship between them.&lt;/p&gt;

&lt;p&gt;We usually indicate &lt;em&gt;V={v1, v2, …, vn}&lt;/em&gt; as the set of vertices, and &lt;em&gt;E = {e1, e2, …, em}&lt;/em&gt; as the set of edges. From there, we can define a graph &lt;em&gt;G&lt;/em&gt; as a structure &lt;em&gt;G(V, E)&lt;/em&gt; which models the relationship between the two sets:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image11.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 2 Graph structure&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It is worth noting that the order of the two sets within parentheses matters, because we usually express the vertices first, followed by the edges. A graph &lt;em&gt;H(X, Y)&lt;/em&gt; is therefore a structure that models the relationship between the set of vertices &lt;em&gt;X&lt;/em&gt; and the set of edges &lt;em&gt;Y&lt;/em&gt;, not the other way around.&lt;/p&gt;

&lt;h2 id=&quot;graph-data-model&quot;&gt;Graph data model&lt;/h2&gt;

&lt;p&gt;Now that we have covered graphs and their typical components, let us move on to graph data models, which help to translate a conceptual view of your data to a logical model. Two common graph data formats are Resource Description Framework (RDF) and Labelled Property Graph (LPG).&lt;/p&gt;

&lt;h3 id=&quot;resource-description-framework-rdf&quot;&gt;Resource Description Framework (RDF)&lt;/h3&gt;

&lt;p&gt;RDF is typically used for metadata and facilitates standardised exchange of data based on their relationships. RDFs typically consist of a triple: a subject, a predicate, and an object. A collection of such triples is an RDF graph. This can be depicted as a node and a directed edge diagram, with each triple representing a node-edge-node graph, as shown in Fig 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image10.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 3 RDF graph&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The three types of nodes that can exist are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Internationalised Resource Identifiers (IRI) - online resource identification code.&lt;/li&gt;
  &lt;li&gt;Literals - data type value, i.e. text, integer, etc.&lt;/li&gt;
  &lt;li&gt;Blank nodes - have no identification; similar to anonymous or existential variables.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us use an example to illustrate this. We have a person with the name Art and we want to plot all his relationships. In this case, the IRI is &lt;em&gt;http://example.org/art&lt;/em&gt; and this can be shortened by defining a prefix like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ex&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this example, the IRI &lt;em&gt;http://xmlns.com/foaf/0.1/knows&lt;/em&gt; defines the relationship &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;knows&lt;/code&gt;. We define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf&lt;/code&gt; as the prefix for &lt;em&gt;http://xmlns.com/foaf/0.1/&lt;/em&gt;. The following code snippet shows how a graph like this will look.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@prefix foaf: &amp;lt;http://xmlns.com/foaf/0.1/&amp;gt;
@prefix ex: &amp;lt;http://example.org/&amp;gt;

ex:art foaf:knows ex:bob
ex:art foaf:knows ex:bea
ex:bob foaf:knows ex:cal
ex:bob foaf:knows ex:cam
ex:bea foaf:knows ex:coe
ex:bea foaf:knows ex:cory
ex:bea foaf:age 23
ex:bea foaf:based_near_:o1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the last two lines, you can see how a literal and blank node would be depicted in an RDF graph. The variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf:age&lt;/code&gt; is a literal node with the integer value of 23, while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf:based_near&lt;/code&gt; is an anonymous spatial entity with a node identifier of underscore. Outside the context of this graph, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;o1&lt;/code&gt; is a data identifier with no meaning.&lt;/p&gt;

&lt;p&gt;Multiple IRIs, intended for use in RDF graphs, are typically stored in an RDF vocabulary. These IRIs often begin with a common substring known as a namespace IRI. In some cases, namespace IRIs are also associated with a short name known as a namespace prefix. In the example above, &lt;em&gt;http://xmlns.com/foaf/0.1/&lt;/em&gt; is the namespace IRI and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ex&lt;/code&gt; are namespace prefixes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: RDF graphs are considered atemporal as they provide a static snapshot of data. They can use appropriate language extensions to communicate information about events or other dynamic properties of entities.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;An RDF dataset is a set of RDF graphs that includes one or more named graphs as well as exactly one default graph. A default graph is one that can be empty, and has no associated IRI or name, while each named graph has an IRI or a blank node corresponding to the RDF graph and its name. If there is no named graph specified in a query, the default graph is queried (hence its name).&lt;/p&gt;

&lt;h3 id=&quot;labelled-property-graph-lpg&quot;&gt;Labelled Property Graph (LPG)&lt;/h3&gt;

&lt;p&gt;A labelled property graph is made up of nodes, links, and properties. Each node is given a label and a set of characteristics in the form of arbitrary key-value pairs. The keys are strings, and the values can be any data type. A relationship is then defined by adding a directed edge that is labelled and connects two nodes with a set of properties.&lt;/p&gt;

&lt;p&gt;In Fig 4, we have an LPG that shows two nodes: art and bea. The bea node has two characteristics, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;age&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proximity&lt;/code&gt;, that are connected by a known edge. This edge has the attribute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;since&lt;/code&gt; because it commemorates the year that art and bea first met.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image12.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 4 Labelled Property Graph: Example 1&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Nodes, edges and properties must be defined when designing an LPG data model. In this scenario, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; might not be applicable to all vertices, but they should be defined. You might be wondering, why not represent the city Seattle as a node and add an edge marked as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; that connects a person and the city?&lt;/p&gt;

&lt;p&gt;In general, if there is a value linked to a large number of other nodes in the network and it requires additional properties to correlate  with other nodes, it should be represented as a node. In this scenario, the architecture defined in Fig 5 is more appropriate for traversing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; connections. It also gives us the ability to link any new attributes to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; relationship.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image8.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 5 Labelled Property Graph: Example 2&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have the context of graphs, let us talk about graph databases, how they help with large data queries and the part they play in Graph Technology.&lt;/p&gt;

&lt;h2 id=&quot;graph-database&quot;&gt;Graph database&lt;/h2&gt;

&lt;p&gt;A graph database is a type of NoSQL database that stores data using network topology. The idea is derived from LPG, which represents data sets with vertices, edges, and attributes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vertices are instances or entities of data that represent any object to be tracked, such as people, accounts, locations, etc.&lt;/li&gt;
  &lt;li&gt;Edges are the critical concepts in graph databases which represent relationships between vertices. The connections have a direction that can be unidirectional (one-way) or bidirectional (two-way).&lt;/li&gt;
  &lt;li&gt;Properties represent descriptive information associated with vertices. In some cases, edges have properties as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Graph databases provide a more conceptual view of data that is closer to reality. Modelling complex linkages becomes simpler because interconnections between data points are given the same weight as the data itself.&lt;/p&gt;

&lt;h3 id=&quot;graph-database-vs-relational-database&quot;&gt;Graph database vs. relational database&lt;/h3&gt;

&lt;p&gt;Relational databases are currently the industry norm and take a structured approach to data, usually in the form of tables. On the other hand, graph databases are agile and focus on immediate relationship understanding. Neither type is designed to replace the other, so it is important to know what each database type has to offer.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image13.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 6 Graph database vs relational database&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There is a domain for both graph and relational databases. Graph databases outperform typical relational databases, especially in use cases involving complicated relationships, as they take a more naturalistic and flowing approach to data.&lt;/p&gt;

&lt;p&gt;The key distinctions between graph and relational databases are summarised in the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Graph&lt;/th&gt;
      &lt;th&gt;Relational&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Format&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Nodes and edges with properties&lt;/td&gt;
      &lt;td&gt;Tables with rows and columns&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Relationships&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Represented with edges between nodes&lt;/td&gt;
      &lt;td&gt;Created using foreign keys between tables&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Flexible&lt;/td&gt;
      &lt;td&gt;Rigid&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Complex queries&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Quick and responsive&lt;/td&gt;
      &lt;td&gt;Requires complex joins&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Use case&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Systems with highly connected relationships&lt;/td&gt;
      &lt;td&gt;Transaction focused systems with more straightforward relationships&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Table. 1 Graph vs. Relational Databases&lt;/i&gt;&lt;/figcaption&gt;

&lt;h3 id=&quot;advantages-and-disadvantages&quot;&gt;Advantages and disadvantages&lt;/h3&gt;

&lt;p&gt;Every database type has its advantages and disadvantages; knowing the distinctions as well as potential options for specific challenges is crucial. Graph databases are a rapidly evolving technology with improved functions compared with other database types.&lt;/p&gt;

&lt;h4 id=&quot;advantages&quot;&gt;Advantages&lt;/h4&gt;

&lt;p&gt;Some advantages of graph databases include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Agile and flexible structures.&lt;/li&gt;
  &lt;li&gt;Explicit relationship representation between entities.&lt;/li&gt;
  &lt;li&gt;Real-time query output - speed depends on the number of relationships.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;disadvantages&quot;&gt;Disadvantages&lt;/h4&gt;

&lt;p&gt;The general disadvantages of graph databases are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No standardised query language; depends on the platform used.&lt;/li&gt;
  &lt;li&gt;Not suitable for transactional-based systems.&lt;/li&gt;
  &lt;li&gt;Small user base, making it hard to find troubleshooting support.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;graph-technology&quot;&gt;Graph technology&lt;/h2&gt;

&lt;p&gt;Graph technology is the next step in improving analytics delivery. Traditional analytics is insufficient to meet complicated business operations, distribution, and analytical concerns as data quantities expand.&lt;/p&gt;

&lt;p&gt;Graph technology aids in the discovery of unknown correlations in data that would otherwise go undetected or unanalysed. When the term graph is used to describe a topic, three distinct concepts come to mind: graph theory, graph analytics, and graph data management.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Graph theory - A mathematical notion that uses stack ordering to find paths, linkages, and networks of logical or physical objects, as well as their relationships. Can be used to model molecules, telephone lines, transport routes, manufacturing processes, and many other things.&lt;/li&gt;
  &lt;li&gt;Graph analytics - The application of graph theory to uncover nodes, edges, and data linkages that may be assigned semantic attributes. Can examine potentially interesting connections in data found in traditional analysis solutions, using node and edge relationships.&lt;/li&gt;
  &lt;li&gt;Graph database - A type of storage for data generated by graph analytics. Filling a knowledge graph, which is a model in data that indicates a common usage of acquired knowledge or data sets expressing a frequently held notion, is a typical use case for graph analytics output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the architecture and terminology are sometimes misunderstood, graph analytics’ output can be viewed through visualisation tools, knowledge graphs, particular applications, and even some advanced dashboard capabilities of business intelligence tools. All three concepts above are frequently used to improve system efficiency and even to assist in dynamic data management. In this approach, graph theory and analysis are inextricably linked, and analysis may always rely on graph databases.&lt;/p&gt;

&lt;h2 id=&quot;graph-centric-user-stories&quot;&gt;Graph-centric user stories&lt;/h2&gt;

&lt;h4 id=&quot;fraud-detection&quot;&gt;Fraud detection&lt;/h4&gt;

&lt;p&gt;Traditional fraud prevention methods concentrate on discrete data points such as individual accounts, devices, or IP addresses. However, today’s sophisticated fraudsters avoid detection by building fraud rings using stolen and fake identities. To detect such fraud rings, we need to look beyond individual data points to the linkages that connect them.&lt;/p&gt;

&lt;p&gt;Graph technology greatly transcends the capabilities of a relational database, by revealing hard-to-find patterns. Enterprise businesses also employ Graph technology to supplement their existing fraud detection skills to tackle a wide range of financial crimes, including first-party bank fraud, fraud, and money laundering.&lt;/p&gt;

&lt;h4 id=&quot;real-time-recommendations&quot;&gt;Real-time recommendations&lt;/h4&gt;

&lt;p&gt;An online business’s success depends on systems that can generate meaningful recommendations in real time. To do so, we need the capacity to correlate product, customer, inventory, supplier, logistical, and even social sentiment data in real time. Furthermore, a real-time recommendation engine must be able to record any new interests displayed during the consumer’s current visit in real time, which batch processing cannot do.&lt;/p&gt;

&lt;p&gt;Graph databases outperform relational and other NoSQL data stores in terms of delivering real-time suggestions. Graph databases can easily integrate different types of data to get insights into consumer requirements and product trends, making them an increasingly popular alternative to traditional relational databases.&lt;/p&gt;

&lt;h4 id=&quot;supply-chain-management&quot;&gt;Supply chain management&lt;/h4&gt;

&lt;p&gt;With complicated scenarios like supply chains, there are many different parties involved and companies need to stay vigilant in detecting issues like fraud, contamination, high-risk areas or unknown product sources. This means that there is a need to efficiently process large amounts of data and ensure transparency throughout the supply chain.&lt;/p&gt;

&lt;p&gt;To have a transparent supply chain, relationships between each product and party need to be mapped out, which means there will be deep linkages. Graph databases are great for these as they are designed to search and analyse data with deep links. This means they can process enormous amounts of data without performance issues.&lt;/p&gt;

&lt;h4 id=&quot;identity-and-access-management&quot;&gt;Identity and access management&lt;/h4&gt;

&lt;p&gt;Managing multiple changing roles, groups, products and authorisations can be difficult, especially in large organisations. Graph technology integrates your data and allows quick and effective identity and access control. It also allows you to track all identity and access authorisations and inheritances with significant depth and real-time insights.&lt;/p&gt;

&lt;h4 id=&quot;network-and-it-operations&quot;&gt;Network and IT operations&lt;/h4&gt;

&lt;p&gt;Because of the scale and complexity of network and IT infrastructure, you need a configuration management database (CMDB) that is far more capable than relational databases. Neptune is an example of a CMDB and graph database that allows you to correlate your network, data centre, and IT assets to aid troubleshooting, impact analysis, and capacity or outage planning.&lt;/p&gt;

&lt;p&gt;A graph database allows you to integrate various monitoring tools and acquire important insights into the complicated relationships that exist between various network or data centre processes. Possible applications of graphs in network and IT operations range from dependency management to automated microservice monitoring.&lt;/p&gt;

&lt;h4 id=&quot;risk-assessment-and-monitoring&quot;&gt;Risk assessment and monitoring&lt;/h4&gt;

&lt;p&gt;Risk assessment is crucial in the fintech business. With multiple sources of credit data such as ecommerce sites, mobile wallets and loan repayment records, it can be difficult to accurately assess an individual’s credit risk. Graph Technology makes it possible to combine these data sources, quantify an individual’s fraud risk and even generate full credit reviews.&lt;/p&gt;

&lt;p&gt;One clear example of this is &lt;a href=&quot;https://www.globenewswire.com/news-release/2018/08/29/1558324/0/en/FinTech-Pioneer-IceKredit-Transforms-the-Credit-Market-With-TigerGraph.html&quot;&gt;IceKredit&lt;/a&gt;, which employs artificial intelligence (AI) and machine learning (ML) techniques to make better risk-based decisions. With Graph technology, IceKredit has also successfully detected unreported links and increased efficiency of financial crime investigations.&lt;/p&gt;

&lt;h4 id=&quot;social-network&quot;&gt;Social network&lt;/h4&gt;

&lt;p&gt;Whether you’re using stated social connections or inferring links based on behaviour, social graph databases like Neptune introduce possibilities for building new social networks or integrating existing social graphs into commercial applications.&lt;/p&gt;

&lt;p&gt;Having a data model that is identical to your domain model allows you to better understand your data, communicate more effectively, and save time. By decreasing the time spent data modelling, graph databases increase the quality and speed of development for your social network application.&lt;/p&gt;

&lt;h4 id=&quot;artificial-intelligence-ai-and-machine-learning-ml&quot;&gt;Artificial intelligence (AI) and machine learning (ML)&lt;/h4&gt;

&lt;p&gt;AI and ML use statistical and analytical approaches to find patterns in data and provide insights. However, there are two prevalent concerns that arise - the quality of data and effectiveness of the analytics. Some AI and ML solutions have poor accuracy because there is not enough training data or variants that have a high correlation to the outcome.&lt;/p&gt;

&lt;p&gt;These ML data issues can be solved with graph databases as it’s possible to connect and traverse links, as well as supplement raw data. With Graph technology, ML systems can recognise each column as a “feature” and each connection as a distinct characteristic, and then be able to identify data patterns and train themselves to recognise these relationships.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Graphs are a great way to visually represent complex systems and can be used to easily detect patterns or relationships between entities. To help improve graphs’ ability to detect patterns early, businesses should consider using Graph technology, which is the next step in improving analytics delivery.&lt;/p&gt;

&lt;p&gt;Graph technology typically consists of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Graph theory&lt;/strong&gt; - Used to find paths, linkages and networks of logical or physical objects.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph analytics&lt;/strong&gt; - Application of graph theory to uncover nodes, edges, and data linkages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph database&lt;/strong&gt; - Storage for data generated by graph analytics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although predominantly used in fraud detection, Graph technology has many other use cases such as making real-time recommendations based on consumer behaviour, identity and access control, risk assessment and monitoring, AI and ML, and many more.&lt;/p&gt;

&lt;p&gt;In our next blog article, we will be talking about how our Graph Visualisation Platform enhances Grab’s fraud detection methods.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.baeldung.com/cs/graph-theory-intro&quot;&gt;https://www.baeldung.com/cs/graph-theory-intro&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/class/cs520/2020/notes/What_Are_Graph_Data_Models.html&quot;&gt;https://web.stanford.edu/class/cs520/2020/notes/What_Are_Graph_Data_Models.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jun 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/graph-concepts</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-concepts</guid>
        
        <category>Security</category>
        
        <category>Graphs concepts</category>
        
        <category>Graph technology</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Automated Experiment Analysis - Making experimental analysis scalable</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Trustworthy experiments are key to making sound decisions, so analysts and data scientists put a lot of effort into analysing them and making business impacts. An extension of &lt;a href=&quot;https://engineering.grab.com/building-grab-s-experimentation-platform&quot;&gt;Grab’s Experimentation (GrabX) platform&lt;/a&gt;, Automated Experiment Analysis is one of Grab’s data products that helps automate statistical analyses of experiments. It also provides automatic experimental data pipelines and customised tests for different types of experiments.&lt;/p&gt;

&lt;p&gt;Designed to help Grab in its journey of innovation and data-driven decision making, the data product helps to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Standardise and automate the basic experiment analysis process on Grab experiments.&lt;/li&gt;
  &lt;li&gt;Ensure post-experiment results are reproducible under a company-wide standard, and easily reviewed by each other.&lt;/li&gt;
  &lt;li&gt;Democratise the institutional knowledge of experimentation across functions.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Today, the GrabX platform provides the ability to define, configure, and execute online controlled experiments (OCEs), often called A/B tests, to gather trustworthy data and make data-driven decisions about how to improve our products.&lt;/p&gt;

&lt;p&gt;Before the automated analysis, each experiment was analysed manually on an ad-hoc basis. This manual and federated model brings in several challenges at the company level:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Inefficiency&lt;/strong&gt;: Repetitive nature of data pipeline building and basic post-experiment analyses incur large costs and deplete the analysts’ bandwidth from running deeper analyses.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lack of quality control&lt;/strong&gt;: Risk of unstandardised, inaccurate or late results as the platform cannot exercise data-governance/control or extend offerings to Grab’s other entities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lack of scalability and availability&lt;/strong&gt;: GrabX users have varied backgrounds and skills, making their approaches to experiments different and not easily transferable/shared. E.g. Some teams may use more advanced techniques to speed up their experiments without using too much resources but these techniques are not transferable without considerable training.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/automated-experiment-analysis/image1.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Architecture diagram&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When users set up experiments on GrabX, they can configure the success metrics they are interested in. These metrics configurations are then stored in the metadata as “bronze”, “silver”, and “gold” datasets depending on the corresponding step in the automated data pipeline process.&lt;/p&gt;

&lt;h4 id=&quot;metrics-configuration-and-bronze-datasets&quot;&gt;Metrics configuration and “bronze” datasets&lt;/h4&gt;

&lt;p&gt;In this project, we have developed a metrics glossary that stores information about what the metrics are and how they are computed. The metrics glossary is stored in CosmoDB and serves as an API Endpoint for GrabX so users can pick from the list of available metrics. If a metric is not available, users can input their custom metrics definition.&lt;/p&gt;

&lt;p&gt;This metrics selection, as an analysis configuration, is then stored as a “bronze” dataset in Azure Data Lake as metadata, together with the experiment configurations. Once the experiment starts, the data pipeline gathers all experiment subjects and their assigned experiment groups from our clickstream tracking system.&lt;/p&gt;

&lt;p&gt;In this case, the experiment subject refers to the facets of the experiment. For example, if the experiment subject is a user, then the user will go through the same experience throughout the entire experimentation period.&lt;/p&gt;

&lt;h4 id=&quot;metrics-computation-and-silver-datasets&quot;&gt;Metrics computation and “silver” datasets&lt;/h4&gt;

&lt;p&gt;In this step, the metrics engine gathers all metrics data based on the metrics configuration and computes the metrics for each experiment subject. This computed data is then stored as a “silver” dataset and is the foundation dataset for all statistical analyses.&lt;/p&gt;

&lt;p&gt;“Silver” datasets are then passed through the “Decision Engine” to get the final “gold” datasets, which contain the experiment results.&lt;/p&gt;

&lt;h4 id=&quot;results-visualisation-andgold-datasets&quot;&gt;Results visualisation and ”gold” datasets&lt;/h4&gt;

&lt;p&gt;In “gold” datasets, we have the result of the experiment, along with some custom messages we want to show our users. These are saved in sets of fact and dim tables (typically used in &lt;a href=&quot;https://docs.microsoft.com/en-us/power-bi/guidance/star-schema&quot;&gt;star schemas&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;For users to visualise the result on GrabX, we leverage the embedded Power BI visualisation. We build the visualisation using a “gold” dataset and embed it to each experiment page with a fixed filter. By doing so, users can experience the end-to-end flow directly from GrabX.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;The implementation consists of four key engineering components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Analysis configuration setup&lt;/li&gt;
  &lt;li&gt;A data pipeline&lt;/li&gt;
  &lt;li&gt;Automatic analysis&lt;/li&gt;
  &lt;li&gt;Results visualisation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Analysis configuration&lt;/strong&gt; is part of the experiment setup process where users select success metrics they are interested in. This is an essential configuration for post-experiment analysis, in addition to the usual experiment configurations (e.g. sampling strategies).&lt;/p&gt;

&lt;p&gt;It ensures that the reported experiment results will align with the hypothesis setup, which helps avoid one of the common pitfalls in OCEs &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;There are three types of metrics available:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pre-defined metrics: These metrics are already defined in the Scribe datamart, e.g. Gross Merchandise Value (GMV) per pax.&lt;/li&gt;
  &lt;li&gt;Event-based metrics: Users can specify an ad-hoc metric in the form of a funnel with event names for funnel start and end.&lt;/li&gt;
  &lt;li&gt;Build your own metrics: Users have the flexibility to define a metric in the form of a SQL query.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;A data pipeline&lt;/strong&gt; here mainly consists of data sourcing and data processing. We use Azure Data Factory to schedule ETL pipelines so we can calculate the metrics and statistical analysis. ETL jobs are written in spark and run using Databricks.&lt;/p&gt;

&lt;p&gt;Data pipelines are streamlined to the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load experiments and metrics metadata, defined at the experiment creation stage.&lt;/li&gt;
  &lt;li&gt;Load experiment and clickstream events.&lt;/li&gt;
  &lt;li&gt;Load experiment assignments. An experiment assignment maps a randomisation unit ID to the corresponding experiment or variant IDs.&lt;/li&gt;
  &lt;li&gt;Merge the data mentioned above for each experiment variant, and obtain sufficient data to do a deeper results analysis.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Automatic analysis&lt;/strong&gt; uses an internal python package “Decision Engine”, which decouples the dataset and statistical tests, so that we can incrementally improve applications of advanced techniques. It provides a comprehensive set of test results at the variant level, which include statistics, p-values, confidence intervals, and the test choices that correspond to the experiment configurations. It’s a crowdsourced project which allows all to contribute what they believe should be included in fundamental post-experiment analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results visualisation&lt;/strong&gt; leverages PowerBI, which is embedded in the GrabX UI, so users can run the experiments and review the results on a single platform. &lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;At the individual user level, Automated Experiment Analysis is designed to enable analysts and data scientists to associate metrics with experiments, and present the experiment results in a standardised and comprehensive manner. It speeds up the decision-making process and frees up the bandwidths of analysts and data scientists to conduct deeper analyses.&lt;/p&gt;

&lt;p&gt;At the user community level, it improves the efficiency of running experimental analysis by capturing all experiments, their results, and the launch decision within a single platform.&lt;/p&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;Automated Experiment Analysis is the first building block to boost the trustworthiness of OCEs in Grab. Not all types of experiments are fully onboard, and they might not need to be. Through this journey, we believe these key learnings would be useful for experimenters and platform teams:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To standardise and simplify several experimental analysis steps, there needs to be automation data pipelines, analytics tools, and a metrics store in the infrastructure.&lt;/li&gt;
  &lt;li&gt;The “Decision Engine” analytics tool should be decoupled from the other engineering components, so that it can be incrementally improved in future.&lt;/li&gt;
  &lt;li&gt;To democratise knowledge and ensure service coverage, many components need to have a crowdsourcing feature, e.g. the metrics store has a BYOM function, and “Decision Engine” is an open-sourced internal python package.&lt;/li&gt;
  &lt;li&gt;Tracking implementation is important. To standardise data pipelines and achieve scalability, we need to standardise the way we implement tracking.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A centralised metric store&lt;/strong&gt; -  We built a metric calculation dictionary, which currently contains around 30-40 basic business metrics, but its functionality is limited to GrabX Experimentation use case.&lt;/p&gt;

&lt;p&gt;If the metric store is expected to serve more general uses, it needs to be further enriched by allowing some “smarts”, e.g. fabric-agnostic metrics computations &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, other types of data slicing, and some considerations with real-time metrics or signals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An end-to-end experiment guide rail&lt;/strong&gt; - Currently, we provide automatic data analysis after an experiment is done, but no guardrail features at multiple experiment stages, e.g. sampling strategy choices, sample size recommendation from the planning stage, and data quality check during/after the experiment windows. Without the end-to-end guardrails, running experiments will be very prone to pitfalls. We therefore plan to add some degree of automation to ensure experiments adhere to the standards used by the post-experimental analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A more comprehensive analysis toolbox&lt;/strong&gt; - The current state of the project mainly focuses on infrastructure development, so it starts with basic frequentist’s A/B testing approaches. In future versions, it can be extended to include sequential testing, CUPED &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, attribution analysis, Causal Forest, heterogeneous treatment effects, etc.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dmitriev, P., Gupta, S., Kim, D. W., &amp;amp; Vaz, G. (2017, August). A dirty dozen: twelve common metric interpretation pitfalls in online controlled experiments. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1427-1436). &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/metric-computation-for-multiple-backends/&quot;&gt;Metric computation for multiple backends&lt;/a&gt;, Craig Boucher, Ulf Knoblich, Dan Miller, Sasha Patotski, Amin Saied, Microsoft Experimentation Platform &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Deng, A., Xu, Y., Kohavi, R., &amp;amp; Walker, T. (2013, February). Improving the sensitivity of online controlled experiments by utilising pre-experiment data. In Proceedings of the sixth ACM international conference on Web search and data mining (pp. 123-132). &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 30 May 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/automated-experiment-analysis</link>
        <guid isPermaLink="true">https://engineering.grab.com/automated-experiment-analysis</guid>
        
        <category>Experiment</category>
        
        <category>Experimental analysis</category>
        
        <category>Azure Databricks</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
  </channel>
</rss>
