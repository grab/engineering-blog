<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 06 Dec 2024 05:57:49 +0000</pubDate>
    <lastBuildDate>Fri, 06 Dec 2024 05:57:49 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>How we seamlessly migrated high volume real-time streaming traffic from one service to another with zero data loss and duplication</title>
        <description>&lt;p&gt;At Grab, we continuously enhance our systems to improve scalability, reliability and cost-efficiency. Recently, we undertook a project to split the read and write functionalities of one of our backend services into separate services. This was motivated by the need to independently scale these operations based on their distinct scalability requirements.&lt;/p&gt;

&lt;p&gt;In this post, we will dive deep into how we migrated the stream processing (write) functionality to a new service with zero data loss and duplication. This was accomplished while handling a high volume of real-time traffic averaging 20,000 reads per second from 16 source Kafka streams writing to other output streams and several DynamoDB tables.&lt;/p&gt;

&lt;h2 id=&quot;migration-challenges-and-strategy&quot;&gt;Migration challenges and strategy&lt;/h2&gt;

&lt;p&gt;Migrating the stream processing to the new service while ensuring zero data loss and duplication posed some interesting challenges, especially given the high volume of real-time data. We needed a strategy that would enable us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Migrate streams one by one gradually.&lt;/li&gt;
  &lt;li&gt;Validate the new service’s processing in production before fully switching over.&lt;/li&gt;
  &lt;li&gt;Perform the switchover with no downtime or data inconsistencies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We considered various options for the switchover such as using feature flags via our unified config management and experimental rollout platform. However, these approaches had some limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There could be some data loss or duplication during the deployment time when toggling the flags, which can be up to a few minutes.&lt;/li&gt;
  &lt;li&gt;There might be data inconsistencies as the flag value could be updated on the services (the existing and and the new one) at slightly different times.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ultimately, we decided on a custom time-based switchover logic implemented in shared code between the two services leveraging our monorepo structure. In the following sections, we will walk you through the steps we took to achieve this seamless migration.&lt;/p&gt;

&lt;h2 id=&quot;step-1-preparation&quot;&gt;Step 1: Preparation&lt;/h2&gt;

&lt;p&gt;First, since both the existing and new services reside in our monorepo, we moved the stream processing code from the existing service to a shared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/commons&lt;/code&gt; directory. This allowed both the old and new services to import and use the same code. We added logic in this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commons&lt;/code&gt; package to selectively turn stream processing on or off based on the service processing them.&lt;/p&gt;

&lt;p&gt;Next, we created temporary “sink” resources  such as streams and DynamoDB tables  for the new service to write the processed data. This allowed us to monitor and validate the new service’s behavior in production without impacting the main resources.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/seamless-migration/figure1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. For a short period, both services consumed the incoming streams, but only the old service continued to write to the actual sink resources while the new service wrote to validation sink resources.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;step-2-scheduling-the-switchover&quot;&gt;Step 2: Scheduling the switchover&lt;/h2&gt;

&lt;p&gt;In the shared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/commons&lt;/code&gt; code, we added a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;map[string]time.Time&lt;/code&gt; to schedule the switchover for each stream.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;map[string]time.Time{
  &quot;streamA&quot;: time.Date(2024, 2, 28, 12, 0, 0, 0, time.UTC),
  &quot;streamB&quot;: time.Date(2024, 3, 10, 12, 0, 0, 0, time.UTC),
  // ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When a stream is added to this map, it means it is scheduled for switchover at the specified time. This logic is shared between both services, so the switchover happens simultaneously. The new service starts writing to the main resources while the old service stops, with no overlap or gap.&lt;/p&gt;

&lt;h2 id=&quot;step-3-deployment-and-monitoring&quot;&gt;Step 3: Deployment and monitoring&lt;/h2&gt;

&lt;p&gt;To perform the switchover, we:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Updated the switchover times for the streams.&lt;/li&gt;
  &lt;li&gt;Deployed both services with enough buffer time before the scheduled switch.&lt;/li&gt;
  &lt;li&gt;Closely monitored the process by creating dedicated monitors for the migration process using our observability tools.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/seamless-migration/figure2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. This timeseries graph shows the stream received at the old and the new service (dotted line), facilitating real time monitoring of the stream processing volume across both services during the validation period.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The old service continued consuming the streams for a short monitoring period post-switchover, but without writing anywhere, ensuring no loss or duplication at the output sink resources. Then, the stream consumption was removed from the old service altogether, completing the entire migration process.&lt;/p&gt;

&lt;h2 id=&quot;results-and-learnings&quot;&gt;Results and learnings&lt;/h2&gt;

&lt;p&gt;Using this time-based approach, we were able to seamlessly migrate the high-volume stream processing to the new service with:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Zero data loss or duplication.&lt;/li&gt;
  &lt;li&gt;No downtime or production issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The whole migration, including the gradual stream-by-stream switchover, was completed in about three weeks.&lt;/p&gt;

&lt;p&gt;One learning was that such custom time-based logic, while effective for our use case, has limitations. If a rollback was needed for any of the two services for some unexpected reasons, some data inconsistency would be unavoidable. Generally, such time-based logic should be used with caution as it can lead to unexpected scenarios if the systems fall out of sync. We went ahead with this approach as it was a temporary measure and we had thoroughly tested it before carrying out the switchover.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Dec 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/seamless-migration</link>
        <guid isPermaLink="true">https://engineering.grab.com/seamless-migration</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Data streaming</category>
        
        <category>Real-time streaming</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Supercharging LLM Application Development with LLM-Kit</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we are committed to leveraging the power of technology to deliver the best services to our users and partners. As part of this commitment, we have developed the LLM-Kit, a comprehensive framework designed to supercharge the setup of production-ready Generative AI applications. This blog post will delve into the features of the LLM-Kit, the problems it solves, and the value it brings to our organisation.&lt;/p&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;p&gt;The introduction of the LLM-Kit has significantly addressed the challenges encountered in LLM application development. The involvement of sensitive data in AI applications necessitates that security remains a top priority, ensuring data safety is not compromised during AI application development.&lt;/p&gt;

&lt;p&gt;Concerns such as scalability, integration, monitoring, and standardisation are common issues that any organisation will face in their LLM and AI development efforts.&lt;/p&gt;

&lt;p&gt;The LLM-Kit has empowered Grab to pursue LLM application development and the rollout of Generative AI efficiently and effectively in the long term.&lt;/p&gt;

&lt;h2 id=&quot;introducing-the-llm-kit&quot;&gt;Introducing the LLM-Kit&lt;/h2&gt;

&lt;p&gt;The LLM-Kit is our solution to these challenges. Since the introduction of the LLM Kit, it has helped onboard hundreds of GenAI applications at Grab and has become the de facto choice for developers. It is a comprehensive framework designed to supercharge the setup of production-ready LLM applications. The LLM-Kit provides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pre-configured structure&lt;/strong&gt;: The LLM-Kit comes with a pre-configured structure containing an API server, configuration management, a sample LLM Agent, and tests.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Integrated tech stack&lt;/strong&gt;: The LLM-Kit integrates with Poetry, Gunicorn, FastAPI, LangChain, LangSmith, Hashicorp Vault, Amazon EKS, and Gitlab CI pipelines to provide a robust and end-to-end tech stack for LLM application development.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability&lt;/strong&gt;: The LLM-Kit features built-in observability with Datadog integration and LangSmith, enabling real-time monitoring of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Config &amp;amp; secret management&lt;/strong&gt;: The LLM-Kit utilises Python’s configparser and Vault for efficient configuration and secret management.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt;: The LLM-Kit provides built-in OpenID Connect (OIDC) auth helpers for authentication to Grab’s internal services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;API documentation&lt;/strong&gt;: The LLM-Kit features comprehensive API documentation using Swagger and Redoc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Redis &amp;amp; vector databases integration&lt;/strong&gt;: The LLM-Kit integrates with Redis and Vector databases for efficient data storage and retrieval.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment pipeline&lt;/strong&gt;: The LLM-Kit provides a deployment pipeline for staging and production environments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluations&lt;/strong&gt;: The LLM-Kit seamlessly integrates with LangSmith, utilising its robust evaluations framework to ensure the quality and performance of the LLM applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to these features, the team has also included a cookbook with many commonly used examples within the organisation providing a valuable resource for developers. Our cookbook includes a diverse range of examples, such as persistent memory agents, Slackbot LLM agents, image analysers and full-stack chatbots with user interfaces, showcasing the versatility of the LLM-Kit.&lt;/p&gt;

&lt;h2 id=&quot;the-value-of-the-llm-kit&quot;&gt;The value of the LLM-Kit&lt;/h2&gt;

&lt;p&gt;The LLM-Kit brings significant value to our teams at Grab:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Increased development velocity&lt;/strong&gt;: By providing a pre-configured structure and integrated tech stack, the LLM-Kit accelerates the development of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improved observability&lt;/strong&gt;: With built-in LangSmith and Datadog integration, teams can monitor their LLM applications in real-time, enabling faster issue detection and resolution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhanced security&lt;/strong&gt;: The LLM-Kit’s built-in OIDC auth helpers and secret management using Vault ensure the secure development and deployment of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficient data management&lt;/strong&gt;: The integration with Vector databases facilitates efficient data storage and retrieval, crucial for the performance of LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Standardisation&lt;/strong&gt;: The LLM-Kit provides a paved-road framework for building LLM applications, promoting best practices and standardisation across teams.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Through the LLM-Kit, we can save an estimate of 1.5 weeks before teams start working on their first feature.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Project development process before LLM-Kit&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Project development process after LLM-Kit&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;architecture-design-and-technical-implementation&quot;&gt;Architecture design and technical implementation&lt;/h2&gt;

&lt;p&gt;The LLM-Kit is designed with a modular architecture that promotes scalability, flexibility, and ease of use.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. LLM-Kit modules&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;automated-steps&quot;&gt;Automated steps&lt;/h3&gt;

&lt;p&gt;To better illustrate the technical implementation of the LLM-Kit, let’s take a look at figure 4 which outlines the step-by-step process of how an LLM application is generated with the LLM-Kit:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Process of generating LLM apps using LLM-Kit&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The process begins when an engineer submits a form with the application name and other relevant details. This triggers the creation of a GitLab project, followed by the generation of a code scaffold specifically designed for the LLM application. GitLab CI files are then generated within the same repository to handle continuous integration and deployment tasks. The process continues with the creation of staging infrastructure, including components like Elastic Container Registry (ECR) and Elastic Kubernetes Service (EKS). Additionally, a Terraform folder is created to provision the necessary infrastructure, eventually leading to the deployment of production infrastructure. At the end of the pipeline, a GPT token is pushed to a secure Vault path, and the engineer is notified upon the successful completion of the pipeline.&lt;/p&gt;

&lt;h3 id=&quot;scaffold-code-structure&quot;&gt;Scaffold code structure&lt;/h3&gt;

&lt;p&gt;The scaffolded code is broken down into multiple folders:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Agents&lt;/strong&gt;: Contains the code to initialise an agent. We have gone ahead with LangChain as the agent framework; essentially the entry point for the endpoint defined in the Routes folder.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Auth&lt;/strong&gt;: Authentication and authorisation module for executing some of the APIs within Grab.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt;: Includes extracting all configurations (i.e. GPT token) and secret decryption for running the LLM application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Models&lt;/strong&gt;: Used to define the structure for the core LLM APIs within Grab.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Routes:&lt;/strong&gt; REST API endpoint definitions for the LLM Applications. It comes with health check, authentication, authorisation, and a simple agent by default.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Includes connectivity with PGVector, our managed vector database within Grab and database schemas.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Functions which are used as tools for the LLM Agent.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tracing&lt;/strong&gt;: Integration with our tracing and monitoring tools to monitor various metrics for a production application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Utils&lt;/strong&gt;: Default folder for utility functions.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Scaffold code structure&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;infrastructure-provisioning-and-deployment&quot;&gt;Infrastructure provisioning and deployment&lt;/h3&gt;

&lt;p&gt;Within the same codebase, we have integrated a comprehensive pipeline that automatically scaffolds the necessary code for infrastructure provisioning, deployment, and build processes. Using Terraform, the pipeline provisions the required infrastructure seamlessly. The deployment pipelines are defined in the .gitlab-ci.yml file, ensuring smooth and automated deployments. Additionally, the build process is specified in the Dockerfile, allowing for consistent builds. This automated scaffolding streamlines the development workflow, enabling developers to focus on writing business logic without worrying about the underlying infrastructure and deployment complexities.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Pipeline infrastructure &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;rag-scaffolding&quot;&gt;RAG scaffolding&lt;/h3&gt;

&lt;p&gt;At Grab, we’ve established a streamlined process for setting up a vector database (PGVector) and whitelisting the service using the LLM-Kit. Once the form (figure 7) is submitted, you can access the credentials and database host path. The secrets will be automatically added to the Vault path. Engineers will then only need to include the DB host path in the configuration file of the scaffolded LLM-Kit application.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm_kit/figure-7.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Form submitted to access credentials and database host path&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The LLM-Kit is a testament to Grab’s commitment to fostering innovation and growth in AI and ML. By addressing the challenges faced by our teams and providing a comprehensive, scalable, and flexible framework for LLM application development, the LLM-Kit is paving the way for the next generation of AI applications at Grab.&lt;/p&gt;

&lt;h2 id=&quot;growth-and-future-plans&quot;&gt;Growth and future plans&lt;/h2&gt;

&lt;p&gt;Looking ahead, the LLM-Kit team aims to significantly enhance the web server’s concurrency and scalability while providing reliable and easy-to-use SDKs. The team plans to offer reusable and composable LLM SDKs, including evaluation and guardrails frameworks, to enable service owners to build feature-rich Generative AI programs with ease. Key initiatives also include the development of a CLI for version updates and dev tooling, as well as a polling-based agent serving function. These advancements are designed to drive innovation and efficiency within the organisation, ultimately providing a more seamless and efficient development experience for engineers.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to acknowledge and thank Pak Zan Tan, Han Su, and Jonathan Ku from the Yoshi team and Chen Fei Lee from the MEKS team for their contribution to this project under the leadership of Padarn George Wilson.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Nov 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/supercharging-llm-application-development-with-llm-kit</link>
        <guid isPermaLink="true">https://engineering.grab.com/supercharging-llm-application-development-with-llm-kit</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we reduced initialisation time of Product Configuration Management SDK</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;GrabX serves as Grab’s central platform for product configuration management. GrabX client services read product configurations through an SDK. This SDK reads the configurations in a way that’s eventually consistent, meaning it takes about a minute for any configuration updates to reach the client SDKs.&lt;/p&gt;

&lt;p&gt;However, some GrabX SDK clients, particularly those that need to read larger configuration data (~400 MB), reported that the SDK takes an extended amount of time to initialise, approximately four minutes. This blog post details how we analysed and addressed this issue.&lt;/p&gt;

&lt;h2 id=&quot;sdk-observations&quot;&gt;SDK Observations&lt;/h2&gt;

&lt;p&gt;GrabX clients have observed that the GrabX SDK requires several minutes to initialise. This results in what is known as ‘cold starts’, where the SDK takes an extended time to begin supporting the reading of configurations at startup. This challenge highlights the importance of efficient SDK start-up management, especially when a service handling a high volume of incoming traffic initiates new SDK instances to manage the load better. However, due to the extended SDK initialisation time, these instances continue to experience stress, potentially leading to service throttling.&lt;/p&gt;

&lt;h2 id=&quot;sdk-initialisation-workflow&quot;&gt;SDK Initialisation Workflow&lt;/h2&gt;

&lt;p&gt;The SDK initialisation flow described below is based on the improvements we proposed to the SDK design in &lt;a href=&quot;reduced-memory-cpu-usage-grabx-sdk/&quot;&gt;our previous post&lt;/a&gt;. In that post, we suggested enhancing the SDK design by:&lt;/p&gt;

&lt;p&gt;A. Implementing service-based data partitioning and storage in the AWS S3 bucket
B. Allowing service-based subscription of data for the SDK&lt;/p&gt;

&lt;p&gt;The following diagram provides a high-level overview of the initialisation process of the GrabX SDK, which can be divided into the following sequential steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set options that drive the behaviour of the SDK.&lt;/li&gt;
  &lt;li&gt;Initialise dependent module clients.&lt;/li&gt;
  &lt;li&gt;Initialise the GrabX client. (Highlighted as A in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download data for the SDK’s subscribed list of services from the AWS S3 bucket and store this data on the SDK instance disk. (Highlighted as B in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download common data needed by the SDK from the AWS S3 bucket and store this data on the SDK instance disk. This data is referred to as ‘common’ because it is required by all different client services.  (Highlighted as C in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download data for the SDK’s subscribed list of services from the AWS S3 bucket and load this data into the SDK instance memory.  (Highlighted as D in the diagram below)&lt;/li&gt;
  &lt;li&gt;Download common data needed by the SDK from the AWS S3 bucket and load this data into the SDK instance memory. (Highlighted as E in the diagram below)&lt;/li&gt;
  &lt;li&gt;Initialise dependent modules for resolving the configuration value.  (Highlighted as F in the diagram below)&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image1.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;proposed-solution&quot;&gt;Proposed Solution&lt;/h2&gt;

&lt;p&gt;In order to address the issue of extended SDK initialisation time, we have decided to enhance the SDK initialisation design in multiple phases. Each phase focused on improving a specific part of the workflow.&lt;/p&gt;

&lt;h3 id=&quot;improvement-phase-1&quot;&gt;Improvement Phase 1&lt;/h3&gt;

&lt;p&gt;As discussed in the previous section, the GrabX SDK needs to load two separate sets of data: the subscribed services data and the common data. These two data sets are currently downloaded from the AWS S3 bucket and sequentially loaded into disk and memory.&lt;/p&gt;

&lt;p&gt;In the first phase of our improvement plan, we decided to change the sequential data load to a concurrent data load for these two data sets, as illustrated in the following diagram. This alteration in the SDK initialisation workflow reduced the initialisation time by approximately 80%.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;improvement-phase-2&quot;&gt;Improvement Phase 2&lt;/h3&gt;

&lt;p&gt;Building on the progress made in Phase 1, we next turned our attention to the issue of large configuration file sizes. As mentioned in the introduction, the extended SDK initialisation time was particularly noticeable for client services that needed to load larger amounts of data.&lt;/p&gt;

&lt;p&gt;In this phase, we decided to implement an SDK design change that allows the SDK to &lt;strong&gt;concurrently&lt;/strong&gt; download data from the AWS S3 bucket and load it into memory for all these large configurations within a subscribed service, as illustrated in the following diagram. This modification to the SDK initialisation workflow further reduced the initialisation time by approximately &lt;strong&gt;6%&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;improvement-phase-3&quot;&gt;Improvement Phase 3&lt;/h3&gt;

&lt;p&gt;Upon examining the SDK’s behaviour, we observed that the SDK is both persisting configuration data downloaded from the AWS S3 bucket to disk and loading the data into memory. We understand that the data is loaded into memory to reduce the latency of configuration reads. The data is stored on disk to support a fallback mechanism, which is activated in a very specific use case: when the client SDK instance restarts and there is a connectivity issue with AWS S3 for downloading configuration files. In this scenario, the SDK will read the configuration data stored on disk. However, this data could be outdated as it is not freshly downloaded from the AWS S3 bucket, and most client services require the most recent data.&lt;/p&gt;

&lt;p&gt;Therefore, we realised that the fallback mechanism, for which data is persisted on disk, actually conflicts with the desired SDK behaviour for most client services. As a result, we decided to eliminate the SDK initialisation step that downloads configuration data from AWS S3 and persists it on disk. If the SDK initialisation fails to connect to the AWS S3 bucket and download data, client services can then take the necessary action, such as retrying initialisation. This modification further reduced the initialisation time by approximately 50% compared to the improvement achieved in Phase 2.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We benchmarked the proposed solution with a variety of services, each having different configuration data sizes. Our findings suggest that the proposed solution has the potential to reduce initialisation time by up to &lt;strong&gt;90%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The following chart illustrates the phase-wise reduction in initialisation time achieved through the improvements made to the GrabX SDK.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-we-reduced-grabx-sdk-initialisation-time/image5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Nov 2024 00:00:01 +0000</pubDate>
        <link>https://engineering.grab.com/how-we-reduced-grabx-sdk-initialisation-time</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-we-reduced-grabx-sdk-initialisation-time</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Metasense V2: Enhancing, improving and productionisation of LLM powered data governance</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the initial article, &lt;a href=&quot;https://engineering.grab.com/llm-powered-data-classification&quot;&gt;LLM Powered Data Classification&lt;/a&gt;, we addressed how we integrated Large Language Models (LLM) to automate governance-related metadata generation. The LLM integration enabled us to resolve challenges in Gemini, such as restrictions on the customisation of machine learning classifiers and limitations of resources to train a customised model. Gemini is a metadata generation service built internally to automate the tag generation process using a third-party data classification service. We also focused on LLM-powered column-level tag classifications. The classified tags, combined with Grab’s data privacy rules, allowed us to determine sensitivity tiers of data entities. The affordability of the model also enables us to scale it to cover more data entities in the company. The initial model scanned more than 20,000 data entries, at an average of 300-400 entities per day. Despite its remarkable performance, we were aware that there was room for improvement in the areas of data classification and prompt evaluation.&lt;/p&gt;

&lt;h2 id=&quot;improving-the-model-post-rollout&quot;&gt;Improving the model post-rollout&lt;/h2&gt;

&lt;p&gt;Since its launch in early 2024, our model has gradually grown to cover the entire data lake. To date, the vast majority of our data lake tables have undergone analysis and classification by our model. This has significantly reduced the workload for Grabbers. Instead of manually classifying all new or existing tables, Grabbers can now rely on our model to assign the appropriate classification tier accurately.&lt;/p&gt;

&lt;p&gt;Despite table classification being automated, the data pipeline still requires owners to manually perform verification to prevent any misclassifications. While it is impossible to entirely eliminate human oversight from critical machine learning workflows, the team has dedicated substantial time post-launch to refining the model, thereby safely minimising the need for human intervention.&lt;/p&gt;

&lt;h3 id=&quot;utilising-post-rollout-data&quot;&gt;Utilising post-rollout data&lt;/h3&gt;

&lt;p&gt;Following the deployment of our model and receipt of extensive feedback from table owners, we have accumulated a large dataset to further enhance the model. This data, coupled with the dataset of manual classifications from the Data Governance Office to ensure compliance with information classification protocols, serves as the training and testing datasets for the second iteration of our model.&lt;/p&gt;

&lt;h3 id=&quot;model-improvements-with-prompt-engineering&quot;&gt;Model improvements with prompt engineering&lt;/h3&gt;

&lt;p&gt;Expanding the evaluation and testing data allowed us to uncover weaknesses in the previous model. For instance, we discovered that seemingly innocuous table columns like “business email” could contain entries with Personal Identifiable Information (PII) data.&lt;/p&gt;

&lt;p&gt;An example of this would be a business that uses a personal email address containing a legal name—a discrepancy that would be challenging for even human reviewers to detect. Additionally, we discovered nested JSON structures occasionally included personal names, phone numbers, and email addresses hidden among other non-PII metadata. Lastly, we identified passenger communications with Grab occasionally mentioning legal names, phone numbers, and other PII, despite most of the content being non-PII.&lt;/p&gt;

&lt;p&gt;Ultimately, we hypothesised the model’s main issue was model capacity. The model displayed difficulty focusing on large data samples containing a mixture of PII and non-PII data despite having a good understanding of what constitutes PII. Just like humans, when given high volumes of tasks to work on simultaneously, the model’s effectiveness is reduced. In the original model, 13 out of 21 tags were aimed at distinguishing different types of non-PII data. This took up significant model capacity and distracted the model from its actual task: identifying PII data.&lt;/p&gt;

&lt;p&gt;To prevent the model from being overwhelmed, large tasks are divided into smaller, more manageable tasks, allowing the model to dedicate more attention to each task. The following measures were taken to free up model capacity:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Splitting the model into two parts to make problem solving more manageable.
    &lt;ul&gt;
      &lt;li&gt;One part for adding PII tags.&lt;/li&gt;
      &lt;li&gt;Another part for adding all other types of tags.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reducing the number of tags for the first part from 21 to 8 by removing all non-PII tags. This simplifies the task of differentiating types of data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using clear and concise language, removing unnecessary detail. This was done by reducing word count in prompt from 1,254 to 737 words for better data analysis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Splitting tables with more than 150 columns into smaller tables. Fewer table rows means that the LLM has sufficient capacity to focus on each column.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;enabling-rapid-prompt-experimentation-and-deployment&quot;&gt;Enabling rapid prompt experimentation and deployment&lt;/h3&gt;

&lt;p&gt;In our quest to facilitate swift experimentation with various prompt versions, we have empowered a diverse team of data scientists and engineers to work together effectively on the prompts and service. This has been made possible by upgrading our model architecture to incorporate the LangChain and LangSmith frameworks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt; introduces a novel framework that streamlines the process from raw input to the desired outcome by chaining interoperable components. &lt;strong&gt;LangSmith&lt;/strong&gt;, on the other hand, is a unified DevOps platform that fosters collaboration among various team members and developers, including product managers, data scientists, and software engineers. It simplifies the processes of development, collaboration, testing, deployment, and monitoring for all involved.&lt;/p&gt;

&lt;p&gt;Our new backend leverages LangChain to construct an updated model that supports classification tasks for both non-PII and PII tagging. Integration with LangSmith enables data scientists to directly develop prompt templates and conduct experiments via the LangSmith user interface. In addition, managing the evaluation dataset on LangSmith provides a clear view of the performance of prompts across multiple custom metrics.&lt;/p&gt;

&lt;p&gt;The integration of LangChain and LangSmith has significantly improved our model architecture, fostering collaboration and continuous improvement. This has not only streamlined our processes but also enhanced the transparency of our performance metrics. By harnessing the power of these innovative tools, we are better equipped to deliver high-quality, efficient solutions.&lt;/p&gt;

&lt;p&gt;The benefits of the LangChain and LangSmith framework enhancements in Metasense are summarised as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streamlined prompt optimisation process.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Data scientists can create, update, and evaluate prompts directly on the LangSmith user interface and save them in commit mode. For rapid deployment, the prompt identifier in service configurations can be easily adjusted.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/metasensev2/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Streamlined prompt optimisation process.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Transparent prompt performance metrics.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LangSmith’s capabilities allow us to effortlessly run evaluations on a dataset and obtain performance metrics across multiple dimensions, such as accuracy, latency, and error rate.&lt;/p&gt;

&lt;h3 id=&quot;assuring-quality-in-perpetuity&quot;&gt;Assuring quality in perpetuity&lt;/h3&gt;

&lt;p&gt;With exceptionally low misclassification rates recorded, table owners can place greater trust in the model’s outputs and spend less time reviewing them. Nevertheless, as a prudent safety measure, we have set up alerts to monitor misclassification rates periodically, sounding an internal alarm if the rate crosses a defined threshold. A model improvement protocol has also been set in place for such alarms.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The integration of LLM into our metadata generation process has significantly improved our data classification capabilities, reducing manual workloads and increasing accuracy. Continuous improvements, including the adoption of LangChain and LangSmith frameworks, have streamlined prompt optimisation and enhanced collaboration among our team. With low misclassification rates and robust safety measures, our system is both reliable and scalable, fostering trust and efficiency. In conclusion, these advancements ensure we remain at the forefront of data governance, delivering high-quality solutions and valuable insights to our stakeholders.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to express our sincere gratitude to Infocomm Media Development Authority (IMDA) for supporting this initative.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Nov 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/metasense-v2</link>
        <guid isPermaLink="true">https://engineering.grab.com/metasense-v2</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we reduced peak memory and CPU usage of the product configuration management SDK</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;GrabX is Grab’s central platform for product configuration management. It has the capacity to control any component within Grab’s backend systems through configurations that are hosted directly on GrabX.&lt;/p&gt;

&lt;p&gt;GrabX clients read these configurations through an SDK, which reads the configurations in a way that’s asynchronous and eventually consistent. As a result, it takes about a minute for any updates to the configurations to reach the client SDKs.&lt;/p&gt;

&lt;p&gt;In this article, we discuss our analysis and the steps we took to reduce the peak memory and CPU usage of the SDK.&lt;/p&gt;

&lt;h2 id=&quot;observations-on-potential-sdk-improvements&quot;&gt;Observations on potential SDK improvements&lt;/h2&gt;

&lt;p&gt;Our GrabX clients noticed that the GrabX SDK tended to require high memory and CPU usage. From this, we saw opportunities for further improvements that could:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimise the tail latencies of client services.&lt;/li&gt;
  &lt;li&gt;Enable our clients to use their resources more effectively.&lt;/li&gt;
  &lt;li&gt;Reduce operation costs and improve the efficiency of using the GrabX SDK.&lt;/li&gt;
  &lt;li&gt;Accelerate the adoption of GrabX by Grab’s internal services.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sdk-design&quot;&gt;SDK design&lt;/h2&gt;

&lt;p&gt;At a high-level, creating, updating, and serving configuration values via the GrabX SDK involved the following process:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/previous-grabx-sdk-design.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Previous GrabX SDK design.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The process begins when GrabX clients either create or update configurations. This is done through the GrabX web portal or by making an API call.&lt;/li&gt;
  &lt;li&gt;Once the configurations are created or updated, the GrabX backend module takes over. It stores the new configuration into an SQL database table.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The GrabX backend ensures that the latest configuration data is available to client SDKs.&lt;/p&gt;

    &lt;p&gt;a. The GrabX backend checks every minute for any newly created or updated configurations.&lt;/p&gt;

    &lt;p&gt;b. If there are new or updated configurations, GrabX backend creates a new JSON file. This file contains all existing and newly created configurations. It’s important to note that all configurations across all services are stored in a single JSON file.&lt;/p&gt;

    &lt;p&gt;c. The backend module uploads this newly created JSON file to an AWS S3 bucket.&lt;/p&gt;

    &lt;p&gt;d. The backend module assigns a version number to the new JSON file and updates a text file in the AWS S3 bucket. This text file stores the latest JSON file version number. The client SDK refers to this version file to check if a newer version of the configuration data is available.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The client SDK performs a check on the version file every minute to determine if a newer version is available. This mechanism is crucial to maintain data consistency across all instances of a service. If any instance fell out of sync, it would be brought back in sync within a minute.&lt;/li&gt;
  &lt;li&gt;If a new version of the configuration JSON file is available, the client SDK downloads this new file. Following the download, it loads the configuration data into memory. Storing the configuration data in memory reduces the read latency for the configurations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;areas-of-improvement-for-existing-sdk-design&quot;&gt;Areas of improvement for existing SDK design&lt;/h2&gt;

&lt;p&gt;In this section we outline the areas of improvement we identified within the SDK design.&lt;/p&gt;

&lt;h3 id=&quot;service-based-data-partitioning&quot;&gt;Service-based data partitioning&lt;/h3&gt;

&lt;p&gt;We saw an opportunity for service-based data partitioning. The configuration data for all services was consolidated into a single JSON file. Upon studying the data read patterns of client services, we observed that most services primarily needed to access configuration data specific to their own service. However, the present design required storing configuration data for all other services. This resulted in unnecessary memory consumption.&lt;/p&gt;

&lt;h3 id=&quot;retaining-only-new-version-of-configuration-in-the-same-file&quot;&gt;Retaining only new version of configuration in the same file&lt;/h3&gt;

&lt;p&gt;By using a single JSON file for storing old and new configuration data, we saw a significant increase in the size of the JSON file.&lt;/p&gt;

&lt;p&gt;The SDK only needs the full data when it starts; the more common case is that it needs to stay updated with the latest configuration. Even in that scenario, the SDK needed to fetch a complete new JSON file every minute no matter the size of the updates. Consequently, the process of downloading, decoding, and loading high volumes of data at a high frequency (every minute) caused the client SDK to spike in memory and CPU usage.&lt;/p&gt;

&lt;h3 id=&quot;more-efficient-json-decoding&quot;&gt;More efficient JSON decoding&lt;/h3&gt;

&lt;p&gt;An additional factor which contributed to memory and CPU usage during the decoding phase was the inefficiency of the default JSON decode library to decode this large (&amp;gt;100MB) JSON file. Decoding this JSON file was heavy on available CPU resources, which tended to starve the service of its ability to handle incoming requests. This manifested as increasing the P99 latency of the service.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/graph-increased-p99-latency.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Graph illustrating the increased P99 latency due to CPU throttling for a service.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;implemented-solution&quot;&gt;Implemented solution&lt;/h2&gt;

&lt;p&gt;We proposed modifications to the existing SDK design, which we discuss in this section.&lt;/p&gt;

&lt;h3 id=&quot;partition-data-by-service&quot;&gt;Partition data by service&lt;/h3&gt;

&lt;p&gt;The proposed solution involved partitioning the data based on services. We chose this approach because a single configuration typically belonged to a single service, and most services primarily needed to read configurations that pertained to their own service.&lt;/p&gt;

&lt;p&gt;Upon analysing the distribution of service-configuration, we discovered that 98% of client services required less than 1% of the total configuration data. Despite this, they were required to maintain and reload 100% of the configuration data. Furthermore, the service with the largest number of configurations only required 20% of the total configuration data.&lt;/p&gt;

&lt;p&gt;Therefore, we proposed a shift towards service-based partitioning of configuration data. This allowed individual client services to access only the data they needed to read.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/service-config.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Graph showing the number of services with varying amounts of configurations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;create-separate-json-files-for-each-configuration&quot;&gt;Create separate JSON files for each configuration&lt;/h3&gt;

&lt;p&gt;Our proposal also included creating a separate JSON file for each configuration in a service. Previously, all data was stored in a single JSON file housed in an AWS S3 bucket, which supported a maximum of 3,500 write/update requests and 5,500 read requests per second.&lt;/p&gt;

&lt;p&gt;By storing each configuration in a separate JSON file, we were able to create a different S3 prefix for each configuration file. These S3 prefixes helped us to maximise S3 throughput by enhancing the read/write performance for each configuration. AWS S3 can handle at least 3,500 PUT/COPY/POST/DELETE requests or 5,500 GET/HEAD requests per second for each partitioned Amazon S3 prefix.&lt;/p&gt;

&lt;p&gt;Therefore, with each configuration’s data stored in a separate S3 file with a different prefix, the GrabX platform could achieve a throughput of 5,500 read requests and 3,500 write/update requests per second per configuration. This was beneficial for boosting read/write capacity when needed.&lt;/p&gt;

&lt;h3 id=&quot;implement-a-service-level-changelog&quot;&gt;Implement a service-level changelog&lt;/h3&gt;

&lt;p&gt;We proposed to create a changelog file at the service level. In other words, a changelog file was created for each service. This file was used to keep track of the latest update version, as well as previous service configuration update versions. This file also recorded the configurations which were created or updated in each version. This enables the SDK to accurately identify the configurations that were created or updated in each update version. This was useful to update the specific configurations belonging to a service on the client side.&lt;/p&gt;

&lt;h3 id=&quot;implement-service-based-sdk&quot;&gt;Implement service-based SDK&lt;/h3&gt;

&lt;p&gt;We proposed that SDK client services should be allowed to subscribe to a list of services for which they need to read configuration data. The SDK was initialised with data of the subscribed services and received updates only for configurations corresponding to the subscribed services.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/sdk-lifecycle-flowchart.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. This flowchart shows our proposed service-based SDK implementation.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The SDK only sought updates for the subscribed services. The client SDK needed to read the changelog file for each of the subscribed services, comparing the latest changelog version against the SDK version number. Whenever a newer changelog version was available, the SDK updated the variables with the latest version.&lt;/p&gt;

&lt;p&gt;This approach significantly reduced the volume of data that the SDK needed to download, decode, and load into memory during both initialisation and each subsequent update.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In summary, we identified ways to optimise CPU and memory usage in the GrabX SDK. Our analysis revealed that frequent high resource consumption hindered the wider adoption of GrabX. We proposed a series of modifications, including partitioning data by service and creating separate JSON files for each configuration.&lt;/p&gt;

&lt;p&gt;After benchmarking the proposed solution with a variety of configuration data sizes, we found that the solution has the potential to reduce memory utilisation by up to 70% and decrease the maximum CPU utilisation by more than 50%. These improvements significantly enhance the performance and scalability of the GrabX SDK.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/bar-charts-before-after.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Bar charts showcasing memory(MB) &amp;amp; CPU(%) utilisation for Service A before and after using the discussed solution.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Moving forward, we plan to continue optimising the GrabX SDK by exploring additional improvements, such as reducing its initialisation time. These efforts aim to make GrabX an even more robust and reliable solution for product configuration management within Grab’s ecosystem.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/reduced-memory-cpu-usage-grabx-sdk</link>
        <guid isPermaLink="true">https://engineering.grab.com/reduced-memory-cpu-usage-grabx-sdk</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>LLM-assisted vector similarity search</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As the complexity of data retrieval requirements continue to grow, traditional search methods often struggle to provide relevant and accurate results, especially for nuanced or conceptual queries. Vector similarity search has emerged as a powerful technique for finding semantically similar information. It refers to finding vectors in a large dataset that are most similar to a given query vector, typically using some distance or similarity measure. The concept originated in the 1960s with the work by Minsky and Papert on nearest neighbour search &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Since then, the idea has evolved substantially with modern approaches often using approximate methods to enable fast search in high-dimensional spaces, such as locality-sensitive hashing &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and graph-based indexing &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Recently, vector similarity search has become a crucial component in many machine learning and information retrieval applications. It is one of the key technologies that popularised the idea of Retrieval Augmented Generation (RAG) &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; which increased the applicability of Transformer &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; based Generative Large Language Models (LLMs) &lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; in domain-specific tasks without requiring any further training or fine-tuning. However, the effectiveness of the vector search can be limited when dealing with intricate queries or contextual nuances. For example, from a typical vector similarity search perspective, “I like fishing” and “I do not like fishing” may be quite close to each other, while in reality, they are the exact opposite. In this blog post, we discuss an approach that we experimented with that combines vector similarity search with LLMs to enhance the relevance and accuracy of search results for such complex and nuanced queries. We leverage the strengths of both techniques: vector similarity search for efficient shortlisting of potential matches, and LLMs for their ability to understand natural language queries and rank the shortlisted results based on their contextual relevance.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h2&gt;
&lt;p&gt;The proposed solution involves a two-step process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Vector similarity search: We first perform a vector similarity search on the dataset to obtain a shortlist of potential matches (e.g., top 10-50 results) for the given query. This step leverages the efficiency of vector similarity search to quickly narrow down the search space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LLM-assisted ranking: The shortlisted results from the vector similarity search are then fed into an LLM, which ranks the results based on their relevance to the original query. The LLM’s ability to understand natural language queries and contextual information helps in identifying the most relevant results from the shortlist.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By combining these two steps, we aim to achieve the best of both worlds: the efficiency of vector similarity search for initial shortlisting, and the contextual understanding and ranking capabilities of LLMs for refining the final results.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-assisted-vector-similarity-search/similarity-search.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Similarity search and the proposed LLM-assisted similarity search.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;To evaluate the effectiveness of our proposed solution, we conducted experiments on two small synthetic datasets in CSV format that we curated using GPT-4o &lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Food dataset&lt;/strong&gt;: A collection of 100 dishes with their titles and descriptions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tourist spots dataset&lt;/strong&gt;: A collection of 100 tourist spots in Asia, including their names, cities, countries, and descriptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to note that we primarily focus on performing similarity search on structured data such as description of various entities in a relational database.&lt;/p&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;Our experimental setup included a Python script for vector similarity search leveraging Facebook AI Similarity Search (FAISS) &lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, a library developed by Facebook that offers efficient similarity search, and OpenAI’s embeddings (i.e., text-embedding-ada-002) &lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; to generate the vector embeddings needed for facilitating the vector search. For our proposed solution, an LLM component (i.e., GPT-4o) was included in the setup in addition to the FAISS-based similarity search component.&lt;/p&gt;

&lt;h3 id=&quot;observations&quot;&gt;Observations&lt;/h3&gt;

&lt;p&gt;To compare the performance of the proposed approach of LLM-assisted vector similarity search as outlined in the “Proposed solution” section with the raw vector similarity search, we conducted both techniques on our two synthetic datasets. With the raw vector search, we get the top three matches for a given query. For our proposed technique, we first get a shortlist of 15 entity matches from FAISS for the same query, and supply the shortlist and the original query to LLM with some descriptive instructions in the prompt to find the top three matches from the provided shortlist.&lt;/p&gt;

&lt;p&gt;From the experiments, in simpler cases where the queries were straightforward and directly aligned with the textual content of the data, both the raw similarity search and the LLM-assisted similarity search demonstrated comparable performance. However, as the queries became more complex, involving additional constraints, negations, or conceptual requirements, the LLM-assisted search exhibited a clear advantage over the raw similarity search. The LLM’s ability to understand context and capture subtleties in the queries allowed it to filter out irrelevant results and rank the most appropriate ones higher, leading to improved accuracy.&lt;/p&gt;

&lt;p&gt;Here are a few examples where the LLM-assisted similarity search performed better:&lt;/p&gt;

&lt;h4 id=&quot;food-dataset&quot;&gt;Food dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: “food with no fish or shrimp”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- title: Tempura, description: A Japanese dish of seafood or vegetables that have been battered and deep fried.
- title: Ceviche, description: A seafood dish popular in Latin America, made from fresh raw fish cured in citrus juices.
- title: Sushi, description: A Japanese dish consisting of vinegared rice accompanied by various ingredients such as seafood and vegetables.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;LLM-assisted similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- title: Chicken Piccata, description: Chicken breasts cooked in a sauce of lemon, butter, and capers.
- title: Chicken Alfredo, description: An Italian-American dish of pasta in a creamy sauce made from butter and Parmesan cheese.
- title: Chicken Satay, description: Grilled chicken skewers served with peanut sauce.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;: The LLM correctly filtered out dishes containing fish or shrimp, while the raw similarity search failed to do so, presumably due to the presence of negation in the query.&lt;/p&gt;

&lt;h4 id=&quot;tourist-spots-dataset&quot;&gt;Tourist spots dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: “exposure to wildlife”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Ocean Park, city: Hong Kong, country: Hong Kong, description: Marine mammal park and oceanarium.
- name: Merlion Park, city: Singapore, country: Singapore, description: Iconic statue with the head of a lion and body of a fish.
- name: Manila Bay, city: Manila, country: Philippines, description: A natural harbor known for its sunset views.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;LLM-assisted similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Ocean Park, city: Hong Kong, country: Hong Kong, description: Marine mammal park and oceanarium.
- name: Chengdu Research Base, city: Chengdu, country: China, description: A research center for giant panda breeding.
- name: Mount Hua, city: Shaanxi, country: China, description: Mountain known for its dangerous hiking trails.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;: Two out of the top three matches by the LLM-assisted technique seem relevant to the query while only one result from the raw similarity search is relevant and the other two being somewhat irrelevant to the query. The LLM identified the relevance of a research base for giant panda breeding to the “exposure to wildlife”, which the raw similarity search ignored in its ranking.&lt;/p&gt;

&lt;p&gt;These examples provide a glimpse into the utility of LLMs in finding more relevant matches in scenarios where the queries involved additional context, constraints, or conceptual requirements beyond simple keyword matching. On the other hand, when the queries were more straightforward and focused on specific keywords or phrases present in the data, both approaches demonstrated comparable performance. For instance, queries like “Japanese food” or “beautiful mountains” yielded similar results from both the raw similarity search and the proposed LLM-assisted approach.&lt;/p&gt;

&lt;p&gt;Overall, the LLM-assisted vector search exhibited a clear advantage in handling complex queries, leveraging its ability to understand natural language and contextual information. However, for simpler queries, the raw similarity search remained a viable option, especially when computational efficiency is a concern.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The experiments demonstrated the potential of combining vector similarity search with LLMs to enhance the relevance and accuracy of search results, particularly for complex and nuanced queries. While vector similarity search alone can provide reasonable results for straightforward queries, the LLM-assisted approach shines when dealing with queries that require a deeper understanding of context, nuances, and conceptual relationships. By leveraging the natural language understanding capabilities of LLMs, this approach can better capture the intent behind complex queries and provide more relevant search results.&lt;/p&gt;

&lt;p&gt;Our experiment was limited to using a small volume of structured data (100 data points in each dataset) with a limited number of queries. However, we have witnessed similar enhancement in search result relevance when we deployed this solution internally within Grab for larger datasets, for example, 4500+ rows of data stored in a relational database.&lt;/p&gt;

&lt;p&gt;Nevertheless, it is important to note that the effectiveness of this approach may still depend on the quality and complexity of the data, as well as the specific use case and query patterns. We believe it is still worthwhile to evaluate the proposed approach for more diverse (e.g., beyond CSV) and larger datasets. An interesting future work can be varying the size of the shortlist from the similarity search and observing how it impacts the overall search relevance when using the proposed approach. In addition, for real world applications, the performance implications in terms of additional latency introduced by the additional LLM query must also be considered.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;M. Minsky and S. Papert, Perceptrons: An Introduction to Computational Geometry. MIT Press, 1969. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;P. Indyk and R. Motwani, “Approximate nearest neighbors: Towards removing the curse of dimensionality,” in Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, 1998. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Y. Malkov and D. Yashunin, “Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in Advances in Neural Information Processing Systems, 2020. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A. Vaswani, “Attention is all you need,” in Advances in Neural Information Processing Systems, 2017. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A. Radford, “Improving language understanding by generative pre-training,” 2018. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“Hello GPT-4o,” OpenAI, May 2024. [Online]. Available: https://openai.com/index/hello-gpt-4o/. [Accessed: Oct. 6, 2024]. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P. E. Mazaré, and H. Jégou, “The faiss library,” arXiv preprint arXiv:2401.08281, 2024. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“Embeddings,” OpenAI API. [Online]. Available: https://platform.openai.com/docs/guides/embeddings. [Accessed: Oct. 6, 2024]. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 23 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-assisted-vector-similarity-search</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-assisted-vector-similarity-search</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        <category>Experiment</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Leveraging RAG-powered LLMs for Analytical Tasks</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Retrieval-Augmented Generation (RAG) is a powerful process that is designed to integrate direct function calling to answer queries more efficiently by retrieving relevant information from a broad database. In the rapidly evolving business landscape, Data Analysts (DAs) are struggling with the growing number of data queries from stakeholders. The conventional method of manually writing and running similar queries repeatedly is time-consuming and inefficient. This is where RAG-powered Large Language Models (LLMs) step in, offering a transformative solution to streamline the analytics process and empower DAs to focus on higher value tasks.&lt;/p&gt;

&lt;p&gt;In this article, we will share how the Integrity Analytics team has built out a data solution using LLMs to help automate tedious analytical tasks like generating regular metric reports and performing fraud investigations.&lt;/p&gt;

&lt;p&gt;While LLMs are known for their proficiency in data interpretation and insight generation, they represent just a fragment of the entire solution. For a comprehensive solution, LLMs must be integrated with other essential tools. The following is required in assembling a solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Internally facing LLM tool -&lt;/strong&gt; Spellvault is a platform within Grab that stores, shares, and refines LLM prompts. It features low/no-code RAG capabilities that lower the barrier of entry for people to create LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data -&lt;/strong&gt; with real time or close to real-time latency to ensure accuracy. It has to be in a standardised format to ensure that all LLM data inputs are accurate.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduler -&lt;/strong&gt;  runs LLM applications at regular intervals. Useful for automating routine tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Messaging Tool -&lt;/strong&gt; a user interface where users can interact with LLM by entering a command to receive reports and insights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introducing-data-arks-the-data-middleware-serving-up-relevant-data-to-the-llm-agents&quot;&gt;Introducing Data-Arks, the data middleware serving up relevant data to the LLM agents&lt;/h2&gt;

&lt;p&gt;For most data use cases, DAs are usually running the same set of SQL queries with minor changes to parameters like dates, age or other filter conditions. In most instances, we already have a clear understanding of the required data and format to accomplish a task. Therefore, we need a tool that can execute the &lt;strong&gt;exact SQL query&lt;/strong&gt; and channel the data output to the LLM.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Data-Arks hosts various APIs which can be called to serve data to applications like SpellVault.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-data-arks&quot;&gt;What is Data-Arks?&lt;/h3&gt;

&lt;p&gt;Data-Arks is an in-house Python-based API platform housing several frequently used SQL queries and python functions packaged into individual APIs. Data-Arks is also integrated with Slack, Wiki, and JIRA APIs, allowing users to parse and fetch information and data from these tools as well. The benefits of Data-Arks are summarised as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration:&lt;/strong&gt; Data-Arks service allows users to upload any SQL query or Python script on the platform. These queries are then surfaced as APIs, which can be called to serve data to the LLM agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Versatility: Data-Arks&lt;/strong&gt; can be extended to everyone. Employees from various teams and functions at Grab can self-serve to upload any SQL query that they want onto the platform, allowing this tool to be used for different teams’ use cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;automating-regular-report-generation-and-summarisation-using-data-arks-and-spellvault&quot;&gt;Automating regular report generation and summarisation using Data-Arks and Spellvault&lt;/h2&gt;

&lt;p&gt;LLMs are just one piece of the puzzle, to build a comprehensive solution, they must be integrated with other tools. Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;p&gt;Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Report Summarizer uses various tools to summarise queries and deliver a summarised report through Slack.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 is an example of a summarised report generated by the Report Summarizer using dummy data.  Report Summarizer calls a Data-Arks API to generate the data in a tabular format and LLM helps summarise and generate a short paragraph of key insights. This automated report generation has helped save an estimated 3-4 hours per report.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Sample of a report generated using dummy data extracted from [https://data.gov.my/](https://data.gov.my/). &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;llm-bots-for-fraud-investigations&quot;&gt;LLM bots for fraud investigations&lt;/h2&gt;

&lt;p&gt;LLMs also excel in helping to streamline fraud investigations, as LLMs are able to contextualise several different data points and information and derive useful insights from them.&lt;/p&gt;

&lt;p&gt;Introducing &lt;strong&gt;A* bot&lt;/strong&gt;, the team’s very own LLM fraud investigation helper.&lt;/p&gt;

&lt;p&gt;A set of frequently used queries for fraud investigation is made available as Data-Arks APIs. Upon a user prompt or query, SpellVault selects the most relevant queries using RAG, executes them and provides a summary of the results to users through Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. A* bot uses Data-Arks and Spellvault to get information for fraud investigations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 shows a sample of fraud investigation responses from A* bot. Scaling to multiple queries for a fraud investigation process, what was once a time-consuming fraud investigation can now be reduced to a matter of minutes, as the A* bot is capable of providing all the necessary information simultaneously.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Sample of fraud investigation responses.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;rag-vs-fine-tuning&quot;&gt;RAG vs fine-tuning&lt;/h2&gt;

&lt;p&gt;On deciding between RAG or fine-tuning to improve LLM accuracy, three key factors tipped the scales in favour of the RAG approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Effort and cost considerations&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires significant computational cost as it involves taking a base model and further training it with smaller, domain specific data and context. RAG is computationally less expensive as it relies on retrieving only relevant data and context to augment a model’s response. As the same base model can be used for different use cases, RAG is the preferred choice due to its flexibility and cost efficiency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ability to respond with the latest information&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires model re-training with each new information update, whereas RAG simply retrieves required context and data from a knowledge base to enhance its response. Thus, by using RAG, LLM is able to answer questions using the most current information from our production database, eliminating the need for model re-training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Speed and scalability&lt;/strong&gt;&lt;br /&gt;
Without the burden of model re-training, the team can rapidly scale and build out new LLM applications with a well managed knowledge base.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The potential of using RAG-powered LLM can be limitless as the ability of GPT is correlated with the tools it equips. Hence, the process does not stop here and we will try to onboard more tools or integration to GPT. In the near future, we plan to utilise Data-Arks to provide images to GPT as GPT-4o is a multimodal model that has vision capabilities. We are committed to pushing the boundaries of what’s possible with RAG-powered LLM, and we look forward to unveiling the exciting advancements that lie ahead.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-what-next.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. What’s next?&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to express our sincere gratitude to the following individuals and teams whose invaluable support and contributions have made this project a reality: &lt;br /&gt;- Meichen Lu, a senior data scientist at Grab, for her guidance and assistance in building the MVP and testing the concept.&lt;br /&gt;- The data engineering team, particularly Jia Long Loh and Pu Li, for setting up the necessary services and infrastructure. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Wed, 09 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</link>
        <guid isPermaLink="true">https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Analytics</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Evolution of Catwalk: Model serving platform at Grab</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As Southeast Asia’s leading super app, Grab serves millions of users across multiple countries every day. Our services range from ride-hailing and food delivery to digital payments and much more. The backbone of our operations? Machine Learning (ML) models. They power our real-time decision-making capabilities, enabling us to provide a seamless and personalised experience to our users. Whether it’s determining the most efficient route for a ride, suggesting a food outlet based on a user’s preference, or detecting fraudulent transactions, ML models are at the forefront.&lt;/p&gt;

&lt;p&gt;However, serving these ML models at Grab’s scale is no small feat. It requires a robust, efficient, and scalable model serving platform, which is where our ML model serving platform, Catwalk, comes in.&lt;/p&gt;

&lt;p&gt;Catwalk has evolved over time, adapting to the growing needs of our business and the ever-changing tech landscape. It has been a journey of continuous learning and improvement, with each step bringing new challenges and opportunities.&lt;/p&gt;

&lt;h1 id=&quot;evolution-of-the-platform&quot;&gt;Evolution of the platform&lt;/h1&gt;

&lt;h2 id=&quot;phase-0-the-need-for-a-model-serving-platform&quot;&gt;Phase 0: The need for a model serving platform&lt;/h2&gt;

&lt;p&gt;Before Catwalk’s debut as our dedicated model serving platform, data scientists across the company employed various ad-hoc approaches to serve ML models. These included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shipping models online using custom solutions.&lt;/li&gt;
  &lt;li&gt;Relying on backend engineering teams to deploy and manage trained ML models.&lt;/li&gt;
  &lt;li&gt;Embedding ML logic within Go backend services.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These methods, however, led to several challenges, undercovering the need for a unified, company-wide platform for serving machine learning models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Operational overhead&lt;/strong&gt;: Data scientists often lacked the necessary expertise to handle the operational aspects of their models, leading to service outages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource wastage&lt;/strong&gt;: There was frequently low resource utilisation (e.g., 1%) for data science services, leading to inefficient use of resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Friction with engineering teams&lt;/strong&gt;: Differences in release cycles and unclear ownership when code was embedded into backend systems resulted in tension between data scientists and engineers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinventing the wheel&lt;/strong&gt;: Multiple teams independently attempted to solve model serving problems, leading to a duplication of effort.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​​These challenges highlighted the need for a company-wide, centralised platform for serving machine learning models.&lt;/p&gt;

&lt;h2 id=&quot;phase-1-no-code-managed-platform-for-tensorflow-serving-models&quot;&gt;Phase 1: No-code, managed platform for TensorFlow Serving models&lt;/h2&gt;

&lt;p&gt;Our initial foray into model serving was centred around creating a managed platform for deploying TensorFlow Serving models. The process involved data scientists submitting their models to the platform’s engineering admin, who could then deploy the model with an endpoint. Infrastructure and networking were managed using Amazon Elastic Kubernetes Service (EKS) and Helm Charts as illustrated below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This phase of our platform, which we also detailed in our &lt;a href=&quot;https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale&quot;&gt;previous article&lt;/a&gt;, was beneficial for some users. However, we quickly encountered scalability challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Codebase maintenance&lt;/strong&gt;: Applying changes to every TensorFlow Serving (TFS) version was cumbersome and difficult to maintain.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited scalability&lt;/strong&gt;: The fully managed nature of the platform made it difficult to scale.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Admin bottleneck&lt;/strong&gt;: The engineering admin’s limited bandwidth became a bottleneck for onboarding new models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited serving types&lt;/strong&gt;: The platform only supported TensorFlow, limiting its usefulness for data scientists using other frameworks like LightGBM, XGBoost, or PyTorch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After a year of operation, only eight models were onboarded to the platform, highlighting the need for a more scalable and flexible solution.&lt;/p&gt;

&lt;h2 id=&quot;phase-2-from-models-to-model-serving-applications&quot;&gt;Phase 2: From models to model serving applications&lt;/h2&gt;

&lt;p&gt;To address the limitations of Phase 1, we transitioned from deploying individual models to self-contained model serving applications. This “low-code, self-serving” strategy introduced several new components and changes as illustrated in the points and diagram below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Support for multiple serving types&lt;/strong&gt;: Users gained the ability to deploy models trained with a variety of frameworks like Open Neural Network Exchange (ONNX), PyTorch, and TensorFlow.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Self-served platform through CI/CD pipelines&lt;/strong&gt;: Data scientists could self-serve and independently manage their model serving applications through CI/CD pipelines.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;New components&lt;/strong&gt;: We introduced these new components to support the self-serving approach:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Catwalk proxy&lt;/strong&gt;, a managed reverse proxy to various serving types.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Catwalk transformer&lt;/strong&gt;, a low-code component to transform input and output data.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amphawa&lt;/strong&gt;, a feature fetching component to augment model inputs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;api-request-flow&quot;&gt;API request flow&lt;/h4&gt;

&lt;p&gt;The Catwalk proxy acts as the orchestration layer. Clients send requests to Catwalk proxy then it orchestrates calls to different components like transformers, feature-store, and so on. A typical end to end request flow is illustrated below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase2-api-request-flow.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Within a year of implementing these changes, the number of models on the platform increased from 8 to 300, demonstrating the success of this approach. However, new challenges emerged:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Complexity of maintaining Helm chart&lt;/strong&gt;: As the platform continued to grow with new components and functionalities, maintaining the Helm chart became increasingly complex. The readability and flow control became more challenging, making the helm chart updating process prone to errors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Process-level mistakes&lt;/strong&gt;: The self-serving approach led to errors such as pushing empty or incompatible models to production, setting too few replicas, or allocating insufficient resources, which resulted in service crashes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We knew that our work was nowhere near done. We had to keep iterating and explore ways to address the new challenges.&lt;/p&gt;

&lt;h2 id=&quot;phase-3-replacing-helm-charts-with-kubernetes-crds&quot;&gt;Phase 3: Replacing Helm charts with Kubernetes CRDs&lt;/h2&gt;

&lt;p&gt;To tackle the deployment challenges and gain more control, we made the significant decision to replace Helm charts with Kubernetes Custom Resource Definitions (CRDs). This required substantial engineering effort, but the outcomes have been rewarding. This transition gave us improved control over deployment pipelines, enabling customisations such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Smart defaults for AutoML&lt;/li&gt;
  &lt;li&gt;Blue-green deployments&lt;/li&gt;
  &lt;li&gt;Capacity management&lt;/li&gt;
  &lt;li&gt;Advanced scaling&lt;/li&gt;
  &lt;li&gt;Application set groupings&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is an example of a simple model serving CRD manifest:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: ml.catwalk.kubebuilder.io/v1
kind: ModelServing
spec:
  hpa:
    desired: 1
    max: 1
    min: 1
  modelMeta:
    modelName: &quot;my-model&quot;
    modelOwner: john.doe
  proxyLayer:
    enableLogging: true
    logHTTPBody: true
  servingLayer:
    servingType: &quot;tensorflow-serving&quot;
    version: &quot;20&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;model-serving-crd-deployment-state-machine&quot;&gt;Model serving CRD deployment state machine&lt;/h4&gt;

&lt;p&gt;Every model serving CRD submission follows a sequence of steps. If there are failures at any step, the controller keeps retrying after small intervals. The major steps on the deployment cycle are described below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Validate whether the new CRD specs are acceptable. Along with sanity checks, we also enforce a lot of platform constraints through this step.&lt;/li&gt;
  &lt;li&gt;Clean up previous non-ready deployment resources. Sometimes a deployment submission might keep crashing and hence it doesn’t proceed to a ready state. On every submission, it’s important to check and clean up such previous deployments.&lt;/li&gt;
  &lt;li&gt;Create resources for the new deployment and ensure that the new deployment is ready.&lt;/li&gt;
  &lt;li&gt;Switch traffic from old deployment to the new deployment.&lt;/li&gt;
  &lt;li&gt;Clean up resources for old deployment. At this point, traffic is already being served by the new deployment resources. So, we can clean up the old deployment.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;phase-4-transition-to-a-high-code-self-served-process-managed-platform&quot;&gt;Phase 4: Transition to a high-code, self-served, process-managed platform&lt;/h2&gt;

&lt;p&gt;As the number of model serving applications and use cases multiplied, clients sought greater control over orchestrations between different models, experiment executions, traffic shadowing, and responses archiving. To cater to these needs, we introduced several changes and components with the Catwalk Orchestrator, a high code orchestration solution, leading the pack.&lt;/p&gt;

&lt;h4 id=&quot;catwalk-orchestrator&quot;&gt;Catwalk orchestrator&lt;/h4&gt;

&lt;p&gt;The &lt;strong&gt;Catwalk Orchestrator&lt;/strong&gt; is a highly abstracted framework for building ML applications that replaced the catwalk-proxy from previous phases. The key difference is that users can now write their own business/orchestration logic. The orchestrator offers a range of utilities, reducing the need for users to write extensive boilerplate code. Key components of the Catwalk Orchestrator include HTTP server, gRPC server, clients for different model serving flavours (TensorFlow, ONNX, PyTorch, etc), client for fetching features from the feature bank, and utilities for logging, metrics, and data lake ingestion.&lt;/p&gt;

&lt;p&gt;The Catwalk Orchestrator is designed to streamline the deployment of machine learning models. Here’s a typical user journey:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Scaffold a model serving application&lt;/strong&gt;: Users begin by scaffolding a model serving application using a command-line tool.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Write business logic&lt;/strong&gt;: Users then write the business logic for the application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy to staging&lt;/strong&gt;: The application is then deployed to a staging environment for testing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Complete load testing&lt;/strong&gt;: Users test the application in the staging environment and complete load testing to ensure it can handle the expected traffic.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy to production&lt;/strong&gt;: Once testing is completed, the application is deployed to the production environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;bundled-deployments&quot;&gt;Bundled deployments&lt;/h4&gt;

&lt;p&gt;To support multiple ML models as part of a single model serving application, we introduced the concept of &lt;strong&gt;bundled deployments&lt;/strong&gt;. Multiple Kubernetes deployments are bundled together as a single model serving application deployment, allowing each component (e.g., models, catwalk-orchestrator, etc) to have its own Kubernetes deployment and to scale independently.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to the major developments, we implemented other changes to enhance our platform’s efficiency. We made &lt;strong&gt;load testing&lt;/strong&gt; mandatory for all ML application updates to ensure robust performance. This testing process was streamlined with a single command that runs the load test in the staging environment, with the results directly shared with the user.&lt;/p&gt;

&lt;p&gt;Furthermore, we boosted &lt;strong&gt;deployment transparency&lt;/strong&gt; by sharing deployment details through Slack and Datadog. This empowered users to diagnose issues independently, reducing the dependency on on-call support. This transparency not only improved our issue resolution times but also enhanced user confidence in our platform.&lt;/p&gt;

&lt;p&gt;The results of these changes speak for themselves. The Catwalk Orchestrator has evolved into our flagship product. In just two years, we have deployed 200 Catwalk Orchestrators serving approximately 1,400 ML models.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h1&gt;

&lt;p&gt;As we continue to innovate and enhance our model serving platform, we are venturing into new territories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Catwalk serverless&lt;/strong&gt;: We aim to further abstract the model serving experience, making it even more user-friendly and efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Catwalk data serving&lt;/strong&gt;: We are looking to extend Catwalk’s capabilities to serve data online, providing a more comprehensive service.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LLM serving&lt;/strong&gt;: In line with the trend towards generative AI and large language models (LLMs), we’re pivoting Catwalk to support these developments, ensuring we stay at the forefront of the AI and machine learning field.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stay tuned as we continue to advance our technology and bring these exciting developments to life.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Oct 2024 00:00:50 +0000</pubDate>
        <link>https://engineering.grab.com/catwalk-evolution</link>
        <guid isPermaLink="true">https://engineering.grab.com/catwalk-evolution</guid>
        
        <category>Machine Learning</category>
        
        <category>Models</category>
        
        <category>Data Science</category>
        
        <category>TensorFlow</category>
        
        <category>Kubernetes</category>
        
        <category>Docker</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Enabling conversational data discovery with LLMs at Grab</title>
        <description>&lt;p&gt;Imagine a world where finding the right data is like searching for a needle in a haystack. In today’s data-driven landscape, companies are drowning in a sea of information, struggling to navigate through countless datasets to uncover valuable insights. At Grab, we faced a similar challenge. With over 200,000 tables in our data lake, along with numerous Kafka streams, production databases, and ML features, locating the most suitable dataset for our Grabber’s use cases promptly has historically been a significant hurdle.&lt;/p&gt;

&lt;h2 id=&quot;problem-space&quot;&gt;Problem Space&lt;/h2&gt;

&lt;p&gt;Our internal data discovery tool, Hubble, built on top of the popular open-source platform Datahub, was primarily used as a reference tool. While it excelled at providing metadata for known datasets, it struggled with true data discovery due to its reliance on Elasticsearch, which performs well for keyword searches but cannot accept and use user-provided context (i.e., it can’t perform semantic search, at least in its vanilla form). The Elasticsearch parameters provided by Datahub out of the box also had limitations: our monthly average click-through rate was only 82%, meaning that in 18% of sessions, users abandoned their searches without clicking on any dataset. This suggested that the search results were not meeting their needs.&lt;/p&gt;

&lt;p&gt;Another indispensable requirement for efficient data discovery that was missing at Grab was documentation. Documentation coverage for our data lake tables was low, with only 20% of the most frequently queried tables (colloquially referred to as P80 tables) having existing documentation. This made it difficult for users to understand the purpose and contents of different tables, even when browsing through them on the Hubble UI.&lt;/p&gt;

&lt;p&gt;Consequently, data consumers heavily relied on tribal knowledge, often turning to their colleagues via Slack to find the datasets they needed. A survey conducted last year revealed that 51% of data consumers at Grab took multiple days to find the dataset they required, highlighting the inefficiencies in our data discovery process.&lt;/p&gt;

&lt;p&gt;To address these challenges and align with Grab’s ongoing journey towards a data mesh architecture, the Hubble team recognised the importance of improving data discovery. We embarked on a journey to revolutionise the way our employees find and access the data they need, leveraging the power of AI and Large Language Models (LLMs).&lt;/p&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;

&lt;p&gt;Given the historical context, our vision was clear: to remove humans in the data discovery loop by automating the entire process using LLM-powered products. We aimed to reduce the time taken for data discovery from multiple days to mere seconds, eliminating the need for anyone to ask their colleagues data discovery questions ever again.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;goals&quot;&gt;Goals&lt;/h2&gt;

&lt;p&gt;To achieve our vision, we set the following goals for ourselves for the first half of 2024:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Build HubbleIQ:&lt;/strong&gt; An LLM-based chatbot that could serve as the equivalent of a Lead Data Analyst for data discovery. Just as a lead is an expert in their domain and can guide data consumers to the right dataset, we wanted HubbleIQ to do the same across all domains at Grab. We also wanted HubbleIQ to be accessible where data consumers hang out the most: Slack.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improve documentation coverage:&lt;/strong&gt; A new Lead Analyst joining the team would require extensive documentation coverage of very high quality. Without this, they wouldn’t know what data exists and where. Thus, it was important for us to improve documentation coverage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhance Elasticsearch:&lt;/strong&gt; We aimed to tune our Elasticsearch implementation to better meet the requirements of Grab’s data consumers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-systematic-path-to-success&quot;&gt;A Systematic Path to Success&lt;/h2&gt;

&lt;h3 id=&quot;step-1-enhance-elasticsearch&quot;&gt;Step 1: Enhance Elasticsearch&lt;/h3&gt;

&lt;p&gt;Through clickstream analysis and user interviews, the Hubble team identified four categories of data search queries that were seen either on the Hubble UI or in Slack channels:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Exact search:&lt;/strong&gt; Queries belonging to this category were a substring of an existing dataset’s name at Grab, with the query length being at least 40% of the dataset’s name.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Partial search:&lt;/strong&gt; The Levenshtein distance between a query in this category and any existing dataset’s name was greater than 80. This category usually comprised queries that closely resembled an existing dataset name but likely contained spelling mistakes or were shorter than the actual name.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Exact and partial searches accounted for 75% of searches on Hubble (and were non-existent on Slack: as a human, receiving a message that just had the name of a dataset would feel rather odd). Given the effectiveness of vanilla Elasticsearch for these categories, the click rank was close to 0.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inexact search:&lt;/strong&gt; This category comprised queries that were usually colloquial keywords or phrases that may be semantically related to a given table, column, or piece of documentation (e.g., “city” or “taxi type”). Inexact searches accounted for the remaining 25% of searches on Hubble. Vanilla Elasticsearch did not perform well in this category since it relied on pure keyword matching and did not consider any additional context.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Semantic search:&lt;/strong&gt; These were free text queries with abundant contextual information supplied by the user. Hubble did not see any such queries as users rightly expected that Hubble would not be able to fulfil their search needs. Instead, these queries were sent by data consumers to data producers via Slack. Such queries were numerous, but usually resulted in data hunting journeys that spanned multiple days - the root of frustration amongst data consumers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two search types can be seen as “reference” queries, where the data consumer already knows what they are looking for. Inexact and contextual searches are considered “discovery” queries. The Hubble team noticed drop-offs in inexact searches because users learned that Hubble could not fulfil their discovery needs, forcing them to search for alternatives.&lt;/p&gt;

&lt;p&gt;Through user interviews, the team discovered how Elasticsearch should be tuned to better fit the Grab context. They implemented the following optimisations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tagging and boosting P80 tables&lt;/li&gt;
  &lt;li&gt;Boosting the most relevant schemas&lt;/li&gt;
  &lt;li&gt;Hiding irrelevant datasets like PowerBI dataset tables&lt;/li&gt;
  &lt;li&gt;Deboosting deprecated tables&lt;/li&gt;
  &lt;li&gt;Improving the search UI by simplifying and reducing clutter&lt;/li&gt;
  &lt;li&gt;Adding relevant tags&lt;/li&gt;
  &lt;li&gt;Boosting certified tables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result of these enhancements, the click-through rate rose steadily over the course of the half to 94%, a 12 percentage point increase.&lt;/p&gt;

&lt;p&gt;While this helped us make significant improvements to the first three search categories, we knew we had to build HubbleIQ to truly automate the last category - semantic search.&lt;/p&gt;

&lt;h3 id=&quot;step-2-build-a-context-store-for-hubbleiq&quot;&gt;Step 2: Build a Context Store for HubbleIQ&lt;/h3&gt;

&lt;p&gt;To support HubbleIQ, we built a documentation generation engine that used GPT-4 to generate documentation based on table schemas and sample data. We refined the prompt through multiple iterations of feedback from data producers.&lt;/p&gt;

&lt;p&gt;We added a “generate” button on the Hubble UI, allowing data producers to easily generate documentation for their tables. This feature also supported the ongoing Grab-wide initiative to certify tables.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image7.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In conjunction, we took the initiative to pre-populate docs for the most critical tables, while notifying data producers to review the generated documentation. Such docs were visible to data consumers with an “AI-generated” tag as a precaution. When data producers accepted or edited the documentation, the tag was removed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image3.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As a result, documentation coverage for P80 tables increased by 70 percentage points to ~90%. User feedback showed that ~95% of users found the generated docs useful.&lt;/p&gt;

&lt;h3 id=&quot;step-3-build-and-launch-hubbleiq&quot;&gt;Step 3: Build and Launch HubbleIQ&lt;/h3&gt;

&lt;p&gt;With high documentation coverage in place, we were ready to harness the power of LLMs for data discovery. To speed up go-to-market, we decided to leverage &lt;a href=&quot;https://www.glean.com/&quot;&gt;Glean&lt;/a&gt;, an enterprise search tool used by Grab.&lt;/p&gt;

&lt;p&gt;First, we integrated Hubble with Glean, making all data lake tables with documentation available on the Glean platform. Next, we used &lt;a href=&quot;https://www.glean.com/product/apps&quot;&gt;Glean Apps&lt;/a&gt; to create the HubbleIQ bot, which was essentially an LLM with a custom system prompt that could access all Hubble datasets that were catalogued on Glean. Finally, we integrated this bot into Hubble search, such that for any search that is likely to be a semantic search, HubbleIQ results are shown on top, followed by regular search results.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image5.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Recently, we integrated HubbleIQ with Slack, allowing data consumers to discover datasets without breaking their flow. Currently, we are working with analytics teams to add the bot to their “ask” channels (where data consumers come to ask contextual search queries for their domains). After integration, HubbleIQ will act as the first line of defence for answering questions in these channels, reducing the need for human intervention.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The impact of these improvements was significant. A follow-up survey revealed that 73% of respondents found it easy to discover datasets, marking a substantial 17 percentage point increase from the previous survey. Moreover, Hubble reached an all-time high in monthly active users, demonstrating the effectiveness of the enhancements made to the platform.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We’ve made significant progress towards our vision, but there’s still work to be done. Looking ahead, we have several exciting initiatives planned to further enhance data discovery at Grab.&lt;/p&gt;

&lt;p&gt;On the documentation generation front, we aim to enrich the generator with more context, enabling it to produce even more accurate and relevant documentation. We also plan to streamline the process by allowing analysts to auto-update data docs based on Slack threads directly from Slack. To ensure the highest quality of documentation, we will develop an evaluator model that leverages LLMs to assess the quality of both human and AI-written docs. Additionally, we will implement Reflexion, an agentic workflow that utilises the outputs from the doc evaluator to iteratively regenerate docs until a quality benchmark is met or a maximum try-limit is reached.&lt;/p&gt;

&lt;p&gt;As for HubbleIQ, our focus will be on continuous improvement. We’ve already added support for metric datasets and are actively working on incorporating other types of datasets as well. To provide a more seamless user experience, we will enable users to ask follow-up questions to HubbleIQ directly on the HubbleUI, with the system intelligently pulling additional metadata when a user mentions a specific dataset.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;By harnessing the power of AI and LLMs, the Hubble team has made significant strides in improving documentation coverage, enhancing search capabilities, and drastically reducing the time taken for data discovery. While our efforts so far have been successful, there are still steps to be taken before we fully achieve our vision of completely replacing the reliance on data producers for data discovery. Nonetheless, with our upcoming initiatives and the groundwork we have laid, we are confident that we will continue to make substantial progress in the right direction over the next few production cycles.&lt;/p&gt;

&lt;p&gt;As we forge ahead, we remain dedicated to refining and expanding our AI-powered data discovery tools, ensuring that Grabbers have every dataset they need to drive Grab’s success at their fingertips. The future of data discovery at Grab is brimming with possibilities, and the Hubble team is thrilled to be at the forefront of this exciting journey.&lt;/p&gt;

&lt;p&gt;To our readers, we hope that our journey has inspired you to explore how you can leverage the power of AI to transform data discovery within your own organisations. The challenges you face may be unique, but the principles and strategies we have shared can serve as a foundation for your own data discovery revolution. By embracing innovation, focusing on user needs, and harnessing the potential of cutting-edge technologies, you too can unlock the full potential of your data and propel your organisation to new heights. The future of data-driven innovation is here, and we invite you to join us on this exhilarating journey.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Sep 2024 00:00:40 +0000</pubDate>
        <link>https://engineering.grab.com/hubble-data-discovery</link>
        <guid isPermaLink="true">https://engineering.grab.com/hubble-data-discovery</guid>
        
        <category>Data Discovery</category>
        
        <category>AI</category>
        
        <category>LLM</category>
        
        <category>Documentation</category>
        
        <category>Elasticsearch</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Bringing Grab’s Live Activity to Android: Enhancing user experience through custom notifications</title>
        <description>&lt;p&gt;In May 2023, Grab unveiled the Live Activity feature for iOS, which received positive feedback from users. Live Activity is a feature that enhances user experience by displaying a user interface (UI) outside of the app, delivering real-time updates and interactive content. At Grab, we leverage this feature to keep users informed about their order updates without requiring them to manually open the app.&lt;/p&gt;

&lt;p&gt;While Live Activity is a native iOS feature provided by Apple, there is currently no official Android equivalent. However, we are determined to bring this immersive experience to Android users. Inspired by the success of Live Activity on iOS, we have embarked on design explorations and feasibility studies to ensure the seamless integration of Live Activity into the Android platform. Our ultimate goal is to provide Android users with the same level of convenience and real-time updates, elevating their Grab experience.&lt;/p&gt;

&lt;h2 id=&quot;product-exploration&quot;&gt;Product Exploration&lt;/h2&gt;

&lt;p&gt;In July 2023, we took a proactive step by forming a dedicated working group with the specific goal of exploring Live Activity on the Android platform. Our mindset was focused on quickly enabling the MVP (Minimum Viable Product) of this feature for Android users. We focused on enabling Grab users to track food and mart orders on Live Activity as our first use-case. We also designed the Live Activity module as an extendable platform, allowing easy adoption by other Grab internal verticals such as the Express and Transport teams.&lt;/p&gt;

&lt;p&gt;The team kicked off by analysing the current solution and end-to-end flow of Live Activity on iOS. The objective was to uncover opportunities on how we could leverage the existing platform approach.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure1.png&quot; alt=&quot;&quot; style=&quot;width:30%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Grab iOS Live Activity flow.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The first thing that caught our attention was that there is no Live Activity Token (also known as Push Token) concept on Android. Push Token is a token generated from the ActivityKit framework and used to remotely start, update, and end Live Activity notifications on iOS.&lt;/p&gt;

&lt;p&gt;Our goal was to match the Live Activity set-up of iOS in Android, which was a challenge due to the missing Push Token. This required us to think outside the box and develop an innovative workaround. After multiple brainstorming sessions, the team developed two potential solutions, Solution 1 and Solution 2, as illustrated below:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Proposed solutions for Live Activity for Android.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We evaluated the two solutions. The first solution is to substitute the Push Token with a placeholder value, serving as a distinctive notification identifier. Whereas, the second solution involves the Hedwig service, our in-house message delivery service. We proposed to bypass the Live Activity token check process specifically for Android devices. Following extensive discussions, we decided to proceed with the first solution, which ensures consistency in the technical approach between Android and iOS platforms. Additionally, this solution allows us to ensure that notifications are only pushed to the devices that support the Live Activity feature. This decision strikes a good balance between efficiency and compatibility.&lt;/p&gt;

&lt;h3 id=&quot;ui-components&quot;&gt;UI Components&lt;/h3&gt;

&lt;p&gt;Starting with a kick-off project meeting where we showcased our plans and proposed solutions to our stakeholders, the engineering team presented two native Android UI components that could be utilised to replicate Live Activity: the Notification View and the Floating View.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Notification View&lt;/strong&gt; is a component located in the notification drawer (and potentially on the Lock Screen) that fulfils the most basic use-case of the Live Activity feature. It enables Android users to access information without the need to open the app. Since the standard notification template only allows developers to display a single content title, a content subtitle, and one image, it falls short of meeting our Live Activity UI requirements. To overcome this limitation, custom notifications with custom layouts are needed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure3.gif&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Early design spec of Grab’s LA using custom notification.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One of the key advantages of custom notifications is that they do not require any additional new permissions, ensuring a smooth user experience. Additionally, Android users are accustomed to checking their notifications from the notification tray, making it a familiar and intuitive interaction. However, it is important to acknowledge that custom notifications rely on a remote view, which can pose restrictions on rendering only specific views. On top of that, custom notifications provide a limited space for content – limited to 48dp when collapsed and 252dp when expanded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Floating View&lt;/strong&gt; is a component that will appear above all the applications in Android. It adds the convenience of accessing the information when the device is unlocked or when the user is on another app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Early design spec of Grab’s LA using floating view.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The use of a Floating View offers greater flexibility to the view by eliminating the reliance on a remote view. However, it’s important to be aware of the potential limitations associated with this approach. These limitations include the requirement for screen space, which can potentially impact other app functionalities and cause frustration for users. Additionally, if we intend to display multiple order updates, we may require even more space, taking into account that Grab allows users to place multiple orders. Furthermore, the Floating View feature requires an extra “Draw over other apps” permission, a setting that allows an app to display information on top of other apps on your screen.&lt;/p&gt;

&lt;p&gt;After thoughtful deliberation, we concluded that custom notifications provide a more consistent and user-friendly solution for implementing Grab’s Live Activity feature on Android. They offer compatibility, non-intrusiveness, no extra permissions, and the flexibility of silent notifications, ensuring an optimised user experience.&lt;/p&gt;

&lt;h2 id=&quot;building-grab-androids-live-activity&quot;&gt;Building Grab Android’s “Live Activity”&lt;/h2&gt;

&lt;p&gt;We began developing the Live Activity feature by focusing on Food and Mart for the MVP. However, we prioritised potential future use cases for other verticals by examining the existing functionality of the Grab iOS Live Activity feature. By considering these factors from the start, we need to make sure that we build an extendable and flexible solution that caters to different verticals and their various use-cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure5.gif&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Grab’s Android Live Activity.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As we set out to design Grab’s Android Live Activity module, we broke down the task into three key components:&lt;/p&gt;

&lt;h3 id=&quot;registering-live-activity-token&quot;&gt;&lt;strong&gt;Registering Live Activity Token&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In order to enable Hedwig services to send Live Activity notifications to devices, it is necessary to register a Live Activity Token for a specific order to Grab Devices services (refer to figure 1 for the iOS flow). As this use-case is applicable across various verticals in iOS, we have designed a LiveActivityIntegrationManager class specifically to handle this functionality.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;interface LiveActivityIntegrationManager {  
    /\*\*  
     \* To start live activity journey  
     \* @param vertical refers to vertical name  
     \* @param id refers to unique id which is used to differentiate live activity UI instances  
     \* eg: Food will use orderID as id, transport can pass rideID  
     \*\*/  
    fun startLiveActivity(vertical: Vertical, id: String): Completable

    fun updateLiveActivity(id: String, attributes: LiveActivityAttributes)

    fun cancelLiveActivity(id: String)  
}  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our goal is to provide developers with an easy implementation of Live Activity in the Grab app. Developers can simply utilize the startLiveActivity() function to register the token to Grab Devices by passing the vertical name and unique ID as parameters.&lt;/p&gt;

&lt;h3 id=&quot;notification-listener-and-payload-mapping&quot;&gt;&lt;strong&gt;Notification Listener and Payload Mapping&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;To handle Live Activity notifications in Android, it is necessary to listen to the Live Activity notification payload and map it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt;. Taking into consideration the initial Live Activity design (refer to figure 3), we need to analyse the variables necessary for this process. As a result, we break down the Live Activity UI into different UI elements and layouts, as follows:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure6.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Android Live Activity view breakdown.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;App Icon&lt;/strong&gt; – labeled as 1 in Figure 6. &lt;br /&gt;
This view always shows the Grab app icon.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Header Icon&lt;/strong&gt; – labeled as 2 in Figure 6.&lt;br /&gt;
This view is an image view that could be set with icon resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content Title View&lt;/strong&gt; – labeled as 3 in Figure 6. &lt;br /&gt;
This view is a placeholder that could be set with a text or custom remote view.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content Text View&lt;/strong&gt; – labeled as 4 in Figure 6. &lt;br /&gt;
This view is a placeholder that could be set with a text or custom remote view.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Footer View&lt;/strong&gt; – labeled as 5 in Figure 6.&lt;br /&gt;
This view is a placeholder that could be set with icon resources, bitmap, or custom remote view.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Decomposing the UI into different parts allows us to clearly understand of the UI components that need to maintain consistency across different use-cases, as well as the elements that can be easily customised and configured based on specific requirements. As a result, we have designed the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt; class that serves as a container that encompasses all the necessary configurations required for rendering the Live Activity.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 
class LiveActivityAttributes private constructor(  
    val iconRes: Int?,  
    val headerIconRes: Int?,  
    val contentTitle: CharSequence?,  
    val contentTitleStyle: ContentStyle.TitleStyle?,  
    val customContentTitleView: LiveActivityCustomView?,  
    val contentText: CharSequence?,  
    val contentTextStyle: ContentStyle.TextStyle?,  
    val customContentTextView: LiveActivityCustomView?,  
    val footerIconRes: Int?,  
    val footerBitmap: Bitmap?,  
    val footerProgressBarProgress: Float?,  
    val footerProgressBarStyle: ProgressBarStyle?,  
    val footerRatingBarAttributes: RatingBarAttributes?,  
    val customFooterView: LiveActivityCustomView?,  
    val contentIntent: PendingIntent?,  
    …  
)  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;payload-rendering&quot;&gt;&lt;strong&gt;Payload Rendering&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;To ensure a clear separation of responsibilities, we have designed a separate class called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityManager&lt;/code&gt;. This dedicated class is responsible for the mapping of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt; to Notifications. The generated notifications are then utilised by Android’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NotificationManager&lt;/code&gt; class to be posted and displayed accordingly.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
interface LiveActivityManager {  
    /\*\*  
     \* Post a Live Activity to be shown in the status bar, stream, etc.  
     \*  
     \* @param id           the ID of the Live Activity  
     \* @param attributes the LiveActivity to post to the system  
     \*/  
    fun notify(id: Int, attributes: LiveActivityAttributes)

    fun cancel(id: Int)  
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;We are delighted to announce that we have successfully implemented Grab’s Android version of the Live Activity feature for Express and Transport products. Furthermore, we plan to extend this feature to the Driver and Merchant applications as well. We understand the value this feature brings to our users and are committed to enhancing it further. Stay tuned for upcoming updates and enhancements to the Live Activity feature as we continue to improve and expand its capabilities across various verticals.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Mon, 23 Sep 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/live-activity-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/live-activity-2</guid>
        
        <category>Engineering</category>
        
        <category>Android</category>
        
        <category>Exploration</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
