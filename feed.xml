<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 13 Jul 2022 03:23:34 +0000</pubDate>
    <lastBuildDate>Wed, 13 Jul 2022 03:23:34 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>How we automated FAQ responses at Grab</title>
        <description>&lt;h2 id=&quot;overview-and-initial-analysis&quot;&gt;Overview and initial analysis&lt;/h2&gt;

&lt;p&gt;Knowledge management is often one of the biggest challenges most companies face internally. Teams spend several working hours trying to either inefficiently look for information or constantly asking colleagues about information already documented somewhere. A lot of time is spent on the internal employee communication channels (in our case, Slack) simply trying to figure out answers to repetitive questions. On our journey to automate the responses to these repetitive questions, we needed first to figure out exactly how much time and effort is spent by on-call engineers answering such repetitive questions.&lt;/p&gt;

&lt;p&gt;We soon identified that many of the internal engineering tools’ on-call activities involve answering users’ (internal users) questions on various Slack channels. Many of these questions have already been asked or documented on the wiki. These inquiries hinder on-call engineers’ productivity and affect their ability to focus on operational tasks. Once we figured out that on-call employees spend a lot of time answering Slack queries, we decided on a journey to determine the top questions.&lt;/p&gt;

&lt;p&gt;We considered smaller groups of teams for this study and found out that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The topmost user queries are “How do I do ABC?” or “Is XYZ broken?”.&lt;/li&gt;
  &lt;li&gt;The second most commonly asked questions revolve around access requests, approvals, or other permissions. The answer to such questions is often URLs to existing documentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These findings informed us that we didn’t just need an artificial intelligence (AI) based autoresponder to repetitive questions. We must, in fact, also leverage these channels’ chat histories to identify patterns.&lt;/p&gt;

&lt;h2 id=&quot;gathering-user-votes-for-shortlisted-vendors&quot;&gt;Gathering user votes for shortlisted vendors&lt;/h2&gt;

&lt;p&gt;In light of saving costs and time and considering the quality of existing solutions already available in the market, we decided not to reinvent the wheel and instead purchase an existing product. And to figure out which product to purchase, we needed to do a comparative analysis. And thus began our vendor comparison journey!&lt;/p&gt;

&lt;p&gt;While comparing the feature sets offered by different vendors, we understood that our users need to play a part in this decision-making process. However, sharing our vendor analysis with our users and allowing them to choose the bot of their choice posed several challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Users could be biased towards known bots (from previous experiences).&lt;/li&gt;
  &lt;li&gt;Users could be biased towards big brands with a preconceived notion that big brands mean better features and better user support.&lt;/li&gt;
  &lt;li&gt;Users may likely pick the most expensive vendor, assuming that a higher cost means higher efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To ensure that we receive unbiased feedback, here’s how we opened users up to voting. We highlighted the top features of each vendor’s bot compared to other shortlisted bots. We hid the names of the bots to avoid brand attraction. At a high level, here’s what the categorisation looked like:&lt;/p&gt;

&lt;table border=&quot;1&quot; style=&quot;text-align:center;&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Vendor 1 (name  hidden)&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Vendor 2 (name  hidden)&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Vendor 3 (name  hidden)&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Enables crowdsourcing, everyone is incentivised to participate. &lt;br /&gt; Participants/SME names are visible. &lt;br /&gt; Everyone can access the web UI and see how the responses configured on the bot.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Lowers discussions on channels by providing easy ways to raise tickets to the team instead of discussing on Slack.&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Only a specific set of admins (or oncall engineers) feed and maintain the bot thus ensuring information authenticity and reliability.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Easy bot feeding mechanism/web UI to update FAQs.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Superior natural language processing capabilities.&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt; &lt;img src=&quot;/img/automated-faq/image1.png&quot; alt=&quot;&quot; style=&quot;width:10%&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;    
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Please vote&lt;/td&gt;
    &lt;td&gt;Vendor 1&lt;/td&gt;
    &lt;td&gt;Vendor 2&lt;/td&gt;
    &lt;td&gt;Vendor 3&lt;/td&gt;    
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Although none of the options had all the features our users wanted, &lt;strong&gt;&lt;em&gt;about 60% chose Vendor 1 (OneBar)&lt;/em&gt;&lt;/strong&gt;. From this, we discovered the core features that our users needed while keeping them involved in the decision-making process.&lt;/p&gt;

&lt;h3 id=&quot;matching-our-requirements-with-available-vendors-feature-sets&quot;&gt;Matching our requirements with available vendors’ feature sets&lt;/h3&gt;

&lt;p&gt;Although our users made their preferences clear, we still needed to ensure that the feature sets available in the market suited our internal requirements in terms of the setup and the features available in portals that we envisioned replacing. As part of our requirements gathering process, here are some of the critical conditions that became more and more prominent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An ability to crowdsource Slack discussions/conclusions and save them directly from Slack (preferably with a single command).&lt;/li&gt;
  &lt;li&gt;An ability to auto-respond to Slack queries without calling the bot manually.&lt;/li&gt;
  &lt;li&gt;The bot must be able to respond to queries only on the preconfigured Slack channel (not a Slack-wide auto-responder that is already available).&lt;/li&gt;
  &lt;li&gt;Ability to auto-detect frequently asked questions on the channels would mean less work for platform engineers to feed the bot manually and periodically.&lt;/li&gt;
  &lt;li&gt;A trusted and secured data storage setup and a responsive customer support team.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proof-of-concept&quot;&gt;Proof of concept&lt;/h2&gt;

&lt;p&gt;We considered several tools (including some of the tools used by our HR for auto-answering employee questions). We then decided to do a complete proof of concept (POC) with OneBar to check if it fulfils our internal requirements.&lt;/p&gt;

&lt;p&gt;These were the phases in which we conducted the POC for the shortlisted vendor (OneBar):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 1&lt;/strong&gt;: Study the traffic, see what insights OneBar shows and what it could/should potentially show. Then think about how an ideal oncall or support should behave in such an environment. i.e. we could identify specific messages in history and describe what should’ve happened to each one of them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 2&lt;/strong&gt;: Create required records in OneBar and configure it to match the desired behaviour as closely as possible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 3&lt;/strong&gt;: Let the tool run for a couple of weeks and then evaluate how well it responds to questions, how often people search directly, how much information they add, etc. Onebar adds all these metrics in the app making it easier to monitor activity.&lt;/p&gt;

&lt;p&gt;In addition to the Onebar POC, we investigated other solutions and did a thorough vendor comparison and analysis. After running the POC and investigating other vendors, we decided to use OneBar as its features best meet our needs.&lt;/p&gt;

&lt;h3 id=&quot;prioritising-slack-channels&quot;&gt;Prioritising Slack channels&lt;/h3&gt;

&lt;p&gt;While we had multiple Slack channels that we’d love to have enabled the shortlisted bot on, our initial contract limited our use of the bot to only 20 channels. We could not use OneBar to auto-scan more than 20 Slack channels.&lt;/p&gt;

&lt;p&gt;Users could still chat directly with the bot to get answers to FAQs based on what was fed to the bot’s knowledge base (KB). They could also access the web login, which displays its KB, other valuable features, and additional features for admins/experts.&lt;/p&gt;

&lt;p&gt;Slack channels that we enabled the licensed features on were prioritised based on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Most messages sent on the channel per month, i.e. most active channels.&lt;/li&gt;
  &lt;li&gt;Most members impacted, i.e. channels with a large member count.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To do this, we used Slack analytics reports and identified the channels that fit our prioritisation criteria.&lt;/p&gt;

&lt;h3 id=&quot;change-is-difficult-but-often-essential&quot;&gt;Change is difficult but often essential&lt;/h3&gt;

&lt;p&gt;Once we’d onboarded the vendor, we began training and educating employees on using this new Knowledge Management system for all their FAQs. It was a challenge as change is always complex but essential for growth.&lt;/p&gt;

&lt;p&gt;A series of tech talks and training conducted across the company and at more minor scales also helped guide users about the bot’s features and capabilities.&lt;/p&gt;

&lt;p&gt;At the start, we suffered from a lack of data resulting in incorrect responses from the bot. But as the team became increasingly aware of the features and learned more about its capabilities, the bot’s number of KB items grew, resulting in a much more efficient experience. It took us around one quarter to feed the bot consistently to see accurate and frequent responses from it.&lt;/p&gt;

&lt;h2 id=&quot;crowdsourcing-our-internal-glossary&quot;&gt;Crowdsourcing our internal glossary&lt;/h2&gt;

&lt;p&gt;With an increasing number of acronyms and company-specific words emerging each year, the number of acronyms and company-specific abbreviations that new joiners face is immense.&lt;/p&gt;

&lt;p&gt;We solved this issue by using the bot’s channel-specific KB feature. We created a specific Slack channel dedicated to storing and retrieving definitions of acronyms and other words. This solution turned out to be a big hit with our users.&lt;/p&gt;

&lt;p&gt;And who fed the bot with the terms and glossary items? Who better than our onboarding employees to train the bot to help other onboarders. A targeted campaign dedicated to feeding the bot excited many of our onboarders. They began to play around with the bot’s features and provide it with as many glossary items as possible, thus winning swags!&lt;/p&gt;

&lt;p&gt;In a matter of weeks, the user base grew from a couple of hundred to around 3000. This effort was also called out in one of our company-wide All Hands meetings, a big win for our team!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jul 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/automated-faq</link>
        <guid isPermaLink="true">https://engineering.grab.com/automated-faq</guid>
        
        <category>Automation</category>
        
        <category>Knowledge management</category>
        
        <category>Productivity</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Graph Networks - 10X investigation with Graph Visualisations</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Detecting fraud schemes used to require investigations using large amounts and varying types of data that come from many different anti-fraud systems. Investigators then need to combine the different types of data and use statistical methods to uncover suspicious claims, which is time consuming and inefficient in most cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We are always looking for ways to improve fraud investigation methods and stay one step ahead of our ever-growing fraudsters. In the &lt;a href=&quot;https://engineering.grab.com/graph-networks&quot;&gt;introductory blog&lt;/a&gt; of this series, we’ve mentioned experimenting with a set of Graph Network technologies, including Graph Visualisation.&lt;/p&gt;

&lt;p&gt;In this post, we will introduce our Graph Visualisation Platform and briefly illustrate how it makes fraud investigations easier and more effective.&lt;/p&gt;

&lt;h2 id=&quot;why-visualise-a-graph&quot;&gt;Why visualise a graph?&lt;/h2&gt;

&lt;p&gt;If you’re a fan of crime shows, you would have come across scenes like a detective putting together evidence, such as pictures, notes and articles, on a board and connecting them with thumb tacks and yarn. When you look at the board, it’s easy to see the relationships between the different pieces of evidence. That’s what graphs do, especially in fraud detection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In the same way, while graph data is the raw material of an investigation, some of the most interesting relationships are often inferred rather than modelled directly in the data. Visualising these relationships can give a unique “big picture” of the data that is difficult or impossible to obtain with traditional relational tables and business intelligence tools.&lt;/p&gt;

&lt;p&gt;On the other hand, graph visualisation enhances the quick identification of relationships and significant structures because it is an intuitive way to help detect patterns. Plus, the human brain processes visual information much faster; that’s where our Graph Visualisation platform comes in.&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-graph-visualisation-platform&quot;&gt;What is the Graph Visualisation platform?&lt;/h2&gt;

&lt;p&gt;Graph Visualisation platform is a full-featured investigation platform that can reveal hidden connections and context in data by transforming raw records into highly visual and interactive maps. From there, investigators can grab any data point and quickly see relationships, patterns, and anomalies, and if necessary, drill down to investigate further.&lt;/p&gt;

&lt;p&gt;This is all done without writing a manual query, switching between anti-fraud systems, or having to think about data science! These are some of the interactions on the platform that easily make anomalies or relevant patterns stand out.&lt;/p&gt;

&lt;h3 id=&quot;expanding-the-data&quot;&gt;Expanding the data&lt;/h3&gt;

&lt;p&gt;To date, we have over three billion nodes and edges in our storage system. It is not possible (nor necessary) to show all of the data at once. The platform allows the user to grab any data point and easily expand to view the relationships.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/expand-data.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;timeline-tracking-and-history-replay&quot;&gt;Timeline tracking and history replay&lt;/h3&gt;

&lt;p&gt;The Graph Visualisation platform’s interactive time filter lets you see temporal relationships within your data and clearly reveals the chronological progression of events. You can start with a specific time of interest, track everything that happens after, then quickly focus on the time and relationships that matter most.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/data-replay.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;10x-investigations&quot;&gt;10X investigations&lt;/h2&gt;

&lt;p&gt;Here are a few examples of how the Graph Visualisation platform facilitates fraud investigations.&lt;/p&gt;

&lt;h3 id=&quot;appeal-confirmation&quot;&gt;Appeal confirmation&lt;/h3&gt;

&lt;p&gt;The following image shows the difference between a true fraudster and a falsely identified one. On the left, we have a Grab rental corporate account that was falsely detected by a fraud rule. Upon review, we discovered that there is no suspicious connection to this account, thus the account got unblocked.&lt;/p&gt;

&lt;p&gt;On the right, we have a passenger that was blocked by the system and they appealed. Investigations showed that the passenger is, in fact, part of an extremely dense device-sharing network, so we maintained our decision to block.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;modus-operandi-discovery&quot;&gt;Modus operandi discovery&lt;/h3&gt;

&lt;h4 id=&quot;passenger-sharing-device&quot;&gt;Passenger sharing device&lt;/h4&gt;

&lt;p&gt;Fraudsters tend to share physical resources to maximise their revenue. With our Graph Visualisation platform, you can see exactly how this pattern looks like. The image below shows a device that is shared by a lot of fraudsters.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;anti-money-laundering-aml&quot;&gt;Anti-money laundering (AML)&lt;/h4&gt;

&lt;p&gt;On the left, we see a pattern of healthy spending on Grab. However, on the right, we can see that passengers are highly connected, and it has frequent large amount transfers to other payment providers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-visualisation/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;Graph Visualisation is an intuitive way to investigate suspicious connections and potential patterns of crime. Investigators can directly interact with any data point to get the details they need and literally view the relationships in the data to make fast, accurate, and defensible decisions.&lt;/p&gt;

&lt;p&gt;While fraud detection is a good use case for Graph Visualisation, it’s not the only possibility. Graph Visualisation can help make anything more efficient and intelligent, especially if you have highly connected data.&lt;/p&gt;

&lt;p&gt;In the next part of this blog series, we will talk about the Graph service platform and the importance of building graph services with graph databases. Check out the other articles in this series:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/graph-networks&quot;&gt;Graph Networks - Striking fraud syndicates in the dark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/graph-concepts&quot;&gt;Graph concepts and applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Jun 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/graph-visualisation</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-visualisation</guid>
        
        <category>Security</category>
        
        <category>Graphs concepts</category>
        
        <category>Graph technology</category>
        
        <category>Graph visualisation</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How facial recognition technology keeps you safe</title>
        <description>&lt;p&gt;Facial recognition technology is one of the many modern technologies that previously only appeared in science fiction movies. The roots of this technology can be traced back to the 1960s and have since grown dramatically due to the rise of deep learning techniques and accelerated digital transformation in recent years.&lt;/p&gt;

&lt;p&gt;In this blog post, we will talk about the various applications of facial recognition technology in Grab, as well as provide details of the technical components that build up this technology.&lt;/p&gt;

&lt;h2 id=&quot;application-of-facial-recognition-technology-&quot;&gt;Application of facial recognition technology  &lt;/h2&gt;

&lt;p&gt;At Grab, we believe in &lt;strong&gt;prevention&lt;/strong&gt;, &lt;strong&gt;protection&lt;/strong&gt;, and &lt;strong&gt;action&lt;/strong&gt; to create a safer every day for our consumers, partners, and the community as a whole. All selfies collected by Grab are handled according to Grab’s Privacy Policy and securely protected under privacy legislation in the countries in which we operate. We will elaborate in detail in a section further below.&lt;/p&gt;

&lt;p&gt;One key incident prevention method is to &lt;strong&gt;verify&lt;/strong&gt; the &lt;strong&gt;identity&lt;/strong&gt; of both our consumers and partners:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From the perspective of protecting the safety of &lt;strong&gt;passengers&lt;/strong&gt;, having a reliable driver authentication process can avoid unauthorized people from delivering a ride. This ensures that trips on Grab are only completed by registered licensed driver-partners that have passed our comprehensive background checks.&lt;/li&gt;
  &lt;li&gt;From the perspective of protecting the safety of &lt;strong&gt;driver-partners&lt;/strong&gt;, verifying the identity of new passengers using facial recognition technology helps to deter crimes targeting our driver-partners and make incident investigations easier.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image7.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image9.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Safety incidents that arise from lack of identity verification&lt;/i&gt;&lt;/figcaption&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Facial recognition technology is also leveraged to improve Grab digital financial services, particularly in facilitating the “electronic Know Your Customer” (&lt;strong&gt;e-KYC&lt;/strong&gt;) process. KYC is a standard regulatory requirement in the financial services industry to verify the identity of customers, which commonly serves to deter financial crime, such as money laundering.&lt;/p&gt;

&lt;p&gt;Traditionally, customers are required to visit a physical counter to verify their government-issued ID as proof of identity. Today, with the widespread use of mobile devices, coupled with the maturity of facial recognition technologies, the process has become much more seamless and can be done entirely digitally.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1: GrabPay wallet e-KYC regulatory requirements in the Philippines&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;overview-of-facial-recognition-technology&quot;&gt;Overview of facial recognition technology&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2: Face recognition flow&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The typical facial recognition pipeline involves multiple stages, which starts with &lt;strong&gt;image preprocessing&lt;/strong&gt;, &lt;strong&gt;face anti-spoof&lt;/strong&gt;, followed by &lt;strong&gt;feature extraction&lt;/strong&gt;, and finally the downstream applications - &lt;strong&gt;face verification&lt;/strong&gt; or &lt;strong&gt;face search&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The most common image preprocessing techniques for face recognition tasks are &lt;strong&gt;face detection&lt;/strong&gt; and &lt;strong&gt;face alignment&lt;/strong&gt;. The face detection algorithm locates the face region in an image, and is usually followed by face alignment, which identifies the key facial landmarks (e.g. left eye, right eye, nose, etc.) and transforms them into a standardised coordinate space. Both of these preprocessing steps aim to ensure a consistent quality of input data for downstream applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Face anti-spoof&lt;/strong&gt; refers to the process of ensuring that the user-submitted facial image is legitimate. This is to prevent fraudulent users from &lt;strong&gt;stealing identities&lt;/strong&gt; (impersonating someone else by using a printed photo or replaying videos from mobile screens) or &lt;strong&gt;hiding identities&lt;/strong&gt; (e.g. wearing a mask). The main approach here is to extract low-level spoofing cues, such as the &lt;em&gt;moiré&lt;/em&gt; pattern, using various machine learning techniques to determine whether the image is spoofed.&lt;/p&gt;

&lt;p&gt;After passing the anti-spoof checks, the user-submitted images are sent for &lt;strong&gt;face feature extraction&lt;/strong&gt;, where important features that can be used to distinguish one person from another are extracted. Ideally, we want the feature extraction model to produce embeddings (i.e. high-dimensional vectors) with &lt;strong&gt;small intra-class distance&lt;/strong&gt; (i.e. faces of the same person) and &lt;strong&gt;large inter-class distance&lt;/strong&gt; (i.e. faces of different people), so that the aforementioned downstream applications (i.e. face verification and face search) become a straightforward task - thresholding the distance between embeddings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Face verification&lt;/strong&gt; is one of the key applications of facial recognition and it answers the question, “&lt;em&gt;Is this the same person?&lt;/em&gt;”. As previously alluded to, this can be achieved by comparing the distance between embeddings generated from a template image (e.g. government-issued ID or profile picture) and a query image submitted by the user. A short distance indicates that both images belong to the same person, whereas a large distance indicates that these images are taken from different people.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Face search&lt;/strong&gt;, on the other hand, tackles the question, “&lt;em&gt;Who is this person?&lt;/em&gt;”, which can be framed as a vector/embedding similarity search problem. Image embeddings belonging to the same person would be highly similar, thus ranked higher, in search results. This is particularly useful for deterring criminals from re-onboarding to our platform by blocking new selfies that match a criminal profile in our criminal denylist database.&lt;/p&gt;

&lt;h4 id=&quot;face-anti-spoof&quot;&gt;Face anti-spoof&lt;/h4&gt;

&lt;p&gt;For face anti-spoof, the most common methods used to attack the facial recognition system are screen replay and printed paper. To distinguish these spoof attacks from genuine faces, we need to solve two main challenges.&lt;/p&gt;

&lt;p&gt;The first challenge is to obtain enough data of spoof attacks to enable the training of models. The second challenge is to carefully train the model to focus on the subtle differences between spoofed and genuine cases instead of overfitting to other background information.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image10.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3: Original face (left), screen replay attack (middle), synthetic data with a moiré pattern (right)&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2110.10444.pdf&quot;&gt;Source&lt;/a&gt; &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Collecting large volumes of spoof data is naturally hard since spoof cases in product flows are very rare. To overcome this problem, one option is to synthesise large volumes of spoof data instead of collecting the real spoof data. More specifically, we synthesise &lt;em&gt;moiré&lt;/em&gt; patterns on genuine face images that we have, and use the synthetic data as the screen replay attack data. This allows our model to use small amounts of real spoof data and sufficiently identify spoofing, while collecting more data to train the model.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4: Data preparation with patch data&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;On the other hand, a spoofed face image contains lots of information with subtle spoof cues such as &lt;em&gt;moiré&lt;/em&gt; patterns that cannot be detected by the naked eye. As such, it’s important to train the model to identify spoof cues instead of focusing on the possible domain bias between the spoof data and genuine data. To achieve this, we need to change the way we prepare the training data.&lt;/p&gt;

&lt;p&gt;Instead of using the entire selfie image as the model input, we firstly detect and crop the face area, then evenly split the cropped face area into several patches. These patches are used as input to train the model. During inference, images are also split into patches the same way and the final result will be the average of outputs from all patches. After this data preprocessing, the patches will contain less global semantic information and more local structure features, making it easier for the model to learn and distinguish spoofed and genuine images.&lt;/p&gt;

&lt;h4 id=&quot;face-verification&quot;&gt;Face verification&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“Data is food for AI.” - Andrew Ng, founder of Google Brain&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The key success factors of artificial intelligence (AI) models are undoubtedly driven by the volume and quality of data we hold. At Grab, we have one of the largest and most comprehensive face datasets, covering a wide range of demographic groups in Southeast Asia. This gives us a strong advantage to build a highly robust and unbiased facial recognition model that serves the region better.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, all selfies collected by Grab are securely protected under privacy legislation in the countries in which we operate. We take reasonable legal, organisational and technical measures to ensure that your Personal Data is protected, which includes measures to prevent Personal Data from getting lost, or used or accessed in an unauthorised way. We limit access to these Personal Data to our employees on a need to know basis. Those processing any Personal Data will only do so in an authorised manner and are required to treat the information with confidentiality.&lt;/p&gt;

&lt;p&gt;Also, selfie data will not be shared with any other parties, including our driver, delivery partners or any other third parties without proper authorisation from the account holder. They are strictly used to improve and enhance our products and services, and not used as a means to collect personal identifiable data. Any disclosure of personal data will be handled in accordance with &lt;a href=&quot;https://www.grab.com/my/terms-policies/privacy-notice/&quot;&gt;Grab Privacy Policy&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5: Semi-Siamese architecture (&lt;a src=&quot;https://arxiv.org/pdf/2007.08398.pdf&quot;&gt;source&lt;/a&gt;)&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Other than data, model architecture also plays an important role, especially when handling less common face verification scenarios, such as ”&lt;strong&gt;&lt;em&gt;selfie to ID photo&lt;/em&gt;&lt;/strong&gt;” and “&lt;strong&gt;&lt;em&gt;selfie to masked selfie&lt;/em&gt;&lt;/strong&gt;” verifications.  &lt;/p&gt;

&lt;p&gt;The main challenge of “&lt;strong&gt;selfie to ID photo&lt;/strong&gt;” verification is the shallow nature of the dataset, i.e. a large number of unique identities, but a low number of image samples per identity. This type of dataset lacks representation in intra-class diversity, which would commonly lead to model collapse during model training. Besides, “selfie to ID photo” verification also poses numerous challenges that are different from general facial recognition, such as aging (old ID photo), attrited ID card (normal wear and tear), and domain difference between printed ID photo and real-life selfie photo.&lt;/p&gt;

&lt;p&gt;To address these issues, we leveraged a novel training method named semi-Siamese training (SST) &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, which is proposed by Du et al. (2020). The key idea is to enlarge intra-class diversity by ensuring that the backbone Siamese networks have similar parameters, but are not entirely identical, hence the name “semi-Siamese”.&lt;/p&gt;

&lt;p&gt;Just like typical Siamese network architecture, feature vectors generated by the subnetworks are compared to compute the loss functions, such as Arc-softmax, Triplet loss, and Large margin cosine loss, all of which aim to reduce intra-class distance while increasing the inter-class distances. With the usage of the semi-Siamese backbone network, intra-class diversity is further promoted as it is guaranteed by the difference between the subnetworks, making the training convergence more stable.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6: Masked face verification&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Another type of face verification problem we need to solve these days is the “&lt;strong&gt;&lt;em&gt;selfie to masked selfie&lt;/em&gt;&lt;/strong&gt;” verification. To pass this type of face verification, users are required to take off their masks as previous face verification models are unable to verify people with masks on. However, removing face masks to do face verification is inconvenient and risky in a crowded environment, which is a pain for many of our driver-partners who need to do verification from time to time.&lt;/p&gt;

&lt;p&gt;To help ease this issue, we developed a face verification model that can verify people even while they are wearing masks. This is done by adding masked selfies into the training data and training the model with both masked and unmasked selfies. This not only enables the model to perform verification for people with masks on, but also helps to increase the accuracy of verifying those without masks. On top of that, masked selfies act as data augmentation and help to train the model with stronger ability of extracting features from the face.&lt;/p&gt;

&lt;h4 id=&quot;face-search&quot;&gt;Face search&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/facial-recognition/image5.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As previously mentioned, once embeddings are produced by the facial recognition models, face search is fundamentally no different from face verification. Both processes use the distance between embeddings to decide whether the faces belong to the same person. The only difference here is that face search is more computationally expensive, since face verification is a 1-to-1 comparison, whereas face search is a 1-to-N comparison (N=size of the database).&lt;/p&gt;

&lt;p&gt;In practice, there are many ways to significantly reduce the complexity of the search algorithm from O(N), such as using Inverted File Index (IVF) and Hierarchical Navigable Small World (HNSW) graphs. Besides, there are also various methods to increase the query speed, such as accelerating the distance computation using GPU, or approximating the distances using compressed vectors. This problem is also commonly known as Approximate Nearest Neighbor (ANN). Some of the great open-sourced vector similarity search libraries that can help to solve this problem are &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/scann&quot;&gt;ScaNN&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; (by Google), &lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;FAISS&lt;/a&gt;&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;(by Facebook), and &lt;a href=&quot;https://github.com/spotify/annoy&quot;&gt;Annoy&lt;/a&gt; (by Spotify).&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;In summary, facial recognition technology is an effective crime prevention and reduction tool to strengthen the safety of our platform and users. While the enforcement of selfie collection by itself is already a strong deterrent against fraudsters misusing our platform, leveraging facial recognition technology raises the bar by helping us to quickly and accurately identify these offenders.&lt;/p&gt;

&lt;p&gt;As technologies advance, face spoofing patterns also evolve. We need to continuously monitor spoofing trends and actively improve our face anti-spoof algorithms to proactively ensure our users’ safety.&lt;/p&gt;

&lt;p&gt;With the rapid growth of facial recognition technology, there is also a growing concern regarding data privacy issues. At Grab, consumer privacy and safety remain our top priorities and we continuously look for ways to improve our existing safeguards.&lt;/p&gt;

&lt;p&gt;In May 2022, Grab was recognised by the Infocomm Media Development Authority in Singapore for its stringent data protection policies and processes through the &lt;a href=&quot;https://www.grab.com/sg/press/others/grab-singapore-is-the-first-superapp-to-secure-data-protection-trustmark-certification-by-imda/&quot;&gt;award of Data Protection Trustmark (DPTM) certification&lt;/a&gt;. This recognition reinforces our belief that we can continue to draw the benefits from facial recognition technology, while avoiding any misuse of it. As the saying goes, &lt;em&gt;“Technology is not inherently good or evil. It’s all about how people choose to use it”&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Niu, D., Guo R., and Wang, Y. (2021). &lt;em&gt;Moiré&lt;/em&gt; Attack (MA): A New Potential Risk of Screen Photos. Advances in Neural Information Processing Systems. https://papers.nips.cc/paper/2021/hash/db9eeb7e678863649bce209842e0d164-Abstract.html &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Du, H., Shi, H., Liu, Y., Wang, J., Lei, Z., Zeng, D., &amp;amp; Mei, T. (2020). Semi-Siamese Training for Shallow Face Learning. European Conference on Computer Vision, 36–53. Springer. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., &amp;amp; Kumar, S. (2020). Accelerating Large-Scale Inference with Anisotropic Vector Quantization. International Conference on Machine Learning. https://arxiv.org/abs/1908.10396 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Johnson, J., Douze, M., &amp;amp; Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3), 535–547. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 09 Jun 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/facial-recognition</link>
        <guid isPermaLink="true">https://engineering.grab.com/facial-recognition</guid>
        
        <category>Security</category>
        
        <category>Facial recognition</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Graph concepts and applications</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In an &lt;a href=&quot;https://engineering.grab.com/graph-networks&quot;&gt;introductory article&lt;/a&gt;, we talked about the importance of Graph Networks in fraud detection. In this article, we will be adding some further context on graphs, graph technology and some common use cases.&lt;/p&gt;

&lt;p&gt;Connectivity is the most prominent feature of today’s networks and systems. From molecular interactions, social networks and communication systems to power grids, shopping experiences or even supply chains, networks relating to real-world systems are not random. This means that these connections are not static and can be displayed differently at different times. Simple statistical analysis is insufficient to effectively characterise, let alone forecast, networked system behaviour.&lt;/p&gt;

&lt;p&gt;As the world becomes more interconnected and systems become more complex, it is more important to employ technologies that are built to take advantage of relationships and their dynamic properties. There is no doubt that graphs have sparked a lot of attention because they are seen as a means to get insights from related data. Graph theory-based approaches show the concepts underlying the behaviour of massively complex systems and networks.&lt;/p&gt;

&lt;h2 id=&quot;what-are-graphs&quot;&gt;What are graphs?&lt;/h2&gt;

&lt;p&gt;Graphs are mathematical models frequently used in network science, which is a set of technological tools that may be applied to almost any subject. To put it simply, graphs are mathematical representations of complex systems.&lt;/p&gt;

&lt;h3 id=&quot;origin-of-graphs&quot;&gt;Origin of graphs&lt;/h3&gt;

&lt;p&gt;The first graph was produced in 1736 in the city of Königsberg, now known as Kaliningrad, Russia. In this city, there were two islands with two mainland sections that were connected by seven different bridges.&lt;/p&gt;

&lt;p&gt;Famed mathematician Euler wanted to plot a journey through the entire city by crossing each bridge only once. Euler proceeded to abstract the four regions of the city and the seven bridges into edges but he demonstrated that the problem was unsolvable. A simplified abstract graph is shown in Fig 1.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image9.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 1 Abstraction graph&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The graph’s four dots represent Königsberg’s four zones, while the lines represent the seven bridges that connect them. Zones connected by an even number of bridges is clearly navigable because several paths to enter and exit are available. Zones connected by an odd number of bridges can only be used as starting or terminating locations because the same route can only be taken once.&lt;/p&gt;

&lt;p&gt;The number of edges associated with a node is known as the node degree. If two nodes have odd degrees and the rest have even degrees, the Königsberg problem could be solved. For example, exactly two regions must have an even number of bridges while the rest have an odd number of bridges. However, as illustrated in Fig 1, no Königsberg location has an even number of bridges, rendering this problem unsolvable.&lt;/p&gt;

&lt;h3 id=&quot;definition-of-graphs&quot;&gt;Definition of graphs&lt;/h3&gt;

&lt;p&gt;A graph is a structure that consists of vertices and edges. Vertices, or nodes, are the objects in a problem, while edges are the links that connect vertices in a graph.  &lt;/p&gt;

&lt;p&gt;Vertices are the fundamental elements that a graph requires to function; there should be at least one in a graph. Vertices are mathematical abstractions that refer to objects that are linked by a condition.&lt;/p&gt;

&lt;p&gt;On the other hand, edges are optional as graphs can still be defined without any edges. An edge is a link or connection between any two vertices in a graph, including a connection between a vertex and itself. The idea is that if two vertices are present, there is a relationship between them.&lt;/p&gt;

&lt;p&gt;We usually indicate &lt;em&gt;V={v1, v2, …, vn}&lt;/em&gt; as the set of vertices, and &lt;em&gt;E = {e1, e2, …, em}&lt;/em&gt; as the set of edges. From there, we can define a graph &lt;em&gt;G&lt;/em&gt; as a structure &lt;em&gt;G(V, E)&lt;/em&gt; which models the relationship between the two sets:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image11.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 2 Graph structure&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It is worth noting that the order of the two sets within parentheses matters, because we usually express the vertices first, followed by the edges. A graph &lt;em&gt;H(X, Y)&lt;/em&gt; is therefore a structure that models the relationship between the set of vertices &lt;em&gt;X&lt;/em&gt; and the set of edges &lt;em&gt;Y&lt;/em&gt;, not the other way around.&lt;/p&gt;

&lt;h2 id=&quot;graph-data-model&quot;&gt;Graph data model&lt;/h2&gt;

&lt;p&gt;Now that we have covered graphs and their typical components, let us move on to graph data models, which help to translate a conceptual view of your data to a logical model. Two common graph data formats are Resource Description Framework (RDF) and Labelled Property Graph (LPG).&lt;/p&gt;

&lt;h3 id=&quot;resource-description-framework-rdf&quot;&gt;Resource Description Framework (RDF)&lt;/h3&gt;

&lt;p&gt;RDF is typically used for metadata and facilitates standardised exchange of data based on their relationships. RDFs typically consist of a triple: a subject, a predicate, and an object. A collection of such triples is an RDF graph. This can be depicted as a node and a directed edge diagram, with each triple representing a node-edge-node graph, as shown in Fig 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image10.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 3 RDF graph&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The three types of nodes that can exist are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Internationalised Resource Identifiers (IRI) - online resource identification code.&lt;/li&gt;
  &lt;li&gt;Literals - data type value, i.e. text, integer, etc.&lt;/li&gt;
  &lt;li&gt;Blank nodes - have no identification; similar to anonymous or existential variables.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us use an example to illustrate this. We have a person with the name Art and we want to plot all his relationships. In this case, the IRI is &lt;em&gt;http://example.org/art&lt;/em&gt; and this can be shortened by defining a prefix like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ex&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this example, the IRI &lt;em&gt;http://xmlns.com/foaf/0.1/knows&lt;/em&gt; defines the relationship &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;knows&lt;/code&gt;. We define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf&lt;/code&gt; as the prefix for &lt;em&gt;http://xmlns.com/foaf/0.1/&lt;/em&gt;. The following code snippet shows how a graph like this will look.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@prefix foaf: &amp;lt;http://xmlns.com/foaf/0.1/&amp;gt;
@prefix ex: &amp;lt;http://example.org/&amp;gt;

ex:art foaf:knows ex:bob
ex:art foaf:knows ex:bea
ex:bob foaf:knows ex:cal
ex:bob foaf:knows ex:cam
ex:bea foaf:knows ex:coe
ex:bea foaf:knows ex:cory
ex:bea foaf:age 23
ex:bea foaf:based_near_:o1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the last two lines, you can see how a literal and blank node would be depicted in an RDF graph. The variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf:age&lt;/code&gt; is a literal node with the integer value of 23, while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf:based_near&lt;/code&gt; is an anonymous spatial entity with a node identifier of underscore. Outside the context of this graph, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;o1&lt;/code&gt; is a data identifier with no meaning.&lt;/p&gt;

&lt;p&gt;Multiple IRIs, intended for use in RDF graphs, are typically stored in an RDF vocabulary. These IRIs often begin with a common substring known as a namespace IRI. In some cases, namespace IRIs are also associated with a short name known as a namespace prefix. In the example above, &lt;em&gt;http://xmlns.com/foaf/0.1/&lt;/em&gt; is the namespace IRI and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foaf&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ex&lt;/code&gt; are namespace prefixes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: RDF graphs are considered atemporal as they provide a static snapshot of data. They can use appropriate language extensions to communicate information about events or other dynamic properties of entities.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;An RDF dataset is a set of RDF graphs that includes one or more named graphs as well as exactly one default graph. A default graph is one that can be empty, and has no associated IRI or name, while each named graph has an IRI or a blank node corresponding to the RDF graph and its name. If there is no named graph specified in a query, the default graph is queried (hence its name).&lt;/p&gt;

&lt;h3 id=&quot;labelled-property-graph-lpg&quot;&gt;Labelled Property Graph (LPG)&lt;/h3&gt;

&lt;p&gt;A labelled property graph is made up of nodes, links, and properties. Each node is given a label and a set of characteristics in the form of arbitrary key-value pairs. The keys are strings, and the values can be any data type. A relationship is then defined by adding a directed edge that is labelled and connects two nodes with a set of properties.&lt;/p&gt;

&lt;p&gt;In Fig 4, we have an LPG that shows two nodes: art and bea. The bea node has two characteristics, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;age&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proximity&lt;/code&gt;, that are connected by a known edge. This edge has the attribute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;since&lt;/code&gt; because it commemorates the year that art and bea first met.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image12.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 4 Labelled Property Graph: Example 1&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Nodes, edges and properties must be defined when designing an LPG data model. In this scenario, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; might not be applicable to all vertices, but they should be defined. You might be wondering, why not represent the city Seattle as a node and add an edge marked as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; that connects a person and the city?&lt;/p&gt;

&lt;p&gt;In general, if there is a value linked to a large number of other nodes in the network and it requires additional properties to correlate  with other nodes, it should be represented as a node. In this scenario, the architecture defined in Fig 5 is more appropriate for traversing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; connections. It also gives us the ability to link any new attributes to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;based_near&lt;/code&gt; relationship.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image8.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 5 Labelled Property Graph: Example 2&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have the context of graphs, let us talk about graph databases, how they help with large data queries and the part they play in Graph Technology.&lt;/p&gt;

&lt;h2 id=&quot;graph-database&quot;&gt;Graph database&lt;/h2&gt;

&lt;p&gt;A graph database is a type of NoSQL database that stores data using network topology. The idea is derived from LPG, which represents data sets with vertices, edges, and attributes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vertices are instances or entities of data that represent any object to be tracked, such as people, accounts, locations, etc.&lt;/li&gt;
  &lt;li&gt;Edges are the critical concepts in graph databases which represent relationships between vertices. The connections have a direction that can be unidirectional (one-way) or bidirectional (two-way).&lt;/li&gt;
  &lt;li&gt;Properties represent descriptive information associated with vertices. In some cases, edges have properties as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Graph databases provide a more conceptual view of data that is closer to reality. Modelling complex linkages becomes simpler because interconnections between data points are given the same weight as the data itself.&lt;/p&gt;

&lt;h3 id=&quot;graph-database-vs-relational-database&quot;&gt;Graph database vs. relational database&lt;/h3&gt;

&lt;p&gt;Relational databases are currently the industry norm and take a structured approach to data, usually in the form of tables. On the other hand, graph databases are agile and focus on immediate relationship understanding. Neither type is designed to replace the other, so it is important to know what each database type has to offer.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-concepts/image13.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 6 Graph database vs relational database&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There is a domain for both graph and relational databases. Graph databases outperform typical relational databases, especially in use cases involving complicated relationships, as they take a more naturalistic and flowing approach to data.&lt;/p&gt;

&lt;p&gt;The key distinctions between graph and relational databases are summarised in the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Graph&lt;/th&gt;
      &lt;th&gt;Relational&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Format&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Nodes and edges with properties&lt;/td&gt;
      &lt;td&gt;Tables with rows and columns&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Relationships&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Represented with edges between nodes&lt;/td&gt;
      &lt;td&gt;Created using foreign keys between tables&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Flexible&lt;/td&gt;
      &lt;td&gt;Rigid&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Complex queries&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Quick and responsive&lt;/td&gt;
      &lt;td&gt;Requires complex joins&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Use case&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Systems with highly connected relationships&lt;/td&gt;
      &lt;td&gt;Transaction focused systems with more straightforward relationships&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Table. 1 Graph vs. Relational Databases&lt;/i&gt;&lt;/figcaption&gt;

&lt;h3 id=&quot;advantages-and-disadvantages&quot;&gt;Advantages and disadvantages&lt;/h3&gt;

&lt;p&gt;Every database type has its advantages and disadvantages; knowing the distinctions as well as potential options for specific challenges is crucial. Graph databases are a rapidly evolving technology with improved functions compared with other database types.&lt;/p&gt;

&lt;h4 id=&quot;advantages&quot;&gt;Advantages&lt;/h4&gt;

&lt;p&gt;Some advantages of graph databases include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Agile and flexible structures.&lt;/li&gt;
  &lt;li&gt;Explicit relationship representation between entities.&lt;/li&gt;
  &lt;li&gt;Real-time query output - speed depends on the number of relationships.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;disadvantages&quot;&gt;Disadvantages&lt;/h4&gt;

&lt;p&gt;The general disadvantages of graph databases are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No standardised query language; depends on the platform used.&lt;/li&gt;
  &lt;li&gt;Not suitable for transactional-based systems.&lt;/li&gt;
  &lt;li&gt;Small user base, making it hard to find troubleshooting support.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;graph-technology&quot;&gt;Graph technology&lt;/h2&gt;

&lt;p&gt;Graph technology is the next step in improving analytics delivery. Traditional analytics is insufficient to meet complicated business operations, distribution, and analytical concerns as data quantities expand.&lt;/p&gt;

&lt;p&gt;Graph technology aids in the discovery of unknown correlations in data that would otherwise go undetected or unanalysed. When the term graph is used to describe a topic, three distinct concepts come to mind: graph theory, graph analytics, and graph data management.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Graph theory - A mathematical notion that uses stack ordering to find paths, linkages, and networks of logical or physical objects, as well as their relationships. Can be used to model molecules, telephone lines, transport routes, manufacturing processes, and many other things.&lt;/li&gt;
  &lt;li&gt;Graph analytics - The application of graph theory to uncover nodes, edges, and data linkages that may be assigned semantic attributes. Can examine potentially interesting connections in data found in traditional analysis solutions, using node and edge relationships.&lt;/li&gt;
  &lt;li&gt;Graph database - A type of storage for data generated by graph analytics. Filling a knowledge graph, which is a model in data that indicates a common usage of acquired knowledge or data sets expressing a frequently held notion, is a typical use case for graph analytics output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the architecture and terminology are sometimes misunderstood, graph analytics’ output can be viewed through visualisation tools, knowledge graphs, particular applications, and even some advanced dashboard capabilities of business intelligence tools. All three concepts above are frequently used to improve system efficiency and even to assist in dynamic data management. In this approach, graph theory and analysis are inextricably linked, and analysis may always rely on graph databases.&lt;/p&gt;

&lt;h2 id=&quot;graph-centric-user-stories&quot;&gt;Graph-centric user stories&lt;/h2&gt;

&lt;h4 id=&quot;fraud-detection&quot;&gt;Fraud detection&lt;/h4&gt;

&lt;p&gt;Traditional fraud prevention methods concentrate on discrete data points such as individual accounts, devices, or IP addresses. However, today’s sophisticated fraudsters avoid detection by building fraud rings using stolen and fake identities. To detect such fraud rings, we need to look beyond individual data points to the linkages that connect them.&lt;/p&gt;

&lt;p&gt;Graph technology greatly transcends the capabilities of a relational database, by revealing hard-to-find patterns. Enterprise businesses also employ Graph technology to supplement their existing fraud detection skills to tackle a wide range of financial crimes, including first-party bank fraud, fraud, and money laundering.&lt;/p&gt;

&lt;h4 id=&quot;real-time-recommendations&quot;&gt;Real-time recommendations&lt;/h4&gt;

&lt;p&gt;An online business’s success depends on systems that can generate meaningful recommendations in real time. To do so, we need the capacity to correlate product, customer, inventory, supplier, logistical, and even social sentiment data in real time. Furthermore, a real-time recommendation engine must be able to record any new interests displayed during the consumer’s current visit in real time, which batch processing cannot do.&lt;/p&gt;

&lt;p&gt;Graph databases outperform relational and other NoSQL data stores in terms of delivering real-time suggestions. Graph databases can easily integrate different types of data to get insights into consumer requirements and product trends, making them an increasingly popular alternative to traditional relational databases.&lt;/p&gt;

&lt;h4 id=&quot;supply-chain-management&quot;&gt;Supply chain management&lt;/h4&gt;

&lt;p&gt;With complicated scenarios like supply chains, there are many different parties involved and companies need to stay vigilant in detecting issues like fraud, contamination, high-risk areas or unknown product sources. This means that there is a need to efficiently process large amounts of data and ensure transparency throughout the supply chain.&lt;/p&gt;

&lt;p&gt;To have a transparent supply chain, relationships between each product and party need to be mapped out, which means there will be deep linkages. Graph databases are great for these as they are designed to search and analyse data with deep links. This means they can process enormous amounts of data without performance issues.&lt;/p&gt;

&lt;h4 id=&quot;identity-and-access-management&quot;&gt;Identity and access management&lt;/h4&gt;

&lt;p&gt;Managing multiple changing roles, groups, products and authorisations can be difficult, especially in large organisations. Graph technology integrates your data and allows quick and effective identity and access control. It also allows you to track all identity and access authorisations and inheritances with significant depth and real-time insights.&lt;/p&gt;

&lt;h4 id=&quot;network-and-it-operations&quot;&gt;Network and IT operations&lt;/h4&gt;

&lt;p&gt;Because of the scale and complexity of network and IT infrastructure, you need a configuration management database (CMDB) that is far more capable than relational databases. Neptune is an example of a CMDB and graph database that allows you to correlate your network, data centre, and IT assets to aid troubleshooting, impact analysis, and capacity or outage planning.&lt;/p&gt;

&lt;p&gt;A graph database allows you to integrate various monitoring tools and acquire important insights into the complicated relationships that exist between various network or data centre processes. Possible applications of graphs in network and IT operations range from dependency management to automated microservice monitoring.&lt;/p&gt;

&lt;h4 id=&quot;risk-assessment-and-monitoring&quot;&gt;Risk assessment and monitoring&lt;/h4&gt;

&lt;p&gt;Risk assessment is crucial in the fintech business. With multiple sources of credit data such as ecommerce sites, mobile wallets and loan repayment records, it can be difficult to accurately assess an individual’s credit risk. Graph Technology makes it possible to combine these data sources, quantify an individual’s fraud risk and even generate full credit reviews.&lt;/p&gt;

&lt;p&gt;One clear example of this is &lt;a href=&quot;https://www.globenewswire.com/news-release/2018/08/29/1558324/0/en/FinTech-Pioneer-IceKredit-Transforms-the-Credit-Market-With-TigerGraph.html&quot;&gt;IceKredit&lt;/a&gt;, which employs artificial intelligence (AI) and machine learning (ML) techniques to make better risk-based decisions. With Graph technology, IceKredit has also successfully detected unreported links and increased efficiency of financial crime investigations.&lt;/p&gt;

&lt;h4 id=&quot;social-network&quot;&gt;Social network&lt;/h4&gt;

&lt;p&gt;Whether you’re using stated social connections or inferring links based on behaviour, social graph databases like Neptune introduce possibilities for building new social networks or integrating existing social graphs into commercial applications.&lt;/p&gt;

&lt;p&gt;Having a data model that is identical to your domain model allows you to better understand your data, communicate more effectively, and save time. By decreasing the time spent data modelling, graph databases increase the quality and speed of development for your social network application.&lt;/p&gt;

&lt;h4 id=&quot;artificial-intelligence-ai-and-machine-learning-ml&quot;&gt;Artificial intelligence (AI) and machine learning (ML)&lt;/h4&gt;

&lt;p&gt;AI and ML use statistical and analytical approaches to find patterns in data and provide insights. However, there are two prevalent concerns that arise - the quality of data and effectiveness of the analytics. Some AI and ML solutions have poor accuracy because there is not enough training data or variants that have a high correlation to the outcome.&lt;/p&gt;

&lt;p&gt;These ML data issues can be solved with graph databases as it’s possible to connect and traverse links, as well as supplement raw data. With Graph technology, ML systems can recognise each column as a “feature” and each connection as a distinct characteristic, and then be able to identify data patterns and train themselves to recognise these relationships.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Graphs are a great way to visually represent complex systems and can be used to easily detect patterns or relationships between entities. To help improve graphs’ ability to detect patterns early, businesses should consider using Graph technology, which is the next step in improving analytics delivery.&lt;/p&gt;

&lt;p&gt;Graph technology typically consists of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Graph theory&lt;/strong&gt; - Used to find paths, linkages and networks of logical or physical objects.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph analytics&lt;/strong&gt; - Application of graph theory to uncover nodes, edges, and data linkages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph database&lt;/strong&gt; - Storage for data generated by graph analytics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although predominantly used in fraud detection, Graph technology has many other use cases such as making real-time recommendations based on consumer behaviour, identity and access control, risk assessment and monitoring, AI and ML, and many more.&lt;/p&gt;

&lt;p&gt;In our next blog article, we will be talking about how our Graph Visualisation Platform enhances Grab’s fraud detection methods.&lt;/p&gt;

&lt;p&gt;Check out the other articles in this series:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/graph-networks&quot;&gt;Graph Networks - Striking fraud syndicates in the dark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/graph-visualisation&quot;&gt;Graph Networks - 10X investigation with Graph Visualisations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.baeldung.com/cs/graph-theory-intro&quot;&gt;https://www.baeldung.com/cs/graph-theory-intro&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/class/cs520/2020/notes/What_Are_Graph_Data_Models.html&quot;&gt;https://web.stanford.edu/class/cs520/2020/notes/What_Are_Graph_Data_Models.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jun 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/graph-concepts</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-concepts</guid>
        
        <category>Security</category>
        
        <category>Graphs concepts</category>
        
        <category>Graph technology</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Automated Experiment Analysis - Making experimental analysis scalable</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Trustworthy experiments are key to making sound decisions, so analysts and data scientists put a lot of effort into analysing them and making business impacts. An extension of &lt;a href=&quot;https://engineering.grab.com/building-grab-s-experimentation-platform&quot;&gt;Grab’s Experimentation (GrabX) platform&lt;/a&gt;, Automated Experiment Analysis is one of Grab’s data products that helps automate statistical analyses of experiments. It also provides automatic experimental data pipelines and customised tests for different types of experiments.&lt;/p&gt;

&lt;p&gt;Designed to help Grab in its journey of innovation and data-driven decision making, the data product helps to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Standardise and automate the basic experiment analysis process on Grab experiments.&lt;/li&gt;
  &lt;li&gt;Ensure post-experiment results are reproducible under a company-wide standard, and easily reviewed by each other.&lt;/li&gt;
  &lt;li&gt;Democratise the institutional knowledge of experimentation across functions.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Today, the GrabX platform provides the ability to define, configure, and execute online controlled experiments (OCEs), often called A/B tests, to gather trustworthy data and make data-driven decisions about how to improve our products.&lt;/p&gt;

&lt;p&gt;Before the automated analysis, each experiment was analysed manually on an ad-hoc basis. This manual and federated model brings in several challenges at the company level:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Inefficiency&lt;/strong&gt;: Repetitive nature of data pipeline building and basic post-experiment analyses incur large costs and deplete the analysts’ bandwidth from running deeper analyses.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lack of quality control&lt;/strong&gt;: Risk of unstandardised, inaccurate or late results as the platform cannot exercise data-governance/control or extend offerings to Grab’s other entities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lack of scalability and availability&lt;/strong&gt;: GrabX users have varied backgrounds and skills, making their approaches to experiments different and not easily transferable/shared. E.g. Some teams may use more advanced techniques to speed up their experiments without using too much resources but these techniques are not transferable without considerable training.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/automated-experiment-analysis/image1.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Architecture diagram&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When users set up experiments on GrabX, they can configure the success metrics they are interested in. These metrics configurations are then stored in the metadata as “bronze”, “silver”, and “gold” datasets depending on the corresponding step in the automated data pipeline process.&lt;/p&gt;

&lt;h4 id=&quot;metrics-configuration-and-bronze-datasets&quot;&gt;Metrics configuration and “bronze” datasets&lt;/h4&gt;

&lt;p&gt;In this project, we have developed a metrics glossary that stores information about what the metrics are and how they are computed. The metrics glossary is stored in CosmoDB and serves as an API Endpoint for GrabX so users can pick from the list of available metrics. If a metric is not available, users can input their custom metrics definition.&lt;/p&gt;

&lt;p&gt;This metrics selection, as an analysis configuration, is then stored as a “bronze” dataset in Azure Data Lake as metadata, together with the experiment configurations. Once the experiment starts, the data pipeline gathers all experiment subjects and their assigned experiment groups from our clickstream tracking system.&lt;/p&gt;

&lt;p&gt;In this case, the experiment subject refers to the facets of the experiment. For example, if the experiment subject is a user, then the user will go through the same experience throughout the entire experimentation period.&lt;/p&gt;

&lt;h4 id=&quot;metrics-computation-and-silver-datasets&quot;&gt;Metrics computation and “silver” datasets&lt;/h4&gt;

&lt;p&gt;In this step, the metrics engine gathers all metrics data based on the metrics configuration and computes the metrics for each experiment subject. This computed data is then stored as a “silver” dataset and is the foundation dataset for all statistical analyses.&lt;/p&gt;

&lt;p&gt;“Silver” datasets are then passed through the “Decision Engine” to get the final “gold” datasets, which contain the experiment results.&lt;/p&gt;

&lt;h4 id=&quot;results-visualisation-andgold-datasets&quot;&gt;Results visualisation and ”gold” datasets&lt;/h4&gt;

&lt;p&gt;In “gold” datasets, we have the result of the experiment, along with some custom messages we want to show our users. These are saved in sets of fact and dim tables (typically used in &lt;a href=&quot;https://docs.microsoft.com/en-us/power-bi/guidance/star-schema&quot;&gt;star schemas&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;For users to visualise the result on GrabX, we leverage the embedded Power BI visualisation. We build the visualisation using a “gold” dataset and embed it to each experiment page with a fixed filter. By doing so, users can experience the end-to-end flow directly from GrabX.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;The implementation consists of four key engineering components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Analysis configuration setup&lt;/li&gt;
  &lt;li&gt;A data pipeline&lt;/li&gt;
  &lt;li&gt;Automatic analysis&lt;/li&gt;
  &lt;li&gt;Results visualisation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Analysis configuration&lt;/strong&gt; is part of the experiment setup process where users select success metrics they are interested in. This is an essential configuration for post-experiment analysis, in addition to the usual experiment configurations (e.g. sampling strategies).&lt;/p&gt;

&lt;p&gt;It ensures that the reported experiment results will align with the hypothesis setup, which helps avoid one of the common pitfalls in OCEs &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;There are three types of metrics available:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pre-defined metrics: These metrics are already defined in the Scribe datamart, e.g. Gross Merchandise Value (GMV) per pax.&lt;/li&gt;
  &lt;li&gt;Event-based metrics: Users can specify an ad-hoc metric in the form of a funnel with event names for funnel start and end.&lt;/li&gt;
  &lt;li&gt;Build your own metrics: Users have the flexibility to define a metric in the form of a SQL query.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;A data pipeline&lt;/strong&gt; here mainly consists of data sourcing and data processing. We use Azure Data Factory to schedule ETL pipelines so we can calculate the metrics and statistical analysis. ETL jobs are written in spark and run using Databricks.&lt;/p&gt;

&lt;p&gt;Data pipelines are streamlined to the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load experiments and metrics metadata, defined at the experiment creation stage.&lt;/li&gt;
  &lt;li&gt;Load experiment and clickstream events.&lt;/li&gt;
  &lt;li&gt;Load experiment assignments. An experiment assignment maps a randomisation unit ID to the corresponding experiment or variant IDs.&lt;/li&gt;
  &lt;li&gt;Merge the data mentioned above for each experiment variant, and obtain sufficient data to do a deeper results analysis.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Automatic analysis&lt;/strong&gt; uses an internal python package “Decision Engine”, which decouples the dataset and statistical tests, so that we can incrementally improve applications of advanced techniques. It provides a comprehensive set of test results at the variant level, which include statistics, p-values, confidence intervals, and the test choices that correspond to the experiment configurations. It’s a crowdsourced project which allows all to contribute what they believe should be included in fundamental post-experiment analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results visualisation&lt;/strong&gt; leverages PowerBI, which is embedded in the GrabX UI, so users can run the experiments and review the results on a single platform. &lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;At the individual user level, Automated Experiment Analysis is designed to enable analysts and data scientists to associate metrics with experiments, and present the experiment results in a standardised and comprehensive manner. It speeds up the decision-making process and frees up the bandwidths of analysts and data scientists to conduct deeper analyses.&lt;/p&gt;

&lt;p&gt;At the user community level, it improves the efficiency of running experimental analysis by capturing all experiments, their results, and the launch decision within a single platform.&lt;/p&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;Automated Experiment Analysis is the first building block to boost the trustworthiness of OCEs in Grab. Not all types of experiments are fully onboard, and they might not need to be. Through this journey, we believe these key learnings would be useful for experimenters and platform teams:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To standardise and simplify several experimental analysis steps, there needs to be automation data pipelines, analytics tools, and a metrics store in the infrastructure.&lt;/li&gt;
  &lt;li&gt;The “Decision Engine” analytics tool should be decoupled from the other engineering components, so that it can be incrementally improved in future.&lt;/li&gt;
  &lt;li&gt;To democratise knowledge and ensure service coverage, many components need to have a crowdsourcing feature, e.g. the metrics store has a BYOM function, and “Decision Engine” is an open-sourced internal python package.&lt;/li&gt;
  &lt;li&gt;Tracking implementation is important. To standardise data pipelines and achieve scalability, we need to standardise the way we implement tracking.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A centralised metric store&lt;/strong&gt; -  We built a metric calculation dictionary, which currently contains around 30-40 basic business metrics, but its functionality is limited to GrabX Experimentation use case.&lt;/p&gt;

&lt;p&gt;If the metric store is expected to serve more general uses, it needs to be further enriched by allowing some “smarts”, e.g. fabric-agnostic metrics computations &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, other types of data slicing, and some considerations with real-time metrics or signals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An end-to-end experiment guide rail&lt;/strong&gt; - Currently, we provide automatic data analysis after an experiment is done, but no guardrail features at multiple experiment stages, e.g. sampling strategy choices, sample size recommendation from the planning stage, and data quality check during/after the experiment windows. Without the end-to-end guardrails, running experiments will be very prone to pitfalls. We therefore plan to add some degree of automation to ensure experiments adhere to the standards used by the post-experimental analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A more comprehensive analysis toolbox&lt;/strong&gt; - The current state of the project mainly focuses on infrastructure development, so it starts with basic frequentist’s A/B testing approaches. In future versions, it can be extended to include sequential testing, CUPED &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, attribution analysis, Causal Forest, heterogeneous treatment effects, etc.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dmitriev, P., Gupta, S., Kim, D. W., &amp;amp; Vaz, G. (2017, August). A dirty dozen: twelve common metric interpretation pitfalls in online controlled experiments. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1427-1436). &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/metric-computation-for-multiple-backends/&quot;&gt;Metric computation for multiple backends&lt;/a&gt;, Craig Boucher, Ulf Knoblich, Dan Miller, Sasha Patotski, Amin Saied, Microsoft Experimentation Platform &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Deng, A., Xu, Y., Kohavi, R., &amp;amp; Walker, T. (2013, February). Improving the sensitivity of online controlled experiments by utilising pre-experiment data. In Proceedings of the sixth ACM international conference on Web search and data mining (pp. 123-132). &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 30 May 2022 00:20:55 +0000</pubDate>
        <link>https://engineering.grab.com/automated-experiment-analysis</link>
        <guid isPermaLink="true">https://engineering.grab.com/automated-experiment-analysis</guid>
        
        <category>Experiment</category>
        
        <category>Experimental analysis</category>
        
        <category>Azure Databricks</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Search architecture revamp</title>
        <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Prior to 2021, Grab’s search architecture was designed to only support textual matching, which takes in a user query and looks for exact matches within the ecosystem through an &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverted_index&quot;&gt;inverted index&lt;/a&gt;. This legacy system meant that only textual matching results could be fetched.&lt;/p&gt;

&lt;p&gt;In the second half of 2021, the Deliveries search team worked on improving this architecture to make it smarter, more scalable and also unlock future growth for different search use cases at Grab. The figure below shows a simplified overview of the legacy architecture.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-architecture-revamp/image2.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Legacy architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;With the legacy system, we noticed several problems.&lt;/p&gt;

&lt;h4 id=&quot;search-results-were-textually-matched-without-considering-intention-and-context&quot;&gt;Search results were textually matched without considering intention and context&lt;/h4&gt;

&lt;p&gt;If a user types in a query “Roti Prata” (flatbread), he is likely looking for Roti Prata dishes and those matches with the dish name should be prioritised compared with matches with the merchant-partner’s name or matches with other entities.&lt;/p&gt;

&lt;p&gt;In the legacy system, all entities whose names partially matched “Roti Prata” were displayed and ranked according to hard coded weights, and matches with merchant-partner names were always prioritised, even if the user intention was clearly to search for the “Roti Prata” dish itself.  &lt;/p&gt;

&lt;p&gt;This problem was more common in Mart, as users often intended to search for items instead of shops. Besides the lack of intention recognition, the search system was also unable to take context into consideration; users searching the same keyword query at different times and locations could have different objectives. E.g. if users search for “Bread” in the day, they may be likely to look for cafes while searches at night could be for breakfast the next day.&lt;/p&gt;

&lt;h4 id=&quot;search-results-from-multiple-business-verticals-were-not-blended-effectively&quot;&gt;Search results from multiple business verticals were not blended effectively&lt;/h4&gt;

&lt;p&gt;In Grab’s context, results from multiple verticals were often merged. For example, in Mart searches, Ads and Mart organic search results were displayed together; in Food searches, Ads, Food and Mart organic results were blended together.&lt;/p&gt;

&lt;p&gt;In the legacy architecture, multiple business verticals were merged on the Deliveries API layer, which resulted in the leak of abstraction and loss of useful data as data from the search recall stage was also not taken into account during the merge stage.&lt;/p&gt;

&lt;h4 id=&quot;inability-to-quickly-scale-to-new-search-use-cases-and-difficulty-in-reusing-existing-components&quot;&gt;Inability to quickly scale to new search use cases and difficulty in reusing existing components&lt;/h4&gt;

&lt;p&gt;The legacy code base was not written in a structured way that could scale to new use cases easily. If new search use cases cannot be built on top of an existing system, it can be rather tedious to keep rebuilding the function every time there is a new search use case.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;In this section, solutions from both architecture and implementation perspectives are presented to address the above problem statements.&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;In the new architecture, the flow is extended from lexical recall only to multi-layer including boosting, multi-recall, and ranking. The addition of boosting enables capabilities like intent recognition and query expansion, while the change from single lexical recall to multi-recall opens up the potential for other recall methods, e.g. embedding based and graph based.&lt;/p&gt;

&lt;p&gt;These help address the &lt;a href=&quot;#search-results-were-textually-matched-without-considering-intention-and-context&quot;&gt;first problem statement&lt;/a&gt;. Furthermore, the multi-recall framework enables fetching results from multiple business verticals, addressing the &lt;a href=&quot;#search-results-from-multiple-business-verticals-were-not-blended-effectively&quot;&gt;second problem statement&lt;/a&gt;. In the new framework, results from different verticals and different recall methods were grouped and ranked together without any leak of abstraction or loss of useful data from search recall stage in ranking.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-architecture-revamp/image1.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Upgraded architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;We believe that the key to a platform’s success is modularisation and flexible assembling of plugins to enable quick product iteration. That is why we implemented a combination of a framework defined by the platform and plugins provided by service teams. In this implementation, plugins are assembled through configurations, which addresses the &lt;a href=&quot;#inability-to-quickly-scale-to-new-search-use-cases-and-difficulty-in-reusing-existing-components&quot;&gt;third problem statement&lt;/a&gt; and has two advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Separation of concern. With the main flow abstracted and maintained by the platform, service team developers could focus on the application logic by writing plugins and fitting them into the main flow. In this case, developers without search experience could quickly enable new search flows.&lt;/li&gt;
  &lt;li&gt;Reusing plugins and economies of scale. With more use cases onboarded, more plugins are written by service teams and these plugins are reusable assets, resulting in scale effect. For example, an Ads recall plugin could be reused in Food keyword or non-keyword searches, Mart keyword or non-keyword searches and universal search flows as all these searches contain non-organic Ads. Similarly, a Mart recall plugin could be reused in Mart keyword or non-keyword searches, universal search and Food keyword search flows, as all these flows contain Mart results. With more plugins accumulated on our platform, developers might be able to ship a new search flow by just reusing and assembling the existing plugins.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Our platform now has a smart search with intent recognition and semantic (embedding-based) search. The process of adding new modules is also more straightforward and adds intention recognition to the boosting step as well as embedding as an additional recall to the multi-recall step. These modules can be easily reused by other use cases.&lt;/p&gt;

&lt;p&gt;On top of that, we also have a mixed Ads and an organic framework. This means that data in the recall stage is taken into consideration and Ads can now be ranked together with organic results, e.g. text relevance.&lt;/p&gt;

&lt;p&gt;With a modularised design and plugins provided by the platform, it is easier for clients to use our platform with a simple onboarding process. Furthermore, plugins can be reused to cater to new use cases and achieve a scale effect.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 17 May 2022 03:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/search-architecture-revamp</link>
        <guid isPermaLink="true">https://engineering.grab.com/search-architecture-revamp</guid>
        
        <category>Architecture</category>
        
        <category>Optimisation</category>
        
        <category>Search</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Embracing a Docs-as-Code approach</title>
        <description>&lt;p&gt;The Docs-as-Code concept has been gaining traction in the past few years as more tech companies start implementing this approach. One of the most widely-known examples is &lt;a href=&quot;https://backstage.io/blog/2020/09/08/announcing-tech-docs&quot;&gt;Spotify&lt;/a&gt;, that ​​uses Docs-as-Code to publish documentation in an internal developer portal.&lt;/p&gt;

&lt;p&gt;Since the start of 2021, Grab has also adopted a Docs-as-Code approach to improve our technical documentation. Before we talk about how this is done at Grab, let’s explain what this concept really means.&lt;/p&gt;

&lt;h2 id=&quot;what-is-docs-as-code&quot;&gt;What is Docs-as-Code?&lt;/h2&gt;

&lt;p&gt;Docs-as-Code is a mindset of creating and maintaining technical documentation. The goal is to empower engineers to write technical documentation frequently and keep it up to date by integrating with their tools and processes.&lt;/p&gt;

&lt;p&gt;This means that technical documentation is placed in the same repository as the code, making it easier for engineers to write and update. Next, we’ll go through the motivations behind this initiative.&lt;/p&gt;

&lt;h2 id=&quot;why-embark-on-this-journey&quot;&gt;Why embark on this journey?&lt;/h2&gt;

&lt;p&gt;After speaking to Grab engineers, we found that some of their biggest challenges are around finding and writing documentation. Like many other companies on the same journey, Grab is rather big and our engineers are split into many different teams. Within each team, technical documentation can be stored on different platforms and in different formats, e.g. Google drive documents, text files, etc. This makes it hard to find relevant information, especially if you are trying to find another team’s documentation.&lt;/p&gt;

&lt;p&gt;On top of that, we realised that the documentation process is disconnected from an engineer’s everyday activities, making technical documentation an awkward afterthought. This means that even if people could find the information, there was a good chance that it would not be up to date.&lt;/p&gt;

&lt;p&gt;To address these issues, we need a centralised platform, a single source of truth, so that people can find and discover technical documentation easily. But first, we need to change how we write technical documentation. This is where Docs-as-Code comes in.  &lt;/p&gt;

&lt;h3 id=&quot;how-does-docs-as-code-solve-the-problem&quot;&gt;How does Docs-as-Code solve the problem?&lt;/h3&gt;

&lt;p&gt;With Docs-as-Code, technical documentation is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Written in plaintext.&lt;/li&gt;
  &lt;li&gt;Editable in a code editor.&lt;/li&gt;
  &lt;li&gt;Stored in the same repository as the source code so it’s easier to update docs whenever a code change is committed.&lt;/li&gt;
  &lt;li&gt;Published on a central platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea is to consolidate all technical documentation on a central platform, making it easier to discover and find content by using an easy-to-navigate information architecture and targeted search.&lt;/p&gt;

&lt;h2 id=&quot;how-is-grabembracing-docs-as-code&quot;&gt;How is Grab embracing Docs-as-Code?&lt;/h2&gt;

&lt;p&gt;We’ve developed an internal developer portal that simplifies the process of writing, reviewing and publishing technical documentation.&lt;/p&gt;

&lt;p&gt;Here’s a brief overview of the process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a dedicated docs folder in a Git repository.&lt;/li&gt;
  &lt;li&gt;Push Markdown files into the docs folder.&lt;/li&gt;
  &lt;li&gt;Configure the developer portal to publish docs from the respective code repository.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The latest version of the documentation will automatically be built and published in the developer portal.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/doc-as-code/image1.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Simplified documentation process&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This way, technical documentation is closer to the source code and integrated into the code development process. Writing and updating technical documentation becomes part of writing code, and this encourages engineers to keep documentation updated.&lt;/p&gt;

&lt;h2 id=&quot;measuring-success&quot;&gt;Measuring success&lt;/h2&gt;

&lt;p&gt;Whenever there’s a change throughout big organisations like Grab, it can be tough to implement. But thankfully, our engineers recognised the importance of improving documentation and making it easier to maintain or update.&lt;/p&gt;

&lt;p&gt;We surveyed our users and here’s what some have said about our Docs-as-Code initiative:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“[W]ith the doc and source code in one place, test backend engineers can now make doc changes via standard code review process and re-use the same content for CLI helper message and documentation.” - Kang Yaw Ong, Test Automation - Engineering Manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;“[Docs-as-Code] is a great initiative, as it keeps documentation in line and up-to-date with the development of a project. Managing documentation using a version control system and the same tools to handle merges and conflicts reduces overhead and friction in an engineer’s workflow.” - Eugene Chiang, Foundations - Engineering Manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;progress-and-future-optimisations&quot;&gt;Progress and future optimisations&lt;/h2&gt;

&lt;p&gt;Since we first started the Docs-as-Code initiative in Grab, we’ve made a lot of progress in terms of adoption - approximately 80% of Grab services will have their technical documentation on the internal portal by April 2022.&lt;/p&gt;

&lt;p&gt;We’ve also improved overall user experience by enhancing stability and performance, improving navigation and content formatting, and enabling feedback. But it doesn’t stop there; we are continuously improving the internal portal and providing more features for our engineers.&lt;/p&gt;

&lt;p&gt;Apart from technical documentation, we are also applying the Docs-as-Code approach to our technical training content. This means moving both self-paced and workshop training content to a centralised repository and providing engineers a single platform for all their learning needs.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to the Tech Learning - Documentation team for their contributions to this blog post.
&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;we-are-hiring&quot;&gt;We are hiring!&lt;/h2&gt;

&lt;p&gt;We are looking for more technical content developers to join the team. If you’re keen on joining our Docs-as-Code journey and improving developer experience, check out our open listings in &lt;a href=&quot;https://grab.careers/jobs/job-details/?id=be841c804fee010177fceb2a4a740001&quot;&gt;Singapore&lt;/a&gt; and &lt;a href=&quot;https://grab.careers/jobs/job-details/?id=2b029fdf01590101835c6a2e830c0002&quot;&gt;Malaysia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Join us in driving this initiative forward and making documentation more approachable for everyone!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 04 May 2022 03:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/doc-as-code</link>
        <guid isPermaLink="true">https://engineering.grab.com/doc-as-code</guid>
        
        <category>Docs-as-Code</category>
        
        <category>Documentation</category>
        
        <category>Technical documentation</category>
        
        <category>Engineering practices</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Graph Networks - Striking fraud syndicates in the dark</title>
        <description>&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-intro/image2.png&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As a leading superapp in Southeast Asia, Grab serves millions of consumers daily. This naturally makes us a target for fraudsters and to enhance our defences, the Integrity team at Grab has launched several hyper-scaled services, such as the &lt;a href=&quot;https://engineering.grab.com/griffin&quot;&gt;Griffin real-time rule engine&lt;/a&gt; and &lt;a href=&quot;https://engineering.grab.com/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;Advanced Feature Engineering&lt;/a&gt;. These systems enable data scientists and risk analysts to develop real-time scoring, and take fraudsters out of our ecosystems.&lt;/p&gt;

&lt;p&gt;Apart from individual fraudsters, we have also observed the fast evolution of the dark side over time. We have had to evolve our defences to deal with professional syndicates that use advanced equipment such as device farms and GPS spoofing apps to perform fraud at scale. These professional fraudsters are able to camouflage themselves as normal users, making it significantly harder to identify them with rule-based detection.&lt;/p&gt;

&lt;p&gt;Since 2020, Grab’s Integrity team has been advancing fraud detection with more sophisticated techniques and experimenting with a range of graph network technologies such as graph visualisations, graph neural networks and graph analytics. We’ve seen a lot of progress in this journey and will be sharing some key learnings that might help other teams who are facing similar issues.&lt;/p&gt;

&lt;h3 id=&quot;what-are-graph-based-prediction-platforms&quot;&gt;What are Graph-based Prediction Platforms?&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“You can fool some of the people all of the time, and all of the people some of the time, but you cannot fool all of the people all of the time.” - Abraham Lincoln&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A Graph-based Prediction Platform connects multiple entities through one or more common features. When such entities are viewed as a macro graph network, we uncover new patterns that are otherwise unseen to the naked eye. For example, when investigating if two users are sharing IP addresses or devices, we might not be able to tell if they are fraudulent or just family members sharing a device.&lt;/p&gt;

&lt;p&gt;However, if we use a graph system and look at all users sharing this device or IP address, it could show us if these two users are part of a much larger syndicate network in a device farming operation. In operations like these, we may see up to hundreds of other fake accounts that were specifically created for promo and payment fraud. With graphs, we can identify fraudulent activity more easily.&lt;/p&gt;

&lt;h3 id=&quot;grabs-graph-based-prediction-platform&quot;&gt;Grab’s Graph-based Prediction Platform&lt;/h3&gt;

&lt;p&gt;Leveraging the power of graphs, the team has primarily built two types of systems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Graph Database Platform&lt;/strong&gt;: An ultra-scalable storage system with over one billion nodes that powers:
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Graph Visualisation&lt;/strong&gt;: Risk specialists and data analysts can review user connections real-time and are able to quickly capture new fraud patterns with over 10 dimensions of features (see Fig 1).&lt;/p&gt;

        &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-intro/image1.gif&quot; alt=&quot;Change Data Capture flow&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 1: Graph visualisation&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Network-based feature system&lt;/strong&gt;: A configurable system for engineers to adjust machine learning features based on network connectivity, e.g. number of hops between two users, numbers of shared devices between two IP addresses.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Graph-based Machine Learning&lt;/strong&gt;: Unlike traditional fraud detection models, Graph Neural Networks (GNN) are able to utilise the structural correlations on the graph and act as a sustainable foundation to combat many different kinds of fraud. The data science team has built large-scale GNN models for scenarios like anti-money laundering and fraud detection.&lt;/p&gt;

    &lt;p&gt;Fig 2 shows a Money Laundering Network where hundreds of accounts coordinate the placement of funds, layering the illicit monies through a complex web of transactions making funds hard to trace, and consolidate funds into spending accounts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-intro/image3.gif&quot; alt=&quot;Change Data Capture flow&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Fig 2: Money Laundering Network&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h3&gt;
&lt;p&gt;In a future article of our Graph Network blog series, we will dive deeper into how we develop the graph infrastructure and database using AWS Neptune.&lt;/p&gt;

&lt;p&gt;Check out the other articles in this series:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/graph-concepts&quot;&gt;Graph concepts and applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/graph-visualisation&quot;&gt;Graph Networks - 10X investigation with Graph Visualisations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Apr 2022 10:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/graph-networks</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-networks</guid>
        
        <category>Graph Networks</category>
        
        <category>Graphs</category>
        
        <category>Fraud detection</category>
        
        <category>Security</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>How we reduced our CI YAML files from 1800 lines to 50 lines</title>
        <description>&lt;p&gt;This article illustrates how the Cauldron Machine Learning (ML) Platform team uses &lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html&quot;&gt;GitLab parent-child pipelines&lt;/a&gt; to dynamically generate GitLab CI files to solve several limitations of GitLab for large repositories, namely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Limitations to the number of includes (&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab/-/issues/207270#:~:text=GitLab%20Next&amp;amp;text=Please%20increase%20the%20maximum%20number,methods%20for%20managing%20CICD%20configurations.&quot;&gt;100&lt;/a&gt; by default).&lt;/li&gt;
  &lt;li&gt;Simplifying the GitLab CI file from 1800 lines to 50 lines.&lt;/li&gt;
  &lt;li&gt;Reducing the need for nested &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci&lt;/code&gt; yml files.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Cauldron is the Machine Learning (ML) Platform team at Grab. The Cauldron team provides tools for ML practitioners to manage the end to end lifecycle of ML models, from training to deployment. GitLab and its tooling are an integral part of our stack, for continuous delivery of machine learning.&lt;/p&gt;

&lt;p&gt;One of our core products is MerLin Pipelines. Each team has a dedicated repo to maintain the code for their ML pipelines. Each pipeline has its own subfolder. We rely heavily on GitLab rules to detect specific changes to trigger deployments for the different stages of different pipelines (for example, model serving with Catwalk, and so on).&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;approach-1-nested-child-files&quot;&gt;Approach 1: Nested child files&lt;/h3&gt;

&lt;p&gt;Our initial approach was to rely heavily on static code generation to generate the child &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; files in individual stages. See Figure 1 for an example directory structure. These nested yml files are pre-generated by our cli and committed to the repository.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1: Example directory structure with nested gitlab-ci.yml files. &quot; src=&quot;img/how-we-reduced-our-ci-yaml/image6.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1: Example directory structure with nested gitlab-ci.yml files. Child `gitlab-ci.yml` files are added by using the &lt;a href=&quot;https://docs.gitlab.com/ee/ci/yaml/#include&quot;&gt;include&lt;/a&gt; keyword.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2: Example root .gitlab-ci.yml file, and include clauses.&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image4.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2: Example root .gitlab-ci.yml file, and include clauses.&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3: Example child .gitlab-ci.yml file for a given stage (Deploy Model) in a pipeline (pipeline 1).
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image1.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3: Example child `.gitlab-ci.yml` file for a given stage (Deploy Model) in a pipeline (pipeline 1).
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;As teams add more pipelines and stages, we soon hit a limitation in this approach:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There was a &lt;a href=&quot;https://docs.gitlab.com/ee/ci/yaml/#include&quot;&gt;soft limit&lt;/a&gt; in the number of includes that could be in the base &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.gitlab-ci.yml&lt;/code&gt; file.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It became evident that this approach would not scale to our use-cases.&lt;/p&gt;

&lt;h3 id=&quot;approach-2-dynamically-generating-a-big-ci-file&quot;&gt;Approach 2: Dynamically generating a big CI file&lt;/h3&gt;

&lt;p&gt;Our next attempt to solve this problem was to try to inject and inline the nested child &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; contents into the root &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; file, so that we no longer needed to rely on the in-built GitLab “include” clause.&lt;/p&gt;

&lt;p&gt;To achieve it, we wrote a utility that parsed a raw &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci&lt;/code&gt; file, walked the tree to retrieve all “included” child &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci&lt;/code&gt; files, and to replace the includes to generate a final big &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Figure 4 illustrates the resulting file is generated from Figure 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4: “Fat” YAML file generated through this approach, assumes the original raw file of Figure 3.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image2.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4: “Fat” YAML file generated through this approach, assumes the original raw file of Figure 3.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This approach solved our issues temporarily. Unfortunately, we ended up with GitLab files that were up to 1800 lines long. There is also a soft limit to the size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab-ci.yml&lt;/code&gt; files. It became evident that we would eventually hit the limits of this approach.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Our initial attempt at using static code generation put us partially there. We were able to pre-generate and infer the stage and pipeline names from the information available to us. Code generation was definitely needed, but upfront generation of code had some key limitations, as shown above. We needed a way to improve on this, to somehow generate GitLab stages on the fly. After some research, we stumbled upon &lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html#dynamic-child-pipelines&quot;&gt;Dynamic Child Pipelines&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Quoting the official website:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Instead of running a child pipeline from a static YAML file, you can define a job that runs your own script to generate a YAML file, which is then used to trigger a child pipeline.&lt;/p&gt;

  &lt;p&gt;This technique can be very powerful in generating pipelines targeting content that changed or to build a matrix of targets and architectures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We were already on the right track. We just needed to combine code generation with child pipelines, to dynamically generate the necessary stages on the fly.&lt;/p&gt;

&lt;h2 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 5: Flow diagram of how we use dynamic yaml generation. The user raises a merge request in a branch, and subsequently merges the branch to master.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image8.png&quot; width=&quot;75%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 5: Flow diagram of how we use dynamic yaml generation. The user raises a merge request in a branch, and subsequently merges the branch to master.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;The user Git flow can be seen in Figure 5, where the user modifies or adds some files in their respective Git team repo. As a refresher, a typical repo structure consists of pipelines and stages (see Figure 1). We would need to extract the information necessary from the branch environment in Figure 5, and have a stage to programmatically generate the proper stages (for example, Figure 3).&lt;/p&gt;

&lt;p&gt;In short, our requirements can be summarized as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Detecting the files being changed in the Git branch.&lt;/li&gt;
  &lt;li&gt;Extracting the information needed from the files that have changed.&lt;/li&gt;
  &lt;li&gt;Passing this to be templated into the necessary stages.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s take a very simple example, where a user is modifying a file in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_1&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_1&lt;/code&gt; in Figure 1. Our desired output would be:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 6: Desired output that should be dynamically generated.&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image7.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 6: Desired output that should be dynamically generated.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Our template would be in the form of:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 7: Example template, and information needed. Let’s call it template\_file.yml.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image5.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 7: Example template, and information needed. Let’s call it template_file.yml.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;First, we need to detect the files being modified in the branch. We achieve this with native &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git diff&lt;/code&gt; commands, checking against the base of the branch to track what files are being modified in the merge request. The output (let’s call it &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diff.txt&lt;/code&gt;) would be in the form of:&lt;/p&gt;
&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;M        pipelines/pipeline_1/stage_1/modelserving.yaml
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
 &lt;small class=&quot;post-image-caption&quot;&gt;Figure 8: Example diff.txt generated from git diff.
 &lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;We must extract the yellow and green information from the line, corresponding to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_name&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_name&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 9: Information that needs to be extracted from the file.&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image9.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 9: Information that needs to be extracted from the file.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;We take a very simple approach here, by introducing a concept called stop patterns.&lt;/p&gt;

&lt;p&gt;Stop patterns are defined as a comma separated list of variable names, and the words to stop at. The colon (:) denotes how many levels before the stop word to stop.&lt;/p&gt;

&lt;p&gt;For example, the stop pattern:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;pipeline_name:pipelines&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;tells the parser to look for the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipelines&lt;/code&gt; and stop before that, extracting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_1&lt;/code&gt; from the example above tagged to the variable name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline_name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The stop pattern with two colons (::):&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;stage_name::pipelines&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;tells the parser to stop two levels before the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipelines&lt;/code&gt;, and extract &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_1&lt;/code&gt; as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stage_name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Our cli tool allows the stop patterns to be comma separated, so the final command would be:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;cauldron_repo_util diff.txt template_file.yml
&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;pipeline_name:pipelines,stage_name::pipelines &amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;generated.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We elected to write the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;util&lt;/code&gt; in Rust due to its high performance, and its rich templating libraries (for example, &lt;a href=&quot;https://github.com/Keats/tera&quot;&gt;Tera&lt;/a&gt;) and decent cli libraries (&lt;a href=&quot;https://github.com/clap-rs/clap&quot;&gt;clap&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Combining all these together, we are able to extract the information needed from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git diff&lt;/code&gt;, and use stop patterns to extract the necessary information to be passed into the template. Stop patterns are flexible enough to support different types of folder structures.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 10: Example Rust code snippet for parsing the Git diff file.
&quot; src=&quot;img/how-we-reduced-our-ci-yaml/image3.png&quot; width=&quot;75%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 10: Example Rust code snippet for parsing the Git diff file.
&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;When triggering pipelines in the master branch (see right side of Figure 5), the flow is the same, with a small caveat that we must retrieve the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diff.txt&lt;/code&gt; file from the source branch. We achieve this by using the rich GitLab API, retrieving the pipeline artifacts and using the same util above to generate the necessary GitLab steps dynamically.&lt;/p&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;After implementing this change, our biggest success was reducing one of the biggest ML pipeline Git repositories from 1800 lines to 50 lines. This approach keeps the size of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.gitlab-ci.yaml&lt;/code&gt; file constant at 50 lines, and ensures that it scales with however many pipelines are added.&lt;/p&gt;

&lt;p&gt;Our users, the machine learning practitioners, also find it more productive as they no longer need to worry about GitLab yaml files.&lt;/p&gt;

&lt;h2 id=&quot;learnings-and-conclusion&quot;&gt;Learnings and conclusion&lt;/h2&gt;

&lt;p&gt;With some creativity, and the flexibility of GitLab Child Pipelines, we were able to invest some engineering effort into making the configuration re-usable, adhering to &lt;a href=&quot;https://en.wikipedia.org/wiki/Don%27t_repeat_yourself&quot;&gt;DRY&lt;/a&gt; principles.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to the Cauldron ML Platform team.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;We might open source our solution.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html&quot;&gt;Parent-child pipelines&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab/-/issues/207270#:~:text=GitLab%20Next&amp;amp;text=Please%20increase%20the%20maximum%20number,methods%20for%20managing%20CICD%20configurations&quot;&gt;Backend: The gitlab-ci.yml is limited to 100 includes&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Apr 2022 15:55:55 +0000</pubDate>
        <link>https://engineering.grab.com/how-we-reduced-our-ci-yaml</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-we-reduced-our-ci-yaml</guid>
        
        <category>CI</category>
        
        <category>Machine Learning</category>
        
        <category>Pipelines</category>
        
        <category>Continuous Integration</category>
        
        <category>Continuous Delivery</category>
        
        <category>Optimisation</category>
        
        <category>Rust</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How Kafka Connect helps move data seamlessly</title>
        <description>&lt;p&gt;Grab’s real-time data platform team a.k.a. Coban has written about &lt;a href=&quot;https://engineering.grab.com/plumbing-at-scale&quot;&gt;Plumbing at scale&lt;/a&gt;, &lt;a href=&quot;https://engineering.grab.com/optimally-scaling-kafka-consumer-applications&quot;&gt;Optimally scaling Kakfa consumer applications&lt;/a&gt;, and &lt;a href=&quot;https://engineering.grab.com/exposing-kafka-cluster&quot;&gt;Exposing Kafka via VPCE&lt;/a&gt;. In this article, we will cover the importance of being able to easily move data in and out of Kafka in a low-code way and how we achieved this with Kafka Connect.&lt;/p&gt;

&lt;p&gt;To build a &lt;a href=&quot;https://www.cio.com/article/220351/what-is-noops-the-quest-for-fully-automated-it-operations.html&quot;&gt;NoOps&lt;/a&gt; managed streaming platform in Grab, the Coban team has:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Engineered an ecosystem on top of Apache Kafka.&lt;/li&gt;
  &lt;li&gt;Successfully adopted it to production for both transactional and analytical use cases.&lt;/li&gt;
  &lt;li&gt;Made it a battle-tested industrial-standard platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In 2021, the Coban team embarked on a new journey (Kafka Connect) that enables and empowers Grabbers to move data in and out of Apache Kafka seamlessly and conveniently.&lt;/p&gt;

&lt;h2 id=&quot;kafka-connect-stack-in-grab&quot;&gt;Kafka Connect stack in Grab&lt;/h2&gt;

&lt;p&gt;This is what Coban’s Kafka Connect stack looks like today. Multiple data sources and data sinks, such as MySQL, S3 and Azure Data Explorer, have already been supported and productionised.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image4.png&quot; alt=&quot;Kafka Connect stack in Grab&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The Coban team has been using Protobuf as the serialisation-deserialisation (SerDes) format in Kafka. Therefore, the role of Confluent schema registry (shown at the top of the figure) is crucial to the Kafka Connect ecosystem, as it serves as the building block for conversions such as Protobuf-to-Avro, Protobuf-to-JSON and Protobuf-to-Parquet.&lt;/p&gt;

&lt;h2 id=&quot;what-problems-are-we-trying-to-solve&quot;&gt;What problems are we trying to solve?&lt;/h2&gt;

&lt;h3 id=&quot;problem-1-change-data-capture-cdc&quot;&gt;Problem 1: Change Data Capture (CDC)&lt;/h3&gt;

&lt;p&gt;In a big organisation like Grab, we handle large volumes of data and changes across many services on a daily basis, so it is important for these changes to be reflected in real time.&lt;/p&gt;

&lt;p&gt;In addition, there are other technical challenges to be addressed:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As shown in the figure below, data is written twice in the code base - once into the database (DB) and once as a message into Kafka. In order for the data in the DB and Kafka to be consistent, the two writes have to be atomic in a two-phase commit protocol (or other atomic commitment protocols), which is non-trivial and impacts availability.&lt;/li&gt;
  &lt;li&gt;Some use cases require data both before and after a change.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image5.png&quot; alt=&quot;Change Data Capture flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;problem-2-message-mirroring-for-disaster-recovery&quot;&gt;Problem 2: Message mirroring for disaster recovery&lt;/h3&gt;

&lt;p&gt;The Coban team has done some research on Kafka MirrorMaker, an open-source solution. While it can ensure better data consistency, it takes significant effort to adopt it onto existing Kubernetes infrastructure hosted by the Coban team and achieve high availability.&lt;/p&gt;

&lt;p&gt;Another major challenge that the Coban team faces is offset mirroring and translation, which is a known challenge in Kafka communities. In order for Kafka consumers to seamlessly resume their work with a backup Kafka after a disaster, we need to cater for &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-545%3A+support+automated+consumer+offset+sync+across+clusters+in+MM+2.0#:~:text=The%20offset%20translation%20is%20great,off%20at%20the%20primary%20cluster%2C&quot;&gt;offset translation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;data-ingestion-into-azure-event-hubs&quot;&gt;Data ingestion into Azure Event Hubs&lt;/h3&gt;

&lt;p&gt;Azure Event Hubs has a Kafka-compatible interface and natively supports JSON and Avro schema. The Coban team uses Protobuf as the SerDes framework, which is not supported by Azure Event Hubs. It means that conversions have to be done for message ingestion into Azure Event Hubs.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;To tackle these problems, the Coban team has picked Kafka Connect because:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It is an open-source framework with a relatively big community that we can consult if we run into issues.&lt;/li&gt;
  &lt;li&gt;It has the ability to plug in transformations and custom conversion logic.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let us see how Kafka Connect can be used to resolve the previously mentioned problems.&lt;/p&gt;

&lt;h3 id=&quot;kafka-connect-with-debezium-connectors&quot;&gt;Kafka Connect with Debezium connectors&lt;/h3&gt;

&lt;p&gt;Debezium is a framework built for capturing data changes on top of Apache Kafka and the Kafka Connect framework. It provides a series of connectors for various databases, such as MySQL, MongoDB and Cassandra.&lt;/p&gt;

&lt;p&gt;Here are the benefits of MySQL &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binlog&lt;/code&gt; streams:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They not only provide changes on data, but also give snapshots of data before and after a specific change.&lt;/li&gt;
  &lt;li&gt;Some producers no longer have to push a message to Kafka after writing a row to a MySQL database. With Debezium connectors, services can choose not to deal with Kafka and only handle MySQL data stores.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;architecture&quot;&gt;Architecture&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image6.png&quot; alt=&quot;Kafka Connect architecture&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;in-case-of-db-upgrades-and-outages&quot;&gt;In case of DB upgrades and outages&lt;/h4&gt;

&lt;p&gt;DB Data Definition Language (DDL) changes, migrations, splits and outages are common in database operations, and each operation type has a systematic resolution.&lt;/p&gt;

&lt;p&gt;The Debezium connector has built-in features to handle DDL changes made by DB migration tools, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pt-online-schema-change&lt;/code&gt;, which is used by the Grab DB Ops team.&lt;/p&gt;

&lt;p&gt;To deal with MySQL instance changes and database splits, the Coban team leverages on the Kafka Connect framework’s ability to change the offsets of connectors. By changing the offsets, Debezium connectors can properly function after DB migrations and resume binlog synchronisation from any position in any binlog file on a MySQL instance.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image1.png&quot; alt=&quot;Database upgrades and outages&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Refer to the &lt;a href=&quot;https://debezium.io/documentation/faq/%23how_to_change_the_offsets_of_the_source_database&quot;&gt;Debezium documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h4 id=&quot;success-stories&quot;&gt;Success stories&lt;/h4&gt;

&lt;p&gt;The CDC project on MySQL via Debezium connectors has been greatly successful in Grab. One of the biggest examples is its adoption in the Elasticsearch optimisation carried out by GrabFood, which has been published in &lt;a href=&quot;https://engineering.grab.com/search-indexing-optimisation&quot;&gt;another blog&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mirrormaker2-with-offset-translation&quot;&gt;MirrorMaker2 with offset translation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/tree/trunk/connect/mirror&quot;&gt;Kafka MirrorMaker2&lt;/a&gt; (MM2), developed in and shipped together with the Apache Kafka project, is a utility to mirror messages and consumer offsets. However, in the Coban team, the MM2 stack is deployed on the Kafka Connect framework per connector because:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A few Kafka Connect clusters have already been provisioned.&lt;/li&gt;
  &lt;li&gt;Compared to launching three connectors bundled in MM2, Coban can have finer controls on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MirrorSourceConnector&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MirrorCheckpointConnector&lt;/code&gt;, and manage both of them in an infrastructure-as-code way via Hashicorp Terraform.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image7.png&quot; alt=&quot;MirrorMaker2 flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;success-stories-1&quot;&gt;Success stories&lt;/h4&gt;

&lt;p&gt;Ensuring business continuity is a key priority for Grab and this includes the ability to recover from incidents quickly. In 2021H2, there was a campaign that ran across many teams to examine the readiness and robustness of various services and middlewares. Coban’s Kafka is one of these services that proved to be robust after rounds of chaos engineering. With MM2 on Kafka Connect to mirror both messages and consumer offsets, critical services and pipelines could safely be replicated and launched across AWS regions if outages occur.&lt;/p&gt;

&lt;p&gt;Because the Coban team has proven itself as the battle-tested Kafka service provider in Grab, other teams have also requested to migrate streams from self-managed Kafka clusters to ones managed by Coban. MM2 has been used in such migrations and brought zero downtime to the streams’ producers and consumers.&lt;/p&gt;

&lt;h3 id=&quot;mirror-to-azure-event-hubs-with-an-in-house-converter&quot;&gt;Mirror to Azure Event Hubs with an in-house converter&lt;/h3&gt;

&lt;p&gt;The Analytics team runs some real time ingestion and analytics projects on Azure. To support this cross-cloud use case, the Coban team has adopted MM2 for message mirroring to Azure Event Hubs.&lt;/p&gt;

&lt;p&gt;Typically, Event Hubs only accept JSON and Avro bytes, which is incompatible with the existing SerDes framework. The Coban team has developed a custom converter that converts bytes serialised in Protobuf to JSON bytes at runtime.&lt;/p&gt;

&lt;p&gt;These steps explain how the converter works:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Deserialise bytes in Kafka to a Protobuf &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DynamicMessage&lt;/code&gt; according to a schema retrieved from the Confluent™ schema registry.&lt;/li&gt;
  &lt;li&gt;Perform a recursive post-order depth-first-search on each field descriptor in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DynamicMessage&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Convert every Protobuf field descriptor to a JSON node.&lt;/li&gt;
  &lt;li&gt;Serialise the root JSON node to bytes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The converter has not been open sourced yet.&lt;/p&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image2.png&quot; alt=&quot;Deployment&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Docker containers are the Coban team’s preferred infrastructure, especially since some production Kafka clusters are already deployed on Kubernetes. The long-term goal is to provide Kafka in a software-as-a-service (SaaS) model, which is why Kubernetes was picked. The diagram below illustrates how Kafka Connect clusters are built and deployed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-connect/image3.png&quot; alt=&quot;Terraform for connectors&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The Coban team is iterating on a unified control plane to manage resources like Kafka topics, clusters and Kafka Connect. In the foreseeable future, internal users should be able to provision Kafka Connect connectors via RESTful APIs and a graphical user interface (GUI).&lt;/p&gt;

&lt;p&gt;At the same time, the Coban team is closely working with the Data Engineering team to make Kafka Connect the preferred tool in Grab for moving data in and out of external storages (S3 and Apache Hudi).&lt;/p&gt;

&lt;h2 id=&quot;coban-is-hiring&quot;&gt;Coban is hiring!&lt;/h2&gt;
&lt;p&gt;The Coban (Real-time Data Platform) team at Grab in Singapore is hiring software and site reliability engineers at all levels as we double down on growing our platform capabilities.&lt;/p&gt;

&lt;p&gt;Join us in building state-of-the-art, mission critical, TB/hour scale data platforms that enable thousands of engineers, data scientists, and analysts to serve millions of consumers, businesses, and partners across Southeast Asia!&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.
Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Apr 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/kafka-connect</link>
        <guid isPermaLink="true">https://engineering.grab.com/kafka-connect</guid>
        
        <category>Kafka</category>
        
        <category>Data processing</category>
        
        <category>Real-Time</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
