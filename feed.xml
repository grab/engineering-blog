<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 22 Dec 2020 13:55:24 +0000</pubDate>
    <lastBuildDate>Tue, 22 Dec 2020 13:55:24 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Pharos - Searching Nearby Drivers on Road Network at Scale</title>
        <description>&lt;p&gt;Have you ever wondered what happens when you click on the book button when arranging a ride home? Actually, many things happen behind this simple action and it would take days and nights to talk about all of them. Perhaps, we should rephrase this question to be more precise. ¬†So, let‚Äôs try again - have you ever thought about how Grab stores and uses driver locations to allocate a driver to you? If so, you will surely find this blog post interesting as we cover how it all works in the backend.&lt;/p&gt;

&lt;h2 id=&quot;what-problems-are-we-going-to-solve&quot;&gt;What problems are we going to solve?&lt;/h2&gt;

&lt;p&gt;One of the fundamental problems of the ride-hailing and delivery industry is to locate the nearest moving drivers in real-time. There are two challenges from serving this request in real time.&lt;/p&gt;

&lt;h3 id=&quot;fast-moving-vehicles&quot;&gt;Fast-moving vehicles&lt;/h3&gt;

&lt;p&gt;Vehicles are constantly moving and sometimes the drivers go at the speed of over 20 meters per second. As shown in Figure 1a and Figure 1b, the two nearest drivers to the pick-up point (blue dot) change as time passes. To provide a high-quality allocation service, it is important to constantly track the objects and update object locations at high frequency (e.g. per second).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/fast-moving-drivers.png&quot; alt=&quot;Figure 1: Fast-moving drivers&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1: Fast-moving drivers&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;routing-distance-calculation&quot;&gt;Routing distance calculation&lt;/h3&gt;

&lt;p&gt;To satisfy business requirements, K nearest objects need to be calculated based on the routing distance instead of straight-line distance. Due to the complexity of the road network, the driver with the shortest straight-line distance may not be the optimal driver as it could reach the pick-up point with a longer routing distance due to detour.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image1.png&quot; alt=&quot;Figure 2: Straight line vs routing&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2: Straight line vs routing&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;As shown in Figure 2, the driver at the top is deemed as the nearest one to pick-up point by straight line distance. However, the driver at the bottom should be the true nearest driver by routing distance. Moreover, routing distance helps to infer the estimated time of arrival (ETA), which is an important factor for allocation, as shorter ETA reduces passenger waiting time thus reducing order cancellation rate and improving order completion rate.&lt;/p&gt;

&lt;p&gt;Searching for the K nearest drivers with respect to a given POI is a well studied topic for all ride-hailing companies, which can be treated as a &lt;em&gt;K Nearest Neighbour (KNN) problem&lt;/em&gt;. Our predecessor, Sextant, searches nearby drivers with the &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;haversine&lt;/a&gt;&lt;/em&gt; distance from driver locations to the pick-up point. By partitioning the region into grids and storing them in a distributed manner, Sextant can handle large volumes of requests with low latency. However, nearest drivers found by the haversine distance may incur long driving distance and ETA as illustrated in Figure 2. For more information about Sextant, kindly refer to the paper, &lt;em&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/8788742&quot;&gt;Sextant: Grab‚Äôs Scalable In-Memory Spatial Data Store for Real-Time K-Nearest Neighbour Search&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To better address the challenges mentioned above, we present the next-generation solution, &lt;strong&gt;Pharos&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image7.png&quot; alt=&quot;Figure 3: Lighthouse of Alexandria&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3: Lighthouse of Alexandria&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-pharos&quot;&gt;What is Pharos?&lt;/h2&gt;

&lt;p&gt;Pharos means lighthouse in Greek. At Grab, it is a scalable in-memory solution that supports large-volume, real-time K nearest search by driving distance or ETA with high object update frequency.&lt;/p&gt;

&lt;p&gt;In Pharos, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenStreetMap&quot;&gt;OpenStreetMap&lt;/a&gt; (OSM) graphs to represent road networks. To support hyper-localized business requirements, the graph is partitioned by cities and verticals (e.g. the road network for a four-wheel vehicle is definitely different compared to a motorbike or a pedestrian). We denote this partition key as &lt;em&gt;map ID&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Pharos loads the graph partitions at service start and stores drivers‚Äô spatial data in memory in a distributed manner to alleviate the scalability issue when the graph or the number of drivers grows. These data are distributed into multiple instances (i.e. machines) with replicas for high stability. Pharos exploits &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2016/papers/leis-icde2013.pdf&quot;&gt;Adaptive Radix Trees&lt;/a&gt; (ART) to store objects‚Äô locations along with their metadata.&lt;/p&gt;

&lt;p&gt;To answer the KNN query by routing distance or ETA, Pharos uses &lt;a href=&quot;http://www.vldb.org/pvldb/vol9/p492-abeywickrama.pdf&quot;&gt;Incremental Network Expansion&lt;/a&gt; (INE) starting from the road segment of the query point. During the expansion, drivers stored along the road segments are incrementally retrieved as candidates and put into the results. As the expansion actually generates an isochrone map, it can be terminated by reaching a predefined radius of distance or ETA, or even simply a maximum number of candidates.&lt;/p&gt;

&lt;p&gt;Now that you have an ¬†overview of Pharos, we would like to go into the design details of it, starting with its architecture.&lt;/p&gt;

&lt;h3 id=&quot;pharos-architecture&quot;&gt;Pharos architecture&lt;/h3&gt;

&lt;p&gt;As a microservice, Pharos receives requests from the upstream, performs corresponding actions and then returns the result back. As shown in Figure 4, the Pharos architecture can be broken down into three layers: &lt;em&gt;Proxy&lt;/em&gt;, &lt;em&gt;Node&lt;/em&gt;, and &lt;em&gt;Model&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Proxy layer&lt;/strong&gt;. This layer helps to pass down the request to the right node, especially when the Node is on another machine.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Node layer&lt;/strong&gt;. This layer stores the index of map IDs to models and distributes the request to the right model for execution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model layer&lt;/strong&gt;. This layer is, where the business logic is implemented, executes the operations and returns the result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a distributed in-memory driver storage, Pharos is designed to handle load balancing, fault tolerance, and fast recovery.&lt;/p&gt;

&lt;p&gt;Taking Figure 4 as an example, Pharos consists of three instances. Each individual instance is able to handle any request from the upstream. Whenever there is a request coming from the upstream, it is distributed into one of the three instances, which achieves the purpose of load balancing.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image2.png&quot; alt=&quot;Figure 4: Pharos architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4: Pharos architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In Pharos, each model has two replicas and they are stored on different instances and different availability zones. If one instance is down, the other two instances are still up for service. The fault tolerance module in Pharos automatically detects the reduction of replicas and creates new instances to load graphs and build the models of missing replicas. This proves the reliability of Pharos even under extreme situations.&lt;/p&gt;

&lt;p&gt;With the architecture of Pharos in mind, let‚Äôs take a look at how it stores driver information.&lt;/p&gt;

&lt;h3 id=&quot;driver-storage&quot;&gt;Driver storage&lt;/h3&gt;

&lt;p&gt;Pharos acts as a driver storage, and rather than being an external storage, it adopts in-memory storage which is faster and more adequate to handle frequent driver position updates and retrieve driver locations for nearby driver queries. Without loss of generality, drivers are assumed to be located on the vertices, i.e. &lt;a href=&quot;https://github.com/Project-OSRM/osrm-backend/wiki/Graph-representation&quot;&gt;Edge Based Nodes&lt;/a&gt; (EBN) of an edge-based graph.&lt;/p&gt;

&lt;p&gt;Model is in charge of the driver storage in Pharos. Driver objects are passed down from upper layers to the model layer for storage. Each driver object contains several fields such as driver ID and metadata, containing the driver‚Äôs business related information e.g. driver status and particular allocation preferences.&lt;/p&gt;

&lt;p&gt;There is also a &lt;em&gt;Latitude and Longitude (LatLon) pair&lt;/em&gt; contained in the object, which indicates the driver‚Äôs current location. Very often, this LatLon pair sent from the driver is off the road (not on any existing road). The computation of routing distance between the query point and drivers is based on the road network. Thus, we need to infer which road segment (EBN) the driver is most probably on.&lt;/p&gt;

&lt;p&gt;To convert a LatLon pair to an exact location on a road is called &lt;strong&gt;Snapping&lt;/strong&gt;. Model begins with finding EBNs which are close to the driver‚Äôs location. After that, as illustrated in Figure 5, the driver‚Äôs location is projected to those EBNs, by drawing perpendicular lines from the location to the EBNs. The projected point is denoted as a &lt;strong&gt;phantom node&lt;/strong&gt;. As the name suggests, these nodes do not exist in the graph. They are merely memory representations of the snapped driver.&lt;/p&gt;

&lt;p&gt;Each phantom node contains information about its projected location such as the ID of EBN it is projected to, projected LatLon and projection ratio, etc. Snapping returns a list of phantom nodes ordered by the haversine distance from the driver‚Äôs LatLon to the phantom node in ascending order. The nearest phantom node is bound with the original driver object to provide information about the driver‚Äôs snapped location.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image12.png&quot; alt=&quot;Figure 5: Snapping and phantom nodes&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5: Snapping and phantom nodes&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To efficiently index drivers from the graph, Pharos uses ART for driver storage. Two ARTs are maintained by each model: &lt;em&gt;Driver ART&lt;/em&gt; and &lt;em&gt;EBN ART&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Driver ART&lt;/strong&gt; is used to store the index of driver IDs to corresponding driver objects, while &lt;strong&gt;EBN ART&lt;/strong&gt; is used to store the index of EBN IDs to the root of an ART, which stores the drivers on that EBN.&lt;/p&gt;

&lt;p&gt;Bi-directional indexing between EBNs and drivers are built because an efficient retrieval from driver to EBN is needed as driver locations are constantly updated. In practice, as index keys, driver IDs, and EBN IDs are both numerical. ART has a better throughput for dense keys (e.g. numerical keys) in contrast to sparse keys such as alphabetical keys, and when compared to other in-memory look-up tables (e.g. hash table). It also incurs less memory than other tree-based methods.&lt;/p&gt;

&lt;p&gt;Figure 6 gives an example of driver ART assuming that the driver ID only has three digits.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image9.png&quot; alt=&quot;Figure 6: Driver ART&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6: Driver ART&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;After snapping, this new driver object is wrapped into an update task for execution. During execution, the model firstly checks if this driver already exists using its driver ID. If it does not exist, the model directly adds it to driver ART and EBN ART. If the driver already exists, the new driver object replaces the old driver object on driver ART. For EBN ART, the old driver object on the previous EBN needs to be deleted first before adding the new driver object to the current EBN.&lt;/p&gt;

&lt;p&gt;Every insertion or deletion modifies both ARTs, which might cause changes to roots. The model only stores the roots of ARTs, and in order to prevent race conditions, a lock is used to prevent other read or write operations to access the ARTs while changing the ART roots.&lt;/p&gt;

&lt;p&gt;Whenever a driver nearby request comes in, it needs to get a snapshot of driver storage, i.e. the roots of two ARTs. A simple example (Figure 7a and 7b) is used to explain how synchronization is achieved during concurrent driver update and nearby requests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/art-synchronization.png&quot; alt=&quot;Figure 7: How ARTs change roots for synchronization&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7: How ARTs change roots for synchronization&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Currently there are two drivers A and B stored and these two drivers reside on the same EBN. When there is a nearby request, the current roots of the two ARTs are returned. When processing this nearby request, there could be driver updates coming and modifying the ARTs, e.g. a new root is resulted due to update of driver C. This driver update has no impact on ongoing driver nearby requests as they are using different roots. Subsequent nearby requests will use the new ART roots to find the nearby drivers. Once the current roots are not used by any nearby request, these roots and their child nodes are ready to be garbage collected.&lt;/p&gt;

&lt;p&gt;Pharos does not delete drivers actively. A deletion of expired drivers is carried out every midnight by populating two new ARTs with the same driver update requests for a duration of driver‚Äôs &lt;em&gt;Time To Live (TTL)&lt;/em&gt;, and then doing a switch of the roots at the end. Drivers with expired TTLs are not referenced and they are ready to be garbage collected. In this way, expired drivers are removed from the driver storage.&lt;/p&gt;

&lt;h3 id=&quot;driver-update-and-nearby&quot;&gt;Driver update and nearby&lt;/h3&gt;

&lt;p&gt;Pharos mainly has two external endpoints: &lt;em&gt;Driver Update&lt;/em&gt; and &lt;em&gt;Driver Nearby&lt;/em&gt;. The following describes how the business logic is implemented in these two operations.&lt;/p&gt;

&lt;h4 id=&quot;driver-update&quot;&gt;Driver update&lt;/h4&gt;

&lt;p&gt;Figure 8 demonstrates the life cycle of a driver update request from upstream. Driver update requests from upstream are distributed to each proxy by a load balancer. The chosen proxy firstly constructs a driver object from the request body.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;RouteTable&lt;/em&gt;, a structure in proxy, stores the index between map IDs and replica addresses. Proxy then uses map ID in the request as the key to check its RouteTable and gets the IP addresses of all the instances containing the model of that map ID.&lt;/p&gt;

&lt;p&gt;Then, proxy forwards the update to other replicas that reside in other instances. Those instances, upon receiving the message, know that the update is forwarded from another proxy. Hence they directly pass down the driver object to the node.&lt;/p&gt;

&lt;p&gt;After receiving the driver object, Node sends it to the right model by checking the index between map ID and model. The remaining part of the update flow is the same as described in Driver Storage. Sometimes the driver updates to replicas are not successful, e.g. request lost or model does not exist, Pharos will not react to such kinds of scenarios.&lt;/p&gt;

&lt;p&gt;It can be observed that data storage in Pharos does not guarantee strong consistency. In practice, Pharos favors high throughput over strong consistency of KNN query results as the update frequency is high and slight inconsistency does not affect allocation performance significantly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image6.png&quot; alt=&quot;Figure 8: Driver update flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 8: Driver update flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;driver-nearby&quot;&gt;Driver nearby&lt;/h4&gt;

&lt;p&gt;Similar to driver update, after a driver nearby request comes from the upstream, it is distributed to one of the machines by the load balancer. In a nearby request, a set of filter parameters is used to match with driver metadata in order to support KNN queries with various business requirements. Note that driver metadata also carries an update timestamp. During the nearby search, drivers with an expired timestamp are filtered.&lt;/p&gt;

&lt;p&gt;As illustrated in Figure 9, upon receiving the nearby request, a nearby object is built and passed to the proxy layer. The proxy first checks RouteTable by map ID to see if this request can be served on the current instance. If so, the nearby object is passed to the Node layer. Otherwise, this nearby request needs to be forwarded to the instances that contain this map ID.&lt;/p&gt;

&lt;p&gt;In this situation, a round-robin fashion is applied to select the right instance for load balancing. After receiving the request, the proxy of the chosen instance directly passes the nearby object to the node. Once the node layer receives the nearby object, it looks for the right model using the map ID as key. Eventually, the nearby object goes to the model layer where K-nearest-driver computation takes place. Model snaps the location of the request to some phantom nodes as described previously - these nodes are used as start nodes for expansion later.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image5.png&quot; alt=&quot;Figure 9: Driver nearby flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 9: Driver nearby flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;k-nearest-driver-search&quot;&gt;K nearest driver search&lt;/h4&gt;

&lt;p&gt;Starting from the phantom nodes found in the &lt;em&gt;Driver Nearby&lt;/em&gt; flow, the K nearest driver search begins. Two priority queues are used during the search: &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; is used to keep track of the nearby EBNs, while &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; keeps track of drivers found during expansion by their driving distance to the query point.&lt;/p&gt;

&lt;p&gt;At first, a snapshot of the current driver storage is taken (using roots of current ARTs) and it shows the driver locations on the road network at the time when the nearby request comes in. From each start node, the parent EBN is found and drivers on these EBNs are appended to driverPQ. After that, KNN search expands to adjacent EBNs and appends these EBNs to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. After iterating all start nodes, there will be some initial drivers in driverPQ and adjacent EBNs waiting to be expanded in &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Each time the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;, drivers located on this EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. After that, the closest driver is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. If the driver satisfies all filtering requirements, it is appended to the array of qualified drivers. This step repeats until driverPQ becomes empty. During this process, if the size of qualified drivers reaches the maximum driver limit, the KNN search stops right away and qualified drivers are returned.&lt;/p&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; becomes empty, adjacent EBNs of the current one are to be expanded and those within the predefined range, e.g. three kilometers, are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. Then the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; and drivers on that EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; again. The whole process continues until &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; becomes empty. The driver array is returned as the result of the nearby query.&lt;/p&gt;

&lt;p&gt;Figure 10 shows the pseudo code of this KNN algorithm.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image8.png&quot; alt=&quot;Figure 10: KNN search algorithm&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 10: KNN search algorithm&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What‚Äôs next?&lt;/h2&gt;

&lt;p&gt;Currently, Pharos is running on the production environment, where it handles requests with &lt;strong&gt;P99 latency time of 10ms for driver update&lt;/strong&gt; and &lt;strong&gt;50ms for driver nearby&lt;/strong&gt;, respectively. Even though the performance of Pharos is quite satisfying, we still see some potential areas of improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pharos uses ART for driver storage. Even though ART proves its ability to handle large volumes of driver update and driver nearby requests, the write operations (driver update) are not carried out in parallel. Hence, we plan to explore other data structures that can achieve high concurrency of read and write, eg. concurrent hash table.&lt;/li&gt;
  &lt;li&gt;Pharos uses OSM &lt;a href=&quot;https://www.google.com/url?q=https://i11www.iti.kit.edu/_media/teaching/theses/ba-hamme-13.pdf&amp;amp;sa=D&amp;amp;ust=1608637926094000&amp;amp;usg=AOvVaw0hO37L1Wqh_C423C51e3S8&quot;&gt;Multi-level Dijkstra&lt;/a&gt; (MLD) graphs to find K nearest drivers. As the predefined range of nearby driver search is often a few kilometers, Pharos does not make use of MLD partitions or support long distance query. Thus, we are interested in exploiting MLD graph partitions to enable Pharos to support long distance query.&lt;/li&gt;
  &lt;li&gt;In Pharos, maps are partitioned by cities and we assume that drivers of a city operate within that city. When finding the nearby drivers, Pharos only allocates drivers of that city to the passenger. Hence, in the future, we want to enable Pharos to support cross city allocation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope this blog helps you to have a closer look at how we store driver locations and how we use these locations to find nearby drivers around you.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;
&lt;p&gt;We would like to thank Chunda Ding, Zerun Dong, and Jiang Liu for their contributions to the distributed layer used in Pharos. Their efforts make Pharos reliable and fault tolerant.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3, Lighthouse of Alexandria is taken from &lt;a href=&quot;https://www.britannica.com/topic/lighthouse-of-Alexandria%23/media/1/455210/187239&quot;&gt;https://www.britannica.com/topic/lighthouse-of-Alexandria#/media/1/455210/187239&lt;/a&gt; authored by Sergey Kamshylin.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5, Snapping and Phantom Nodes, is created by Minbo Qiu. We would like to thank him for the insightful elaboration of the snapping mechanism.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cover Photo by Kevin Huang on Unsplash&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Dec 2020 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</guid>
        
        <category>Real-Time K Nearest Neighbour Search</category>
        
        <category>Spatial Data Store</category>
        
        <category>Distributed System</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Reflecting on the five years of Bug Bounty at Grab</title>
        <description>&lt;p&gt;Security has always been a top-priority at Grab; our product security team works round-the-clock to ensure that our customers‚Äô data remains safe. Five years ago, we launched our private bug bounty program on &lt;a href=&quot;https://hackerone.com/grab&quot;&gt;HackerOne&lt;/a&gt;, which evolved into a public program in August 2017. The idea was to complement the security efforts our team has been putting through to keep Grab secure. We were a pioneer in South East Asia to implement a public bug bounty program, and now we stand among the &lt;a href=&quot;https://www.hackerone.com/resources/e-book/top-20-public-bug-bounty-programs&quot;&gt;Top 20 programs on HackerOne&lt;/a&gt; worldwide.&lt;/p&gt;

&lt;p&gt;We started as a private bug bounty program which provided us with fantastic results, thus encouraging us to increase our reach and benefit from the vibrant security community across the globe which have helped us iron-out security issues 24x7 in our products and infrastructure. We then publicly launched our bug bounty program offering competitive rewards and hackers can even earn additional bonuses if their report is well-written and display an innovative approach to testing.&lt;/p&gt;

&lt;p&gt;In 2019, we also enrolled ourselves in the &lt;a href=&quot;https://hackerone.com/googleplay&quot;&gt;Google Play Security Reward Program (GPSRP)&lt;/a&gt;, Offered by Google Play, GPSRP allows researchers to re-submit their resolved mobile security issues directly and get additional bounties if the report qualifies under the GPSRP rules. A selected number of Android applications are eligible, including Grab‚Äôs Android mobile application. Through the participation in GPSP, we hope to give researchers the recognition they deserve for their efforts.&lt;/p&gt;

&lt;p&gt;In this blog post, we‚Äôre going to share our journey of running a bug bounty program, challenges involved and share the learnings we had on the way to help other companies in SEA and beyond to establish and build a successful bug bounty program.&lt;/p&gt;

&lt;h2 id=&quot;transitioning-from-private-to-a-public-program&quot;&gt;Transitioning from Private to a Public Program&lt;/h2&gt;

&lt;p&gt;At Grab, before starting the private program, we defined &lt;a href=&quot;https://docs.hackerone.com/programs/policy-and-scope.html&quot;&gt;policy and scope&lt;/a&gt;, allowing us to communicate the objectives of our bug bounty program and list the targets that can be tested for security issues. We did a security sweep of the targets to eliminate low-hanging security issues, assigned people from the security team to take care of incoming reports, and then launched the program in private mode on HackerOne with a few chosen researchers having demonstrated a history of submitting quality submissions.&lt;/p&gt;

&lt;p&gt;One of the benefits of running a &lt;a href=&quot;https://docs.hackerone.com/programs/private-vs-public-programs.html&quot;&gt;private bug bounty program&lt;/a&gt; is to have some control over the number of incoming submissions of potential security issues and researchers who can report issues. This ensures the quality of submissions and helps to control the volume of bug reports, thus avoiding overwhelming a possibly small security team with a deluge of issues so that they won‚Äôt be overwhelming for the people triaging potential security issues. The invited researchers to the program are limited, and it is possible to invite researchers with a known track record or with a specific skill set, further working in the program‚Äôs favour.&lt;/p&gt;

&lt;p&gt;The results and lessons from our private program were valuable, making our program and processes mature enough to &lt;a href=&quot;https://www.techinasia.com/grab-public-bug-bounty&quot;&gt;open the bug bounty program&lt;/a&gt; to security researchers across the world. We still did another security sweep, reworded the policy, redefined the targets by expanding the scope, and allocated enough folks from our security team to take on the initial inflow of reports which was anticipated to be in tune with other public programs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reflecting-on-the-five-years-of-bug-bounty-at-grab/image1.png&quot; alt=&quot;Submissions&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Noticeable spike in the number of incoming reports as we went public in July 2017.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned-from-the-public-program&quot;&gt;Lessons Learned from the Public Program&lt;/h2&gt;

&lt;p&gt;Although we were running our bug bounty program in private for sometime before going public, we still had not worked much on building standard operating procedures and processes for managing our bug bounty program up until early 2018. Listed below, are our key takeaways from 2018 till July 2020 in terms of improvements, challenges, and other insights.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Response Time&lt;/strong&gt;: No researcher wants to work with a bug bounty team that doesn‚Äôt respect the time that they are putting into reporting bugs to the program. We initially didn‚Äôt have a formal process around response times, because we wanted to encourage all security engineers to pick-up reports. Still, we have been consistently delivering a first response to reports in a matter of hours, which is significantly lower than the top 20 bug bounty programs running on HackerOne. Know what structured (or unstructured) processes work for your team in this area, because your program can see significant rewards from fast response times.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time to Bounty&lt;/strong&gt;: In most bug bounty programs the payout for a bug is made in one of the following ways: full payment after the bug has been resolved, full payment after the bug has been triaged, or paying a portion of the bounty after triage and the remaining after resolution. We opt to pay the full bounty after triage. While we‚Äôre always working to speed up resolution times, that timeline is in our hands, not the researcher‚Äôs. Instead of making them wait, we pay them as soon as impact is determined to incentivize long-term engagement in the program.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noise Reduction&lt;/strong&gt;: With &lt;a href=&quot;https://www.hackerone.com/services&quot;&gt;HackerOne Triage&lt;/a&gt; and &lt;a href=&quot;https://www.hackerone.com/blog/Double-your-signal-double-your-fun&quot;&gt;Human-Augmented Signal&lt;/a&gt;, we‚Äôre able to focus our team‚Äôs efforts on resolving unique, valid vulnerabilities. Human-Augmented Signal flags any reports that are likely false-positives, and Triage provides a validation layer between our security team and the report inbox. Collaboration with the HackerOne Triage team has been fantastic and ultimately allows us to be more efficient by focusing our energy on valid, actionable reports. In addition, we take significant steps to block traffic coming from networks running automated scans against our Grab infrastructure and we‚Äôre constantly exploring this area to actively prevent automated external scanning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Team Coverage&lt;/strong&gt;: We introduced a team scheduling process, in which we assign a security engineer (chosen during sprint planning) on a weekly basis, whose sole responsibility is to review and respond to bug bounty reports. We have integrated our systems with HackerOne‚Äôs API and PagerDuty to ensure alerts are for valid reports and verified as much as possible.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;In one area we haven‚Äôt been doing too great is ensuring higher rates of participation in our core mobile applications; some of the pain points researchers have informed us about while testing our applications are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Researchers‚Äô accounts are getting blocked due to our &lt;a href=&quot;https://engineering.grab.com/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;anti-fraud checks&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Researchers are not able to register driver accounts (which is understandable as our driver-partners have to go through manual verification process)&lt;/li&gt;
  &lt;li&gt;Researchers who are not residing in the Southeast Asia region are unable to complete end-to-end flows of our applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are open to community feedback and how we can improve. We want to hear from you! Please drop us a note at &lt;a href=&quot;mailto:infosec.bugbounty@grab.com&quot;&gt;infosec.bugbounty@grab.com&lt;/a&gt; for any program suggestions or feedback.&lt;/p&gt;

&lt;p&gt;Last but not least, we‚Äôd like to thank all researchers who have contributed to the Grab program so far. Your immense efforts have helped keep Grab‚Äôs businesses and users safe. Here‚Äôs a shoutout to our program‚Äôs top-earning hackers &lt;a href=&quot;https://emojipedia.org/trophy/%23:~:text%3DThe%2520trophy%2520emoji%2520is%2520a,the%2520bottom%2520detailing%2520the%2520award.%26text%3DTrophy%2520was%2520approved%2520as%2520part,to%2520Emoji%25201.0%2520in%25202015.&quot;&gt;üèÜ&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overall Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/quanyang?type%3Duser&quot;&gt;@quanyang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/ngocdh?type%3Duser&quot;&gt;@ngocdh&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Year 2019/2020 Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/alexeypetrenko?type%3Duser&quot;&gt;@alexeypetrenko&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/chaosbolt?type%3Duser&quot;&gt;@chaosbolt&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lastly, here is a special shoutout to &lt;a href=&quot;https://hackerone.com/bagipro&quot;&gt;@bagipro&lt;/a&gt; who has done some great work and testing on our Grab mobile applications!&lt;/p&gt;

&lt;p&gt;Well done and from everyone on the Grab team, we look forward to seeing you on the program!&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</guid>
        
        <category>Security</category>
        
        <category>HackerOne</category>
        
        <category>Bug Bounty</category>
        
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>How Grab is Blazing Through the Super App Bazel Migration</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we build a seamless user experience that addresses more and more of the daily lifestyle needs of people across South East Asia. We‚Äôre proud of our Grab rides, payments, and delivery services, and want to provide a unified experience across these offerings.&lt;/p&gt;

&lt;p&gt;Here is couple of examples of what Grab does for millions of people across South East Asia every day:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image2.jpg&quot; alt=&quot;Grab Service Offerings&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Grab Service Offerings&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The Grab Passenger application reached super app status more than a year ago and continues to provide hundreds of life-changing use cases in dozens of areas for millions of users.&lt;/p&gt;

&lt;p&gt;With the big product scale, it brings with it even bigger technical challenges. Here are a couple of dimensions that can give you a sense of the scale we‚Äôre working with.&lt;/p&gt;

&lt;h3 id=&quot;engineering-and-product-structure&quot;&gt;Engineering and product structure&lt;/h3&gt;

&lt;p&gt;Technical and product teams work in close collaboration to outserve our customers. These teams are combined into dedicated groups to form Tech Families and focus on similar use cases and areas.&lt;/p&gt;

&lt;p&gt;Grab consists of many Tech Families who work on food, payments, transport, and other services, which are supported by hundreds of engineers. The diverse landscape makes the development process complicated and requires the industry‚Äôs best practices and approaches.&lt;/p&gt;

&lt;h3 id=&quot;codebase-scale-overview&quot;&gt;Codebase scale overview&lt;/h3&gt;

&lt;p&gt;The Passenger Applications (Android and iOS) contain more than &lt;strong&gt;2.5 million lines of code&lt;/strong&gt; each and it keeps growing. We have &lt;strong&gt;1000+ modules&lt;/strong&gt; in the Android App and &lt;strong&gt;700+ targets&lt;/strong&gt; in the iOS App. Hundreds of commits are merged by all the mobile engineers on a daily basis.&lt;/p&gt;

&lt;p&gt;To maintain the health of the codebase and product stability, we run &lt;strong&gt;40K+ unit tests&lt;/strong&gt; on Android and &lt;strong&gt;30K+ unit tests&lt;/strong&gt; on iOS, as well as thousands of UI tests and hundreds of end-to-end tests on both platforms.&lt;/p&gt;

&lt;h2 id=&quot;build-time-challenges&quot;&gt;Build time challenges&lt;/h2&gt;

&lt;p&gt;The described complexity and scale do not come without challenges. A huge codebase propels the build process to the ultimate extreme- challenging the efficiency of build systems and hardware used to compile the super app, and creating out of the line challenges to be addressed.&lt;/p&gt;

&lt;h3 id=&quot;local-build-time&quot;&gt;Local build time&lt;/h3&gt;

&lt;p&gt;Local build time (the build on engineers‚Äô laptop) is one of the most obvious challenges. More code goes in the application binary, hence the build system requires more time to compile it.&lt;/p&gt;

&lt;h4 id=&quot;adr-local-build-time&quot;&gt;ADR local build time&lt;/h4&gt;

&lt;p&gt;The Android ecosystem provides a great out-of-the-box tool to build your project called &lt;em&gt;Gradle&lt;/em&gt;. It‚Äôs flexible and user friendly, and ¬†provides huge capabilities for a reasonable cost. But is this always true? It appears to not be the case due to multiple reasons. Let‚Äôs unpack these reasons below.&lt;/p&gt;

&lt;p&gt;Gradle performs well for medium sized projects with say 1 million line of code. Once the code surpasses that 1 million mark (or so), Gradle starts failing in giving engineers a reasonable build time for the given flexibility. And that‚Äôs exactly what we have observed in our Android application.&lt;/p&gt;

&lt;p&gt;At some point in time, the Android local build became ridiculously long. We even encountered cases ¬†where engineers‚Äô laptops simply failed to build the project due to hardware resources limits. Clean builds took by the hours, and incremental builds easily hit dozens of minutes.&lt;/p&gt;

&lt;h4 id=&quot;ios-local-build-time&quot;&gt;iOS local build time&lt;/h4&gt;

&lt;p&gt;Xcode behaved a bit better compared to Gradle. The Xcode build cache was somehow bearable for incremental builds and didn‚Äôt exceed a couple of minutes. Clean builds still took dozens of minutes though. When Xcode failed to provide the valid cache, engineers had to rerun everything as a clean build, which killed the experience entirely.&lt;/p&gt;

&lt;h3 id=&quot;ci-pipeline-time&quot;&gt;CI pipeline time&lt;/h3&gt;

&lt;p&gt;Each time an engineer submits a Merge Request (MR), our CI kicks in running a wide variety of jobs to ensure the commit is valid and doesn‚Äôt introduce regression to the master branch. The feedback loop time is critical here as well, and the pipeline time tends to skyrocket alongside the code base growth. We found ourselves on the trend where the feedback loop came in by the hours, which again was just breaking the engineering experience, and prevented ¬†us from delivering the world‚Äôs best features to our customers.&lt;/p&gt;

&lt;p&gt;As mentioned, we have a large number of unit tests (30K-40K+) and UI tests (700+) that we run on a pre-merge pipeline. This brings us to hours of execution time before we could actually allow MRs to land to the master branch.&lt;/p&gt;

&lt;p&gt;The number of daily commits, which is by the hundreds, adds another stone to the basket of challenges.&lt;/p&gt;

&lt;p&gt;All this clearly indicated the area of improvement. We were missing opportunities in terms of engineering productivity.&lt;/p&gt;

&lt;h2 id=&quot;the-extra-mile&quot;&gt;The extra mile&lt;/h2&gt;

&lt;p&gt;The biggest question for us to answer was how to put all this scale into a reasonable experience with minimal engineering idle time and fast feedback loop.&lt;/p&gt;

&lt;h3 id=&quot;build-time-critical-path-optimization&quot;&gt;Build time critical path optimization&lt;/h3&gt;

&lt;p&gt;The most reasonable thing to do was to pay attention to the utilization of the hardware resources and make the build process optimal.&lt;/p&gt;

&lt;p&gt;This literally boiled down to the simplest approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Decouple building blocks&lt;/li&gt;
  &lt;li&gt;Make building blocks as small as possible&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This approach is valid for any build system and applies ¬†for both iOS and Android. The first thing we focused on was to understand what our build graph looked like, how dependencies were distributed, and which blocks were bottlenecks.&lt;/p&gt;

&lt;p&gt;Given the scale of the apps, it‚Äôs practically not possible to manage a dependency tree manually, thus we created a tool to help us.&lt;/p&gt;

&lt;h4 id=&quot;critical-path-overview&quot;&gt;Critical path overview&lt;/h4&gt;

&lt;p&gt;We introduced the Critical Path concept:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The critical path is the longest (time) chain of sequential dependencies, which must be built one after the other.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image3.png&quot; alt=&quot;Critical Path&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Critical Path build&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Even with an infinite number of parallel processors/cores, the total build time cannot be less than the critical path time.&lt;/p&gt;

&lt;p&gt;We implemented the tool that parsed the dependency trees (for both Android and iOS), aggregated modules/target build time, and calculated the critical path.&lt;/p&gt;

&lt;p&gt;The concept of the critical path introduced a number of action items, which we prioritized:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The critical path must be as short as possible.&lt;/li&gt;
  &lt;li&gt;Any huge module/target on the critical path must be split into smaller modules/targets.&lt;/li&gt;
  &lt;li&gt;Depend on interfaces/bridges rather than implementations to shorten the critical path.&lt;/li&gt;
  &lt;li&gt;The presence of other teams‚Äô implementation modules/targets in the critical path of the given team is a red flag.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image1.png&quot; alt=&quot;Stack representation of the Critical Path build time&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Stack representation of the Critical Path build time&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;projects-scale-factor&quot;&gt;Project‚Äôs scale factor&lt;/h4&gt;

&lt;p&gt;To implement the conceptually easy action items, we ran a Grab-wide program. The program has impacted almost every mobile team at Grab and involved &lt;strong&gt;200+ engineers&lt;/strong&gt; to some degree. The whole implementation took 6 months to complete.&lt;/p&gt;

&lt;p&gt;During this period of time, we assigned engineers who were responsible to review the changes, provide support to the engineers across Grab, and monitor the results.&lt;/p&gt;

&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;

&lt;p&gt;Even though the overall plan seemed to be good on paper, the results were minimal - it just flattened the build time curve of the upcoming trend introduced by the growth of the codebase. The estimated impact was almost the same for both platforms and gave us about a &lt;strong&gt;7%-10% cut in the CI and local build time&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;open-source-plan&quot;&gt;Open source plan&lt;/h4&gt;

&lt;p&gt;The critical path tool proved to be effective to illustrate the projects‚Äô bottlenecks in a dependency tree configuration. It is currently widely used by mobile teams at Grab to analyze their dependencies and cut out or limit an unnecessary impact on the respective scope.&lt;/p&gt;

&lt;p&gt;The tool is currently considered to be open-sourced as we‚Äôd like to hear feedback from other external teams and see what can be built on top of it. We‚Äôll provide more details on this in future posts.&lt;/p&gt;

&lt;h3 id=&quot;remote-build&quot;&gt;Remote build&lt;/h3&gt;

&lt;p&gt;Another pillar of the ¬†build process is the hardware where the build runs. The solution is ¬†really straightforward - put more muscles on your build to get it stronger and to run faster.&lt;/p&gt;

&lt;p&gt;Clearly, our engineers‚Äô laptops could not be considered fast enough. To have a fast enough build we were looking at something with &lt;em&gt;20+ cores, ~200Gb of RAM&lt;/em&gt;. None of the desktop or laptop computers can reach those numbers within reasonable pricing. We hit a bottleneck in hardware. Further parallelization of the build process didn‚Äôt give any significant improvement as all the build tasks were just queueing and waiting for the resources to be released. And that‚Äôs where cloud computing came into the picture where a huge variety of available options is ready to be used.&lt;/p&gt;

&lt;h4 id=&quot;adr-mainframer&quot;&gt;ADR mainframer&lt;/h4&gt;

&lt;p&gt;We took advantage of the &lt;a href=&quot;https://github.com/buildfoundation/mainframer&quot;&gt;Mainframer&lt;/a&gt; tool. When the build must run, the code diff is pushed to the remote executor, gets compiled, and then the generated artifacts are pushed back to the local machine. An engineer might still benefit from indexing, debugging, and other features available in the IDE.&lt;/p&gt;

&lt;p&gt;To make the infrastructure mature enough, we‚Äôve introduced Kubernetes-based autoscaling based on the load. Currently, we have a stable infrastructure that accommodates &lt;strong&gt;100+ Android engineers scaling up and down (saving costs)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This strategy gave us a &lt;strong&gt;40-50% improvement in the local build time&lt;/strong&gt;. Android builds finished, in the extreme case, &lt;strong&gt;x2 faster&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;ios&quot;&gt;iOS&lt;/h4&gt;

&lt;p&gt;Given the success of the Android remote build infrastructure, we have immediately turned our attention to the iOS builds. It was an obvious move for us - we wanted the same infrastructure for iOS builds. The idea looked good on paper and was proven with Android infrastructure, but the reality was a bit different for our iOS builds.&lt;/p&gt;

&lt;p&gt;Our ¬†very first roadblock was that Xcode is not that flexible and the process of delegating build to remote is way more complicated compared to Android. We tackled a series of blockers such as running indexing on a remote machine, sending and consuming build artifacts, and even running the remote build itself.&lt;/p&gt;

&lt;p&gt;The reality was that the remote build was absolutely possible for iOS. There were ¬†minor tradeoffs impacting engineering experience alongside obvious gains from utilizing cloud computing resources. But the problem is that legally iOS builds are only allowed to be built on an Apple machine.&lt;/p&gt;

&lt;p&gt;Even if we get¬†the most powerful hardware - a macPro - ¬†the specs are still not ideal and are unfortunately not optimized for the build process. A &lt;em&gt;24 core, 194Gb RAM macPro&lt;/em&gt; could have given about x2 improvement on the build time, but when it had to ¬†run 3 builds simultaneously for different users, the build efficiency immediately dropped to the baseline value.&lt;/p&gt;

&lt;p&gt;Android remote machines with the above same specs are capable of running up to &lt;strong&gt;8 simultaneous builds&lt;/strong&gt;. This allowed us to accommodate up to &lt;strong&gt;30-35 engineers&lt;/strong&gt; per machine, whereas iOS‚Äô infrastructure would require to keep this balance at &lt;strong&gt;5-6 engineers&lt;/strong&gt; per machine. This solution didn‚Äôt seem to be scalable at all, causing us to abandon the idea of the remote builds for iOS at that moment.&lt;/p&gt;

&lt;h3 id=&quot;test-impact-analysis&quot;&gt;Test impact analysis&lt;/h3&gt;

&lt;p&gt;The other battlefront was the CI pipeline time. Our efforts in dependency tree optimizations complemented with comparably powerful hardware played a good part in achieving a reasonable build time on CI.&lt;/p&gt;

&lt;p&gt;CI validations also include the execution of unit and UI tests and may easily take 50%-60% of the pipeline time. The problem was getting worse as the number of tests was constantly growing. We were to face incredibly huge tests‚Äô execution time in the near future. We could mitigate the problem by a muscle approach - throwing more runners and shredding tests - but it won‚Äôt make finance executives happy.&lt;/p&gt;

&lt;p&gt;So the time for smart solutions came again. It‚Äôs a known fact that the simpler solution is more likely to be correct. The simplest solution was to stop running &lt;em&gt;ALL&lt;/em&gt; tests. The idea was to run only those tests that were impacted by the codebase change introduced in the given MR.&lt;/p&gt;

&lt;p&gt;Behind this simple idea, we‚Äôve found a huge impact. Once the &lt;em&gt;Test Impact Analysis&lt;/em&gt; was applied to the pre-merge pipelines, we‚Äôve managed to cut down the total number of executed tests by up to &lt;strong&gt;90%&lt;/strong&gt; without any impact on the codebase quality or applications‚Äô stability. As a result, &lt;strong&gt;we cut the pipeline for both platforms by more than 30%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Today, the Test Impact Analysis is coupled with our codebase. We are looking to ¬†invest some effort to make it available for open sourcing. We are excited to be ¬†on this path.&lt;/p&gt;

&lt;h2 id=&quot;the-end-of-the-native-build-systems&quot;&gt;The end of the Native Build Systems&lt;/h2&gt;

&lt;p&gt;One might say that our journey was long and we won the battle for the build time.&lt;/p&gt;

&lt;p&gt;Today, we hit a limit to the native build systems‚Äô efficiency and hardware for both Android and iOS. And it‚Äôs clear to us that in our current setup, we would not be able to scale up while delivering high engineering experience.&lt;/p&gt;

&lt;h2 id=&quot;lets-move-to-bazel&quot;&gt;Let‚Äôs move to Bazel&lt;/h2&gt;

&lt;p&gt;To introduce another big improvement to the build time, we needed to make some ground-level changes. And this time, we focused on the ¬†build system itself.&lt;/p&gt;

&lt;p&gt;Native build systems are designed to work well for small and medium-sized projects, however they have not been as successful in large scale projects such as the Grab Passenger applications.&lt;/p&gt;

&lt;p&gt;With these assumptions, we considered options and found the Bazel build system to be a good contender. The deep comparison of build systems disclosed that Bazel was promising better results almost in all key areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bazel enables remote builds out of box&lt;/li&gt;
  &lt;li&gt;Bazel provides sustainable cache capabilities (local and remote). This cache can be reused across all consumers - local builds, CI builds&lt;/li&gt;
  &lt;li&gt;Bazel was designed with the big codebase as a cornerstone requirement&lt;/li&gt;
  &lt;li&gt;The majority of the tooling may be reused across multiple platforms&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ways-of-adopting&quot;&gt;Ways of adopting&lt;/h3&gt;

&lt;p&gt;On paper, Bazel was awesome and shining. All our playground investigations showed positive results:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cache worked great&lt;/li&gt;
  &lt;li&gt;Incremental builds were incredibly fast&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the effort to shift to this new build system was huge. We made sure that we foresee all possible pitfalls and impediments. It took us about 5 months to estimate the impact and put together a sustainable proof of concept, which reflected the majority of our use cases.&lt;/p&gt;

&lt;h4 id=&quot;migration-limitations&quot;&gt;Migration limitations&lt;/h4&gt;

&lt;p&gt;After those 5 months of investigation, we got the endless list of incompatible features and major blockers to be addressed. Those blockers touched even such obvious things as indexing and the &lt;em&gt;jump to definition&lt;/em&gt; IDE feature, which we used to take for granted.&lt;/p&gt;

&lt;p&gt;But the biggest challenge was the need to keep the pace of the product release. There was no compromise of stopping the product development even for a day. The way out appeared to be a &lt;strong&gt;hybrid build&lt;/strong&gt; concept. We figured out how to marry native and Bazel build systems to live together in harmony. This move gave us a chance to start migrating target by target, project by project moving from the bottom to top of the dependency graph.&lt;/p&gt;

&lt;p&gt;This approach was a valid enabler, however we were still faced with a challenge of our app‚Äôs ¬†scale. The codebase of over 2.5 million of LOC cannot be migrated overnight. The initial estimation was based on the idea of manually migrating the whole codebase, which would have required us to invest dozens of person-months.&lt;/p&gt;

&lt;h4 id=&quot;team-capacity-limitations&quot;&gt;Team capacity limitations&lt;/h4&gt;

&lt;p&gt;This approach was immediately pushed back by multiple teams arguing with the priority and concerns about the impact on their own product roadmap.&lt;/p&gt;

&lt;p&gt;We were left with not much ¬†choice. On one hand, we had a pressingly long build time. And on the other hand, we were asking for a huge effort from teams. We clearly needed to get buy-ins from all of our stakeholders to push things forward.&lt;/p&gt;

&lt;h3 id=&quot;getting-buy-in&quot;&gt;Getting buy-in&lt;/h3&gt;

&lt;p&gt;To get all needed buy-ins, all stakeholders were grouped and addressed separately. We defined key factors for each group.&lt;/p&gt;

&lt;h4 id=&quot;key-factors&quot;&gt;Key factors&lt;/h4&gt;

&lt;p&gt;C-level stakeholders:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;. The migration impact must be significant - at least a 40% decrease on the build time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Costs&lt;/strong&gt;. Migration costs must be paid back in a reasonable time and the positive impact is extended to ¬†the future.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Engineering experience&lt;/strong&gt;. The user experience must not be compromised. All tools and features engineers used must be available during migration and even after.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Engineers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Engineering experience&lt;/strong&gt;. Similar to the criteria established at the C-level factor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Early adopters engagement&lt;/strong&gt;. A common ¬†core experience must be created across the mobile engineering community to support other engineers in the later stages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Education&lt;/strong&gt;. Awareness campaigns must be in place. Planned and conducted a series of tech talks and workshops to raise awareness among engineers and cut the learning curve. We wrote hundreds of pages of documentation and guidelines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Product teams:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;No roadmap impact&lt;/strong&gt;. Migration must not affect the product roadmap.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Minimize the engineering effort&lt;/strong&gt;. Migration must not increase the efforts from engineering.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;migration-automation-separate-talks&quot;&gt;Migration automation (separate talks)&lt;/h4&gt;

&lt;p&gt;The biggest concern for the majority of the stakeholders appeared to be the estimated migration effort, which impacted the cost, the product roadmap, and the engineering experience. It became evident that we needed to streamline the process and reduce the effort for migration.&lt;/p&gt;

&lt;p&gt;Fortunately, the actual migration process was routine in nature, so we had opportunities for automation. We investigated ideas on automating the whole migration process.&lt;/p&gt;

&lt;h4 id=&quot;the-tools-weve-created&quot;&gt;The tools we‚Äôve created&lt;/h4&gt;

&lt;p&gt;We found that it‚Äôs relatively easy to create a bunch of tools that read the native project structure and create an equivalent Bazel set up. This was a game changer.&lt;/p&gt;

&lt;p&gt;Things moved pretty smoothly for both Android and iOS projects. We managed to roll out tooling to migrate the codebase in a single click/command (well with some exceptions as of now. Stay tuned for another blog post on this). With this tooling combined with the hybrid build concept, we addressed all the key buy-in factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Migration cost dropped by at least 50%.&lt;/li&gt;
  &lt;li&gt;Less engineers required for the actual migration. There was no need to engage the wide engineering community as a small group of people can manage the whole process.&lt;/li&gt;
  &lt;li&gt;There is no more impact on the product roadmap.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;where-do-we-stand-today&quot;&gt;Where do we stand today&lt;/h2&gt;

&lt;p&gt;When we were in the middle of the actual migration, we decided to take a pragmatic path and migrate our applications in phases to ensure everything was under control and that there were no unforeseen issues.&lt;/p&gt;

&lt;p&gt;The hybrid build time is racing alongside our migration progress. It has a linear dependency on the amount of the migrated code. The figures look positive and we are confident in achieving our impact goal of &lt;strong&gt;decreasing at least 40% of the build time&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;plans-to-open-source&quot;&gt;Plans to open source&lt;/h3&gt;

&lt;p&gt;The automated migration tooling we‚Äôve created is planned to be open sourced. We are doing a bit better on the Android side decoupling it from our applications‚Äô implementation details and plan to open source it in the near future.&lt;/p&gt;

&lt;p&gt;The iOS tooling is a bit behind, and we expect it to be available for open-sourcing by the end of Q1‚Äô2021.&lt;/p&gt;

&lt;h2 id=&quot;is-it-worth-it-all&quot;&gt;Is it worth it all?&lt;/h2&gt;

&lt;p&gt;Bazel is not a silver bullet for the build time and your project. There are a lot of edge cases you‚Äôll never know until it punches you straight in your face.&lt;/p&gt;

&lt;p&gt;It‚Äôs far from industry standard and you might find yourself having difficulty hiring engineers with such knowledge. It has a steep learning curve as well. It‚Äôs absolutely an overhead for small to medium-sized projects, but it‚Äôs undeniably essential once you start playing in a high league of super apps.&lt;/p&gt;

&lt;p&gt;If you were to ask whether we‚Äôd go this path again, the answer would come in a &lt;strong&gt;fast and correct&lt;/strong&gt; way - yes, without any doubts.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Sergii Grechukha on behalf of the Passenger App team at Grab. Special thanks to Madushan Gamage, Mikhail Zinov, Nguyen Van Minh, Mihai Costiug, Arunkumar Sampathkumar, Maryna Shaposhnikova, Pavlo Stavytskyi, Michael Goletto, Nico Liu, and Omar Gawish for their contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Dec 2020 04:30:00 +0000</pubDate>
        <link>https://engineering.grab.com/how-grab-is-blazing-through-the-super-app-bazel-migration</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-grab-is-blazing-through-the-super-app-bazel-migration</guid>
        
        <category>Bazel</category>
        
        <category>Android</category>
        
        <category>iOS</category>
        
        <category>Build Time</category>
        
        <category>Xcode</category>
        
        <category>Gradle</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Democratizing Fare Storage at scale using Event Sourcing</title>
        <description>&lt;p&gt;From humble beginnings, Grab has expanded across different markets in the last couple of years. We‚Äôve added a wide range of features to the Grab platform to continue to delight our customers and driver-partners. We had to incessantly find ways to improve our existing solutions to better support new features.&lt;/p&gt;

&lt;p&gt;In this blog, we discuss how we built &lt;em&gt;Fare Storage&lt;/em&gt;, Grab‚Äôs single source of truth fare data store, and how we overcame the challenges to make it more reliable and scalable to support our expanding features.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image1.jpg&quot; alt=&quot;High-level Flow&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To set some context for this blog, let‚Äôs define some key terms before proceeding. A &lt;em&gt;Fare&lt;/em&gt; is a dollar amount calculated to move someone or something from point A to point B. And, a &lt;em&gt;Fee&lt;/em&gt; is a dollar amount added to or subtracted from the original fare amount for any additional service.&lt;/p&gt;

&lt;p&gt;Now that you‚Äôre acquainted with the key concepts, let‚Äôs look take a look at the following image. It illustrates that features such as &lt;em&gt;Destination Change Fee&lt;/em&gt;, &lt;em&gt;Waiting Fee&lt;/em&gt;, &lt;em&gt;Cancellation Fee&lt;/em&gt;, &lt;em&gt;Tolls&lt;/em&gt;, &lt;em&gt;Promos&lt;/em&gt;, &lt;em&gt;Surcharges&lt;/em&gt;, and many others store additional fee breakdown along with the original fare. This set of information is crucial for generating receipts and debugging processes. However, our legacy storage system wasn‚Äôt designed to host massive quantities of information effectively.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image2.jpg&quot; alt=&quot;Sample Flow with Fee Breakdown&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In our legacy architecture, we stored all the booking and fare-related information in a single relational database table. Adding new fare fields and breakdowns required changes in our critical booking system, making iterations prohibitively expensive and hindering innovation.&lt;/p&gt;

&lt;p&gt;The need to store the fare information and metadata for every additional feature along with other booking information resulted in a bloated booking entity. With millions of bookings created every day at Grab, this posed a scaling and stability threat to our booking service storage. Moreover, the legacy storage only tracked the latest value of fare and lacked a holistic view of all the modifications to the fare. So, debugging the fare was also a massive chore for our Engineering and Tech Operations teams.&lt;/p&gt;

&lt;h2 id=&quot;drafting-a-solution&quot;&gt;Drafting a solution&lt;/h2&gt;

&lt;p&gt;The shortcomings of our legacy system led us to explore options for decoupling the fare and its metadata storage from the booking details. We wanted to build a platform that can store and provide access to both fare and its audit trail.&lt;/p&gt;

&lt;p&gt;High-level functional requirements for the new fare store were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide a platform to store and retrieve fare and associated breakdowns, with no tight coupling between services.&lt;/li&gt;
  &lt;li&gt;Act as a single source-of-truth for fare and associated fees in the Grab ecosystem.&lt;/li&gt;
  &lt;li&gt;Enable clients to access the metadata of fare change events in real-time, enabling the Product team to innovate freely.&lt;/li&gt;
  &lt;li&gt;Provide smooth access to a fare‚Äôs audit trail, improving the response time to our customers‚Äô queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Non-functional requirements for the fare store were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High availability for the read and write APIs, with few milliseconds latency.&lt;/li&gt;
  &lt;li&gt;Handle concurrent updates to the fare gracefully.&lt;/li&gt;
  &lt;li&gt;Detect duplicate events for a booking for the same transaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;storing-change-sequence-with-event-sourcing&quot;&gt;Storing change sequence with Event Sourcing&lt;/h2&gt;

&lt;p&gt;Our legacy storage solution used a defined schema and only stored the latest state of the fare. We needed an audit trail-based storage system with fast querying capabilities that can store and retrieve changes in chronological order.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Event Sourcing pattern&lt;/em&gt; stood out as a flexible architectural pattern as it allowed us to store and query the sequence of changes in the order it occurred. In Martin Fowler‚Äôs &lt;a href=&quot;https://martinfowler.com/eaaDev/EventSourcing.html&quot;&gt;blog&lt;/a&gt;, he described &lt;a href=&quot;https://microservices.io/patterns/data/event-sourcing.html&quot;&gt;Event Sourcing&lt;/a&gt; as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‚ÄúThe fundamental idea of Event Sourcing is to ensure that every change to the state of an application is captured in an event object and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.‚Äù&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the Event Sourcing pattern, we store all the fare changes as events in the order they occurred for a booking. We iterate through these events to retrieve a complete snapshot of the modifications to the fare.&lt;/p&gt;

&lt;p&gt;A sample Fare Event looks like this:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Event&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// type of the event, ADD, SUB, SET, resilient&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;EventType&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// value which was added, subtracted or modified&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// fare for the booking after applying discount&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fare&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// description bytes generated by SDK&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;bytes&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//transactionID for the EventType&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;transactionID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Event Sourcing pattern also enable us to use the Command Query Responsibility Segregation (&lt;a href=&quot;https://martinfowler.com/bliki/CQRS.html&quot;&gt;CQRS&lt;/a&gt;) pattern to decouple the read responsibility for different use cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image4.jpg&quot; alt=&quot;CQRS Pattern&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Clients of the fare life cycle read the current fare and create events to change the fare value as per their logic. Clients can also access fare events, when required. This pattern enable clients to modify fares independently, and give them visibility to the sequence for different business needs.&lt;/p&gt;

&lt;p&gt;The diagram below describes the overall fare life cycle from creation, modification to display using the event store.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image5.png&quot; alt=&quot;Overall Fare Life Cycle&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;architecture-overview&quot;&gt;Architecture overview&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image6.jpg&quot; alt=&quot;Fare Cycle Architecture&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Clients interact with the Fare LifeCycle service through an SDK. The SDK offers various features such as metadata serialization, deserialization, retries, and timeouts configurations, some of which are discussed later.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Fare LifeCycle Store&lt;/em&gt; service uses DynamoDB as Event Store to persist and read the fare change events backed by a cache for eventually consistent reads. For further processing, such as archiving and generation of receipts, the successfully updated events are streamed out to a message queue system.&lt;/p&gt;

&lt;h2 id=&quot;ensuring-the-integrity-of-the-fare-sequence&quot;&gt;Ensuring the integrity of the fare sequence&lt;/h2&gt;

&lt;p&gt;Democratizing the responsibility of fare modification means that multiple services might try to update the fare in parallel without prior synchronization. Concurrent fare updates for the same booking might result in a race condition. Concurrency and consistency problems are always highlights of distributed storage systems.&lt;/p&gt;

&lt;p&gt;Let‚Äôs understand why the ordering of fare updates are important. Business rules for different cities and countries regulate the pricing features based on local market conditions and prevailing laws. For example, in some scenarios, &lt;em&gt;Tolls&lt;/em&gt; and &lt;em&gt;Waiting Fees&lt;/em&gt; may not be eligible for discounts or promotions. The service applying discounts needs to consider this information while applying a discount. Therefore, updates to the fare are not independent of the previous fare events.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image3.jpg&quot; alt=&quot;Fare Integrity&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We needed a mechanism to detect race conditions and handle them appropriately to ensure the integrity of the fare. To handle race conditions based on our use case, we explored &lt;a href=&quot;https://en.wikipedia.org/wiki/Lock_(computer_science)&quot;&gt;Pessimistic and Optimistic locking mechanisms&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All the expected fare change events happen based on certain conditions being true or false. For example, less than 1% of the bookings have a payment change request initiated by passengers during a ride. And, the probability of multiple similar changes happening on the same booking is rather low. &lt;em&gt;Optimistic Locking&lt;/em&gt; offers both efficiency and correctness for our requirements where the chances of race conditions are low, and the records are independent of each other.&lt;/p&gt;

&lt;p&gt;The logic to calculate the fare/surcharge is coupled with the business logic of the system that calculates the fare component or fees. So, handling data race conditions on the data store layer was not an acceptable option either. It made more sense to let the clients handle it and keep the storage system decoupled from the business logic to compute the fare.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image8.jpg&quot; alt=&quot;Optimistic Locking&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To achieve &lt;em&gt;Optimistic Locking&lt;/em&gt;, we store a fare version and increment it on every successful update. The client must pass the version they read to modify the fare. Should there be a version mismatch between the update query and the current fare, the update is rejected. On version mismatches, the clients read the updated checksum(version) and retry with the recalculated fare.&lt;/p&gt;

&lt;h2 id=&quot;idempotency-of-event-updates&quot;&gt;Idempotency of event updates&lt;/h2&gt;

&lt;p&gt;The next challenge we came across was how to handle client retries - ensuring that we do not duplicate the same event for the booking. Clients might encounter sporadic errors as a result of network-related issues, although the update was successful. Under such circumstances, clients retry to update the same event, resulting in duplicate events. Duplicate events not only result in an extra space requirement, but it also impairs the clients‚Äô understanding on whether we‚Äôve taken an action multiple times on the fare.&lt;/p&gt;

&lt;p&gt;As discussed in the previous section, retrying with the same version would fail due to the version mismatch. If the previous attempt successfully modified the fare, it would also update the version.&lt;/p&gt;

&lt;p&gt;However, clients might not know if their update modified the version or if any other clients updated the data. Relying on clients to check for event duplication makes the client-side complex and leaves a chance of error if the clients do not handle it correctly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image9.jpg&quot; alt=&quot;Solution for Duplicate Events&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To handle the duplicate events, we associate each event with a unique UUID (&lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt;) generated from the client-side using a UUID library from the Fare LifeCycle service SDK. We check whether the &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt; is already part of successful transaction IDs before updating the fare. If we identify a non-unique &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt;, we return duplicate event errors to the client.&lt;/p&gt;

&lt;p&gt;For unique &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionIDs&lt;/code&gt;, we append it to the list of transactionIDs and save it to the Event Store along with the event.&lt;/p&gt;

&lt;h2 id=&quot;schema-less-metadata&quot;&gt;Schema-less metadata&lt;/h2&gt;

&lt;p&gt;Metadata are the breakdowns associated with the fare. We require the metadata for specific fee/fare calculation for the generation of receipts and debugging purposes. Thus, for the storage system and multiple clients, they need not know the metadata definition of all events.&lt;/p&gt;

&lt;p&gt;One goal for our data store was to give our clients the flexibility to add new fields to existing metadata or to define new metadata without changing the API. We adopted an SDK-based approach for metadata, where clients interact with the Fare LifeCycle service via SDK. The SDK has the following responsibilities for metadata:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Serialize the metadata into bytes before making an API call to the Fare LifeCycle service.&lt;/li&gt;
  &lt;li&gt;Deserialize the bytes metadata returned from the Fare LifeCycle service into a Go struct for client access.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratizing-fare-storage-at-scale-using-event-sourcing/image7.jpg&quot; alt=&quot;Fare LifeCycle SDK&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Serializing and deserializing the metadata on the client-side decoupled it from the Fare LifeCycle Store API. This helped teams update the metadata without deploying the storage service each time.&lt;/p&gt;

&lt;p&gt;For reading the breakdown, the clients pass the metadata bytes to the SDK along with the Event Type, and then it converts them back into the corresponding proto schema. With this approach, clients can update the metadata without changing the Data Store Service.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Fare LifeCycle service enabled us to revolutionize the fare storage at scale for Grab‚Äôs ecosystem of services. Further benefits realized with the system are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The feature agnostic platform helped us to reduce the time-to-market for our hyper-local features so that we can further outserve our customers and driver-partners.&lt;/li&gt;
  &lt;li&gt;Decoupling the fare information from the booking information also helped us to achieve a better separation of concerns between services.&lt;/li&gt;
  &lt;li&gt;Improve the overall reliability and scalability of the Grab platform by decoupling fare and booking information, allowing them to scale independently of each other.&lt;/li&gt;
  &lt;li&gt;Reduce unnecessary coupling between services to fetch fare related information and update fare.&lt;/li&gt;
  &lt;li&gt;The audit-trail of fare changes in the chronological order reduced the time to debug fare and improved our response to customers for fare-related queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope this post helped you to have a closer look at how we used the Event Source pattern for building a data store and how we handled a few caveats and challenges in the process.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Sourabh Suman on behalf of the Pricing team at Grab. Special thanks to Karthik Gandhi, Kurni Famili, ChandanKumar Agarwal, Adarsh Koyya, Niteesh Mehra, Sebastian Wong, Matthew Saw, Muhammad Muneer, and Vishal Sharma for their contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Nov 2020 06:21:00 +0000</pubDate>
        <link>https://engineering.grab.com/democratizing-fare-storage-at-scale-using-event-sourcing</link>
        <guid isPermaLink="true">https://engineering.grab.com/democratizing-fare-storage-at-scale-using-event-sourcing</guid>
        
        <category>Pricing</category>
        
        <category>Event Sourcing</category>
        
        <category>Fare Storage</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Keeping 170 libraries up to date on a large scale Android App</title>
        <description>&lt;p&gt;To scale up to the needs of our customers, we‚Äôve adopted ways to efficiently deliver our services through our everyday superapp - whether it‚Äôs through continuous process improvements or coding best practices. For one, &lt;a href=&quot;https://en.wikipedia.org/wiki/Library_(computing)&quot;&gt;libraries&lt;/a&gt; have made it possible for us to increase our development velocity. In the Passenger App Android team, we‚Äôve a mix of libraries - from libraries that we‚Äôve built in-house to open source ones.&lt;/p&gt;

&lt;p&gt;Every week, we release a new version of our Passenger App. Each update contains on average between five to ten library updates. In this article, we will explain how we keep all libraries used by our app up to date, and the different actions we take to avoid defect leaks into production.&lt;/p&gt;

&lt;h2 id=&quot;how-many-libraries-are-we-using&quot;&gt;How many libraries are we using?&lt;/h2&gt;

&lt;p&gt;Before we add a new library to a project, it goes through a rigorous assessment process covering many parts, such as security issue detection and usability tests measuring the impact on the app size and app startup time. This process ensures that only libraries up to our standards are added.&lt;/p&gt;

&lt;p&gt;In total, there are more than &lt;strong&gt;170 libraries&lt;/strong&gt; powering the SuperApp, including 55 AndroidX artifacts and 22 libraries used for the sole purpose of writing automation testing (Unit Testing or UI Testing).&lt;/p&gt;

&lt;h2 id=&quot;who-is-responsible-for-updating&quot;&gt;Who is responsible for updating&lt;/h2&gt;

&lt;p&gt;While we do have an internal process on how to update the libraries, it doesn‚Äôt mention who and how often it should be done. In fact, it‚Äôs everyone‚Äôs responsibility to make sure our libraries are up to date. Each team should be aware of the libraries they‚Äôre using and whenever a new version is released.&lt;/p&gt;

&lt;p&gt;However, this isn‚Äôt really the case. We‚Äôve a few developers taking ownership of the libraries as a whole and trying to maintain it. With more than 170 external libraries, we surveyed the Android developer community on how they manage libraries in the company. The result can be summarized as follow:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/keeping-170-libraries-up-to-date/infography.png&quot; alt=&quot;Survey Results&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Survey Results&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;While most developers are aware of updates, they don‚Äôt update a library because the risk of defects leaking into production is too high.&lt;/p&gt;

&lt;h2 id=&quot;risk-management&quot;&gt;Risk management&lt;/h2&gt;
&lt;p&gt;The risk is to have a defect leaking into production. It can cause regressions on existing features or introduce new crashes in the app. In a worst case scenario, if this isn‚Äôt caught before publishing, it can force us to make a hotfix and a certain number of users will be impacted.&lt;/p&gt;

&lt;p&gt;Before updating (bump) a library, we evaluate two metrics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the usage of this library in the codebase.&lt;/li&gt;
  &lt;li&gt;the number of changes introduced in the library between the current version and the targeted version.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The risk needs to be assessed between the number of usages of a certain library and the size of the changes. The following chart illustrate this point.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/keeping-170-libraries-up-to-date/radar.png&quot; alt=&quot;Risk Assessment Radar&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Risk Assessment Radar&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;This arbitrary scale helps us in deciding if we will require additional signoff from the QA team. If the estimation places the item on the bottom-left corner, the update will be less risky while if it‚Äôs on the top-right corner, it means we should follow extra verification to reduce the risk.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A good practice to reduce the risks of updating a library is to update it frequently, decreasing the diffs hence reducing the scope of impact.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;reducing-the-risk&quot;&gt;Reducing the risk&lt;/h2&gt;

&lt;p&gt;The first thing we‚Äôre doing to reduce the risk is to update our libraries on a weekly basis. As described above, small changes are always less risky than large changes even if the usage of this partial library is wide. By following incremental updates, we avoid accumulating potential issues over a longer period of time.&lt;/p&gt;

&lt;p&gt;For example, the Android Jetpack and Firebase libraries follow a two-week release train. So every two weeks, we check for new updates, read the changelogs, and proceed with the update.&lt;/p&gt;

&lt;p&gt;In case of a defect detected, we can easily revert the change until we figure out a proper solution or raise the issue to the library owner.&lt;/p&gt;

&lt;h3 id=&quot;automation&quot;&gt;Automation&lt;/h3&gt;

&lt;p&gt;To reduce risk on any merge request (not limited to library update), we‚Äôve spent a tremendous amount of effort on automating tests. For each new feature we‚Äôve a set of test cases written in Gherkin syntax.&lt;/p&gt;

&lt;p&gt;Automation is implemented as UI tests that run on continuous integration (CI) for every merge request. If those tests fail, we won‚Äôt be able to merge any changes.&lt;/p&gt;

&lt;p&gt;To further elaborate, let‚Äôs take this example: Team A developed a lot of features and now has a total of 1,000 test cases. During regression testing before each release, only a subset of those are executed manually based on the impacted area. With automation in place, team A now has 60% of those tests executed as part of CI. So, when all the tests successfully pass, we‚Äôre already 60% confident that no defect is detected. This tremendously increases our confidence level while reducing manual testing.&lt;/p&gt;

&lt;h3 id=&quot;qa-signoff&quot;&gt;QA signoff&lt;/h3&gt;

&lt;p&gt;When the update is in the risk threshold area and the automation tests are insufficient, the developer works with QA engineers on analyzing impacted areas. They would then execute test cases related to the impacted area.&lt;/p&gt;

&lt;p&gt;For example, if we‚Äôre updating Facebook library, the impacted area would be the ‚ÄúLogin with Facebook‚Äù functionality. QA engineers would then run test cases related to social login.&lt;/p&gt;

&lt;p&gt;A single or multiple team can be involved. In some cases, QA signoff can be required by all the teams if they‚Äôre all affected by the update.&lt;/p&gt;

&lt;p&gt;This process requires a lot of effort from different teams and can affect the current roadmap. To avoid falling into this category, we refine the impacted area analysis to be as specific as possible.&lt;/p&gt;

&lt;h2 id=&quot;update-before-it-becomes-mandatory&quot;&gt;Update before it becomes mandatory&lt;/h2&gt;

&lt;p&gt;Google updates the Google Play requirements regularly to ensure that published apps are fully compatible with the latest Android version.&lt;/p&gt;

&lt;p&gt;For example, starting 1st November 2020 all apps must target API 29. This change causes &lt;a href=&quot;https://developer.android.com/about/versions/10/behavior-changes-10&quot;&gt;behavior changes&lt;/a&gt; for some API. New behavior has to be supported and verified for our code, but also for all the libraries we use. Libraries bundled inside our app are also affected if they‚Äôre using Android API. However, the support for newer API is done by each library maintainer. By keeping our libraries up to date, we ensure compatibility with the latest Android API.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Keep updating your libraries. If they‚Äôre following a release plan, try to match it so it won‚Äôt accumulate too many changes. For every new release at Grab, we ship a new version each week, which includes between 5 to 10 libraries bump.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each update, identify the potential risks on your app and find the correct balance between risk and effort required to mitigate this. Don‚Äôt overestimate the risk, especially if the changes are minimal and only include some minor bug fixing. Some library updates don‚Äôt even change any single line of code and are only documentation updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Invest in robust automation testing to create a high confidence level when making changes, including potentially large changes like a huge library bump.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Lucas Nelaupe on behalf of the Grab Android Development team. Special thanks to Tridip Thrizu and Karen Kue for the design and copyediting contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Oct 2020 04:39:00 +0000</pubDate>
        <link>https://engineering.grab.com/keeping-170-libraries-up-to-date-on-a-large-scale-android-app</link>
        <guid isPermaLink="true">https://engineering.grab.com/keeping-170-libraries-up-to-date-on-a-large-scale-android-app</guid>
        
        <category>Mobile</category>
        
        <category>Android</category>
        
        <category>Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Optimally scaling Kafka consumer applications</title>
        <description>&lt;p&gt;Earlier this year, we took you on a journey on how we built and deployed our event sourcing and stream processing framework at Grab. We‚Äôre happy to share that we‚Äôre able to reliably maintain our uptime and continue to service close to 400 billion events a week. We haven‚Äôt stopped there though. To ensure that we can scale our framework as the Grab business continuously grows, we have spent efforts optimizing our infrastructure.&lt;/p&gt;

&lt;p&gt;In this article, we will dive deeper into our Kubernetes infrastructure setup for our &lt;a href=&quot;https://engineering.grab.com/plumbing-at-scale&quot;&gt;stream processing framework&lt;/a&gt;. We will cover why and how we focus on optimal scalability and availability of our infrastructure.&lt;/p&gt;

&lt;h2 id=&quot;quick-architecture-recap&quot;&gt;Quick Architecture Recap&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Coban Platform Architecture&quot; src=&quot;/img/optimally-scaling-kafka-consumer-applications/image2.png&quot; /&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The Coban platform provides lightweight &lt;a href=&quot;https://medium.com/learning-the-go-programming-language/writing-modular-go-programs-with-plugins-ec46381ee1a9&quot;&gt;Golang plugin&lt;/a&gt; architecture-based data processing pipelines running in Kubernetes. These are essentially Kafka consumer pods that consume data, process it, and then materialize the results into various sinks (RDMS, other Kafka topics).&lt;/p&gt;

&lt;h2 id=&quot;anatomy-of-a-processing-pod&quot;&gt;Anatomy of a Processing Pod&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Anatomy of a Processing Pod&quot; src=&quot;/img/optimally-scaling-kafka-consumer-applications/image1.png&quot; /&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Each stream processing pod (the smallest unit of a pipeline‚Äôs deployment) has three top level components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Trigger&lt;/strong&gt;: An interface that connects directly to the source of the data and converts it into an event channel.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Runtime&lt;/strong&gt;: This is the app‚Äôs entry point and the orchestrator of the pod. It manages the worker pools, triggers, event channels, and lifecycle events.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipeline plugin&lt;/strong&gt;: This is provided by the user, and conforms to a contract that the platform team publishes. It contains the domain logic for the pipeline and houses the pipeline orchestration defined by a user based on our Stream Processing Framework.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-scaling&quot;&gt;Optimal Scaling&lt;/h3&gt;

&lt;p&gt;We initially architected our Kubernetes setup around &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale&quot;&gt;horizontal pod autoscaling&lt;/a&gt; (HPA), which scales the number of pods per deployment based on CPU and memory usage. HPA keeps CPU and memory per pod specified in the deployment manifest and scales horizontally as the load changes.&lt;/p&gt;

&lt;p&gt;These were the areas of application wastage we observed on our platform:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As Grab‚Äôs traffic is uneven, we‚Äôd always have to provision for peak traffic. As users would not (or could not) always account for ramps, they would be fairly liberal with setting limit values (CPU and memory), leading to resource wastage.&lt;/li&gt;
  &lt;li&gt;Pods often had uneven traffic distribution despite fairly even partition load distribution in Kafka. The Stream Processing Framework(SPF) is essentially Kafka consumers consuming from Kafka topics, hence the number of pods scaling in and out resulted in unequal partition load per pod.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vertically-scaling-with-fixed-number-of-pods&quot;&gt;Vertically Scaling with Fixed Number of Pods&lt;/h3&gt;

&lt;p&gt;We initially kept the number of pods for a pipeline equal to the number of partitions in the topic the pipeline consumes from. This ensured even distribution of partitions to each pod providing balanced consumption. In order to abstract this from the end user, we automated the application deployment process to directly call the Kafka API to fetch the number of partitions during runtime.&lt;/p&gt;

&lt;p&gt;After achieving a fixed number of pods for the pipeline, we wanted to move away from HPA. We wanted our pods to scale up and down as the load increases or decreases without any manual intervention. &lt;a href=&quot;https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler&quot;&gt;Vertical pod autoscaling&lt;/a&gt; (VPA) solves this problem as it relieves us from any manual operation for setting up resources for our deployment.&lt;/p&gt;

&lt;p&gt;We just deploy the application and let VPA handle the resources required for its operation. It‚Äôs known to not be very susceptible to quick load changes as it trains its model to monitor the deployment‚Äôs load trend over a period of time before recommending an optimal resource. This process ensures the optimal resource allocation for our pipelines considering the historic trends on throughput.&lt;/p&gt;

&lt;p&gt;We saw a &lt;em&gt;~45%&lt;/em&gt; reduction in our total resource usage vs resource requested after moving to VPA with a fixed number of pods from HPA.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Anatomy of a Processing Pod&quot; src=&quot;/img/optimally-scaling-kafka-consumer-applications/image3.png&quot; /&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;managing-availability&quot;&gt;Managing Availability&lt;/h3&gt;

&lt;p&gt;We broadly classify our workloads as latency sensitive (critical) and latency tolerant (non-critical). As a result, we could optimize scheduling and cost efficiency using &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption&quot;&gt;priority classes&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubernetes-sigs/cluster-proportional-autoscaler&quot;&gt;overprovisioning&lt;/a&gt; on heterogeneous node types on AWS.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-priority-classes&quot;&gt;Kubernetes Priority Classes&lt;/h2&gt;

&lt;p&gt;The main cost of running EKS in AWS is attributed to the EC2 machines that form the worker nodes for the Kubernetes cluster. Running &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand&quot;&gt;On-Demand&lt;/a&gt; brings all the guarantees of instance availability but it is definitely very expensive. Hence, our first action to drive cost optimisation was to include &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html&quot;&gt;Spot instances&lt;/a&gt; in our worker node group.&lt;/p&gt;

&lt;p&gt;With the uncertainty of losing a spot instance, we started assigning priority to our various applications. We then let the user choose the priority of their pipeline depending on their use case. Different priorities would result in different node affinity to different kinds of instance groups (On-Demand/Spot). For example, Critical pipelines (latency sensitive) run on On-Demand worker node groups and Non-critical pipelines (latency tolerant) on Spot instance worker node groups.&lt;/p&gt;

&lt;p&gt;We use priority class as a method of preemption, as well as a node affinity that chooses a certain priority pipeline for the node group to deploy to.&lt;/p&gt;

&lt;h2 id=&quot;overprovisioning&quot;&gt;Overprovisioning&lt;/h2&gt;

&lt;p&gt;With spot instances running we realised a need to make our cluster quickly respond to failures. We wanted to achieve quick rescheduling of evicted pods, hence we added overprovisioning to our cluster. This means we keep some noop pods occupying free space running in our worker node groups for the quick scheduling of evicted or deploying pods.&lt;/p&gt;

&lt;p&gt;The overprovisioned pods are the lowest priority pods, thus can be preempted by any pod waiting in the queue for scheduling. We used cluster proportional autoscaler to decide the right number of these overprovisioned pods, which scales up and down proportionally to cluster size (i.e number of nodes and CPU in worker node group). This relieves us from tuning the number of these noop pods as the cluster scales up or down over the period keeping the free space proportional to current cluster capacity.&lt;/p&gt;

&lt;p&gt;Lastly, overprovisioning also helped improve the deployment time because there is no ¬†dependency on the time required for &lt;a href=&quot;https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html&quot;&gt;Auto Scaling Groups&lt;/a&gt; (ASG) to add a new node to the cluster every time we want to deploy a new application.&lt;/p&gt;

&lt;h2 id=&quot;future-improvements&quot;&gt;Future Improvements&lt;/h2&gt;

&lt;p&gt;Evolution is an ongoing process. In the next few months, we plan to work on custom resources for combining VPA and fixed deployment size. Our current architecture setup works fine for now, but we would like to create a more tuneable in-house &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources&quot;&gt;CRD&lt;/a&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources&quot;&gt;(Custom Resource Definition)&lt;/a&gt; for VPA that incorporates rightsizing our Kubernetes deployment horizontally.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored By Shubham Badkur on behalf of the Coban team at Grab - Ryan Ooi, Karan Kamath, Hui Yang, Yuguang Xiao, Jump Char, Jason Cusick, Shrinand Thakkar, Dean Barlan, Shivam Dixit, Andy Nguyen, and Ravi Tandon.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Tue, 13 Oct 2020 02:13:54 +0000</pubDate>
        <link>https://engineering.grab.com/optimally-scaling-kafka-consumer-applications</link>
        <guid isPermaLink="true">https://engineering.grab.com/optimally-scaling-kafka-consumer-applications</guid>
        
        <category>Event Sourcing</category>
        
        <category>Stream Processing</category>
        
        <category>Kubernetes</category>
        
        <category>Backend</category>
        
        <category>Platform</category>
        
        <category>Go</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Our Journey to Continuous Delivery at Grab (Part 1)</title>
        <description>&lt;p&gt;This blog post is a two-part presentation of the effort that went into improving the &lt;a href=&quot;https://continuousdelivery.com/&quot;&gt;continuous delivery&lt;/a&gt; processes for backend services at Grab in the past two years. In the first part, we take stock of where we started two years ago and describe the software and tools we created while introducing some of the integrations we‚Äôve done to automate our software delivery in our staging environment.
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;post-quotations&quot;&gt;
  &lt;i&gt;Continuous Delivery is the ability to get changes of all types‚Äîincluding new features, configuration changes, bug fixes and experiments‚Äîinto production, or into the hands of users, safely and quickly in a sustainable way.&lt;/i&gt;
  &lt;br /&gt;
  &lt;i&gt;‚Äî &lt;a href=&quot;https://continuousdelivery.com/&quot;&gt;continuousdelivery.com&lt;/a&gt;&lt;/i&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
As a backend engineer at Grab, nothing matters more than the ability to innovate quickly and safely. Around the end of 2018, Grab‚Äôs transportation and deliveries backend architecture consisted of roughly 270 services (the majority being microservices). The deployment process was lengthy, required careful inputs and clear communication. The care needed to push changes in production and the risk associated with manual operations led to the introduction of a Slack bot to coordinate deployments. The bot ensures that deployments occur only during off-peak and within work hours:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Overview of the Grab Delivery Process&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image4.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Overview of the Grab Delivery Process&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Once the build was completed, engineers who desired to deploy their software to the Staging environment would copy release versions from the build logs, and paste them in a Jenkins job‚Äôs parameter. Tests needed to be manually triggered from another dedicated Jenkins job.&lt;/p&gt;

&lt;p&gt;Prior to production deployments, engineers would generate their release notes via a script and update them manually in a wiki document. Deployments would be scheduled through interactions with a Slack bot that controls release notes and deployment windows. Production deployments were made once again by pasting the correct parameters into two dedicated Jenkins jobs, one for the canary (a.k.a. one-box) deployment and the other for the full deployment, spread one hour apart. During the monitoring phase, engineers would continuously observe metrics reported on our dashboards.&lt;/p&gt;

&lt;p&gt;In spite of the fragmented process and risky manual operations impacting our velocity and stability, around 614 builds were running each business day and changes were deployed on our staging environment at an average rate of 300 new code releases per business day, while production changes averaged a rate of 28 new code releases per business day.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Our Deployment Funnel, Towards the End of 2018&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image10.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Our Deployment Funnel, Towards the End of 2018&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These figures meant that, on average, it took 10 business days between each service update in production, and only 10% of the staging deployments were eventually promoted to production.&lt;/p&gt;

&lt;h2 id=&quot;automating-continuous-deployments-at-grab&quot;&gt;Automating Continuous Deployments at Grab&lt;/h2&gt;

&lt;p&gt;With an increased focus on Engineering efficiency, in 2018 we started an internal initiative to address frictions in deployments that became known as Conveyor. To build Conveyor with a small team of engineers, we had to rely on an already mature platform which exhibited properties that are desirable to us to achieve our mission.&lt;/p&gt;

&lt;h3 id=&quot;hands-off-deployments&quot;&gt;Hands-off deployments&lt;/h3&gt;

&lt;p&gt;Deployments should be an afterthought. Engineers should be as removed from the process as possible, and whenever possible, decisions should be taken early, during the code review process. The machine will do the heavy lifting, and only when it can‚Äôt decide for itself, should the engineer be involved. Notifications can be leveraged to ensure that engineers are only informed when something goes wrong and a human decision is required.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Hands-off Deployment Principle&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image12.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Hands-off Deployment Principle&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;confidence-in-deployments&quot;&gt;Confidence in Deployments&lt;/h3&gt;

&lt;p&gt;Grab‚Äôs focus on gathering internal Engineering NPS feedback helped us collect valuable metrics. One of the metrics we cared about was our engineers‚Äô confidence in their production deployments. A team‚Äôs entire deployment process to production could last for more than a day and may extend up to a week for teams with large infrastructures running critical services. The possibility of losing progress in deployments when individual steps may last for hours is detrimental to the improvement of Engineering efficiency in the organisation. The deployment automation platform is the bedrock of that confidence. If the platform itself fails regularly or does provide a path of upgrade that is transparent to end-users, any features built on top of it would suffer from these downtimes and ultimately erode confidence in deployments.&lt;/p&gt;

&lt;h3 id=&quot;tailored-to-most-but-extensible-for-the-few&quot;&gt;Tailored To Most But Extensible For The Few&lt;/h3&gt;

&lt;p&gt;Our backend engineering teams are working on diverse stacks, and so are their deployment processes. Right from the start, we wanted our product to benefit the largest population of engineers that had adopted the same process, so as to maximize returns on our investments. To ease adoption, we decided to tailor a deployment pipeline such that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It would model the exact sequence of manual processes followed by this population of engineers.&lt;/li&gt;
  &lt;li&gt;Switching to use that pipeline should require as little work as possible by service teams.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, in cases where this model would not fit a team‚Äôs specific process, our deployment platform should be open and extensible and support new customizations even when they are not originally supported by the product‚Äôs ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;cloud-agnosticity&quot;&gt;Cloud-Agnosticity&lt;/h3&gt;

&lt;p&gt;While we were going to target a specific process and team, to ensure that our solution would stand the test of time, we needed to ensure that our solution would support the variety of environments currently used in production. This variety was also likely to increase, and we wanted a platform that would mature together with the rest of our ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-conveyor&quot;&gt;Overview Of Conveyor&lt;/h2&gt;

&lt;h3 id=&quot;setting-sail-with-spinnaker&quot;&gt;Setting Sail With Spinnaker&lt;/h3&gt;

&lt;p&gt;Conveyor is based on &lt;a href=&quot;https://spinnaker.io/&amp;amp;usg=AOvVaw1a93_1MJmR_1SZQ0mlu4Ow&quot;&gt;Spinnaker&lt;/a&gt;, an open-source, multi-cloud continuous delivery platform. We‚Äôve chosen Spinnaker over other platforms because it is a mature deployment platform with no single point of failure, supports complex workflows (referred to as pipelines in Spinnaker), and already supports a large array of cloud providers. Since Spinnaker is open-source and extensible, it allowed us to add the features we needed for the specificity of our ecosystem.&lt;/p&gt;

&lt;p&gt;To further ease adoption within our organization, we built a tailored ¬†user interface and created our own domain-specific language (DSL) to manage its pipelines as code.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Outline of Conveyor's Architecture&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image3.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Outline of Conveyor's Architecture&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;onboarding-to-a-simpler-interface&quot;&gt;Onboarding To A Simpler Interface&lt;/h3&gt;

&lt;p&gt;Spinnaker comes with its own interface, it has all the features an engineer would want from an advanced continuous delivery system. However, Spinnaker interface is vastly different from Jenkins and makes for a steep learning curve.&lt;/p&gt;

&lt;p&gt;To reduce our barrier to adoption, we decided early on to create a simple interface for our users. In this interface, deployment pipelines take the center stage of our application. Pipelines are objects managed by Spinnaker, they model the different steps in the workflow of each deployment. Each pipeline is made up of stages that can be assembled like lego-bricks to form the final pipeline. An instance of a pipeline is called an execution.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Conveyor dashboard. Sensitive information like authors and service names are redacted.&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image2.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Conveyor Dashboard&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;With this interface, each engineer can focus on what matters to them immediately: the pipelines they have started, or those started by other teammates working on the same services as they are. Conveyor also provides a search bar (on the top) and filters (on the left) that work in concert to explore all pipelines executed at Grab.&lt;/p&gt;

&lt;p&gt;We adopted a consistent set of colours to model all information in our interface:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;blue: represent stages that are currently running;&lt;/li&gt;
  &lt;li&gt;red: stages that have failed or important information;&lt;/li&gt;
  &lt;li&gt;yellow: stages that require human interaction;&lt;/li&gt;
  &lt;li&gt;and finally, in green: stages that were successfully completed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Conveyor also provides a task and notifications area, where all stages requiring human intervention are listed in one location. Manual interactions are often no more than just YES or NO questions:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Conveyor tasks. Sensitive information like author/service names is redacted.&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image9.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Conveyor Tasks&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Finally, in addition to supporting automated deployments, we greatly simplified the start of manual deployments. Instead of being required to copy/paste information, each parameter can be selected on the interface from a set of predefined items, sorted chronologically, and presented with contextual information to help engineers in their decision.&lt;/p&gt;

&lt;p&gt;Several parameters are required for our deployments and their values are selected from the UI to ensure correctness.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Simplified manual deployments&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image8.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Simplified Manual Deployments&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ease-of-adoption-with-our-pipeline-as-code-dsl&quot;&gt;Ease Of Adoption With Our Pipeline-As-Code DSL&lt;/h3&gt;

&lt;p&gt;Ease of adoption for the team is not simply about the learning curve of the new tools. We needed to make it easy for teams to configure their services to deploy with Conveyor. Since we focused on automating tasks that were already performed manually, we needed only to configure the layer that would enable the integration.&lt;/p&gt;

&lt;p&gt;We set on creating a pipeline-as-code implementation when none were widely being developed in the Spinnaker community. It‚Äôs interesting to see that two years on, this idea has grown in parallel in the community, with the birth of other &lt;a href=&quot;https://docs.armory.io/docs/spinnaker/using-dinghy/&quot;&gt;pipeline-as-code implementations&lt;/a&gt;. Our pipeline-as-code is referred to as the Pipeline DSL, and its configuration is located inside each team‚Äôs repository. Artificer is the name of our Pipeline DSL interpreter and it runs with every change inside our monorepository:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Artificer: Our Pipeline DSL&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image6.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Artificer: Our Pipeline DSL&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Pipelines are being updated at every commit if necessary.&lt;/p&gt;

&lt;p&gt;Creating a &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; file within the service‚Äôs directory of our monorepository with the few lines below is all that‚Äôs required for Artificer to do its work and get the benefits of automation provided by Conveyor‚Äôs pipeline:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'default.libsonnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;service-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
       &lt;span class=&quot;s2&quot;&gt;&quot;group-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample minimal &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration to onboard services.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In this file, engineers simply specify the name of their service and the group that a user should belong to, to have deployment rights for the service.&lt;/p&gt;

&lt;p&gt;Once the build is completed, teams can log in to Conveyor and start manual deployments of their services with our pipelines. Three pipelines are provided by default: the integration pipeline used for tests and developments, the staging pipeline used for pre-production tests, and the production pipeline for production deployment.&lt;/p&gt;

&lt;p&gt;Thanks to the simplicity of this minimal configuration file, we were able to generate these configuration files for all existing services of our monorepository. This resulted in the automatic onboarding of a large number of teams and was a major contributing factor to the adoption of Conveyor throughout our organisation.&lt;/p&gt;

&lt;h2 id=&quot;our-journey-to-engineering-efficiency-for-backend-services&quot;&gt;Our Journey To Engineering Efficiency (for backend services)&lt;/h2&gt;

&lt;p&gt;The sections below relate some of the improvements in engineering efficiency we‚Äôve delivered since Conveyor‚Äôs inception. They were not made precisely in this order but for readability, they have been mapped to each step of the software development lifecycle.&lt;/p&gt;

&lt;h3 id=&quot;automate-deployments-at-build-time&quot;&gt;Automate Deployments at Build Time&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Continuous Integration Job&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image7.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Continuous Integration Job&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Continuous delivery begins with a pushed code commit in our trunk-based development flow. Whenever a developer pushes changes onto their development branch or onto the trunk, a continuous integration job is triggered on Jenkins. The products of this job (binaries, docker images, etc) are all uploaded into our artefact repositories. We‚Äôve made two additions to our continuous integration process.&lt;/p&gt;

&lt;p&gt;The first modification happens at the step ‚ÄúUpload &amp;amp; Register artefacts‚Äù. At this step, each artefact created is now registered in Conveyor with its associated metadata. When and if an engineer needs to trigger a deployment manually, Conveyor can display the list of versions to choose from, eliminating the need for error-prone manual inputs:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot; Staging&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image5.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Staging&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Each selectable version shows contextual information: title, author, version and link to the code change where it originated. During registration, the commit time is also recorded and used to order entries chronologically in the interface. To ensure this integration is not a single point of failure for deployments, manual input is still available optionally.&lt;/p&gt;

&lt;p&gt;The second modification implements one of the essential feature continuous delivery: your deployments should happen often, automatically. Engineers are now given the possibility to start automatic deployments once continuous integration has successfully completed, by simply modifying their project‚Äôs continuous integration settings:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;AfterBuild&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;AutoDeploy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;OnDiff&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;OnLand&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;TYPE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;conveyor&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// other settings...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample settings needed to trigger auto-deployments. &lt;code class=&quot;highlighter-rouge&quot;&gt;Diff&lt;/code&gt; refers to code review submissions, and &lt;code class=&quot;highlighter-rouge&quot;&gt;Land&lt;/code&gt; refers to merged code changes.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;staging-pipeline&quot;&gt;Staging Pipeline&lt;/h3&gt;

&lt;p&gt;Before deploying a new artefact to a service in production, changes are validated on the staging environment. During the staging deployment, we verify that canary (one-box) deployments and full deployments with automated smoke and functional tests suites.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Staging Pipeline&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image1.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Staging Pipeline&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We start by acquiring a deployment lock for this service and this environment. This prevents another deployment of the same service on the same environment to happen concurrently, other deployments will be waiting in a FIFO queue until the lock is released.&lt;/p&gt;

&lt;p&gt;The stage &lt;em&gt;‚ÄúCompute Changeset‚Äù&lt;/em&gt; ensures that the deployment is not a rollback. It verifies that the new version deployed does not correspond to a rollback by comparing the ancestry of the commits provided during the artefact registration at build time: since we automate deployments after the build process has completed, cases of rollback may occur when two changes are created in quick succession and the latest build completes earlier than the older one.&lt;/p&gt;

&lt;p&gt;After the stage &lt;em&gt;‚ÄúDeploy Canary‚Äù&lt;/em&gt; has completed, smoke test run. There are three kinds of tests executed at different stages of the pipeline: smoke, functional and security tests. Smoke tests directly reach the canary instance‚Äôs endpoint, by-passing load-balancers. If the smoke tests fail, the canary is immediately rolled back and this deployment is terminated.&lt;/p&gt;

&lt;p&gt;All tests are generated from the same builds as the artefact being tested and their versions must match during testing. To ensure that the right version of the test run and distinguish between the different kind of tests to perform, we provide additional metadata that will be passed by Conveyor to the tests system, known internally as Gandalf:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'default.libsonnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;service-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;group-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_smoke_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/smoke/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_functional_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/functional/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_security_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/security/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration with integration tests added.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, in parallel to the execution of the smoke tests, the canary is also being monitored from the moment its deployment has completed and for a predetermined duration. We leverage our integration with Datadog to allow engineers to select the alerts to monitor. If an alert is triggered during the monitoring period, and while the tests are executed, the canary is again rolled back, and the pipeline is terminated. Engineers can specify the alerts by adding them to the &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration file together with the monitoring duration:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'default.libsonnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;service-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;group-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_smoke_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/smoke/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_functional_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/functional/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gandalf_security_tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;repo.internal/path/to/my/security/tests&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;stg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;duration_seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;alarms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;datadog&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;alert_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12345678&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;datadog&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;alert_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23456789&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
&lt;p&gt;&lt;em&gt;Sample &lt;code class=&quot;highlighter-rouge&quot;&gt;conveyor.jsonnet&lt;/code&gt; configuration with alerts in staging added.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;When the smoke tests and monitor pass and the deployment of new artefacts is completed, the pipeline execution triggers functional and security tests. Unlike smoke tests, functional &amp;amp; security tests run only after that step, as they communicate with the cluster through load-balancers, impersonating other services.&lt;/p&gt;

&lt;p&gt;Before releasing the lock, release notes are generated to inform engineers of the delta of changes between the version they just released and the one currently running in production. Once the lock is released, the stage &lt;em&gt;‚ÄúCheck Policies‚Äù&lt;/em&gt; verifies that the parameters and variable of the deployment obeys a specific set of criteria, for example: if its service metadata is up-to-date in our service inventory, or if the base image used during deployment is sufficiently recent.&lt;/p&gt;

&lt;p&gt;Here‚Äôs how the policy stage, the engine, and the providers interact with each other:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Check Policy Stage&quot; src=&quot;/img/our-journey-to-continuous-delivery-at-grab/image11.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Check Policy Stage&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Spinnaker, each event of a pipeline‚Äôs execution updates the pipeline‚Äôs state in the database. The current state of the pipeline can be fetched by its API as a single JSON document, describing all information related to its execution: including its parameters, the contextual information related to each stage or even the response from the various interfacing components. The role of our &lt;em&gt;‚ÄúPolicy Check‚Äù&lt;/em&gt; stage is to query this JSON representation of the pipeline, to extract and transform the variables which are forwarded to our policy engine for validation. Our policy engine gathers judgements passed by different policy providers. If the validation by the policy engine fails, the deployment is not rolled back this time; however, promotion to production is not possible and the pipeline is immediately terminated.&lt;/p&gt;

&lt;p&gt;The journey through staging deployment finally ends with the stage &lt;em&gt;‚ÄúRegister Deployment‚Äù&lt;/em&gt;. This stage registers that a successful deployment was made in our staging environment as an artefact. Similarly to the policy check above, certain parameters of the deployment are picked up and consolidated into this document. We use this kind of artefact as proof for upcoming production deployment.&lt;/p&gt;

&lt;h3 id=&quot;continuing-our-journey-to-engineering-efficiency&quot;&gt;Continuing Our Journey to Engineering Efficiency&lt;/h3&gt;

&lt;p&gt;With the advancements made in continuous integration and deployment to staging, Conveyor has reduced the efforts needed by our engineers to just three clicks in its interface, when automated deployment is used. Even when the deployment is triggered manually, Conveyor gives the assurance that the parameters selected are valid and it does away with copy/pasting and human interactions across heterogeneous tools.&lt;/p&gt;

&lt;p&gt;In the sequel to this blog post, we‚Äôll dive into the improvements that we‚Äôve made to our production deployments and introduce a crucial concept that led to the creation of our proof for successful staging deployment. Finally, we‚Äôll cover the impact that Conveyor had on the continuous delivery of our backend services, by comparing our deployment velocity when we started two years ago versus where we are today.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;All these improvements in efficiency for our engineers would never have been possible without the hard work of all team members involved in the project, past and present: Evan Sebastian, Tanun Chalermsinsuwan, Aufar Gilbran, Deepak Ramakrishnaiah, Repon Kumar Roy (Kowshik), Su Han, Voislav Dimitrijevikj, Qijia Wang, Oscar Ng, Jacob Sunny, Subhodip Mandal, and many others who have contributed and collaborated with them.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Wed, 23 Sep 2020 10:23:44 +0000</pubDate>
        <link>https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab</guid>
        
        <category>Deployment</category>
        
        <category>CI</category>
        
        <category>Continuous Integration</category>
        
        <category>Continuous Deployment</category>
        
        <category>Deployment Process</category>
        
        <category>Cloud Agnostic</category>
        
        <category>Spinnaker</category>
        
        <category>Continuous Delivery</category>
        
        <category>Multi Cloud</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Uncovering the truth behind Lua and Redis data consistency</title>
        <description>&lt;p&gt;Our team at Grab uses Redis as one of our message queues. The Redis server is deployed in a master/replica setup. Quite recently, we have been noticing a spike in the CPU usage of the Redis replicas every time we deploy our service, even when the replicas are not in use and when there‚Äôs no read traffic to it. However, the issue is resolved once we reboot the replica.&lt;/p&gt;

&lt;p&gt;Because a reboot of the replica fixes the issue every time, we thought that it might be due to some Elasticache replication issues and didn‚Äôt pursue further. However, a recent Redis failover brought this to our attention again. After the failover, the problematic replica becomes the new master and its CPU immediately goes to 100% with the read traffic, which essentially means the cluster is not functional after the failover. And this time we investigated the issue with new vigour. What we found in our investigation led us to deep dive into the details of Redis replication and its implementation of Hash.&lt;/p&gt;

&lt;p&gt;Did you know that Redis master/replica can become inconsistent in certain scenarios?&lt;/p&gt;

&lt;p&gt;Did you know the encoding of Hash objects on the master and the replica are different even if the writing operations are exactly the same and in the same order? Read on to find out why.&lt;/p&gt;

&lt;h2 id=&quot;the-problem&quot;&gt;The problem&lt;/h2&gt;

&lt;p&gt;The following graph shows the CPU utilization of the master vs. the replica immediately after our service is deployed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/uncovering-the-truth-behind-lua-and-redis-data-consistency/cpuUtilization.png&quot; alt=&quot;Architecture diagram&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;CPU Utilization&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;From the graph, you can see the following CPU usage trends. Replica‚Äôs CPU usage:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Increases immediately after our service is deployed.&lt;/li&gt;
  &lt;li&gt;Spikes higher than the master after a certain time.&lt;/li&gt;
  &lt;li&gt;Get‚Äôs back to normal after a reboot.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cursory-investigation&quot;&gt;Cursory investigation&lt;/h2&gt;

&lt;p&gt;Because the spike occurs only when we deploy our service, we scrutinised all the scripts that were triggered immediately after the deployment. Lua monitor script was identified as a possible suspect. The script redistributes inactive service instances‚Äô messages in the queue to active service instances so that messages can be processed by other healthy instances.&lt;/p&gt;

&lt;p&gt;We ran a few experiments related to the Lua monitor script using the Redis &lt;a href=&quot;https://redis.io/commands/monitor&quot;&gt;monitor&lt;/a&gt; command to compare the script‚Äôs behaviour on master and the replica. A side note, because this command causes performance degradation, use it with discretion. Coming back to the script, we were surprised to note that the monitor script behaves differently between the master and the replica:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Redis executes the script separately on the master and the replica. We expected the script to execute only on master and the resulting changes to be replicated to the secondary.&lt;/li&gt;
  &lt;li&gt;The Redis command &lt;code class=&quot;highlighter-rouge&quot;&gt;HGETALL&lt;/code&gt; used in the script returns the hash keys in a different order on master compared to the replica.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Due to the above reasons, the script causes data inconsistencies between the master and its replica. From that point on, the data between the master and the replica keeps diverging till they become completely distinct. Due to the inconsistency, the data on the secondary does not get deleted correctly thereby growing into an extremely large dataset. Any further operations on the large dataset requires a higher CPU usage, which explains why the replica‚Äôs CPU usage is higher than the master.&lt;/p&gt;

&lt;p&gt;During replica reboots, the data gets synced and consistent again, which is why the CPU usage gets to normal values after rebooting.&lt;/p&gt;

&lt;h2 id=&quot;diving-deeper-on-hgetall&quot;&gt;Diving deeper on HGETALL&lt;/h2&gt;

&lt;p&gt;We knew that the keys of a hash are not ordered and we should not rely on the order. But it still puzzled us that the order is different even when the writing sequence is the same between the master and the replica. Plus the fact that the orders are always the same in our local environment with a similar setup made us even more curious.&lt;/p&gt;

&lt;p&gt;So to better understand the underlying magic of Redis and to avoid similar bugs in the future, we decided to hammer on and read the Redis source code to get more details.&lt;/p&gt;

&lt;h2 id=&quot;hgetall-command-handling-code&quot;&gt;HGETALL command handling code&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;HGETALL&lt;/code&gt; command is handled by the function &lt;code class=&quot;highlighter-rouge&quot;&gt;genericHgetallCommand&lt;/code&gt; and it further calls &lt;code class=&quot;highlighter-rouge&quot;&gt;hashTypeNext&lt;/code&gt; to iterate through the Hash object. A snippet of the code is shown as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/* Move to the next entry in the hash. Return C_OK when the next entry
 * could be found and C_ERR when the iterator reaches the end. */
int hashTypeNext(hashTypeIterator *hi) {
    if (hi-&amp;gt;encoding == OBJ_ENCODING_ZIPLIST) {
        // call zipListNext
    } else if (hi-&amp;gt;encoding == OBJ_ENCODING_HT) {
        // call dictNext
    } else {
        serverPanic(&quot;Unknown hash encoding&quot;);
    }
    return C_OK;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the code snippet, you can see that the Redis Hash object actually has two underlying representations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ZIPLIST&lt;/li&gt;
  &lt;li&gt;HASHTABLE (dict)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A bit of research online helped us understand that, to save memory, Redis chooses between the two hash representations based on the following limits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;By default, Redis stores the Hash object as a &lt;a href=&quot;https://redis.io/topics/memory-optimization&quot;&gt;zipped list&lt;/a&gt; when the hash has less than 512 entries and when each element‚Äôs size is smaller than 64 bytes.&lt;/li&gt;
  &lt;li&gt;If either limit is exceeded, Redis converts the list to a &lt;a href=&quot;https://github.com/antirez/redis/blob/3.2/src/t_hash.c#L40&quot;&gt;hashtable&lt;/a&gt;, and this is irreversible. That is, Redis won‚Äôt convert the hashtable back to a list again, even if the entries/size falls below the limit.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;eureka-moment&quot;&gt;Eureka moment&lt;/h2&gt;

&lt;p&gt;Based on this understanding, we checked the encoding of the problematic hash in staging.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;stg-bookings-qu-002.pcxebj.0001.apse1.cache.amazonaws.com:6379&amp;gt; object encoding queue_stats
&quot;hashtable&quot;

stg-bookings-qu-001.pcxebj.0001.apse1.cache.amazonaws.com:6379&amp;gt; object encoding queue_stats
&quot;ziplist&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;To our surprise, the encodings of the Hash object on the master and its replica were different.&lt;/strong&gt; Which means if we add or delete elements in the hash, the sequence of the keys won‚Äôt be the same due to hashtable operation vs. list operation!&lt;/p&gt;

&lt;p&gt;Now that we have identified the root cause, we were still curious about the difference in encoding between the master and the replica.&lt;/p&gt;

&lt;h3 id=&quot;how-could-the-underlying-representations-be-different&quot;&gt;How could the underlying representations be different?&lt;/h3&gt;

&lt;p&gt;We reasoned, ‚Äú&lt;em&gt;If the master and its replica‚Äôs writing operations are exactly the same and in the same order, why are the underlying representations still different?&lt;/em&gt;‚Äù&lt;/p&gt;

&lt;p&gt;To answer this, we further looked through the Redis source to find all the possible places that a Hash object‚Äôs representation could be changed and soon found the following code snippet:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/* Load a Redis object of the specified type from the specified file.
 * On success a newly allocated object is returned, otherwise NULL. */
robj *rdbLoadObject(int rdbtype, rio *rdb) {
  //...
  if (rdbtype == RDB_TYPE_HASH) {
    //...
    o = createHashObject();  // ziplist

    /* Too many entries? Use a hash table. */
    if (len &amp;gt; server.hash_max_ziplist_entries)
        hashTypeConvert(o, OBJ_ENCODING_HT);

    //...
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Reading through the code we understand the following behaviour:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When restoring from an RDB file, Redis creates a ziplist first for Hash objects.&lt;/li&gt;
  &lt;li&gt;Only when the size of the Hash object is greater than the &lt;code class=&quot;highlighter-rouge&quot;&gt;hash_max_ziplist_entries&lt;/code&gt;, the ziplist is converted to a hashtable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, if you have a Redis Hash object encoded as a &lt;code class=&quot;highlighter-rouge&quot;&gt;hashtable&lt;/code&gt; with its length less than &lt;code class=&quot;highlighter-rouge&quot;&gt;hash_max_ziplist_entries&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;(512)&lt;/code&gt; in the master, when you set up a replica, it is encoded as a &lt;code class=&quot;highlighter-rouge&quot;&gt;ziplist&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We were able to verify this behaviour in our local setup as well.&lt;/p&gt;

&lt;h3 id=&quot;how-did-we-fix-it&quot;&gt;How did we fix it?&lt;/h3&gt;

&lt;p&gt;We could use the following two approaches to address this issue:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable&lt;a href=&quot;https://redis.io/commands/eval#replicating-commands-instead-of-scripts&quot;&gt; script effect replication mode&lt;/a&gt;. This tells Redis to replicate the commands generated by the script instead of running the whole script on the replica. One disadvantage to using this approach is that it adds network traffic between the master and the replica.&lt;/li&gt;
  &lt;li&gt;Ensure the behaviour of the Lua monitor script is deterministic. In our case, we can do this by sorting the outputs of HKEYS/HGETALL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We chose the latter approach because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Hash object is pretty small ( &amp;lt; 30 elements) so the sorting overhead is low, less than 1ms for 100 elements based on our tests.&lt;/li&gt;
  &lt;li&gt;Replicating our script effect would end up replicating thousands of Redis writing commands on the secondary causing a much higher overhead compared to replicating just the script.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After the fix, the CPU usage of the replica remained in range after each deployment. This also prevented the Redis cluster from being destroyed in the event of a master failover.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;

&lt;p&gt;In addition to writing clear and maintainable code, it‚Äôs equally important to understand the underlying storage layer that you are dealing with to produce efficient and bug-free code.&lt;/p&gt;

&lt;p&gt;The following are some of the key learnings on Redis:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Redis does not guarantee the consistency between master and its replica nodes when Lua scripts are used. You have to ensure that the behaviour of the scripts are deterministic to avoid data inconsistency.&lt;/li&gt;
  &lt;li&gt;Redis replicates the whole Lua script instead of the resulting commands to the replica. However, this is the default behaviour and you can disable it.&lt;/li&gt;
  &lt;li&gt;To save memory, Redis uses different representations for Hash. Your Hash object could be stored as a list in memory or a hashtable. This is not guaranteed to be the same across the master and its replicas.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 08:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/uncovering-the-truth-behind-lua-and-redis-data-consistency</link>
        <guid isPermaLink="true">https://engineering.grab.com/uncovering-the-truth-behind-lua-and-redis-data-consistency</guid>
        
        <category>Redis</category>
        
        <category>Lua Scripts</category>
        
        <category>High CPU Usage</category>
        
        <category>Data Consistency</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Securing and managing multi-cloud Presto Clusters with Grab‚Äôs DataGateway</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Data is the lifeblood of Grab and the insights we gain from it drive all the most critical business decisions made by Grabbers and our leaders every day.&lt;/p&gt;

&lt;p&gt;Grab‚Äôs Data Engineering (DE) team is responsible for maintaining the data platform, which consists of data pipelines, job schedulers, and the query/computation engines that are the key components for generating insights from data. SQL is the core language for analytics at Grab and as of early 2020, our Presto platform serves about 200 user groups that add up to 500 users who run 350,000 queries every day. These queries span across 10,000 tables that process up to 1PB of data daily.&lt;/p&gt;

&lt;p&gt;In 2016, we started the DataGateway project to enable us to manage data access for the hundreds of Grabbers who needed access to &lt;a href=&quot;https://aws.amazon.com/big-data/what-is-presto/&quot;&gt;Presto&lt;/a&gt; for their work. Since then, DataGateway has grown to become much more than just an access control mechanism for Presto. In this blog, we want to share what we‚Äôve achieved since the initial launch of the project.&lt;/p&gt;

&lt;h2 id=&quot;the-problems-we-wanted-to-solve&quot;&gt;The problems we wanted to solve&lt;/h2&gt;

&lt;p&gt;As we were reviewing the key challenges around data access in Grab and assessing possible solutions, we came up with this prioritized list of user requirements we wanted to work on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use a single endpoint to serve everyone.&lt;/li&gt;
  &lt;li&gt;Manage user access to clusters, schemas, tables, and fields.&lt;/li&gt;
  &lt;li&gt;Provide seamless user experience when presto clusters are scaled up/down, in/out, or provisioned/decommissioned.&lt;/li&gt;
  &lt;li&gt;Capture audit trail of user activities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To provide Grabbers with the critical need of interactive querying, as well as performing extract, transform, load (ETL) jobs, we evaluated several technologies. Presto was among the ones we evaluated, and was what we eventually chose although it didn‚Äôt meet all of our requirements out of the box. In order to address these gaps, we came up with the idea of a security gateway for the Presto compute engine that could also act as a load balancer/proxy, this is how we ended up creating the DataGateway.&lt;/p&gt;

&lt;p&gt;DataGateway is a service that sits between clients and Presto clusters. It is essentially a smart HTTP proxy server that is an abstraction layer on top of the Presto clusters that handles the following actions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Parse incoming SQL statements to get requested schemas, tables, and fields.&lt;/li&gt;
  &lt;li&gt;Manage user Access Control List (ACL) to limit users‚Äô data access by checking against the SQL parsing results.&lt;/li&gt;
  &lt;li&gt;Manage users‚Äô cluster access.&lt;/li&gt;
  &lt;li&gt;Redirect users‚Äô traffic to the authorized clusters.&lt;/li&gt;
  &lt;li&gt;Show meaningful error messages to users whenever the query is rejected or exceptions from clusters are encountered.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;anatomy-of-datagateway&quot;&gt;Anatomy of DataGateway&lt;/h2&gt;

&lt;p&gt;The DataGateway‚Äôs key components are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;API Service&lt;/li&gt;
  &lt;li&gt;SQL Parser&lt;/li&gt;
  &lt;li&gt;Auth framework&lt;/li&gt;
  &lt;li&gt;Administration UI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We leveraged Kubernetes to run all these components as microservices.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Figure 1. DataGateway Key Components&quot; height=&quot;90%&quot; width=&quot;90%&quot; src=&quot;/img/data-gateway/image3.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Figure 1. DataGateway Key Components&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;api-service&quot;&gt;API Service&lt;/h3&gt;

&lt;p&gt;This is the component that manages all users and cluster-facing processes. We integrated this service with the Presto API, which means it appears to be the same as a Presto cluster to a client. It accepts query requests from clients, gets the parsing result and runs authorization from the SQL Parser and the Auth Framework.&lt;/p&gt;

&lt;p&gt;If everything is good to go, the API Service forwards queries to the assigned clusters and continues the entire query process.&lt;/p&gt;

&lt;h3 id=&quot;auth-framework&quot;&gt;Auth Framework&lt;/h3&gt;

&lt;p&gt;This handles both authentication and authorization requests. It stores the ACL of users and communicates with the API Service and the SQL Parser to run the entire authentication process. But why is it a microservice instead of a module in API Service, you ask? It‚Äôs because we keep evolving the security checks at Grab to ensure that everything is compliant with our security requirements, especially when dealing with data.&lt;/p&gt;

&lt;p&gt;We wanted to make it flexible to fulfill ad-hoc requests from the security team without affecting the API Service. Furthermore, there are different authentication methods out there that we might need to deal with (OAuth2, SSO, you name it). The API Service supports multiple authentication frameworks that enable different authentication methods for different users.&lt;/p&gt;

&lt;h3 id=&quot;sql-parser&quot;&gt;SQL Parser&lt;/h3&gt;

&lt;p&gt;This is a SQL parsing engine to get schema, tables, and fields by reading SQL statements. Since Presto SQL parsing works differently in each version, we would compile multiple SQL Parsers that are identical to the Presto clusters we run. The SQL Parser becomes the single source of truth.&lt;/p&gt;

&lt;h3 id=&quot;admin-ui&quot;&gt;Admin UI&lt;/h3&gt;

&lt;p&gt;This is a UI for Presto administrators to manage clusters and user access, as well as to select an authentication framework, making it easier for the administrators to deal with the entire ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;how-we-deployed-datagateway-using-kubernetes&quot;&gt;How we deployed DataGateway using Kubernetes&lt;/h2&gt;

&lt;p&gt;In the past couple of years, we‚Äôve had significant growth in workloads from analysts and data scientists. As we were very enthusiastic about Kubernetes, DataGateway was chosen as one of the earliest services for deployment in Kubernetes. DataGateway in Kubernetes is known to be highly available and fully scalable to handle traffic from users and systems.&lt;/p&gt;

&lt;p&gt;We also tested the &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough&quot;&gt;HPA feature of Kubernetes&lt;/a&gt;, which is a dynamic scaling feature to scale in or out the number of pods based on actual traffic and resource consumption.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Figure 2. DataGateway deployment using Kubernetes&quot; height=&quot;90%&quot; width=&quot;90%&quot; src=&quot;/img/data-gateway/image1.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Figure 2. DataGateway deployment using Kubernetes&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;functionality-of-datagateway&quot;&gt;Functionality of DataGateway&lt;/h2&gt;

&lt;p&gt;This section highlights some of the ways we use DataGateway to manage our Presto ecosystem efficiently.&lt;/p&gt;

&lt;h3 id=&quot;restrict-users-based-on-schematable-level-access&quot;&gt;Restrict users based on Schema/Table level access&lt;/h3&gt;

&lt;p&gt;In a setup where a Presto cluster is deployed on &lt;a href=&quot;https://aws.amazon.com/emr&quot;&gt;AWS Amazon Elastic MapReduce (EMR)&lt;/a&gt; or &lt;a href=&quot;https://aws.amazon.com/eks&quot;&gt;Elastic Kubernetes Service (EKS)&lt;/a&gt;, we configure an &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html&quot;&gt;IAM&lt;/a&gt; role and attach it to the EMR or EKS nodes. The IAM role is set to limit the access to S3 storage. However, the IAM only provides bucket-level and file-level control; it doesn‚Äôt meet our requirements to have schema, table, and column-level ACLs. That‚Äôs how DataGateway is found useful in such scenarios.&lt;/p&gt;

&lt;p&gt;One of the DataGateway services is an SQL Parser. As previously covered, this is a service that parses and digs out schemas and tables involved in a query. The API service receives the parsing result and checks against the ACL of users, and decides whether to allow or reject the query. This is a remarkable improvement in our security control since we now have another layer to restrict access, on top of the S3 storage. We‚Äôve implemented an SQL-based access control down to table level.&lt;/p&gt;

&lt;p&gt;As shown in the Figure 3, user A is trying run a SQL statement &lt;code class=&quot;highlighter-rouge&quot;&gt;select * from locations.cities&lt;/code&gt;. The SQL Parser reads the statement and tells the API service that user A is trying to read data from the table &lt;code class=&quot;highlighter-rouge&quot;&gt;cities&lt;/code&gt; in the schema &lt;code class=&quot;highlighter-rouge&quot;&gt;locations&lt;/code&gt;. Then, the API service checks against the ACL of user A. The service finds that user A has only read access to table &lt;code class=&quot;highlighter-rouge&quot;&gt;countries&lt;/code&gt; in schema &lt;code class=&quot;highlighter-rouge&quot;&gt;locations&lt;/code&gt;. Eventually, the API service denies this attempt because user A doesn‚Äôt have read access to table &lt;code class=&quot;highlighter-rouge&quot;&gt;cities&lt;/code&gt; in the schema &lt;code class=&quot;highlighter-rouge&quot;&gt;locations&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Figure 3. An example of how to check user access to run SQL statements&quot; height=&quot;90%&quot; width=&quot;90%&quot; src=&quot;/img/data-gateway/image5.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Figure 3. An example of how to check user access to run SQL statements&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The above flow shows an access denied result because the user doesn‚Äôt have the appropriate permissions.&lt;/p&gt;

&lt;h3 id=&quot;seamless-user-experience-during-the-emr-migration&quot;&gt;Seamless User Experience during the EMR migration&lt;/h3&gt;

&lt;p&gt;We use AWS EMR to deploy Presto as an SQL query engine since deployment is really easy. However, without DataGateway, any EMR operations such as terminations, new cluster deployment, config changes, and version upgrades, would require quite a bit of user involvement. We would sometimes need users to make changes on their side. For example, request users to change the endpoints to connect to suitable clusters.&lt;/p&gt;

&lt;p&gt;With DataGateway, ACLs exist for each of the user accounts. The ACL includes the list of EMR clusters that users are allowed to access. As a Presto access management platform, here the DataGateway redirects user traffics to an appropriate cluster based on the ACL, like a proxy. Users always connect to the same endpoint we offer, which is the DataGateway. To switch over from one cluster to another, we just need to edit the cluster ACL and everything is handled seamlessly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Figure 4. Cluster switching using DataGateway&quot; height=&quot;90%&quot; width=&quot;90%&quot; src=&quot;/img/data-gateway/image4.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Figure 4. Cluster switching using DataGateway&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 4 highlights the case when we‚Äôre switching EMR from one cluster to another. No changes are required from users.&lt;/p&gt;

&lt;p&gt;We executed the migration of our entire Presto platform from an AWS EMR instance to another AWS EMR instance using the same methodology. The migrations were executed with little to no disruption for our users. We were able to move 40 clusters with hundreds of users. They were able to issue millions of queries daily in a few phases over a couple of months.&lt;/p&gt;

&lt;p&gt;In most cases, users didn‚Äôt have to make any changes on their end, they just continued using Presto as usual while we made the changes in the background.&lt;/p&gt;

&lt;h3 id=&quot;multi-cloud-data-lakepresto-cluster-maintenance&quot;&gt;Multi-Cloud Data Lake/Presto Cluster maintenance&lt;/h3&gt;

&lt;p&gt;Recently, we started to build and maintain data lakes not just in one cloud, but two - in AWS and Azure. Since most end-users are AWS-based, and each team has their own AWS sub-account to run their services and workloads, it would be a nightmare to bridge all the connections and access routes between these two clouds from end-to-end, sub-account by sub-account.&lt;/p&gt;

&lt;p&gt;Here, the DataGateway plays the role of the multi-cloud gateway. Since all end-users‚Äô AWS sub-accounts have peered to DataGateway‚Äôs network, everything becomes much easier to handle.&lt;/p&gt;

&lt;p&gt;For end-users, they retain the same Presto connection profile. The DE team then handles the connection setup from DataGateway to Azure, and also the deployment of Presto clusters in Azure.&lt;/p&gt;

&lt;p&gt;When all is set, end-users use the same endpoint to DataGateway. We offer a feature called &lt;em&gt;Cluster Switch&lt;/em&gt; that allows users to switch between AWS Presto cluster and Azure Presto Cluster on the fly by filling in parameters on the connection string. This feature allows users to switch to their target Presto cluster without any endpoint changes. The switch works instantly whenever they do the change. That means users can run different queries in different clusters based on their requirements.&lt;/p&gt;

&lt;p&gt;This feature has helped the DE team to maintain Presto Cluster easily. We can spin up different Presto clusters for different teams, so that each team has their own query engine to run their queries with dedicated resources.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Figure 5. Sub-account connections and Queries&quot; height=&quot;75%&quot; width=&quot;75%&quot; src=&quot;/img/data-gateway/image6.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Figure 5. Sub-account connections and Queries&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 shows an example of how sub-accounts connect to DataGateway and run queries on resources in different clouds and clusters.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;figure&gt;
    &lt;img alt=&quot;Figure 6. Sample scenario without DataGateway&quot; height=&quot;90%&quot; width=&quot;90%&quot; src=&quot;/img/data-gateway/image2.png&quot; /&gt;
    &lt;figcaption&gt;&lt;em&gt;Figure 6. Sample scenario without DataGateway&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 6 shows a scenario of what would happen if DataGatway doesn‚Äôt exist. Each of the accounts would have to maintain its own connections, &lt;a href=&quot;https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html&quot;&gt;Virtual Private C&lt;/a&gt;&lt;a href=&quot;https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html&quot;&gt;loud (VPC)&lt;/a&gt; peering, and express link to connect to our Presto resources.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;DataGateway is playing a key role in Grab‚Äôs entire Presto ecosystem. It helps us manage user access and cluster selections on a single endpoint, ensuring that everyone is running their Presto queries on the same place. It also helps distribute workload to different types and versions of Presto clusters.&lt;/p&gt;

&lt;p&gt;When we started to deploy the DataGateway on Kubernetes, our vision for the Presto ecosystem underwent an epic change as it further motivated us to continuously improve. Since then, we‚Äôve had new ideas on deployment method/pipeline, microservice implementations, scaling strategy, resource control, we even made use of Kubernetes and designed an on-demand, container-based Presto cluster provisioning engine. We‚Äôll share this in another engineering blog, so do stay tuned!.&lt;/p&gt;

&lt;p&gt;We also made crucial enhancements on data access control as we extended Presto‚Äôs access controls down to the schema/table-level.&lt;/p&gt;

&lt;p&gt;In day-to-day operations, especially when we started to implement data lake in multiple clouds, DataGateway solved a lot of implementation issues. DataGateway made it simpler to switch a user‚Äôs Presto cluster from one cloud to another or allow a user to use a different Presto cluster using parameters. DataGateway allowed us to provide a seamless experience to our users.&lt;/p&gt;

&lt;p&gt;Looking forward, we‚Äôve more and more ideas for our Presto ecosystem, such Spark DataGateway or AWS Athena integrations, to keep our data safe at any time and to provide our users with a smoother experience when dealing with data used for analysis or research.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Vinnson Lee on behalf of the Presto Development Team at Grab - Edwin Law, Qui Hieu Nguyen, Rahul Penti, Wenli Wan, Wang Hui and the Data Engineering Team.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Aug 2020 08:12:56 +0000</pubDate>
        <link>https://engineering.grab.com/data-gateway</link>
        <guid isPermaLink="true">https://engineering.grab.com/data-gateway</guid>
        
        <category>Engineering</category>
        
        <category>Presto</category>
        
        <category>Data</category>
        
        <category>Data Pipeline</category>
        
        <category>Access Control</category>
        
        <category>Workload Distribution</category>
        
        <category>Cluster</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Go Modules- A guide for monorepos (Part 2)</title>
        <description>&lt;p&gt;This is the second post on the Go module series, which highlights Grab‚Äôs experience working with Go modules in a multi-module monorepo. In this article, we‚Äôll focus on suggested solutions for catching unexpected changes to the &lt;code class=&quot;highlighter-rouge&quot;&gt;go.mod&lt;/code&gt; file and addressing dependency issues. We‚Äôll also cover automatic upgrades and other learnings uncovered from the initial obstacles in using Go modules.&lt;/p&gt;

&lt;h2 id=&quot;vendoring-process-issues&quot;&gt;Vendoring process issues&lt;/h2&gt;

&lt;p&gt;Our previous vendoring process fell solely on the developer who wanted to add or update a dependency. However, it was often the case that the developer came across many unexpected changes, due to previous vendoring attempts, accidental imports and changes to dependencies.&lt;/p&gt;

&lt;p&gt;The developer would then have to resolve these issues before being able to make a change, costing time and causing frustration with the process. It became clear that it wasn‚Äôt practical to expect the developer to catch all of the potential issues while vendoring, especially since Go modules itself was new and still in development.&lt;/p&gt;

&lt;h2 id=&quot;avoiding-unexpected-changes&quot;&gt;Avoiding unexpected changes&lt;/h2&gt;

&lt;p&gt;Reluctantly, we added a check to our CI process which ran on every merge request. This helped ensure that there are no unexpected changes required to go mod. This added time to every build and often flagged a failure, but it saved a lot of post-merge hassle. We then realized that we should have done this from the beginning.&lt;/p&gt;

&lt;p&gt;Since we hadn‚Äôt enabled Go modules for builds yet, we couldn‚Äôt rely on the &lt;a href=&quot;https://godoc.org/cmd/go%23hdr-Maintaining_module_requirements&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;\mod=readonly&lt;/code&gt;&lt;/a&gt; flag. We implemented the check by running &lt;code class=&quot;highlighter-rouge&quot;&gt;go mod vendor&lt;/code&gt; and then checking the resulting difference.&lt;/p&gt;

&lt;p&gt;If there were any changes to &lt;code class=&quot;highlighter-rouge&quot;&gt;go.mod&lt;/code&gt; or the vendor directory, the merge request would get rejected. This worked well in ensuring the integrity of our &lt;code class=&quot;highlighter-rouge&quot;&gt;go.mod&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;roadblocks-and-learnings&quot;&gt;Roadblocks and learnings&lt;/h2&gt;

&lt;p&gt;However, as this was the first time we were using Go modules on our CI system, it uncovered some more problems.&lt;/p&gt;

&lt;h3 id=&quot;private-repository-access&quot;&gt;Private repository access&lt;/h3&gt;

&lt;p&gt;There was the problem of accessing private repositories. We had to ensure that the CI system was able to clone all of our private repositories as well as the main monorepo, by adding the relevant SSH deploy keys to the repository.&lt;/p&gt;

&lt;h3 id=&quot;false-positives&quot;&gt;False positives&lt;/h3&gt;

&lt;p&gt;The check sometimes fired &lt;code class=&quot;highlighter-rouge&quot;&gt;false positives&lt;/code&gt; - detecting a go mod failure when there were no changes. This was often due to network issues, especially when the modules are hosted by less reliable third-party servers. This is somewhat solved in Go 1.13 onwards with the introduction of &lt;a href=&quot;https://golang.org/cmd/go/%23hdr-Module_downloading_and_verification&quot;&gt;proxy servers&lt;/a&gt;, but our workaround was simply to retry the command several times.&lt;/p&gt;

&lt;p&gt;We also avoided adding dependencies hosted by a domain that we haven‚Äôt seen before, unless absolutely necessary.&lt;/p&gt;

&lt;h3 id=&quot;inconsistent-go-versions&quot;&gt;Inconsistent Go versions&lt;/h3&gt;

&lt;p&gt;We found several inconsistencies between Go versions - running go mod vendor on one Go version gave different results to another. One example was a &lt;a href=&quot;https://github.com/golang/go/issues/29278&quot;&gt;change to the checksums&lt;/a&gt;. These inconsistencies are less common now, but still remain between Go 1.12 and later versions. The only solution is to stick to a single version when running the vendoring process.&lt;/p&gt;

&lt;h2 id=&quot;automated-upgrades&quot;&gt;Automated upgrades&lt;/h2&gt;

&lt;p&gt;There are benefits to using Go modules for vendoring. It‚Äôs faster than previous solutions, better supported by the community and it‚Äôs part of the language, so it doesn‚Äôt require any extra tools or wrappers to use it.&lt;/p&gt;

&lt;p&gt;One of the most useful benefits from using Go modules is that it enables automated upgrades of dependencies in the go.mod file - and it becomes more useful as more third-party modules adopt Go modules and semantic versioning.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/go-module-a-guide-for-monorepos-part-2/image1.png&quot; alt=&quot;Automated updates workflow&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Automated updates workflow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We call our solution for automating updates at Grab the AutoVend Bot. It is built around a single Go command, &lt;code class=&quot;highlighter-rouge&quot;&gt;go list -m -u all&lt;/code&gt;, which finds and lists available updates to the dependencies listed in &lt;code class=&quot;highlighter-rouge&quot;&gt;go.mod&lt;/code&gt; (add &lt;code class=&quot;highlighter-rouge&quot;&gt;\json&lt;/code&gt; for JSON output). We integrated the bot with our development workflow and change-request system to take the output from this command and create merge requests automatically, one per update.&lt;/p&gt;

&lt;p&gt;Once the merge request is approved (by a human, after verifying the test results), the bot would push the change. We have hundreds of dependencies in our main monorepo module, so we‚Äôve scheduled it to run a small number each day so we‚Äôre not overwhelmed.&lt;/p&gt;

&lt;p&gt;By reducing the manual effort required to update dependencies to almost nothing, we have been able to apply hundreds of updates to our dependencies, and ensure our most critical dependencies are on the latest version. This not only helps keep our dependencies free from bugs and security flaws, but it makes future updates far easier and less impactful by reducing the set of changes needed.&lt;/p&gt;

&lt;h2 id=&quot;in-summary&quot;&gt;In Summary&lt;/h2&gt;

&lt;p&gt;Using Go modules for vendoring has given us valuable and low-risk exposure to the feature. We have been able to detect and solve issues early, without affecting our regular builds, and develop tooling that‚Äôll help us in future.&lt;/p&gt;

&lt;p&gt;Although Go modules is part of the standard Go toolchain, it shouldn‚Äôt be viewed as a complete &lt;em&gt;off the shelf&lt;/em&gt; solution that can be dropped into a codebase, especially a monorepo.&lt;/p&gt;

&lt;p&gt;Like many other Go tools, the Modules feature comprises many small, focused tools that work best when combined together with other code. By embracing this concept and leveraging things like go list, go mod graph and go mod vendor, Go modules can be made to integrate into existing workflows, and deliver the benefits of structured versioning and reproducible builds.&lt;/p&gt;

&lt;p&gt;I hope you have enjoyed this article on using Go modules and vendoring within a monorepo.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;

&lt;h4 id=&quot;credits&quot;&gt;Credits&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The cute Go gopher logo for this blog‚Äôs cover image was inspired by Renee French‚Äôs original work.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 12 Aug 2020 10:02:00 +0000</pubDate>
        <link>https://engineering.grab.com/go-module-a-guide-for-monorepos-part-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/go-module-a-guide-for-monorepos-part-2</guid>
        
        <category>Go</category>
        
        <category>Monorepo</category>
        
        <category>Vendoring</category>
        
        <category>Vendors</category>
        
        <category>Libraries</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
