<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 29 Jan 2026 07:35:27 +0000</pubDate>
    <lastBuildDate>Thu, 29 Jan 2026 07:35:27 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>Cursor at Grab: Adoption and impact</title>
        <description>&lt;h2 id=&quot;adoption-overview&quot;&gt;Adoption overview&lt;/h2&gt;

&lt;p&gt;The illustration below encapsulates how Cursor is scaled across Grab, achieving rapid and widespread adoption that accelerated software development and empowered non-technical teams to build solutions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cursor-at-grab/cursor-figure-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Adoption overview of AI tool Cursor in Grab.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;multi-tool-strategy&quot;&gt;Multi-tool strategy&lt;/h3&gt;

&lt;p&gt;Grab embraces a multi-tool strategy for AI coding assistants. Rather than committing to a single solution, we experiment with multiple tools simultaneously, allowing us to compare outcomes and adopt what works. This approach keeps us flexible in a space that evolves quickly. We covered this philosophy in a &lt;a href=&quot;https://www.grab.com/sg/inside-grab/stories/beyond-one-size-fits-all-why-grab-embraces-multiple-ai-coding-assistants/&quot;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;growth&quot;&gt;Growth&lt;/h3&gt;

&lt;p&gt;We introduced &lt;a href=&quot;https://cursor.com/&quot;&gt;Cursor&lt;/a&gt; in late 2024 as one of several tools in our AI engineering toolkit. Adoption grew quickly—98% of tech Grabbers became monthly active users, and about 75% use it weekly. For comparison, Google’s &lt;a href=&quot;https://services.google.com/fh/files/misc/2025_state_of_ai_assisted_software_development.pdf&quot;&gt;2025 State of AI-Assisted Software Development&lt;/a&gt; report highlights that even among high-performing teams, AI coding tool adoption seldom surpasses 70%. Notably, Cursor’s appeal extended beyond engineering, with non-technical teams incorporating it into their workflows.&lt;/p&gt;

&lt;p&gt;A standout metric is Cursor’s suggestion acceptance rate, which is around 50%, surpassing the industry average of 30%. This indicates two key insights: first, the suggestions are sufficiently relevant for engineers to accept them half of the time; second, engineers maintain a critical review process rather than accepting suggestions indiscriminately. We attribute this relevance to continuous feedback loops and environment-specific tuning, ensuring suggestions remain aligned with Grab’s codebase and conventions.&lt;/p&gt;

&lt;h2 id=&quot;extent-of-adoption&quot;&gt;Extent of adoption&lt;/h2&gt;

&lt;p&gt;Raw adoption figures don’t provide the complete picture. We aimed to determine whether engineers were truly incorporating Cursor into their daily workflows or merely experimenting with it sporadically.&lt;/p&gt;

&lt;p&gt;The data indicates genuine integration. Approximately half of Cursor users engage with it 10 or more days each month, with some teams achieving full adoption. Over 98% of merge requests now incorporate Cursor in some capacity. Engineers actively share tips and workflows via a dedicated Slack channel, fostering an organic knowledge base.&lt;/p&gt;

&lt;p&gt;Across various teams, we’ve observed significant transitions from light usage to moderate and power user levels over the past six months.&lt;/p&gt;

&lt;h2 id=&quot;engineer-utilization-patterns&quot;&gt;Engineer utilization patterns&lt;/h2&gt;

&lt;p&gt;The most common patterns we see are unit test generation, code refactoring, cross-repository navigation, bug fixing, and automation of routine tasks like API scaffolding or commit messages.&lt;/p&gt;

&lt;p&gt;Test generation is particularly popular. Writing tests manually is tedious, and Cursor’s ability to generate and iteratively refine tests has become a standard part of many engineers’ workflows. Cross-repository navigation helps with onboarding and context-switching—engineers can ask Cursor questions about unfamiliar codebases rather than hunting through documentation.&lt;/p&gt;

&lt;p&gt;Qualitative feedback confirms what the adoption numbers suggest: tasks that took a full day to complete now take hours. Engineers report tackling refactors and test additions they would have otherwise skipped due to time pressure. Cursor doesn’t just speed up existing work; it makes previously impractical work feasible.&lt;/p&gt;

&lt;h2 id=&quot;integration-with-grabs-stack&quot;&gt;Integration with Grab’s stack&lt;/h2&gt;

&lt;p&gt;Integrating Cursor effectively at Grab required custom tooling. We built solutions for monorepo indexing to handle Grab’s scale and to distribute preconfigured rules that align Cursor’s suggestions with Grab-specific coding conventions. This integration ensures that Cursor understands our environment rather than offering generic suggestions.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;Cursor is one tool in a broader toolkit. Our multi-tool strategy means we’re also investing in terminal-based workflows and &lt;a href=&quot;https://engineering.grab.com/the-birth-of-grab-gpt&quot;&gt;GrabGPT&lt;/a&gt; for internal knowledge retrieval. Different tools suit different workflows. The aim is to empower users, not to restrict them.&lt;/p&gt;

&lt;p&gt;Beyond engineering, we’re expanding AI-assisted development to new personas. Our AI Upskilling workshops have trained several hundred Grabbers across five countries, including executive committee members and senior leaders who have built and deployed their own apps. Non-engineers in Financial Planning and Analysis (FP&amp;amp;A), Operations, and regional teams are now building tools with the assitance of AI to solve their own pain points.&lt;/p&gt;

&lt;p&gt;Our product design team has launched an initiative empowering designers to directly implement production fixes. Designers have successfully merged hundreds of merge requests, often with same-day turnaround, facilitating quicker iterations on UI fixes without the engineering queue delay. This process requires designers to be trained in Git fundamentals prior to gaining access, with initial reviews conducted by design managers.&lt;/p&gt;

&lt;p&gt;Cursor has become part of daily work at Grab. But adoption is only half the question — the other half is impact. We’ve been running a parallel effort to measure productivity effects rigorously, using fixed-effects regression to isolate Cursor’s contribution from other factors. Early findings show a dose-response relationship: productivity gains scale with usage intensity, and the effects hold up to statistical scrutiny.&lt;/p&gt;

&lt;p&gt;We will address the measurement methodology and present our findings in a subsequent post.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://www.grab.careers/en/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Jan 2026 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/cursor-at-grab-adoption-and-impact</link>
        <guid isPermaLink="true">https://engineering.grab.com/cursor-at-grab-adoption-and-impact</guid>
        
        <category>AI</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Docker lazy loading at Grab: Accelerating container startup times</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we’ve been exploring ways to dramatically reduce container startup times for our data platforms. Large container images for services like Airflow and Spark Connect were taking minutes to download, causing slow cold starts and poor auto-scaling performance. This blog post shares our journey implementing Docker image lazy loading using eStargz and Seekable OCI (SOCI) technologies, the results we achieved, and the lessons learned along the way.&lt;/p&gt;

&lt;h2 id=&quot;results-the-numbers-speak-for-themselves&quot;&gt;Results: The numbers speak for themselves&lt;/h2&gt;

&lt;h3 id=&quot;benchmark-results&quot;&gt;Benchmark results&lt;/h3&gt;

&lt;p&gt;Our initial testing on fresh nodes (nodes without cached images) showed dramatic improvements in image pull times as shown in &lt;strong&gt;Figure 1&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/docker-lazy-loading/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Table of results.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The key advantage of lazy loading is the reduction in image pull time, especially on “fresh” nodes that do not have the image cached. By analyzing detailed pod events, we can see the precise impact of using the stargz snapshotter.&lt;/p&gt;

&lt;p&gt;During our SOCI benchmark testing, we observed an important distinction between SOCI and eStargz: &lt;strong&gt;SOCI maintains the same application startup time as standard images, while eStargz takes longer&lt;/strong&gt;. For example, with Airflow, both overlayFS and SOCI achieved 5.0 seconds startup time, while eStargz took 25.0 seconds. This demonstrates that lazy loading doesn’t eliminate download time; it redistributes it. SOCI’s approach of maintaining separate indexes allows it to optimize the download-to-startup time trade-off more effectively, keeping application startup performance on par with standard images while still dramatically reducing image pull time.&lt;/p&gt;

&lt;h2 id=&quot;production-performance&quot;&gt;Production performance&lt;/h2&gt;

&lt;p&gt;The production deployment of SOCI lazy loading has delivered significant, measurable improvements across our data platforms. Both Airflow and Spark Connect now experience 30-40% faster startup times, directly improving our ability to handle traffic spikes and scale efficiently. These improvements translate to better auto-scaling responsiveness, reduced resource waste during initialization, and improved user experience for data processing workloads. The sustained performance gains observed over time demonstrate that lazy loading is a stable, production-ready optimization that delivers consistent value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2 and 3&lt;/strong&gt; illustrates the P95 startup time improvements for both services:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/docker-lazy-loading/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Production results: Airflow P95 startup time. &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/docker-lazy-loading/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Production results: Spark Connect P95 startup time.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It is important to note that P95 startup time includes both the image download/pull time and the application startup time itself. This metric captures the entire system performance for both cold and hot starts on fresh and hot nodes, showing the overall system improvement rather than just cold start performance.&lt;/p&gt;

&lt;p&gt;During the production deployment and monitoring, we gained valuable insights on SOCI configuration tuning. Following AWS’s recommended configuration from their blog on &lt;a href=&quot;https://aws.amazon.com/blogs/containers/introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/&quot;&gt;Introducing Seekable OCI: Parallel Pull Mode for Amazon EKS&lt;/a&gt;, we optimized our SOCI snapshotter settings:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Increased &lt;em&gt;max_concurrent_downloads_per_image&lt;/em&gt; from 5 to 10.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increased &lt;em&gt;max_concurrent_unpacks_per_image&lt;/em&gt; from 3 to 10.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increased &lt;em&gt;concurrent_download_chunk_size&lt;/em&gt; from 8MB to 16MB (aligning with AWS’s recommendation for Elastic Container Registry (ECR)).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This configuration tuning led to a significant performance improvement: &lt;strong&gt;image download time on a fresh node was reduced from 60 seconds to 24 seconds, representing a 60% improvement&lt;/strong&gt;. The key lesson here is that default SOCI configurations may not be optimal for all environments, and tuning these parameters based on your infrastructure (especially when using ECR) can yield substantial gains.&lt;/p&gt;

&lt;h2 id=&quot;technical-background-how-docker-lazy-loading-works&quot;&gt;Technical background: How Docker lazy loading works&lt;/h2&gt;

&lt;h3 id=&quot;container-root-filesystem-rootfs-and-file-organization&quot;&gt;Container root filesystem (rootfs) and file organization&lt;/h3&gt;

&lt;p&gt;A container’s root filesystem, or rootfs, is the directory structure that the container sees as its root &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(/)&lt;/code&gt;. It contains all the files and directories necessary for an application to run, including the application itself, its dependencies, system libraries, and configuration files. It’s an isolated filesystem, separate from the host machine’s filesystem.&lt;/p&gt;

&lt;p&gt;The rootfs is built from a series of read-only layers that come from the container image. Each instruction in an image’s Dockerfile creates a new layer, representing a set of filesystem changes. When a container is launched, a new writable layer, often called the “container layer,” is added on top of the stack of read-only image layers. Any changes made to the running container, such as writing new files or modifying existing ones, are written to this writable layer. The underlying image layers remain untouched. This is known as a copy-on-write (CoW) mechanism.&lt;/p&gt;

&lt;p&gt;In containerd, a snapshotter is a plugin responsible for managing container filesystems. Its primary job is to take the layers of an image and assemble them into a rootfs for a container. The default snapshotter in containerd is &lt;strong&gt;overlayFS&lt;/strong&gt;, which uses the Linux kernel’s OverlayFS driver to efficiently stack layers. To assemble the rootfs, the overlayFS snapshotter creates a “merged” view of the read-only image layers:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/docker-lazy-loading/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. How OverlayFS assembles the container filesystem.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;lowerdir&lt;/strong&gt;: The read-only image layers are used as the lowerdir in OverlayFS. These are the immutable layers from the container image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;upperdir&lt;/strong&gt;: A new, empty directory is created to be the upperdir. This is the writable layer for the container where any changes are stored.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;merged&lt;/strong&gt;: The merged directory is the unified view of the lowerdir and upperdir. This is what is presented to the container as its rootfs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a container reads a file, it’s read from the merged view. When a container writes a file, it’s written to the upperdir using a copy-on-write mechanism. This is an efficient way to manage container filesystems, as it avoids duplicating files and allows for fast container startup.&lt;/p&gt;

&lt;h3 id=&quot;the-problem-traditional-container-image-pull&quot;&gt;The problem: Traditional container image pull&lt;/h3&gt;

&lt;p&gt;To understand the benefits of lazy loading, we first need to understand the traditional container image pull process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Download layers&lt;/strong&gt;: The container runtime downloads all layer tarballs that make up the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unpack layers&lt;/strong&gt;: Each layer is unpacked and extracted onto the host’s disk.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Create snapshot&lt;/strong&gt;: The snapshotter combines these layers into a single, unified filesystem, known as the container’s rootfs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Start container&lt;/strong&gt;: Only after all layers are downloaded and unpacked can the container start.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This process is slow, especially for large images, as the entire image must be present on the host before the container can launch.&lt;/p&gt;

&lt;h3 id=&quot;the-solution-remote-snapshotter&quot;&gt;The solution: Remote snapshotter&lt;/h3&gt;

&lt;p&gt;To address the slow startup issue with large images, we use a &lt;strong&gt;remote snapshotter&lt;/strong&gt; solution. A remote snapshotter is a special type of snapshotter that doesn’t require all image data to be locally present. Instead of downloading and unpacking all the layers, it creates a “snapshot” that points to the remote location of the data (like a container registry). The actual file content is then fetched on-demand when the container tries to read a file for the first time.&lt;/p&gt;

&lt;p&gt;While a traditional snapshotter like overlayFS uses directories on the local disk as its lowerdir, a remote snapshotter creates a virtual lowerdir that is backed by the remote registry. This is typically done using FUSE (Filesystem in Userspace). The remote snapshotter creates a FUSE filesystem that presents the contents of the remote layer as if it were a local directory. This FUSE mount is then used as the lowerdir for the overlayFS driver. This allows the remote snapshotter to integrate with the existing overlayFS infrastructure while adding the capability of lazy-loading data from a remote source.&lt;/p&gt;

&lt;p&gt;There are two main formats that enable remote snapshotters: &lt;strong&gt;eStargz&lt;/strong&gt; and &lt;strong&gt;SOCI&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;estargz-format&quot;&gt;eStargz format&lt;/h3&gt;

&lt;p&gt;eStargz is a backward-compatible extension of the standard OCI &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tar.gz&lt;/code&gt; layer format. It has several key features that enable lazy loading:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Individually compressed files&lt;/strong&gt;: Each file within the layer (and even chunks of large files) is compressed individually. This is the key that allows for random access to file contents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;TOC (table of contents)&lt;/strong&gt;: A JSON file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stargz.index.json&lt;/code&gt; is located at the end of the layer. This TOC contains metadata for every file, including its name, size, and, most importantly, its offset within the layer blob.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Footer&lt;/strong&gt;: A small footer at the very end of the layer contains the offset of the TOC, allowing it to be easily located by reading only the last few bytes of the layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Chunking and verification&lt;/strong&gt;: Large files can be broken down into smaller chunks, each with its own entry in the TOC. Each chunk also has a chunkDigest in its TOC entry, allowing for independent verification of each downloaded piece of data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Prefetch landmark&lt;/strong&gt;: A special file, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.prefetch.landmark&lt;/code&gt;, can be placed in the layer to mark the end of “prioritized files”. This allows the snapshotter to intelligently prefetch the most important files for the container’s workload.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The stargz snapshotter uses the eStargz format to enable lazy loading. Here’s how it works:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mount request&lt;/strong&gt;: When containerd calls the Mount function, it’s the main entry point for creating a new filesystem for a layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Resolve and read TOC&lt;/strong&gt;: The snapshotter fetches the layer’s footer, then fetches the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stargz.index.json&lt;/code&gt; TOC from the remote registry. This TOC contains all the file metadata needed to create a virtual filesystem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mount FUSE filesystem&lt;/strong&gt;: With the TOC in memory, the snapshotter creates a virtual filesystem using FUSE. The container can now start, as it has a valid rootfs, even though most of the file content has not been downloaded.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;On-demand fetching&lt;/strong&gt;: When the container performs a file operation like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read()&lt;/code&gt;, the FUSE filesystem intercepts the call. The snapshotter checks a local disk cache for the requested bytes. If the data is not cached, it issues an HTTP Range request to the container registry to download only the required chunk of the layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Remote fetching and caching&lt;/strong&gt;: The downloaded data is returned to the container and also written to the local cache for subsequent reads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Prefetching for optimization&lt;/strong&gt;: After the FUSE filesystem is mounted, a background goroutine begins downloading the prioritized files (up to the &lt;em&gt;.prefetch.landmark&lt;/em&gt;) and can also be configured to download the entire rest of the layer in the background.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For a deeper understanding of the eStargz format and stargz snapshotter, see the &lt;a href=&quot;https://github.com/containerd/stargz-snapshotter/blob/main/docs/overview.md&quot;&gt;stargz-snapshotter overview documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;soci-format&quot;&gt;SOCI format&lt;/h3&gt;

&lt;p&gt;SOCI is a technology open sourced by AWS that enables containers to launch faster by lazily loading the container image. SOCI works by creating an index (SOCI Index) of the files within an existing container image. SOCI borrows some of the design principles from stargz-snapshotter but takes a different approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Separate index&lt;/strong&gt;: A SOCI index is generated separately from the container image and is stored in the registry as an OCI Artifact, linked back to the container image by OCI Reference Types.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;No image conversion&lt;/strong&gt;: This means that the container images do not need to be converted, image digests do not change, and image signatures remain valid.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Native Bottlerocket support&lt;/strong&gt;: SOCI is natively supported on Bottlerocket OS.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a deeper understanding of the SOCI format, see the &lt;a href=&quot;https://github.com/awslabs/soci-snapshotter/blob/main/docs/index.md&quot;&gt;soci-snapshotter documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;building-and-deploying-lazy-loaded-images&quot;&gt;Building and deploying lazy-loaded images&lt;/h2&gt;

&lt;h3 id=&quot;setting-up-snapshotters-in-eks&quot;&gt;Setting up snapshotters in EKS&lt;/h3&gt;

&lt;p&gt;When using EKS with containerd as the container runtime, you can configure remote snapshotters to enable lazy loading. Here’s how to set them up:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For stargz-snapshotter (eStargz)&lt;/strong&gt;: You need to install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containerd-stargz-grpc&lt;/code&gt; service first, then register it as a proxy plugin in containerd’s configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-textproto&quot;&gt;# /etc/containerd/config.toml
[proxy_plugins]
[proxy_plugins.stargz]
type = &quot;snapshot&quot;
address = &quot;/run/containerd-stargz-grpc/containerd-stargz-grpc.sock&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For detailed installation instructions, see the &lt;a href=&quot;https://github.com/containerd/stargz-snapshotter/blob/main/docs/INSTALL.md&quot;&gt;stargz-snapshotter installation documentation&lt;/a&gt;. The setup can be baked into an AMI for production use or tested via user data from node bootstrap scripts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For SOCI snapshotter (Bottlerocket)&lt;/strong&gt;: On Bottlerocket nodes, enable the SOCI snapshotter via user data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-textproto&quot;&gt;# Enable SOCI snapshotter
[settings.container-runtime]
snapshotter = &quot;soci&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SOCI is natively supported on Bottlerocket, so no additional daemon installation is required.&lt;/p&gt;

&lt;h3 id=&quot;building-lazy-loaded-images&quot;&gt;Building lazy-loaded images&lt;/h3&gt;

&lt;p&gt;eStargz images can be built natively using Docker Buildx by setting the output compression to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;estargz&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker buildx build 
  &lt;span class=&quot;nt&quot;&gt;--platform&lt;/span&gt; linux/amd64 
  &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;registry,oci-mediatypes&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;,compression&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;estargz,force-compression&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; 
  &lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ECR_REGISTRY&lt;/span&gt;/airflow:&lt;span class=&quot;nv&quot;&gt;$TAG&lt;/span&gt; 
  &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;SOCI doesn’t require rebuilding images; you only need to generate a SOCI index for existing images. Since Docker doesn’t natively support SOCI index generation yet, workaround solutions include using the &lt;a href=&quot;https://awslabs.github.io/cfn-ecr-aws-soci-index-builder/#_overview&quot;&gt;AWS SOCI Index Builder Using Lambda Functions&lt;/a&gt; or integrating SOCI index generation into your CI/CD pipeline as described in this &lt;a href=&quot;https://pabis.eu/blog/2025-06-17-Faster-ECS-Startup-SOCI-Index-GitLab-Pipeline.html&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaway-why-we-chose-soci&quot;&gt;Key takeaway: Why we chose SOCI&lt;/h2&gt;

&lt;p&gt;We started our exploration with eStargz but ultimately chose SOCI for production deployment. The key reason is scalability and alignment with our strategy to use Bottlerocket OS for enhancing Kubernetes pod startup and security. SOCI is natively supported by Bottlerocket, which means service teams don’t need to set up and maintain the more complicated stargz snapshotter across all EKS clusters. This makes the implementation easier to maintain and provides better support from AWS.&lt;/p&gt;

&lt;p&gt;Additionally, we learned that lazy loading doesn’t eliminate the time required to download image data; it redistributes it from startup time to runtime. While this dramatically improves cold start performance, it’s important to monitor application performance closely and tune configuration parameters based on your workload and infrastructure. We achieved a 60% improvement by optimizing SOCI’s parallel pull mode settings, demonstrating the value of proper configuration tuning.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Docker image lazy loading with SOCI offers a significant opportunity to improve the performance and efficiency of our services at Grab. Our testing and production deployments have shown:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;4x faster image pull times on fresh nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;29-34% improvement in P95 startup times for production workloads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;60% improvement in image download times with proper configuration tuning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The implementation path is clear, low-risk, and builds on proven components. This technology is production-ready, and we’re continuing to scale it across more services.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Databricks:&lt;/strong&gt; &lt;a href=&quot;https://www.databricks.com/blog/2021/09/08/booting-databricks-vms-7x-faster-for-serverless-compute.html&quot;&gt;Booting Databricks VMs 7x Faster for Serverless Compute&lt;/a&gt; - Industry case study showing how major tech companies achieve fast container startup at scale&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;BytePlus:&lt;/strong&gt; &lt;a href=&quot;https://docs.byteplus.com/en/docs/vke/Container-image-lazy-loading-solution&quot;&gt;Container Image Lazy Loading Solution&lt;/a&gt; - Enterprise implementation guide for lazy loading in production Kubernetes environments&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AWS:&lt;/strong&gt; &lt;a href=&quot;https://aws.amazon.com/blogs/containers/introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/&quot;&gt;Introducing Seekable OCI: Parallel Pull Mode for Amazon EKS&lt;/a&gt; - AWS’s guide to SOCI configuration and optimization&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdockerlazyloading&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Jan 2026 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/docker-lazy-loading</link>
        <guid isPermaLink="true">https://engineering.grab.com/docker-lazy-loading</guid>
        
        <category>Database</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>From deployment slop to production reality: How BriX bridges the gap with enterprise-grade AI infrastructure</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;You’ve vibe-coded an AI assistant that’s a game-changer for your team. It works perfectly on your laptop. But when you try to deploy it company-wide, everything falls apart.&lt;/p&gt;

&lt;p&gt;This is what is known as “deployment slop”—the messy reality when quick AI prototypes hit the enterprise world. Your tool suddenly becomes unreliable, insecure, and impossible to maintain. Different teams run different versions. Security flags it. IT won’t touch it. Your innovation dies.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BriX solves this&lt;/strong&gt;. It’s a platform that takes your working AI prototype and makes it production-ready—without forcing you to become a full-stack developer. BriX handles the hard parts such as security, scaling, and data connections, so you can focus on building great tools. Switch between AI models like Claude or GPT with a click. Connect securely to your company’s data sources. Deploy once, and it just works—for everyone.&lt;/p&gt;

&lt;p&gt;This article shows how BriX transforms AI deployment from an engineering bottleneck into a configuration task, enabling domain experts to ship enterprise-grade AI tools in days instead of months.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Building AI tools has never been easier. With ChatGPT, Claude, and other Large Language Models (LLMs), anyone can prototype a useful AI assistant in an afternoon. Data analysts build metric query tools; product managers create research assistants. This rapid experimentation—”vibe coding”—has sparked innovation across organizations.&lt;/p&gt;

&lt;p&gt;But then comes the hard part: &lt;em&gt;deployment&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;That brilliant tool you built on your laptop? It works great for you. But when your boss asks you to “roll it out to the whole company,” you hit a wall. Suddenly you need:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Security reviews (Is it leaking sensitive data?)&lt;/li&gt;
  &lt;li&gt;Reliability guarantees (What happens when 500 people use it at once?)&lt;/li&gt;
  &lt;li&gt;Access controls (Who can see what data?)&lt;/li&gt;
  &lt;li&gt;Audit trails (Who asked what, and when?)&lt;/li&gt;
  &lt;li&gt;Consistent behavior (Why does it give different answers to different people?)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most builders aren’t DevOps engineers. They’re domain experts who had a good idea. So these tools either:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Never get deployed (innovation dies in a Jupyter notebook); or&lt;/li&gt;
  &lt;li&gt;Get deployed badly (creating “Deployment Slop”—a mess of insecure, unreliable scripts).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-three-failure-modes-of-deployment-slop&quot;&gt;The three failure modes of deployment slop&lt;/h3&gt;

&lt;h4 id=&quot;the-chaos-problem-everyones-running-a-different-version&quot;&gt;The chaos problem: Everyone’s running a different version&lt;/h4&gt;

&lt;p&gt;Marketing copies your script and tweaks the prompts. Finance changed the model from GPT-4 to Claude because it’s cheaper. Sales adds their own data sources. Within weeks, you have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Five different versions of “the same tool”.&lt;/li&gt;
  &lt;li&gt;Wildly different answers to the same question.&lt;/li&gt;
  &lt;li&gt;No one knows which version is “correct”.&lt;/li&gt;
  &lt;li&gt;Teams making decisions based on inconsistent data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Potential risk&lt;/strong&gt;: A senior executive receiving conflicting answers from different teams, resulting in a loss of trust.&lt;/p&gt;

&lt;h4 id=&quot;the-reliability-problem-it-works-until-it-doesnt&quot;&gt;The reliability problem: It works until it doesn’t&lt;/h4&gt;

&lt;p&gt;Your laptop script was built for one user (you). Now 50 people are using it simultaneously. The result:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Timeouts and crashes during peak hours.&lt;/li&gt;
  &lt;li&gt;No error handling (users see cryptic Python stack traces).&lt;/li&gt;
  &lt;li&gt;Rate limits hit on API calls.&lt;/li&gt;
  &lt;li&gt;No monitoring or alerts when things break.&lt;/li&gt;
  &lt;li&gt;You become the “on-call” support person for a side project.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Potential risk&lt;/strong&gt;: The tool fails during a critical metric review leaving folks to find the solution manually.&lt;/p&gt;

&lt;h4 id=&quot;the-security-problem-accidental-data-leaks&quot;&gt;The security problem: Accidental data leaks&lt;/h4&gt;

&lt;p&gt;Your prototype connects directly to production databases. It has your personal credentials hardcoded. There’s no:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Access control (everyone sees all data, including sensitive info).&lt;/li&gt;
  &lt;li&gt;Audit trail (no record of who queried what).&lt;/li&gt;
  &lt;li&gt;Data governance (PII might be exposed).&lt;/li&gt;
  &lt;li&gt;Compliance review (legal and security teams don’t even know it exists).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Potential risk&lt;/strong&gt;: An employee inadvertently querying PII, resulting in a potential breach.&lt;/p&gt;

&lt;h3 id=&quot;who-gets-hit-hardest&quot;&gt;Who gets hit hardest?&lt;/h3&gt;

&lt;p&gt;This problem is especially painful for semi-technical builders—the domain experts who understand the business problem but aren’t DevOps engineers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Product Managers who write SQL but not Kubernetes configs.&lt;/li&gt;
  &lt;li&gt;Data Analysts who know Python but not cloud security.&lt;/li&gt;
  &lt;li&gt;Marketing Ops who build dashboards but not CI/CD pipelines.&lt;/li&gt;
  &lt;li&gt;HR Analytics who understand people data but not infrastructure scaling.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The traditional solution is to “hand it to Engineering,” but they are backlogged for months. By the time they rebuild your tool “properly,” the business need has changed.&lt;/p&gt;

&lt;h2 id=&quot;solution-enter-brix-from-prototype-to-production-in-days-not-months&quot;&gt;Solution: Enter BriX: From prototype to production in days, not months&lt;/h2&gt;

&lt;p&gt;BriX is a platform that solves the deployment problem by centralizing all the hard infrastructure work. Instead of forcing every builder to become a DevOps expert, BriX provides the production-ready foundation so you can focus on building great AI tools.&lt;/p&gt;

&lt;p&gt;The core insight: Deployment doesn’t have to be an engineering problem. It can be a configuration problem.&lt;/p&gt;

&lt;h3 id=&quot;what-brix-does&quot;&gt;What BriX does&lt;/h3&gt;

&lt;p&gt;Think of BriX as the “production layer” for AI tools. You bring your working prototype. BriX handles security, scaling, data connections, monitoring, audit trails, and consistent behavior across teams.&lt;/p&gt;

&lt;p&gt;You configure. BriX deploys.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/brix-infrastructure.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. BriX infrastructure&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-three-core-capabilities&quot;&gt;The three core capabilities&lt;/h3&gt;

&lt;h4 id=&quot;choose-your-ai-model-model-agnosticism&quot;&gt;Choose your AI model (Model agnosticism)&lt;/h4&gt;

&lt;p&gt;Different tasks need different models. BriX lets you switch between models with a dropdown—Claude, GPT, Gemini, or others. Test which works best. Change models without rewriting code. Optimize for cost vs. performance.&lt;/p&gt;

&lt;p&gt;Example: Your finance tool uses GPT-4 for complex analysis, but a new better model is available. Change it in BriX with one click—no code changes needed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/model-selection-interface.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Model selection interface&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;connect-to-enterprise-data-securely-model-context-protocols&quot;&gt;Connect to enterprise data securely (Model Context Protocols)&lt;/h4&gt;

&lt;p&gt;This is where BriX really shines. Your AI tool needs data—metrics, customer info, documentation. But connecting to enterprise systems securely is hard.&lt;/p&gt;

&lt;p&gt;Model Context Protocols (MCPs) are BriX’s solution. Think of them as secure, pre-built connectors to your company’s data sources.&lt;/p&gt;

&lt;p&gt;Why MCPs matter:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Security built-in: No hardcoded credentials, proper access controls.&lt;/li&gt;
  &lt;li&gt;Certified data: Connect only to approved, governed data sources.&lt;/li&gt;
  &lt;li&gt;No custom integration: Pre-built connectors, not custom API code.&lt;/li&gt;
  &lt;li&gt;Audit trails: Every query is logged automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Your marketing tool can query the metrics system to get conversion rates, search the knowledge base for campaign guidelines, and pull customer data from the data lake —all through secure, governed connections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Technical note&lt;/strong&gt;: MCPs use a standardized protocol, so adding new data sources doesn’t require rebuilding your tool. BriX handles the complexity.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/chat-user-interface.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. BriX chat user interface&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;ensure-consistent-behavior-system-prompts-and-context&quot;&gt;Ensure consistent behavior (System prompts and context)&lt;/h4&gt;

&lt;p&gt;Remember the “chaos problem” where everyone runs different versions? BriX solves this with centralized configurations by allowing you to lock it down for the users:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;System prompts: Define your AI’s personality, tone, and guardrails once.&lt;/li&gt;
  &lt;li&gt;Context files: Upload reference documents that every instance uses.&lt;/li&gt;
  &lt;li&gt;Global enforcement: All users get the same behavior automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Your customer support tool has a system prompt that says “Always be empathetic, never make promises about refunds, escalate to humans for complaints.” Every support agent’s AI follows these rules—no exceptions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/builder-view-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/builder-view-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. The builder’s view&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;additional-feature-flexible-interfaces-and-collaboration&quot;&gt;Additional feature: Flexible interfaces and collaboration&lt;/h4&gt;

&lt;p&gt;Beyond the core infrastructure, BriX offers flexible ways to consume these tools. BriX goes beyond conversational interfaces—you can host custom UIs built with any frontend framework while BriX handles the AI backend. Users can also generate and share analyses as persistent reports, turning individual queries into institutional knowledge accessible across teams via shareable links—complete with data, visualizations, and AI insights.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/share-feature-interface.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Share feature interface&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-brix-workflow-a-real-example&quot;&gt;The BriX workflow: A real example&lt;/h3&gt;
&lt;p&gt;Let’s see how a product manager would use BriX:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: Upload your prototype&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You’ve built a Jupyter notebook that queries metrics and generates reports.&lt;/li&gt;
  &lt;li&gt;Upload it to BriX (or connect your GitHub repo).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step 2: Configure (Not code)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Choose your AI model: Claude 4.5 Sonnet&lt;/li&gt;
  &lt;li&gt;Connect data sources: Midas (metrics), Hubble (data lake)&lt;/li&gt;
  &lt;li&gt;Set system prompt: “You’re a data analyst. Always cite sources. Format numbers with commas.”&lt;/li&gt;
  &lt;li&gt;Upload context: Your company’s metrics definitions guide.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Lock&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lock all the configurations of your BriX.&lt;/li&gt;
  &lt;li&gt;Share with your team.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/brix-landing-page.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. BriX landing page&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/user-view-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/user-view-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. The user’s view (Locks and edit not available)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 4: It just works&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Certification by design with Brick Quality residing with the brick admin.&lt;/li&gt;
  &lt;li&gt;Focused use cases have specific system prompts, context - minimizing hallucination concerns.&lt;/li&gt;
  &lt;li&gt;People can use it simultaneously (BriX handles scaling).&lt;/li&gt;
  &lt;li&gt;Everyone gets consistent answers (same model, same prompts).&lt;/li&gt;
  &lt;li&gt;All queries are logged (audit trail automatic).&lt;/li&gt;
  &lt;li&gt;The security team is happy (proper access controls).&lt;/li&gt;
  &lt;li&gt;You’re not on-call (BriX monitors and alerts).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Time to production: 3 Days, not 3 months.&lt;/p&gt;

&lt;h3 id=&quot;under-the-hood-the-brix-architecture&quot;&gt;Under the hood: The BriX architecture&lt;/h3&gt;

&lt;p&gt;BriX is built on a synchronous streaming architecture—a design that prioritizes real-time responsiveness without sacrificing enterprise security. Think of it like a live sports broadcast: you see the action as it happens, not a delayed replay.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/brix/brix-architecture.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. BriX architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Here’s how a single user request flows through the system, from question to answer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The request journey: Six layers&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;User Question
      ↓
[1] The Frontend — Real-Time Streaming
      ↓
[2] The Gateway — FastAPI Backend
      ↓
[3] The Brain — LangGraph Orchestration
      ↓
[4] Memory — Hot and Cold Storage
      ↓
[5] Security — Identity Propagation (&quot;On-Behalf-Of&quot; Flow)
      ↓
[6] Data Processing — Full Context, Not Fragments
      ↓
Response streams back to user in real-time
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s break down each layer.&lt;/p&gt;

&lt;h4 id=&quot;layer-1-the-frontend--real-time-streaming&quot;&gt;Layer 1: The frontend — Real-time streaming&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Technology&lt;/strong&gt;: React (TypeScript)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;User experience&lt;/strong&gt;: ChatGPT-style interface&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The User types a question: “What’s our conversion rate in Singapore last month?”&lt;/p&gt;

&lt;p&gt;The frontend opens a persistent connection to BriX servers. As the AI processes the question, updates stream back instantly:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“🤔 Thinking…”&lt;/li&gt;
  &lt;li&gt;“📊 Querying metrics database…”&lt;/li&gt;
  &lt;li&gt;“✅ Found 3 relevant data points…”
[Final answer appears]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why streaming matters:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Traditional approach&lt;/th&gt;
      &lt;th&gt;BriX approach&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ User waits 30 seconds, sees nothing, then gets full answer (feels broken).&lt;/td&gt;
      &lt;td&gt;✅ User sees progress every second (feels responsive and trustworthy).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Technical implementation: Server-Sent Events (SSE) for real-time updates without WebSocket complexity.&lt;/p&gt;

&lt;h4 id=&quot;layer-2-the-gateway--fastapi-backend&quot;&gt;Layer 2: The Gateway — FastAPI backend&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Technology&lt;/strong&gt;: FastAPI (Python)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Role&lt;/strong&gt;: Central traffic controller&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Receives all incoming requests&lt;/li&gt;
  &lt;li&gt;Authenticates users (checks SSO tokens)&lt;/li&gt;
  &lt;li&gt;Routes requests to the appropriate agent&lt;/li&gt;
  &lt;li&gt;Manages rate limiting (prevents abuse)&lt;/li&gt;
  &lt;li&gt;Handles errors gracefully&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Why FastAPI?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;⚡ Fast (async/await for concurrent requests)&lt;/li&gt;
  &lt;li&gt;🔒 Secure (built-in authentication)&lt;/li&gt;
  &lt;li&gt;📈 Scalable (handles thousands of concurrent users)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Layer 3: The Brain — LangGraph orchestration&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Technology&lt;/strong&gt;: LangGraph (AI workflow framework)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Role&lt;/strong&gt;: The “main agent” that coordinates everything.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Think of LangGraph as a smart router that understands intent and delegates work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example flow&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;User asks&lt;/em&gt;&lt;/strong&gt;: “Compare our Singapore and Malaysia conversion rates, then explain why they differ”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;LangGraph analyzes the question&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Task 1: Query metrics (needs Midas MCP)&lt;/li&gt;
  &lt;li&gt;Task 2: Compare data (needs calculation)&lt;/li&gt;
  &lt;li&gt;Task 3: Explain differences (needs context/knowledge base)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LangGraph delegates to specialized “MCPs”:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Midas MCP: Queries Midas for conversion data&lt;/li&gt;
  &lt;li&gt;LLM Agent: Calculates the difference&lt;/li&gt;
  &lt;li&gt;Glean MCP: Searches knowledge base for regional factors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LangGraph synthesizes: Combines results into coherent answer
Why modular “Bricks”?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;✅ Reliability: Each Brick is specialized (fewer hallucinations)&lt;/li&gt;
  &lt;li&gt;✅ Maintainability: Update one Brick without breaking others&lt;/li&gt;
  &lt;li&gt;✅ Extensibility: Add new Bricks for new use cases&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;layer-4-memory--hot-and-cold-storage&quot;&gt;Layer 4: Memory — Hot and cold storage&lt;/h4&gt;

&lt;p&gt;BriX uses a two-tier memory system to balance speed and durability:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Hot memory (Redis)&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;⚡ Ultra-fast: In-memory storage (microsecond access).&lt;/li&gt;
  &lt;li&gt;🔄 Session management: Tracks active conversations.&lt;/li&gt;
  &lt;li&gt;🔒 Distributed locks: Prevents race conditions when multiple requests happen simultaneously.&lt;/li&gt;
  &lt;li&gt;💨 Temporary: Data expires after session ends.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Cold memory (PostgreSQL)&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;💾 Persistent: Data stored permanently&lt;/li&gt;
  &lt;li&gt;📜 Audit trail: Every query, response, and action logged&lt;/li&gt;
  &lt;li&gt;🔍 Searchable: Users can search past conversations&lt;/li&gt;
  &lt;li&gt;📊 Analytics: Track usage patterns and performance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example scenario&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You ask BriX a question → Hot memory tracks your active session&lt;/li&gt;
  &lt;li&gt;You close the browser → Session data moves to cold memory&lt;/li&gt;
  &lt;li&gt;You return tomorrow → BriX loads your history from cold memory&lt;/li&gt;
  &lt;li&gt;You continue the conversation → New session in hot memory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/strong&gt;: Fast responses + complete history + full auditability&lt;/p&gt;

&lt;h4 id=&quot;layer-5-security--identity-propagation-on-behalf-of-flow&quot;&gt;Layer 5: Security — Identity propagation (“On-Behalf-Of” flow)&lt;/h4&gt;

&lt;p&gt;This is where BriX’s security model shines. Instead of using a single “service account” to access all data, BriX uses your credentials for every query.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;How it works&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: Authentication (Login)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You log in via SSO (e.g., Okta, Azure AD)&lt;/li&gt;
  &lt;li&gt;BriX receives a secure token that represents your identity&lt;/li&gt;
  &lt;li&gt;This token includes your permissions (what data you can access)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step 2: Identity propagation (Query execution)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You ask: “Show me customer revenue data”&lt;/li&gt;
  &lt;li&gt;BriX doesn’t use its own credentials to query the database&lt;/li&gt;
  &lt;li&gt;Instead, BriX carries your token to the data source&lt;/li&gt;
  &lt;li&gt;The data source checks: “Does this user have permission to see revenue data?”
    &lt;ul&gt;
      &lt;li&gt;If yes → Returns data&lt;/li&gt;
      &lt;li&gt;If no → Access denied&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Audit trail&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Every query is logged with:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Who&lt;/strong&gt; asked (your user ID)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;What&lt;/strong&gt; they asked (the question)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;What data&lt;/strong&gt; was accessed (the query)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;When&lt;/strong&gt; it happened (timestamp)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why this matters&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Traditional approach&lt;/th&gt;
      &lt;th&gt;BriX approach&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ Service account has access to ALL data.&lt;/td&gt;
      &lt;td&gt;✅ Each user only sees their authorized data.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ Can&apos;t tell who accessed what.&lt;/td&gt;
      &lt;td&gt;✅ Complete audit trail per user.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ Security team nervous about AI tools.&lt;/td&gt;
      &lt;td&gt;✅ Security team approves (same controls as existing tools).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ One compromised credential = full breach.&lt;/td&gt;
      &lt;td&gt;✅ Breach limited to single user&apos;s permissions.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Real-world example&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Finance analyst asks about revenue → Sees all financial data (authorized)&lt;/li&gt;
  &lt;li&gt;Marketing analyst asks same question → Sees only marketing budget (restricted)&lt;/li&gt;
  &lt;li&gt;Same AI tool, different permissions → Security enforced automatically&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Technical term&lt;/em&gt;&lt;/strong&gt;: This is called “identity propagation” or “on-behalf-of flow” in enterprise security.&lt;/p&gt;

&lt;h4 id=&quot;layer-6-data-processing--full-context-not-fragments&quot;&gt;Layer 6: Data processing — Full context, not fragments&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The old way (Retrieval Augmented Generation (RAG))&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;User asks a question.&lt;/li&gt;
  &lt;li&gt;System searches for relevant document chunks.&lt;/li&gt;
  &lt;li&gt;System sends top 5 chunks to AI.&lt;/li&gt;
  &lt;li&gt;AI answers based on fragments.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: AI might miss context from other parts of the document.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The BriX way (Full context)&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;User uploads a document.&lt;/li&gt;
  &lt;li&gt;BriX feeds the entire document into the AI’s context window.&lt;/li&gt;
  &lt;li&gt;AI reads and understands the full document.&lt;/li&gt;
  &lt;li&gt;AI answers with complete context.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Why this works now&lt;/strong&gt;: Modern AI models (Claude, GPT-4) have massive context windows (100K+ tokens). They can process entire documents, not just snippets—resulting in more accurate answers and fewer hallucinations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Question: “What’s our refund policy for international orders?”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RAG approach: Finds 3 snippets about refunds → Might miss international-specific rules&lt;/li&gt;
  &lt;li&gt;BriX approach: Reads entire policy document → Finds exact international refund section&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;architecture-summary-why-this-design-works&quot;&gt;Architecture summary: Why this design works&lt;/h4&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Design choice&lt;/th&gt;
      &lt;th&gt;Benefit&lt;/th&gt;
      &lt;th&gt;User impact&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;Streaming architecture&lt;/td&gt;
      &lt;td&gt;Real-time feedback&lt;/td&gt;
      &lt;td&gt;Feels fast and responsive&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;Modular Bricks&lt;/td&gt;
      &lt;td&gt;Specialized agents&lt;/td&gt;
      &lt;td&gt;Fewer errors, more reliable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;Hot/Cold memory&lt;/td&gt;
      &lt;td&gt;Speed + durability&lt;/td&gt;
      &lt;td&gt;Fast responses + full history&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;Identity propagation&lt;/td&gt;
      &lt;td&gt;User-level security&lt;/td&gt;
      &lt;td&gt;Only see authorized data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;Full context processing&lt;/td&gt;
      &lt;td&gt;Complete understanding&lt;/td&gt;
      &lt;td&gt;More accurate answers&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;The result&lt;/strong&gt;: An AI platform that feels as fast as ChatGPT but with enterprise-grade security and reliability.&lt;/p&gt;

&lt;h3 id=&quot;what-using-brix-actually-feels-like&quot;&gt;What using BriX actually feels like&lt;/h3&gt;

&lt;p&gt;All the technical architecture is invisible to end users. Here’s what they actually see and experience.&lt;/p&gt;

&lt;h4 id=&quot;login-one-click-no-new-passwords&quot;&gt;Login: One click, no new passwords&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What users see&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit BriX URL&lt;/li&gt;
  &lt;li&gt;Click “Log in with SSO” (uses your existing company login)&lt;/li&gt;
  &lt;li&gt;Redirects to familiar authentication screen&lt;/li&gt;
  &lt;li&gt;Logged in automatically&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What users DON’T see&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No new account creation&lt;/li&gt;
  &lt;li&gt;No password to remember&lt;/li&gt;
  &lt;li&gt;No security questionnaire&lt;/li&gt;
  &lt;li&gt;BriX inherits your existing permissions automatically&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why this matters: Zero onboarding friction. If you can access your email, you can use BriX.&lt;/p&gt;

&lt;h4 id=&quot;the-app-library-your-companys-ai-tools&quot;&gt;The app library: Your company’s AI tools&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What users see&lt;/em&gt;&lt;/strong&gt;: Company’s internal “App Store” for AI tools.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each tool is pre-configured and vetted&lt;/li&gt;
  &lt;li&gt;Click to launch (no installation)&lt;/li&gt;
  &lt;li&gt;Tools are tailored to company’s data and processes&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;using-a-tool-chatgpt-style-interface&quot;&gt;Using a Tool: ChatGPT-style interface&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What users see&lt;/em&gt;&lt;/strong&gt;:
See the AI “thinking” and “querying”—no black box waiting. Builds trust (“I can see it’s actually checking the data”).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Source citations&lt;/em&gt;&lt;/strong&gt;:
Every answer includes a data source. Click to view original data. No “trust me” answers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Conversational follow-ups&lt;/em&gt;&lt;/strong&gt;:
“Why did it increase?” | “Compare to Malaysia” | “Show me a chart”&lt;/p&gt;

&lt;p&gt;BriX remembers the context.&lt;/p&gt;

&lt;h4 id=&quot;data-upload-drag-drop-analyze&quot;&gt;Data upload: Drag, drop, analyze&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What users have&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Files are processed securely (encrypted).&lt;/li&gt;
  &lt;li&gt;AI reads the full content.&lt;/li&gt;
  &lt;li&gt;Users can ask questions about the files.&lt;/li&gt;
  &lt;li&gt;Files are only visible to the uploader (privacy).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;trustworthy-answers-certified-data-not-hallucinations&quot;&gt;Trustworthy answers: Certified data, not hallucinations&lt;/h4&gt;

&lt;p&gt;The problem BriX solves:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ChatGPT/Generic AI&lt;/th&gt;
      &lt;th&gt;BriX&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ Makes up data (&quot;hallucinations&quot;)&lt;/td&gt;
      &lt;td&gt;✅ Only uses your company&apos;s real data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ No source citations&lt;/td&gt;
      &lt;td&gt;✅ Every answer cites the source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ Can&apos;t access internal data&lt;/td&gt;
      &lt;td&gt;✅ Connects to your data lakes, metrics, docs&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;❌ Same answer for everyone&lt;/td&gt;
      &lt;td&gt;✅ Respects your permissions (you only see your data)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Why users trust it:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;✅ Specific number (not vague)&lt;/li&gt;
  &lt;li&gt;✅ Source cited (can verify)&lt;/li&gt;
  &lt;li&gt;✅ Certified data (governance approved)&lt;/li&gt;
  &lt;li&gt;✅ Timestamp (know it’s current)&lt;/li&gt;
  &lt;li&gt;✅ Can export/verify (transparency)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-impact-what-brix-actually-changes&quot;&gt;The impact: What BriX actually changes&lt;/h3&gt;

&lt;p&gt;BriX shifts how organizations build AI tools. Here’s what that looks like in practice.&lt;/p&gt;

&lt;h4 id=&quot;from-months-to-days&quot;&gt;From months to days&lt;/h4&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Traditional path&lt;/th&gt;
      &lt;th&gt;BriX path&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;1. Domain expert has idea.&lt;/td&gt;
      &lt;td&gt;1. Domain expert has idea&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;2. Submits request to engineering.&lt;/td&gt;
      &lt;td&gt;2. Configures the idea in BriX.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;3. Waits in backlog (weeks to months).&lt;/td&gt;
      &lt;td&gt;3. Tests with small group.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;4. Engineering rebuilds it &quot;properly&quot;.&lt;/td&gt;
      &lt;td&gt;4. Deploys to production.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;1&quot;&gt;5. Tool finally launches.&lt;/td&gt;
      &lt;td&gt;5. Shares with team.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What changes&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;⚡ Speed (hours instead of months)&lt;/li&gt;
  &lt;li&gt;👤 Ownership (domain experts maintain their tools)&lt;/li&gt;
  &lt;li&gt;🔄 Iteration (refine based on feedback immediately)&lt;/li&gt;
  &lt;li&gt;✅ Success rate (ideas get tested instead of dying in backlog)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;true-democratization&quot;&gt;True democratization&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Who builds tools with BriX&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The shift isn’t just engineers anymore. We’re seeing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Product managers building feature analysis tools.&lt;/li&gt;
  &lt;li&gt;Data analysts creating custom dashboards.&lt;/li&gt;
  &lt;li&gt;Marketing ops building campaign trackers.&lt;/li&gt;
  &lt;li&gt;Sales ops creating pipeline monitors.&lt;/li&gt;
  &lt;li&gt;HR analytics building retention tools.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What this means&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Domain expertise stays with domain experts (no translation loss). Engineering focuses on platforms (not individual tool requests). Innovation happens at business speed (not constrained by engineering capacity).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The reality check&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Not every domain expert will build tools (and that’s fine). Some tools still need engineering (complex integrations, custom logic). But the bottleneck shifts from “engineering capacity” to “good ideas.”&lt;/p&gt;

&lt;h4 id=&quot;flexibility-without-fragility&quot;&gt;Flexibility without fragility&lt;/h4&gt;

&lt;p&gt;What you can change without rewriting code:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Swap AI models&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dropdown menu selection (GPT-5, Claude, Gemini)&lt;/li&gt;
  &lt;li&gt;Different teams can setup different models for their BriX&lt;/li&gt;
  &lt;li&gt;Can test new models without rebuilding tools&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Add data sources&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;New MCP connector (one-time setup)&lt;/li&gt;
  &lt;li&gt;All existing tools can access the new source&lt;/li&gt;
  &lt;li&gt;No need to update individual tools&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Update behavior globally&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Change system prompt in one place&lt;/li&gt;
  &lt;li&gt;All instances follow new rules immediately&lt;/li&gt;
  &lt;li&gt;Useful for policy updates, compliance changes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Real example&lt;/strong&gt;: When a company needs to update data access policies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Traditional approach: Update each tool individually (days/weeks)&lt;/li&gt;
  &lt;li&gt;BriX approach: Update system prompt once (minutes)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;security-that-enables-not-blocks&quot;&gt;Security that enables (Not blocks)&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The traditional trade-off&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Secure tools = slow approval, limited functionality&lt;/li&gt;
  &lt;li&gt;Fast tools = security nightmares, compliance issues&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BriX’s approach: Security is built into the platform, not added per tool.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What’s automatic&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SSO authentication (no passwords to manage)&lt;/li&gt;
  &lt;li&gt;Identity propagation (users see only their authorized data)&lt;/li&gt;
  &lt;li&gt;Audit logging (every query tracked)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What this changes&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Security team reviews the platform once (not every tool)&lt;/li&gt;
  &lt;li&gt;Builders don’t need to become security experts&lt;/li&gt;
  &lt;li&gt;Compliance is automatic (audit trails, access controls)&lt;/li&gt;
  &lt;li&gt;Tools can move fast without sacrificing governance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Real impact&lt;/em&gt;&lt;/strong&gt;: Security teams that previously rejected most AI proposals can pre-approve BriX. Then tools built on BriX inherit those security controls automatically.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;BriX will&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide infrastructure for rapid AI tool deployment.&lt;/li&gt;
  &lt;li&gt;Make it easier for domain experts to productionize ideas.&lt;/li&gt;
  &lt;li&gt;Centralize security and governance.&lt;/li&gt;
  &lt;li&gt;Reduce (not eliminate) the engineering bottleneck.&lt;/li&gt;
  &lt;li&gt;Give you a path from prototype to production.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-real-impact&quot;&gt;The real impact&lt;/h4&gt;

&lt;p&gt;The biggest change isn’t technical. It’s organizational.&lt;/p&gt;

&lt;p&gt;BriX changes the conversation from:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Can engineering build this for us?”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Let me try building this and see if it works”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;That shift—from asking permission to testing ideas—is the real impact.
Some ideas will fail. That’s fine. The cost of testing is now low enough that failure is acceptable.&lt;/p&gt;

&lt;p&gt;The ideas that succeed can scale immediately. That’s what matters.&lt;/p&gt;

&lt;h2 id=&quot;adoption-from-zero-to-production-reality&quot;&gt;Adoption: From zero to production reality&lt;/h2&gt;

&lt;p&gt;This isn’t theoretical. Real teams are using BriX right now:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The Universal Playground&lt;/strong&gt; - Data analysts and product managers drop in to run quick analyses or ask questions—no setup, no credentials to configure. Just connect and go. It’s become the default “let me check something” tool.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Country Intelligence Assistant&lt;/strong&gt; - Country Analytics built a specialized assistant that answers country-specific questions—market data, regulations, operational metrics. It’s now the go-to source for regional teams making local decisions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Medallion Architecture Validator&lt;/strong&gt; - A data engineer created a tool that validates table compliance with medallion architecture standards. What used to take manual reviews now happens instantly. Teams query it before deployments to catch issues early.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conversion Funnel Analyzer&lt;/strong&gt; - Product analyst built an assistant that tracks user conversion funnels step-by-step in a custom UI. Marketing and product teams use it daily to understand drop-off points without writing SQL.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/conclusion&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The promise&lt;/strong&gt;: Anyone can build AI tools.
&lt;strong&gt;The reality&lt;/strong&gt;: Anyone can build prototypes, but production requires engineering expertise most people don’t have.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BriX bridges that gap&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;what-brix-does-1&quot;&gt;What BriX does&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;For domain experts&lt;/strong&gt;: Build and own tools without becoming DevOps experts. Iterate in hours, not months.
&lt;strong&gt;For engineering&lt;/strong&gt;: Stop being the bottleneck. Secure the platform once, not every tool.
&lt;strong&gt;For the organization&lt;/strong&gt;: Test more ideas. Scale what works. Automatic security and compliance.&lt;/p&gt;

&lt;h3 id=&quot;why-brix-works-three-design-principles&quot;&gt;Why BriX works: Three design principles&lt;/h3&gt;

&lt;p&gt;Building BriX taught us that successful enterprise AI platforms require:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Specialization over generalization&lt;/strong&gt;
Users prefer 5 focused tools over 1 unpredictable tool. That’s why BriX uses modular “Bricks”—each specialized for specific tasks (data analysis, trend detection, document search). Narrow scope = better reliability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Enablement over control&lt;/strong&gt;
Deployment slop isn’t a problem to eliminate—it’s evidence of demand. Don’t kill experimentation; provide the path to production. BriX lets teams experiment locally, then offers the infrastructure to scale what works.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reliability over features&lt;/strong&gt;
Users forgive missing features. They don’t forgive unreliability. One slow response or wrong answer = they never come back. That’s why BriX prioritizes real-time streaming, certified data sources, and source citations over adding more capabilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The result&lt;/strong&gt;: A platform that feels as fast as ChatGPT but with enterprise-grade security and governance.&lt;/p&gt;

&lt;h3 id=&quot;configure-once-analyze-everywhere-act-fast&quot;&gt;Configure once. Analyze everywhere. Act fast.&lt;/h3&gt;

&lt;p&gt;BriX makes AI tool deployment a configuration problem, not an engineering problem.&lt;/p&gt;

&lt;p&gt;Your domain experts have the ideas. BriX gives them the path to production.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;BriX solves deployment, but we’re not stopping there.&lt;/p&gt;

&lt;h3 id=&quot;more-data-sources&quot;&gt;More data sources&lt;/h3&gt;

&lt;p&gt;We’re expanding the MCP library. If our company uses it, BriX should connect to it—securely and without custom engineering work.&lt;/p&gt;

&lt;h3 id=&quot;bring-your-own-code&quot;&gt;Bring your own code&lt;/h3&gt;

&lt;p&gt;For technical builders who want custom logic without DevOps headaches, we’re launching a mono repo setup:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;App owners own: Their code and business logic&lt;/li&gt;
  &lt;li&gt;BriX owns: Platform, security, scaling, maintenance&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;more-brix&quot;&gt;More BriX&lt;/h3&gt;

&lt;p&gt;Onboarding more BriX for different tech and non-tech personas.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebbrix&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Jan 2026 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/brix</link>
        <guid isPermaLink="true">https://engineering.grab.com/brix</guid>
        
        <category>AI</category>
        
        <category>LLM</category>
        
        <category>Deployment</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Demystifying user journeys: Revolutionizing troubleshooting with auto tracking</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Troubleshooting critical issues by deciphering a user’s journey on the Grab app is an extremely challenging task. With countless user journeys and multiple paths through the User Interface (UI), it’s akin to searching for a needle in a vast haystack. This challenge frequently resonates with us, the dedicated developers at Grab, as we strive to understand user behaviors, views, and interactions.&lt;/p&gt;

&lt;h2 id=&quot;the-challenge&quot;&gt;The challenge&lt;/h2&gt;

&lt;p&gt;The distinction between resolving an issue effectively versus spending hours on a wild goose chase is understanding our user journey in real-time.&lt;/p&gt;

&lt;p&gt;The development team initially attempted to address the issue of the incomplete user journey tracking by implementing a system where a click stream event would be sent with every user interaction. However, this approach presented significant challenges due to the sheer volume of UI components—often numbering in the hundreds—and the reliance on individual developers to correctly instrument each one.&lt;/p&gt;

&lt;p&gt;A common pitfall was that developers would occasionally overlook or forget to instrument certain user interactions, leading to breaks in the recorded user journey. This created a highly frustrating situation for both the development and product teams, as the integrity of the user journey data was consistently compromised. Despite continuous efforts to patch these bugs and address the omissions, the team found themselves in a perpetual state of reaction, constantly trying to catch up with newly discovered breaches rather than proactively preventing them. This reactive approach consumed valuable resources and hindered the ability to gain a complete and accurate understanding of user behavior.&lt;/p&gt;

&lt;p&gt;Diagnosing system failures, application bugs, or poor user experiences in complex applications becomes inefficient without real-time performance metrics and detailed session tracking. When engineering teams rely on outdated or fragmented data, they are forced to piece together issue narratives reactively, long after the issues occur. This significantly delays the Mean Time To Resolution (MTTR). Such a reactive approach leads to increased downtime, higher operational costs, customer dissatisfaction, and a waste of developers’ time, as they spend more time “hunting” for clues rather than deploying solutions or new features.&lt;/p&gt;

&lt;h2 id=&quot;our-eureka-moment-autotrack-sdk&quot;&gt;Our ‘Eureka’ moment: AutoTrack SDK&lt;/h2&gt;

&lt;p&gt;The pivotal breakthrough that provides our unique advantage was the creation of auto tracking user journeys—our “Eureka” moment. To deliver this, we developed the new Software Development Kit (SDK) called AutoTrack.&lt;/p&gt;

&lt;p&gt;AutoTrack is system that comprehensively records application state, UI view state, as well as user interactions - a solution that pieces together a chronicle of the user journey, from launch to interactions, as they navigate through the screens. AutoTrack SDK is built on the three core pillars:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Application state&lt;/li&gt;
  &lt;li&gt;User interactions&lt;/li&gt;
  &lt;li&gt;UI screens&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s delve deeper into the mechanics of how this operates.&lt;/p&gt;

&lt;h3 id=&quot;application-state&quot;&gt;Application state&lt;/h3&gt;

&lt;p&gt;Understanding the application state is fundamental to comprehending user behavior and, consequently, executing effective troubleshooting. The application state provides crucial insights into how a user interacts with the app, particularly concerning its visibility and how it was initiated. This encompasses tracking when the app moves between the background and foreground, as well as the various launch mechanisms.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Application state user flow.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Key aspects of application state that are vital to monitor include:&lt;br /&gt;
&lt;strong&gt;Application lifecycle transitions:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Background state:&lt;/strong&gt; When the app is running but not actively displayed to the user (e.g., the user switches to another app, or the device is locked). Understanding how frequently and for how long an app resides in the background can inform power consumption analysis and the effectiveness of background tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Foreground state:&lt;/strong&gt; When the app is actively in use and displayed to the user. Monitoring transitions into and out of the foreground provides a real-time view of user engagement.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inactive state:&lt;/strong&gt; A temporary state where the app is in the foreground but not receiving events (e.g., an incoming call temporarily interrupts the app).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Suspended state:&lt;/strong&gt; An app that is in the background and has been explicitly suspended by the operating system to free up resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Terminated state:&lt;/strong&gt; When the app has been completely closed or crashed. Differentiating between intentional termination and crashes is critical for identifying stability issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Application launch mechanisms:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The way an app is launched significantly impacts the initial user experience and can influence subsequent interactions. Tracking these different launch types is essential for understanding user entry points and for debugging issues that might be specific to a particular launch method.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Explicit user launch:&lt;/strong&gt; This is the most straightforward launch mechanism, where the user directly taps on the app icon from their device’s home screen or app drawer. This indicates a deliberate intent to use the app and often signifies a primary entry point for regular users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deeplinks:&lt;/strong&gt; Deeplinks are URLs that, when clicked, open a specific page or section within a mobile app rather than a web page. They are powerful tools for enhancing user experience and engagement by providing direct access to relevant content.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Push notifications:&lt;/strong&gt; Push notifications are messages sent by an app to a user’s device even when the app is not actively in use. Tapping on a push notification often launches the app and directs the user to a specific context related to the notification’s content.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Code sample for tracking application lifecycle transition.&lt;/figcaption&gt; &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;user-interactions&quot;&gt;User interactions&lt;/h3&gt;

&lt;p&gt;Real-time session tracking is a crucial component in understanding user behavior and optimizing app performance. By meticulously tracking a wide array of user interactions, the system provides invaluable insights into how users navigate and engage with the app. This granular data forms the bedrock for constructing comprehensive user journeys, allowing development teams to visualise the path a user takes from their initial entry point to achieving their goals within the app.&lt;/p&gt;

&lt;p&gt;This deep understanding of user interactions is the most important pillar in creating accurate and insightful user journey maps. These maps, in turn, are instrumental in identifying patterns of user behavior, both positive and negative. For instance, tracking helps to identify pain points, bugs, or areas of confusion that might lead to user frustration or abandonment.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Sample code for real-time session tracking.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ui-screen&quot;&gt;UI screen&lt;/h3&gt;

&lt;p&gt;The system leverages lifecycle events from UIViewController (iOS), Activity (Android), and Fragments (Android) to accurately identify and track which specific screen is currently displayed to the user. This granular level of screen tracking is crucial because it significantly enriches the contextual information available to us. By understanding the precise UI that users are interacting with, we can account for the dynamic nature of our app. Different geographical regions, diverse user segments, and varying operational scenarios can lead to distinct user interfaces being presented. This capability ensures that our analysis and troubleshooting efforts are always based on the actual user experience, allowing for more precise problem identification and more effective solutions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-4-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt; &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Sample code of UIViewController configuration.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ui-screen-data&quot;&gt;UI screen data&lt;/h3&gt;
&lt;p&gt;On top of that, whenever the screen appears, we capture the screen metadata where we read the full screen hierarchy. With the Screen hierarchy JSON data at hand, we employ it to train an AI model. This model, consequently, can generate an HTML file, which mirrors the user’s screen and interaction.&lt;/p&gt;

&lt;p&gt;Disclaimer: information is redacted in compliance with GDPR/PDPA, personal data protection laws.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/auto-tracking/figure-7.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Screen hierarchy.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;applications-of-autotrack&quot;&gt;Applications of AutoTrack&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Key applications of AutoTrack data:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reconstructing user journeys and reproducing elusive bugs:&lt;/strong&gt; One of the most significant benefits of AutoTrack is its ability to meticulously record user interactions within the app. This detailed session data allows our teams to precisely recreate the user journey that led to a reported issue. For bugs that are notoriously difficult to reproduce, this capability is a game-changer, eliminating hours of manual guesswork and dramatically accelerating the identification and resolution of underlying problems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automated issue assignment:&lt;/strong&gt; When an issue is reported, AutoTrack data can be leveraged to automatically assign it to the most relevant team. By analysing the context of the issue within the recorded session, including the specific features or modules involved, the system can intelligently route the problem to the engineers best equipped to address it. This automation reduces triage time, ensures issues are handled by subject matter experts, and improves overall response efficiency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automating UI test case generation:&lt;/strong&gt; The rich dataset provided by AutoTrack offers a powerful foundation for automating the creation of UI test cases. By observing how users interact with the interface, we can automatically generate test scripts that mimic real-world usage patterns. This not only speeds up the testing phase but also leads to more comprehensive test coverage, identifying edge cases and user flows that might otherwise be missed by manually written tests.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Understanding analytics event triggers:&lt;/strong&gt; AutoTrack data provides a granular view into when and why specific analytics events are triggered within the application. This allows us to validate the accuracy of our analytics instrumentation, ensure that events are firing as expected, and gain deeper insights into user behavior. By understanding the precise context surrounding event triggers, we can refine our data collection strategies and derive more meaningful insights from our analytics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-takeaways-and-whats-next&quot;&gt;Key takeaways and what’s next&lt;/h2&gt;

&lt;p&gt;AutoTrack replaces fragile manual instrumentation with a unified, real-time view of application state, screen context, and user interactions. That end-to-end trace makes elusive bugs reproducible, routes issues to the right owners, and seeds reliable UI tests—turning guesswork into grounded evidence so teams can ship fixes faster and with greater confidence.&lt;/p&gt;

&lt;p&gt;Looking ahead, we are expanding AutoTrack across surfaces and deepening the context it captures—pairing sessions with network and performance signals, strengthening privacy guardrails, and integrating with automated triage and test generation. Look forward to reading more of our deep dives on auto-generated UI tests and how these journeys will power proactive quality across Grab’s app.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebautocheck&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Dec 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/auto-track-sdk</link>
        <guid isPermaLink="true">https://engineering.grab.com/auto-track-sdk</guid>
        
        <category>mobile</category>
        
        <category>ios</category>
        
        <category>android</category>
        
        <category>tracking</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>How Grab is accelerating growth with real-time personalization using Customer Data Platform scenarios</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Delivering personalized user experiences in real-time is central to Grab’s strategy, but achieving this at scale poses significant engineering challenges. Grab’s Customer Data Platform (CDP) and Growth team has successfully delivered several real-time campaigns, driving significant business impact through enhanced personalization. These initiatives include high-impact use cases like immediate mall offers, timely traveler recommendations, precise ad retargeting, and proactive interventions during key user journey moments. At the core of these successes is Grab’s CDP, which rapidly deploys advanced real-time personalization via a powerful new capability called “Scenarios.”&lt;/p&gt;

&lt;h2 id=&quot;about-grabs-cdp&quot;&gt;About Grab’s CDP&lt;/h2&gt;

&lt;p&gt;Grab’s CDP is a centralized, reliable repository for user attributes, designed for freshness, governance, and reusability. Built on &lt;a href=&quot;https://engineering.grab.com/signals-market-place&quot;&gt;Grab’s Signal Marketplace&lt;/a&gt; framework, the CDP streamlines data management through automation and integration, supporting seamless interactions with internal services and toolings that power marketing, experimentation, ads, Machine Learning (ML) features, and external platforms, including Facebook, Google Ads, and TikTok.&lt;/p&gt;

&lt;p&gt;The platform currently manages over 1,000 batch user attributes for Passengers, Drivers, and Merchants, powering diverse use cases from targeted marketing campaigns to operational decision-making across Grab’s entire ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-real-time-personalization&quot;&gt;The need for real-time personalization&lt;/h2&gt;

&lt;p&gt;In our current CDP setup, user segments are primarily created for targeting using batch attributes that update once daily. While these batch updates provide valuable historical insights, they are not suitable for scenarios requiring real-time responsiveness. This delay prevents timely engagement with users, particularly when immediate actions can significantly enhance user experiences and conversion rates.&lt;/p&gt;

&lt;p&gt;For example, when travelers land at an airport, they immediately benefit from timely suggestions for rides, dining options, or local attractions. Traditional batch processing cannot deliver the agility and responsiveness required for these dynamic scenarios.&lt;/p&gt;

&lt;p&gt;Historically, real-time personalization at Grab relied heavily on engineering resources, which resulted in limited scalability and agility. Marketers and product teams often found themselves blocked by engineering bandwidth constraints, restricting experimentation and innovation.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;The limitations of Grab’s existing personalization frameworks include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Batch attribute delays&lt;/strong&gt;: Daily updates are insufficient for scenarios requiring immediate user responses.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Limited dynamic enrichment&lt;/strong&gt;: Difficulties in dynamically integrating real-time events with historical user data, weakens personalization effectiveness.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High engineering overhead&lt;/strong&gt;: Custom solutions require extensive resources, limiting agility and innovation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To overcome these challenges and support Grab’s vision for comprehensive personalization – including proactive recommendations and assistance – CDP needed robust real-time capabilities.&lt;/p&gt;

&lt;h2 id=&quot;cdp-scenarios-real-time-personalization-made-simple&quot;&gt;CDP Scenarios: Real-time personalization made simple&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Scenario&lt;/strong&gt; feature revolutionizes real-time targeting within the CDP by utilizing user-initiated events, geo-fencing, historical profile data, and on-the-fly predictions. This empowers the business to deliver easy, quick, and flexible personalization without the need for complex engineering efforts.&lt;/p&gt;

&lt;p&gt;Scenarios enable innovative use cases such as these:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mall personalization&lt;/strong&gt;: Real-time personalized offers upon arrival.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Traveler assistance&lt;/strong&gt;: Immediate recommendations at airports or hotels.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ad retargeting&lt;/strong&gt;: Enhanced real-time ad targeting.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conversion optimization&lt;/strong&gt;: Timely intervention during user drop-off points.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Imagine predicting a user’s intent to drop off at a mall using both real-time and historical context. For instance, when a user books a ride to a mall, factors such as destination, time, cuisine preferences, and past behavior (e.g., affluence level) can help predict whether the user’s purpose is retail therapy, grocery shopping, or dining out. This prediction accounts for elements like time of day, day of the week, and mall location. Grab’s engineering teams can leverage this predicted intent (signal) to offer personalized actions, such as GrabPay discounts for shopping or exclusive dining offers for dinner.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Scenario in CDP.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;key-features&quot;&gt;Key features&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Event-driven personalization&lt;/strong&gt;: Real-time Scenarios triggered by Scribe events (Grab’s comprehensive event collection and tracking platform) combined with geo-fencing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Historical context integration&lt;/strong&gt;: Optionally enrich Scenarios using historical CDP data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Predictive modeling&lt;/strong&gt;: Deploy pre-trained models for instant user behavior predictions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Self-serve graphical user interface (GUI)&lt;/strong&gt;: Enable marketers to create complex event sequences and validate Scenarios with synthetic data processed through Flink pipelines.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Headless application programming interfaces (APIs)&lt;/strong&gt;: Allow programmatic access and management of Scenarios.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-2.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Attributes for a scenario in CDP.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;self-serve-scenario-creation&quot;&gt;Self-serve Scenario creation&lt;/h3&gt;

&lt;p&gt;We designed an intuitive self-serve UI, embedded within the Grab app, empowering marketers to quickly define and deploy Scenarios. Users can specify event triggers, configure geo-fencing, incorporate historical user attributes, and select predictive models. Marketers can also validate Scenarios using synthetic data before deployment, ensuring accurate and realistic outcomes.&lt;/p&gt;

&lt;p&gt;How it works:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Select event triggers&lt;/strong&gt;: Choose predefined events or define custom intra-session sequences via the GUI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Configure geo-fencing&lt;/strong&gt;: Define Scenario activation locations, like airports or malls.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Include historical attributes (optional)&lt;/strong&gt;: Utilize batch attributes from the CDP to enrich Scenarios.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Select predictive models (optional)&lt;/strong&gt;: Train custom classifiers or pick from pre-trained Catwalk models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Define data sink&lt;/strong&gt;: Choose between Amphawa (DynamoDB), Kafka, or both; potentially extendable to external destinations (e.g., Appsflyer).&lt;/li&gt;
  &lt;li&gt;Once configured, metadata synchronizes automatically with our streaming service, and Scenarios become available for real-time consumption within an hour.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;proven-impact-real-world-success&quot;&gt;Proven impact: Real-world success&lt;/h2&gt;

&lt;p&gt;CDP Scenarios are already delivering measurable business results, with over 12 live production implementations. For instance, in a case study addressing Grab Unlimited subscription signup abandonment, we leveraged CDP Scenarios to increase signups by engaging users in real time within 15 minutes of them leaving the signup process.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Grab Unlimited sign-up journey.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To enhance conversion rates, personalized real-time nudges were deployed through Scenarios. For example, users who started the signup process but failed to complete it within 15 minutes received a follow-up notification, prompting them to finalize their registration.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Scenario flow for Grab Unlimited registration.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This scenario alone achieved more than a 3% uplift in subscriber conversions vs non-real-time acquisition campaigns, demonstrating Scenarios’ potential to significantly boost business outcomes.&lt;/p&gt;

&lt;h2 id=&quot;technical-architecture-low-latency-high-reliability&quot;&gt;Technical architecture: Low latency, high reliability&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cdp-scenario/figure-6.jpg&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. High-level scenario flow. Scenarios are designed for low latency (under 15 seconds) and high reliability.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Event registration&lt;/strong&gt;: Popular UI events from Scribe are whitelisted and immediately available; custom events are onboarded via the CDP web portal.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scenario creation&lt;/strong&gt;: Users configure Scenarios through a user-friendly GUI, defining events, historical contexts, and predictive models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time Flink processing&lt;/strong&gt;: Incoming events trigger Scenarios, evaluating user historical data via StarRocks and performing real-time predictions using pre-trained models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time data sync&lt;/strong&gt;: Outcomes are synced back to Kafka or Amphawa (Grab’s internal feature store built on AWS DynamoDB), enriching data for use by subsequent services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consumption by downstream services&lt;/strong&gt;: Kafka streams or CDP’s Profile SDK facilitates immediate, personalized user experiences.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;advancing-the-future-of-real-time-personalization&quot;&gt;Advancing the future of real-time personalization&lt;/h2&gt;

&lt;p&gt;As we continue to innovate, we are focused on enhancing the capabilities of CDP Scenarios to support more complex and scalable personalization use cases. Here are some key areas of improvement we are exploring:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Optimized Scenario sharding for scalable processing&lt;/strong&gt;: To accommodate the growing number of use cases, we plan to scale and orchestrate our Flink pipeline fleet in a headless manner. This approach will improve system stability and enable seamless management of complex Scenarios across the pipeline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Enhanced signal distribution across multiple destinations&lt;/strong&gt;: Currently, Scenario outputs are limited to a single topic or sink. To address the increasing diversity of use cases, we aim to expand signal distribution, allowing downstream consumers to access Scenario outcomes through multiple scalable and reliable channels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Advanced scheduling and delayed triggering&lt;/strong&gt;: While real-time computation of Scenario signals is effective, certain use cases require delayed activation for maximum impact. We are exploring ways to compute signals instantly but trigger actions at scheduled times, such as sending a push notification for booking a return Grab ride based on the average wait time at the drop-off location.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-revolutionizing-real-time-personalization&quot;&gt;Conclusion: Revolutionizing real-time personalization&lt;/h2&gt;

&lt;p&gt;The launch of CDP Scenarios represents a significant milestone for Grab, paving the way for scalable, efficient, and user-friendly real-time personalization. Initial successes have demonstrated its immense potential, delivering notable improvements in user engagement and conversion rates. Looking ahead, we are committed to continuously advancing Scenarios by expanding its features, integrations, and applications to further elevate user experiences across the Grab ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdef1&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Thu, 18 Dec 2025 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/cdp-scenarios</link>
        <guid isPermaLink="true">https://engineering.grab.com/cdp-scenarios</guid>
        
        <category>Database</category>
        
        <category>FlinkSQL</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>A Decade of Defense: Celebrating Grab&apos;s 10th Year Bug Bounty Program</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Ten years ago, we launched our bug bounty program in partnership with &lt;a href=&quot;https://www.hackerone.com/blog&quot;&gt;HackerOne&lt;/a&gt;. Beyond a security initiative, it represented an open invitation to collaborative development.
As pioneers in Southeast Asia, we began the program with 23 initial researchers, and it has since evolved into a global community of security researchers.&lt;/p&gt;

&lt;p&gt;The strategic structure and scope of our Bug Bounty Program, combined with our continuous innovation and experimentation, have successfully captured the attention of the global security research community. Over the past decade, we have partnered with more than 850 active security researchers from HackerOne’s community of over 2 million cybersecurity professionals worldwide. These dedicated researchers work alongside us across borders and time zones, forming a collaborative defense network that helps protect over 187 million users throughout Southeast Asia. Their ongoing participation demonstrates both the maturity of our program and the trust we’ve built within the security research community.&lt;/p&gt;

&lt;p&gt;This milestone reflects the strength of shared purpose and our sustained partnership with the HackerOne platform. It demonstrates the value of human connection and the collective understanding that security is stronger through collaboration. Here’s to a decade of partnership and to many more years of building a safer future, one collaboration at a time!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/decade-of-defense/figure-1.jpg&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Ten years of achievements with our HackerOne partnership.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;evolution-and-growth-adapting-to-a-dynamic-threat-landscape&quot;&gt;Evolution and growth: Adapting to a dynamic threat landscape&lt;/h3&gt;

&lt;p&gt;Over the past ten years, our program has consistently adapted to the dynamic threat landscape and integrated invaluable feedback from our research community. We have grown from a private initiative to a program that consistently ranks among the top 20 worldwide and among the top 3 in Asia on HackerOne. Key milestones from our journey include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expanding our horizons:&lt;/strong&gt; Our scope significantly broadened in 2023-2024, continuously adding new assets and prominently including financial services in Indonesia and AI systems. This expansion provides researchers with more avenues to contribute to Grab’s security.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Focused mobile security:&lt;/strong&gt; We introduced a dedicated bounty table for mobile-specific issues, recognizing the unique challenges of mobile security.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incentivizing excellence:&lt;/strong&gt; We regularly experiment with campaigns of various types and targets, diversifying our reward methods to include both financial rewards and recognition.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evolving vulnerability focus:&lt;/strong&gt; We’ve observed a significant shift in the types of vulnerabilities reported over the decade, moving from foundational issues in early years to more sophisticated and emerging categories recently.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/decade-of-defense/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. The journey of our bug bounty program.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-global-stage-connecting-with-the-best&quot;&gt;The global stage: Connecting with the best&lt;/h3&gt;

&lt;p&gt;Our program’s success is deeply rooted in its vibrant global community, which we actively foster through continuous engagement. Our strategy extends beyond the platform to major live hacking events, including the &lt;strong&gt;ThreatCon Live Hacking Event 2023&lt;/strong&gt; &lt;strong&gt;in Nepal&lt;/strong&gt; and &lt;strong&gt;DEFCON 32’s Live Recon Village 2024 in Las Vegas.&lt;/strong&gt; These initiatives have been instrumental in connecting us with a diverse pool of new talent and strengthening relationships with researchers across different continents. By meeting hackers where they are, we’ve not only brought new expertise into our ecosystem but also demonstrated our commitment to being an accessible and collaborative partner on a global scale.&lt;/p&gt;

&lt;p&gt;The high participation and quality submissions from these events demonstrate the effectiveness of this approach. They’ve expanded our global security testing coverage and strengthened our standing within the worldwide cybersecurity community. Through ongoing interactions and submitted reports, we continue to see that security is a collaborative effort with no borders.&lt;/p&gt;

&lt;h3 id=&quot;exclusive-anniversary-celebrations-global-club-campaigns&quot;&gt;Exclusive anniversary celebrations: Global club campaigns&lt;/h3&gt;

&lt;p&gt;To commemorate our 10th anniversary, we launched three exclusive, invite-only campaigns with HackerOne’s regional clubs in &lt;strong&gt;Germany, Morocco, and India&lt;/strong&gt;. These campaigns served as cultural exchanges, bringing fresh perspectives from outside our core Southeast Asian consumer markets. By engaging with these clubs, we expanded our researcher community and connected with security experts who understand different threat landscapes and methodologies, bringing outside perspectives to our systems.&lt;/p&gt;

&lt;p&gt;In August, we also ran a broader anniversary campaign that drew significant participation from the researcher community, resulting in 461 submissions. &lt;a href=&quot;https://hackerone.com/xchopath?type=user&quot;&gt;xchopath&lt;/a&gt; was awarded the Best Hacker Bonus for their contributions during this campaign.&lt;/p&gt;

&lt;p&gt;These campaigns expanded our global security testing coverage and strengthened relationships with international researcher communities. Beyond vulnerability reports, they functioned as knowledge-sharing initiatives. We connected directly with researchers to learn from their experience and feedback, creating a continuous loop of improvement. This international collaboration also informed our global expansion security strategy by providing insights into how different regions approach digital payments and authentication.&lt;/p&gt;

&lt;p&gt;The anniversary campaigns allowed us to validate our security frameworks against diverse regulatory environments and advanced testing methodologies from established security markets, reinforcing our commitment to maintaining robust security standards.&lt;/p&gt;

&lt;h3 id=&quot;voices-from-our-community&quot;&gt;Voices from our community&lt;/h3&gt;

&lt;p&gt;Behind every vulnerability report is a researcher who chose to help make Grab safer. Their perspectives reveal the human side of our security evolution. These individuals are not just cybersecurity experts; they are partners in our mission to protect millions of users and ensure a safe digital environment. Here are a few testimonies from participants in our past campaigns:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“The triage was very fast despite the time difference, which I really appreciated. The triaging experience was better than other programs. The huge scope and business portal with different user roles made it especially interesting to explore.” – &lt;a href=&quot;https://hackerone.com/artsec?type=user&quot;&gt;&lt;em&gt;ArtSec&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Germany club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“I liked that different countries have different features—this gives me more attack surface to explore. Response time was great, triage was very fast, and I appreciated Grab’s effort in providing fast responses. The scope was huge with a lot of wildcards for reconnaissance.” – &lt;a href=&quot;https://hackerone.com/sicksec?type=user&quot;&gt;&lt;em&gt;Sicksec&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Morocco club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“More than 20 bugs were reported, and was particularly happy that bounties were being paid upon triage. The Germany team spent a lot of time on the educational part, especially for newcomers. Communication overall was very good, and the immediate response even outside working hours was really cool. SSO and authentication is my expertise and I liked that aspect of exploring the platform.” – &lt;a href=&quot;https://hackerone.com/lauritz?type=user&quot;&gt;&lt;em&gt;Lauritz&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Germany club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-road-ahead-our-commitment-to-a-secure-future&quot;&gt;The road ahead: Our commitment to a secure future&lt;/h3&gt;

&lt;p&gt;With a strong community of security researchers across countries and a decade of collaboration, we’ve built meaningful partnerships. Every vulnerability report represents trust, and every discovery reflects dedication to our shared mission. The program demonstrates our choice to build together rather than work in isolation, to protect rather than exploit, and to collaborate rather than compete.&lt;/p&gt;

&lt;p&gt;While we celebrate our external community, the success of our program relies equally on our dedicated internal teams. Our cybersecurity teams form the operational foundation of this initiative. Their consistent responsiveness and researcher-focused approach have enabled vulnerability reporting to evolve into a genuine partnership, maintaining researcher trust and keeping Grab secure.&lt;/p&gt;

&lt;p&gt;The next ten years will bring challenges we can’t yet imagine, from emerging threats in artificial intelligence to novel cryptographic approaches in a quantum-powered world. We will face them together as a community that spans cultures, time zones, and expertise.&lt;/p&gt;

&lt;p&gt;Together, we’ll continue securing Southeast Asia’s digital future, one partnership, one discovery, one shared achievement at a time.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility, and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people every day to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdef&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Dec 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/a-decade-of-defense</link>
        <guid isPermaLink="true">https://engineering.grab.com/a-decade-of-defense</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Real-time data quality monitoring: Kafka stream contracts with syntactic and semantic test</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In today’s data-driven landscape, monitoring data quality has become a critical need for ensuring reliable and efficient data usage across domains. High-quality data is the backbone of AI innovation, driving efficiency and unlocking new opportunities. As decentralized data ownership grows, the ability to effectively monitor data quality is essential for maintaining reliability in data systems.&lt;/p&gt;

&lt;p&gt;Kafka streams, as a vital component of real-time data processing, play a significant role in this ecosystem. However, unreliable data within Kafka streams can lead to errors and inefficiencies for downstream users, and monitoring the quality of data within these streams has always been a challenge. This blog introduces a solution that empowers stream users to define a data contract, specifying the rules that Kafka stream data must adhere to. By leveraging this user-defined data contract, the solution performs automated real-time data quality checks, identifies problematic data as it occurs, and promptly notifies stream owners. This ensures timely action, enabling effective monitoring and management of Kafka stream data quality while supporting the broader goals of data mesh and AI-driven innovation.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;In the past, monitoring Kafka stream data processing lacked an effective solution for data quality validation. This limitation made it challenging to identify bad data, notify users in a timely manner, and prevent the cascading impact on downstream users from further escalating.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Challenges in syntactic and semantic issue identification&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Syntactic issues&lt;/strong&gt;: Refers to schema mismatches between producers and consumers, which can lead to deserialization errors. While schema backward compatibility can be validated upon schema evolution, there are scenarios where the actual data in the Kafka topic does not align with the defined schema. For example, this can occur when a rogue Kafka producer is not using the expected schema for a given Kafka topic. Identifying the specific fields causing these syntactic issues is a typical challenge.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semantic issues&lt;/strong&gt;: Refers to inconsistencies or misalignments between producers and consumers about the expected pattern or significance of each field. Unlike Kafka stream schemas, which act as a data structure contract between producers and consumers, there is no existing framework for stakeholders to define and enforce field-level semantic rules, for example, the expected length or pattern of an identifier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Timeliness challenge in data quality monitoring&lt;/strong&gt;: There is no real-time mechanism to automatically validate data against predefined rules, timely identify quality issues, and promptly alert stream stakeholders. Without real-time stream validation, data quality issues can sometimes persist for periods of time, impacting various online and offline downstream systems before being discovered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Observability challenge for troubleshooting bad data&lt;/strong&gt;: Even when problematic data is identified, stream users face difficulties in pinpointing the exact “poison data” and understanding which fields are incompatible with the schema or violate semantic rules. This lack of visibility complicates Root Cause Analysis and resolution efforts.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Our &lt;a href=&quot;https://engineering.grab.com/an-elegant-platform&quot;&gt;Coban platform&lt;/a&gt; offers a standardized data quality test and observability solution at the platform level, consisting of the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Contract Definition&lt;/strong&gt;: Enables Kafka stream stakeholders to define contracts that include schema agreements, semantic rules that Kafka topic data must comply with, and Kafka stream ownership details for alerting and notifications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automated Test Execution&lt;/strong&gt;: Provides a long running Test Runner to automatically execute real-time tests based on the defined contract.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time Data Quality Issue Identification&lt;/strong&gt;: Detects data issues at both syntactic and semantic levels in real-time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alerts and Result Observability&lt;/strong&gt;: Alerts users, simplifying observation of data quality issues via the platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;p&gt;The solution includes three components: &lt;em&gt;Data Contract Definition, Test Execution &amp;amp; Data Quality Issue Identification, and Result Observability as shown in the architecture diagram in figure 1&lt;/em&gt;. All mentions of “Flow” from here onwards refer to the corresponding processes illustrated in figure 1.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/coban-architecture.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Real-time Kafka Stream Data Quality Monitoring Architecture diagram.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;data-contract-definition&quot;&gt;Data Contract Definition&lt;/h4&gt;

&lt;p&gt;The Coban Platform streamlines the process of defining Kafka stream data contracts, serving as a formal agreement among Kafka stream stakeholders. This includes the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Schema&lt;/strong&gt;: Represents the schema used by the Kafka topic under test and helps the Test Runner to validate schema compatibility across data streams (Flow 1.1).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Configuration&lt;/strong&gt;: Encompasses essential configurations such as the endpoint and topic name, which the platform automatically populates (Flow 1.2).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability Metadata&lt;/strong&gt;: Provides contact information for notifying Kafka stream stakeholders about data quality issues and includes alert configurations for monitoring (Flow 1.3).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Semantic Test Rules&lt;/strong&gt;: Empowers users to define intuitive semantic test rules at the field level. These rules include checks for string patterns, number ranges, constant values, etc. (Flow 1.5).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LLM-Based Semantic Test Rules Recommendation&lt;/strong&gt;: Defining dozens if not hundreds of field-specific test rules can overwhelm users. To simplify this process, the Coban Platform uses LLM-based recommendations to predict semantic test rules using provided Kafka stream schemas and anonymized sample data (Flow 1.4). This feature helps users set up semantic rules efficiently, as demonstrated in the sample UI in figure 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/sample-ui.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Sample UI showcasing LLM-based Kafka stream schema field-level semantic test rules. Note that the data shown is entirely fictional.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;data-contract-transformation&quot;&gt;Data Contract Transformation&lt;/h4&gt;

&lt;p&gt;Once defined, the Coban Platform’s transformation engine converts the data contract into configurations that the Test Runner can interpret (Flow 2.1). This transformation process includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Schema&lt;/strong&gt;: Translates the schema defined in the data contract into a schema reference that the Test Runner can parse.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Configuration&lt;/strong&gt;: Sets up the Kafka stream as a source for the Test Runner.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability metadata&lt;/strong&gt;: Sets contact information as configurations of the Test Runner.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Semantic Test Rules&lt;/strong&gt;: Transforms human-readable semantic test rules into an inverse SQL query to capture the data that violates the defined rules.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/semantic-test-rules.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Illustration of semantic test rules being converted from human-readable formats into inverse SQL queries.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;test-execution--data-quality-issue-identification&quot;&gt;Test Execution &amp;amp; Data Quality Issue Identification&lt;/h3&gt;

&lt;p&gt;Once the Test Configuration Transformation Engine generates the Test Runner configuration (Flow 2.1), the platform automatically deploys the Test Runner.&lt;/p&gt;

&lt;h4 id=&quot;test-runner&quot;&gt;Test Runner&lt;/h4&gt;

&lt;p&gt;The Test Runner utilises FlinkSQL as the compute engine to execute the tests. FlinkSQL was selected for its flexibility in defining test rules as straightforward SQL statements, enabling our platform to efficiently convert data contracts into enforceable rules.&lt;/p&gt;

&lt;h4 id=&quot;test-execution-workflow-and-problematic-data-identification&quot;&gt;Test Execution Workflow And Problematic Data Identification&lt;/h4&gt;

&lt;p&gt;FlinkSQL consumes data from the Kafka topic under test (Flow 2.2) using its own consumer group, ensuring it doesn’t impact other consumers. It runs the inverse SQL query (Flow 2.3) to identify any data that violates the semantic rules or that is syntactically incorrect in the first place. Test Runner captures such data, packages it into a data quality issue event enriched with a test summary, the total count of bad records, and sample bad data, and publishes it to a dedicated Kafka topic (Flow 3.2). Additionally, the platform sinks all such data quality events to an AWS S3 bucket (Flow 3.1) to enable deeper observability and analysis.&lt;/p&gt;

&lt;h3 id=&quot;result-observability&quot;&gt;Result Observability&lt;/h3&gt;

&lt;p&gt;Grab’s in-house data quality observability platform, Genchi, consumes problematic data captured by the Test Runner (Flow 3.3).&lt;/p&gt;

&lt;h4 id=&quot;alerting&quot;&gt;Alerting&lt;/h4&gt;
&lt;p&gt;Genchi sends Slack notifications (Flow 3.5) to stream owners specified in the data contract observability metadata. These notifications include detailed information about stream issues, such as links to sample data in Coban UI, observed windows, counts of bad records, and other relevant details.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/slack-notification.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Sample Slack notifications
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;observability&quot;&gt;Observability&lt;/h4&gt;

&lt;p&gt;Users can access the Coban UI (Flow 3.4), displaying Kafka stream test rules and sample bad records, highlighting fields and values that violate rules.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/sample-test-result.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. In this Sample Test Result, the highlighted fields indicate violations of the semantic test rules.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since its deployment earlier this year, the solution has enabled Kafka stream users to define contracts with syntactic and semantic rules, automate test execution, and alert users when problematic data is detected, prompting timely action. It has been actively monitoring data quality across 100+ critical Kafka topics. The solution offers the capability to immediately identify and halt the propagation of invalid data across multiple streams.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We implemented and rolled out a solution to assist Grab engineers in effectively monitoring data quality in their Kafka streams. This solution empowers them to establish syntactic and semantic tests for their data. Our platform’s automatic testing feature enables real-time tracking of data quality, with instant alerts for any discrepancies. Additionally, we provide detailed visibility into test results, facilitating the easy identification of specific data fields that violate the rules. This accelerates the process of diagnosing and resolving issues, allowing users to swiftly address production data challenges.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;While our current solution emphasizes monitoring the quality of Kafka streaming data, further exploration will focus on tracing producers to pinpoint the origin of problematic data, as well as enabling more advanced semantic tests such as cross-field validations. Additionally, we aim to expand monitoring capabilities to cover broader aspects like data completeness and freshness, and integrate with &lt;a href=&quot;https://www.gable.ai/&quot;&gt;Gable AI&lt;/a&gt; to detect Data Transfer Object (DTO) changes and semantic regressions in Go producers upon committing code to the Git repository. These enhancements will pave the way for a more robust, multidimensional data quality testing solution across a wider range.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.oreilly.com/library/view/driving-data-quality/9781837635009/&quot;&gt;Driving Data Quality with Data Contracts: A Comprehensive Guide to Building Reliable, Trusted, and Effective Data Platforms&lt;/a&gt; by &lt;a href=&quot;https://www.oreilly.com/search?q=author:%22Andrew%20Jones%22&quot;&gt;Andrew Jones&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdatam&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/real-time-data-quality-monitoring</link>
        <guid isPermaLink="true">https://engineering.grab.com/real-time-data-quality-monitoring</guid>
        
        <category>Engineering</category>
        
        <category>Kafka</category>
        
        <category>Performance</category>
        
        <category>Data science</category>
        
        <category>Data processing</category>
        
        <category>Real-time streaming</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>SpellVault’s evolution: Beyond LLM apps, towards the agentic future</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, innovation isn’t just about building new features; it’s about evolving our platforms to meet the changing needs of our users and the broader technological landscape. &lt;a href=&quot;https://www.grab.com/sg/inside-grab/stories/ai-llm-productivity-tool-apps-coding/&quot;&gt;SpellVault&lt;/a&gt;, our internal AI platform, exemplifies this philosophy. When SpellVault was first launched, our vision was straightforward: empower everyone at Grab to effortlessly build and manage AI-powered apps without the need for coding. Built on the principles of Retrieval-Augmented Generation (RAG) and enhanced by plugin support, SpellVault rapidly evolved into a powerful productivity engine for the organization, enabling the creation of thousands of apps that drive automation, foster experimentation, and support production use cases.&lt;/p&gt;

&lt;p&gt;As the AI landscape has evolved, SpellVault has grown alongside it. Initially launched as a straightforward no-code app builder for Large Language Models (LLMs), it has now evolved into a cutting-edge platform that embraces the agentic future—a future where AI goes beyond generating responses to reasoning, acting, and dynamically adapting through the use of tools and contextual understanding.&lt;/p&gt;

&lt;p&gt;This article outlines SpellVault’s journey towards an agentic future and how we empower users to build AI Agents that are smarter, more adaptable, and ready for the future.&lt;/p&gt;

&lt;h2 id=&quot;a-no-code-platform-for-building-llm-apps&quot;&gt;A no-code platform for building LLM apps&lt;/h2&gt;

&lt;p&gt;SpellVault was founded with a clear mission: to democratize access to AI for everyone at Grab, regardless of their technical expertise. Initially launched as a no-code LLM app builder, the platform was built on a foundation of RAG pipelines and basic plugin support.&lt;/p&gt;

&lt;p&gt;Early on, we recognized that the true potential of AI apps extends beyond the capabilities of language models alone. Their real value lies in the ability to seamlessly interact with external systems and diverse data sources. This insight drove our commitment to minimizing barriers and ensuring users could access data from various sources with ease. From the very beginning, we centered our efforts on three key focus areas:&lt;/p&gt;

&lt;h4 id=&quot;comprehensive-rag-solution-with-useful-integrations&quot;&gt;Comprehensive RAG solution with useful integrations&lt;/h4&gt;

&lt;p&gt;From the start, the SpellVault team prioritized enabling users to enhance their LLM apps with data through RAG. Rather than solely relying on the LLM’s internal information, we wanted the apps to ground their responses in up-to-date, contextually relevant, and factual information. SpellVault has built-in integrations with knowledge sources such as Wikis, Google Docs, as well as plain text and PDF uploads. These capabilities empower users to build assistants that reference relevant knowledge and provide more accurate, verifiable answers.&lt;/p&gt;

&lt;h4 id=&quot;plugins-to-fetch-information-on-demand&quot;&gt;Plugins to fetch information on demand&lt;/h4&gt;

&lt;p&gt;To move beyond static knowledge retrieval, we needed a way for apps to act dynamically. This was made possible through SpellVault plugins—modular components that allow apps to interact with internal systems (e.g. service dashboards, incident trackers) and external APIs (e.g. search engines, weather data). Rather than being confined to their initial prompt and data, these plugins can fetch fresh information at runtime. From the available plugin types, users can create their own instances of plugins with custom settings, enabling highly specialized functionality tailored to their specific workflows. For instance, with SpellVault’s HTTP plugin, users can define custom endpoints and credentials, enabling their AI apps to make tailored HTTP calls during runtime. These custom plugins have become the backbone of many of our most impactful apps, empowering teams to seamlessly integrate SpellVault with their existing systems and processes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. SpellVault’s early architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;making-spellvault-accessible-via-common-interfaces-web-slack-api&quot;&gt;Making SpellVault accessible via common interfaces: Web, Slack, API&lt;/h4&gt;

&lt;p&gt;One of our primary goals was to make AI seamlessly accessible and useful within the tools users already use—whether it’s a browser or Slack. With SpellVault, users can make their AI apps in minutes and start using them via browser or Slack messaging immediately and intuitively, without requiring any additional setup. We also exposed APIs that enabled other internal services to integrate with SpellVault apps for a variety of use cases. This multi-channel approach ensured that SpellVault wasn’t just a standalone sandbox but a platform woven into existing tools and processes.&lt;/p&gt;

&lt;p&gt;Users quickly adopted the platform, creating thousands of apps for internal productivity gains, automation, and even production use cases. The platform’s success validated our hypothesis that there was significant demand for democratized AI tools within the organization.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. SpellVault’s web interface for LLM App configuration and chat.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;evolution-over-time&quot;&gt;Evolution over time&lt;/h2&gt;

&lt;p&gt;The AI landscape over the past few years has been defined by relentless change. New frameworks, execution paradigms, and standards have emerged in quick succession, each promising to make AI systems more powerful, more reliable, or more extensible. At Grab, we recognized that for SpellVault to stay relevant, it could not remain static. It needed to evolve in tandem with the ever-changing ecosystem, continuously incorporating valuable advancements while ensuring a seamless experience for our users.&lt;/p&gt;

&lt;p&gt;This philosophy of continuous adaptation has guided SpellVault’s journey. From its early days as a simple RAG-powered app builder with a few plugins, the platform grew to support an extensive number of plugin types, richer execution models, and eventually a unified approach to tools. Each step was a response both to the needs of our users and to the shifting definition of what “building with AI” meant in practice. Rather than opting for a complete overhaul, SpellVault has embraced incremental advancements, ensuring that users can seamlessly benefit from new capabilities without disruption.&lt;/p&gt;

&lt;p&gt;This approach to evolution has naturally positioned SpellVault to transition from a platform for LLM apps to one designed for AI agents. The following section delves into this transition in greater detail.&lt;/p&gt;

&lt;h3 id=&quot;expanding-capabilities&quot;&gt;Expanding capabilities&lt;/h3&gt;

&lt;p&gt;Over time, we introduced numerous new capabilities to SpellVault, driven both by user feedback and our commitment to innovation and staying ahead of industry trends. For instance, we extended support for different plugin types, enabling integrations with tools like Slack and Kibana, and continuously added more integrations to enhance the platform’s versatility. We implemented auto-updates for users’ Knowledge Vaults, ensuring their data remained current. With more users building with the platform, ensuring the trustworthiness of responses generated by SpellVault apps became increasingly important. We included citation capability to mitigate some of that concern. Recognizing the need for more precise answers to mathematical problems, we developed a feature that enabled LLMs to solve such problems using Python runtime. Additionally, many users requested an automated way to trigger their LLM apps, which led to the creation of a Task Scheduler feature that allows LLMs to schedule actions based on natural language user input.&lt;/p&gt;

&lt;p&gt;A significant milestone in SpellVault’s evolution was the introduction of “Workflow,” a drag-and-drop interface within the platform that empowered users to design deterministic workflows. These workflows enabled users to seamlessly combine various components from the SpellVault ecosystem—such as LLM calls, Python code execution, and Knowledge Vault lookups—in a predefined and structured manner. This enabled advanced use cases for many users.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Evolving tools landscape of SpellVault with increasing integrations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;shifting-the-execution-model&quot;&gt;Shifting the execution model&lt;/h3&gt;

&lt;p&gt;As SpellVault evolved, a fundamental shift took place in the way its apps were executed internally. We transitioned from our &lt;a href=&quot;https://python.langchain.com/docs/how_to/agent_executor/&quot;&gt;legacy executor system&lt;/a&gt;, which facilitated one-off information retrieval from the Knowledge Vault or user plugins, to a more advanced &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/low_level/&quot;&gt;graph based executor&lt;/a&gt;. This empowered SpellVault’s app execution with nodes, edges, and states that supported branching, looping, and modularity. This laid the groundwork for more sophisticated agent behaviors, moving beyond the linear input-output paradigm.&lt;/p&gt;

&lt;p&gt;This transformed all existing SpellVault apps into ‘Reasoning and Acting’ agents, better known as &lt;a href=&quot;https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html&quot;&gt;ReAct agents&lt;/a&gt; - a “one size fits many” solution that significantly enhanced the capabilities of these apps. By enabling them to leverage the Knowledge Vault and plugins in a more agentic and dynamic manner, the ReAct agent framework allowed apps to perform more complex tasks while seamlessly preserving their existing functionality, ensuring no disruption to their behavior.&lt;/p&gt;

&lt;p&gt;In addition, the internal decoupling of the executor and prompt engineering components enabled us to design multiple execution pathways with ease. This allowed us to provide generic Deep Research capability to any SpellVault app via a simple UI checkbox, as well as sophisticated internal workflows that cater to high-ROI complex use cases like on-call alert analysis. The Deep Research capability came with SpellVault’s ability to search across internal information repositories (e.g., Slack messages, Wiki, Jira) within Grab, as well as searching online for relevant information.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. SpellVault’s evolved architecture with more dynamic context gathering and advanced interaction modes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;towards-an-agentic-framework&quot;&gt;Towards an agentic framework&lt;/h3&gt;

&lt;p&gt;Over time, several capabilities were added to SpellVault, including features like Python code execution and internal repository search. Initially, these functionalities were integrated directly into the core PromptBuilder class. For users, these features were primarily accessible through simple checkboxes in the user interface. As SpellVault gradually transitioned towards giving more agency to user-crafted apps, we recognized that these capabilities should instead be positioned as “Tools” for LLMs to use with greater autonomy, similar to how ReAct agent–backed apps have been using SpellVault’s user plugins. We also understood that this shift could bring a clearer mental model for users where they were no longer simply toggling features but creating AI agents with access to a defined set of tools. The agents could then decide when and how to use those tools intelligently to accomplish tasks, making the overall experience more natural and intuitive.&lt;/p&gt;

&lt;p&gt;This recognition led to the consolidation of these scattered capabilities into a unified framework called “Native Tools.” These Native Tools, along with SpellVault’s existing user plugins—rebranded as “Community Built Tools”—formed a comprehensive collection of tools that LLMs could dynamically invoke at runtime. Despite being grouped under the same umbrella, a key distinction was maintained: Native Tools required no user-specific configuration (e.g., performing internet searches), whereas Community Built Tools were custom, user-configured entities (e.g., invoking specific HTTP endpoints) created from available plugin types, often requiring credentials or other personalized settings.&lt;/p&gt;

&lt;p&gt;This consolidation of capabilities under a unified Tools abstraction and enabling SpellVault apps to invoke them with greater autonomy marked a pivotal milestone in the platform’s evolution. It meaningfully shifted SpellVault toward making agentic behavior more natural, discoverable, and extensible for every app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. SpellVault’s Unified Tools housing both Native Tools and Community Built Tools.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;spellvault-as-an-mcp-service&quot;&gt;SpellVault as an MCP service&lt;/h3&gt;

&lt;p&gt;As we streamlined SpellVault’s internal capabilities into a unified tools framework, we also turned our focus outward to align with industry standards. The growing adoption of the &lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;Model Context Protocol&lt;/a&gt; (MCP) presented an opportunity for agents and clients to seamlessly interact without requiring custom integrations. To remain at the forefront of innovation, we adapted SpellVault to function as an MCP service, enabling it to actively participate in this evolving ecosystem. This extension brought two key advancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SpellVault apps as MCP tools&lt;/strong&gt;: Each app created in SpellVault can now be exposed through the MCP protocol. This allows other agents or MCP-compatible clients, such as IDEs or external orchestration frameworks, to treat a SpellVault app as a callable tool. Instead of living only inside our web user interface or Slack interface, these apps become accessible building blocks that other systems can invoke dynamically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;RAG as an MCP tool&lt;/strong&gt;: We extended the same idea to our Knowledge Vaults. Through MCP, external clients can search, retrieve, and even add information to Vaults. This effectively turns SpellVault’s RAG pipeline into an MCP-native service, making contextual grounding available to agents beyond SpellVault itself.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While building the SpellVault MCP Server, we also created &lt;a href=&quot;https://github.com/grab/tinymcp&quot;&gt;TinyMCP&lt;/a&gt; - a lightweight open-source Python library that adds MCP capabilities to an existing FastAPI app as just another router, instead of mounting a separate app.&lt;/p&gt;

&lt;p&gt;By exposing both apps and RAG through MCP, we shifted SpellVault from being a self-contained platform to becoming an interoperable service provider in the agentic ecosystem. Users still benefit from the no-code simplicity inside SpellVault. However, the output of their work, apps, and knowledge, are now usable by other agents and tools outside of it.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;SpellVault’s evolution shows how a platform can adapt with the AI landscape while staying true to its original mission of making powerful technology accessible to everyone. What began as a no-code builder for LLM apps has steadily expanded into an agentic platform - one where apps can act with more intelligence, agency, and context and interact with the systems around them.&lt;/p&gt;

&lt;p&gt;This progress wasn’t the result of a single breakthrough, but of steady, incremental improvements that introduced new capabilities while preserving ease of use. By layering in these advancements thoughtfully but boldly, SpellVault has managed to support more sophisticated agentic behaviors without compromising its original goal of democratizing AI at Grab.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebspell&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/spellvault-evolution-beyond-llm</link>
        <guid isPermaLink="true">https://engineering.grab.com/spellvault-evolution-beyond-llm</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Grab&apos;s Mac Cloud Exit supercharges macOS CI/CD</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In our mission to optimize continuous integration and delivery (CI/CD), we have taken a bold step by relocating our infrastructure from a cloud vendor in the US to a colocation cluster within Southeast Asia, closer to our Git server infrastructure. This change has dramatically improved the performance of our macOS builds, primarily by reducing the network traffic delays associated with distant data centers. By bringing our infrastructure closer to home, we have not only accelerated CI/CD job completion times but also massively slashed operational costs.&lt;/p&gt;

&lt;p&gt;Join us as we delve into the Mac Cloud Exit journey and the significant improvements it has brought to our workflows.&lt;/p&gt;

&lt;p&gt;Our macOS CI/CD infrastructure has evolved from 1 Physical Mac Pro running in our office to a cluster of 250 Mac minis fully occupied during peak hours of the day. There were multiple stages in the journey to transition to the current state. The following diagram shows the focus area for this blog post.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Infrastructure transition path&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;before-and-after-visualizing-the-evolution&quot;&gt;Before and after: Visualizing the evolution&lt;/h3&gt;

&lt;p&gt;We began our journey with a much simpler setup.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image2.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Photo of the setup when we started&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Today, that infrastructure has scaled significantly to meet the growing demands of Grab&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image3.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Mac mini cluster today&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;economy-at-scale-the-rent-vs-own-equation&quot;&gt;Economy at scale: The rent vs. own equation&lt;/h2&gt;

&lt;p&gt;At the beginning, it was a no-brainer to rent when our demand for macOS hardware increased from 1 MacPro to 20 times that size. However, when that grew to over 200 machines, the total cost became significant, prompting us to consider:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the desired reliability for this cluster?&lt;/li&gt;
  &lt;li&gt;What would be the total cost of ownership for us to build this cluster ourselves compared to cloud-based options?&lt;/li&gt;
  &lt;li&gt;What kind of operational leverage would it bring us by controlling end-to-end stack by ourselves?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-is-grabs-scale&quot;&gt;What is Grab’s scale&lt;/h3&gt;

&lt;p&gt;At Grab, our iOS build needs have scaled quite significantly, so we went from running some builds on a single Mac Pro to running them on an army of 250+ Mac minis. And so did the cost.&lt;/p&gt;

&lt;h4 id=&quot;active-jobs-trend&quot;&gt;Active jobs trend&lt;/h4&gt;

&lt;p&gt;The total number of jobs trend is one of the data points to understand the demand situation. The following chart is a snapshot from our demand curve in 2022. Peak demand often started to exceed the available supply, creating queues for the jobs.&lt;/p&gt;

&lt;p&gt;We estimated we would need 200+ machines to comfortably supply for the peak demand and projected a demand for 400+ machines in 2025.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Active macOS CI/CD jobs&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-our-workload&quot;&gt;What is our workload&lt;/h3&gt;

&lt;p&gt;We have several iOS apps that share a common macOS compute cluster for their CI/CD workloads. &lt;br /&gt;
This includes, but is not limited to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grab-taxi-ride-food-delivery/id647268330&quot;&gt;Grab app&lt;/a&gt; (Largest iOS code base with approximately 2.5M+ total lines of code)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grab-driver-app-for-partners/id1257641454&quot;&gt;Grab Driver app&lt;/a&gt; (Second largest iOS code base with approximately 0.7M+ total lines of code)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/kartalink/id6450411148&quot;&gt;KartaLink&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grabmerchant/id1282271764&quot;&gt;GrabMerchant&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/kartaview/id1089548849&quot;&gt;KartaView&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/us/app/ovo/id1142114207&quot;&gt;OVO&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/ph/app/move-it-fast-moto-taxi-ride/id1481198245&quot;&gt;Move It: Fast Moto Taxi Ride&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/ph/app/move-it-driver-app/id6446633186&quot;&gt;Move It Driver App&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The workload primarily involves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Building apps&lt;/li&gt;
  &lt;li&gt;Execution of tests&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-evaluation-cloud-vs-colocation-vs-on-premises&quot;&gt;The evaluation: Cloud vs colocation vs on-premises&lt;/h2&gt;

&lt;p&gt;We did a comprehensive comparison and total cost of ownership (TCO) estimation to compare many different options, including cloud vendors and colocation in different places.&lt;/p&gt;

&lt;h3 id=&quot;cost-of-macos-compute&quot;&gt;Cost of macOS compute&lt;/h3&gt;

&lt;p&gt;The expense of macOS compute is notably higher, particularly in continuous integration (CI) setups, posing challenges for optimal configuration. Several factors contribute to these increased costs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apple’s restrictive EULA mandates a minimum lease period of 24 hours for macOS instances, which alters the utilization equation.&lt;/li&gt;
  &lt;li&gt;Economies of scale are not favorable for available macOS hardware configurations compared to alternatives. Optimized server hardware designed for racking offers various configurations that reduce operational costs, unlike macOS options such as Mac Mini and Mac Pro.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, although not a direct comparison, the &lt;a href=&quot;https://docs.github.com/en/billing/managing-billing-for-your-products/managing-billing-for-github-actions/about-billing-for-github-actions#per-minute-rates-for-standard-runners&quot;&gt;pricing for GitHub Actions build minutes&lt;/a&gt; shows macOS is ten times more costly than Linux. This reflects the pricing GitHub can offer after &lt;a href=&quot;https://www.youtube.com/watch?v=I2J2MzKjcqY&quot;&gt;implementing racking optimizations.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Initially, we conducted rough estimations to assess the total cost of ownership differences between cloud, colocation, and on-premises setups. Even with conservative estimates for manpower and engineering costs, colocation or on-premises setups proved more cost-effective at our scale. This cost disparity became even more pronounced when focusing on cloud vendors providing macOS compute physically located in Southeast Asia.&lt;/p&gt;

&lt;p&gt;We opted to conduct an in-depth evaluation of the following options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Establishing a macOS cluster at our headquarters in Singapore, which was swiftly dismissed due to scalability and cost concerns making it an unsuitable long-term solution.&lt;/li&gt;
  &lt;li&gt;Colocating in a Southeast Asian country where we have operational presence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;choice-of-location&quot;&gt;Choice of location&lt;/h3&gt;

&lt;p&gt;As a Southeast Asian company, we maintain offices in each country where we operate, some of which boast advanced data center infrastructures. We focused our location choices on Singapore and Malaysia, assessing them based on several criteria, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The maturity of existing data center infrastructure.&lt;/li&gt;
  &lt;li&gt;The proximity of the data centers to our offices, ensuring staff availability for infrastructure setup.&lt;/li&gt;
  &lt;li&gt;The cost and reliability of power.&lt;/li&gt;
  &lt;li&gt;The proximity to our Git servers and the expense of establishing direct network connections.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Eventually we concluded to go ahead with a decision to colocate in a data center in Malaysia &lt;a href=&quot;https://www.edgeconnex.com/news/edge-blog/southeast-asias-data-center-powerhouse-malaysia/&quot;&gt;which is one of the emerging data center powerhouses in the region&lt;/a&gt; with relatively low energy cost compared to Singapore.&lt;/p&gt;

&lt;h3 id=&quot;choice-of-mac-hardware&quot;&gt;Choice of Mac hardware&lt;/h3&gt;

&lt;p&gt;Our choice of hardware model for our build and test workload was guided by a cost-benefit analysis. We decided to use bare-metal setups without virtualization, simplifying migration processes, which may be revisited in the future. We ensured we neither over-specified nor under-specified the bare-metal hardware. We had a clear understanding of the resource consumption of our most demanding workload on a few reference models, as illustrated in the following graphs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. User and system CPU usage during build&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Memory usage&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;virtualization-vs-bare-metal&quot;&gt;Virtualization vs bare-metal&lt;/h3&gt;

&lt;p&gt;Virtualization offers significant advantages in managing and provisioning clusters, including the flexibility to create ephemeral builds. However, our experience with macOS virtualization has been mixed. While off-the-shelf virtualization solutions provide maintenance benefits, they often come at the cost of performance or stability.&lt;/p&gt;

&lt;p&gt;Key points:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Improved utilization&lt;/strong&gt;: Virtualization can improve resource utilization by consolidating multiple workloads on fewer physical servers, thereby reducing the overall cost.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Performance penalty&lt;/strong&gt;: However, the performance penalty associated with virtualization can sometimes negate these cost benefits. This is particularly true for macOS virtualization, where we have observed trade-offs in performance or stability.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evolution of virtualization&lt;/strong&gt;: The virtualization space has been evolving and making good progress. We may re-evaluate these solutions in the future as they continue to mature and potentially address current performance and stability issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our conclusion was to stick to bare-metal for the time-being as the benefits didn’t justify the downside and cost.&lt;/p&gt;

&lt;h2 id=&quot;execution&quot;&gt;Execution&lt;/h2&gt;

&lt;h3 id=&quot;progressive-migration&quot;&gt;Progressive migration&lt;/h3&gt;

&lt;p&gt;Any disruption to the macOS CI/CD cluster would be hugely disruptive to the company given our scale highlighted above. So, we enabled new cluster partially for part of the workload for a reasonably long period of time and monitored and compared:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Job failure rate&lt;/li&gt;
  &lt;li&gt;Jobs performance&lt;/li&gt;
  &lt;li&gt;Reliability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once we were confident, we made the full switch and terminated vendor contracts at due.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image7.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Total active jobs trend&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;The migration yielded better results overall than our initial conservative estimates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cost savings: Estimated over 2.4 million USD over three years&lt;/li&gt;
  &lt;li&gt;Performance improvement: Between 20-40% depending on the use case&lt;/li&gt;
  &lt;li&gt;Stability: No compromise&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A strategic investment in our mission to drive Southeast Asia forward by onshoring critical Mac infrastructure into the region.&lt;/p&gt;

&lt;h3 id=&quot;cost&quot;&gt;Cost&lt;/h3&gt;

&lt;p&gt;We anticipate a three-year replacement cycle for our hardware. While some equipment may be utilized beyond this period, it provides a reasonable lifespan for cost estimation purposes.&lt;/p&gt;

&lt;p&gt;The lifecycle of networking equipment involves both physical reliability, following the bathtub curve, and technological obsolescence, often necessitating replacement every 3 to 5 years. Mac minis could become outdated after approximately three years, making the opportunity cost of extended use potentially higher than the net replacement cost after benefits.&lt;/p&gt;

&lt;p&gt;Importantly, the experience gained during this cycle could significantly reduce the engineering costs associated with future replacements.&lt;/p&gt;

&lt;p&gt;Overall, we project total cost of ownership savings of approximately 2.4 million USD over a three-year period compared to our last cloud-based setup rented from a vendor.&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;We measured the performance gains in two of our largest iOS apps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Grab app&lt;/li&gt;
  &lt;li&gt;Grab Driver app&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;overall-gains&quot;&gt;Overall gains&lt;/h4&gt;

&lt;p&gt;The following table summarizes the total time measured before and after the migration for total CI pipeline time and building the app codebase. Measurements are presented in 3 percentiles (p50, p75, p95).&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th rowspan=&quot;2&quot;&gt;App / Metric&lt;/th&gt;
            &lt;th rowspan=&quot;2&quot;&gt;&lt;/th&gt;
            &lt;th colspan=&quot;3&quot;&gt;Time (Minutes)&lt;/th&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;th&gt;p50&lt;/th&gt;
            &lt;th&gt;p75&lt;/th&gt;
            &lt;th&gt;p95&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;CI pipeline time trend for the Grab app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;43&lt;/td&gt;
            &lt;td&gt;54&lt;/td&gt;
            &lt;td&gt;67&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;33&lt;/td&gt;
            &lt;td&gt;42&lt;/td&gt;
            &lt;td&gt;49&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;23.26%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;22.22%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;26.87%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;App build time trend for the Grab app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;10.7&lt;/td&gt;
            &lt;td&gt;13.2&lt;/td&gt;
            &lt;td&gt;17.6&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;6.45&lt;/td&gt;
            &lt;td&gt;9&lt;/td&gt;
            &lt;td&gt;10.8&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;39.72%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;31.82%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.64%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;Pipeline time trend for the Grab Driver app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;47&lt;/td&gt;
            &lt;td&gt;50&lt;/td&gt;
            &lt;td&gt;52&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;26&lt;/td&gt;
            &lt;td&gt;31&lt;/td&gt;
            &lt;td&gt;32&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;44.68%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.00%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.46%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;App build time trend for the Grab Driver app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;10&lt;/td&gt;
            &lt;td&gt;13&lt;/td&gt;
            &lt;td&gt;14&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;6&lt;/td&gt;
            &lt;td&gt;8&lt;/td&gt;
            &lt;td&gt;8.5&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;40.00%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.46%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;39.29%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;a-different-perspective-trends&quot;&gt;A different perspective: Trends&lt;/h4&gt;

&lt;p&gt;The following trend illustrations show how the performance of various tasks has improved while we progressively migrated to the new colocation setup.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image8.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. 14 day aggregate percentiles of p50, p75 and p95 for total CI pipeline times for the Grab app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image9.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. Pipeline time pulse for the Grab app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image10.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10. 14 day aggregate percentiles of p50, p75 and p95 for total CI pipeline times for the Grab Driver app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;stability&quot;&gt;Stability&lt;/h3&gt;

&lt;p&gt;We measured overall job failure rates between both clusters for extended periods as a guardrail metric and ensured the stability of the new cluster before shutting down the old one.&lt;/p&gt;

&lt;h2 id=&quot;colocation-setup-and-rack-configuration&quot;&gt;Colocation setup and rack configuration&lt;/h2&gt;

&lt;p&gt;The following table provides an overview of the layout of our new Mac mini cluster.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;Component&lt;/th&gt;
            &lt;th&gt;Description&lt;/th&gt;
            &lt;th&gt;Redundancy&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;Rack&lt;/td&gt;
            &lt;td&gt;We have got four 42RU (600x1200x42RU) racks housing 200+ Mac minis, plus some spare racks to house upcoming scheduled capacity upgrades.&lt;/td&gt;
            &lt;td&gt;Racks have shared resources which have their own redundancy. Generally rack separation does provide some level of redundancy for total compute.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Power&lt;/td&gt;
            &lt;td&gt;2 power sources power the cluster. Each rack is powered by these 2 power sources. It is 1U, 2-post rack mount.&lt;/td&gt;
                &lt;td&gt;Losing 1 power source will reduce 50% of capacity.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Mac Mini&lt;/td&gt;
            &lt;td&gt;We rack 2 Mac minis in a row on a mounting tray, typically racking 70 minis in one rack in total. Except for the first rack which requires extra rack units (RUs) for core switches and firewalls.&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;KVM&lt;/td&gt;
            &lt;td&gt;KVM switches with adaptor for keyboard and mouse emulation when required.&lt;/td&gt;
            &lt;td&gt;N/A&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Networking Setup&lt;/td&gt;
            &lt;td&gt;Networking consists of Core Switches, Access Switches, Firewalls, Internet and Direct Connect Links.&lt;/td&gt;
            &lt;td&gt;Mostly active/active redundancy.&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;provisioning-and-configuration&quot;&gt;Provisioning and configuration&lt;/h2&gt;

&lt;h3 id=&quot;zero-touch-provisioning&quot;&gt;Zero-touch provisioning&lt;/h3&gt;

&lt;p&gt;Zero-touch provisioning is a streamlined method for setting up and configuring devices with minimal manual intervention. This section outlines the process and benefits of zero-touch provisioning using Jamf for Mac minis.&lt;/p&gt;

&lt;p&gt;We have a setup that enables these machines to start accepting jobs once they are racked up and connected (Power and network cables). Here is how it works:&lt;/p&gt;

&lt;h4 id=&quot;mobile-device-management-mdm-configuration-and-automated-device-enrollment-ade&quot;&gt;Mobile Device Management (MDM) configuration and Automated Device Enrollment (ADE)&lt;/h4&gt;

&lt;p&gt;ADE, previously known as Device Enrollment Program (DEP), is an Apple service that facilitates automatic enrollment. When a new Mac Mini is acquired and registered in the organization’s ADE account, it is primed for automatic enrollment. Administrators create a PreStage enrollment configuration within Jamf Pro, encompassing account settings (e.g., creating a local admin account, hiding it in Users &amp;amp; Groups, skipping account creation for the user), configuration profiles (defining device settings, security policies, and restrictions), and enrollment packages (including necessary software and scripts).&lt;/p&gt;

&lt;h4 id=&quot;device-setup-activation-and-redirection&quot;&gt;Device setup: Activation and redirection&lt;/h4&gt;

&lt;p&gt;Upon powering on and connecting to the internet, the Mac Mini communicates with Apple’s activation servers. The activation servers identify the device as part of the organization’s ADE and redirect it to the Jamf MDM server, ensuring automatic enrollment without user input.&lt;/p&gt;

&lt;h4 id=&quot;enrollment-and-configuration&quot;&gt;Enrollment and configuration&lt;/h4&gt;

&lt;p&gt;The Mac Mini enrolls into the Jamf MDM system automatically. Jamf applies predefined configuration profiles to set up the device’s settings, installs required applications based on configured policies, and enforces security policies such as encryption and authentication settings to ensure compliance.&lt;/p&gt;

&lt;h4 id=&quot;key-benefits-of-zero-touch-provisioning&quot;&gt;Key benefits of zero-touch provisioning&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Devices are ready to use right out of the box, reducing the time and effort required by IT staff.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Ensures that all devices are configured uniformly according to organizational policies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Enforces security policies from the moment the device is first powered on, reducing vulnerabilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Easily manage and configure a large number of devices without manual intervention.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learnings-and-insights&quot;&gt;Learnings and insights&lt;/h2&gt;

&lt;h3 id=&quot;supply-chain-is-as-fast-as-the-last-essential-component-you-need&quot;&gt;Supply chain is as fast as the last essential component you need&lt;/h3&gt;

&lt;p&gt;The efficiency of a supply chain hinges on the delivery of its final essential component. Despite being a fundamental principle, it’s worth reiterating. Our timely launch was facilitated by a buffer period for unexpected delays. Interestingly, one of the last critical items to arrive was the rack mounting trays. The brief delay underscored the importance of prioritizing and planning for on-time delivery of every essential component, irrespective of its manufacturing simplicity.&lt;/p&gt;

&lt;h3 id=&quot;consistently-address-the-question-how-will-this-scale&quot;&gt;Consistently address the question: How will this scale?&lt;/h3&gt;

&lt;p&gt;From the outset, our goal was to develop a scalable infrastructure. As the cluster expands, tasks such as preparing Mac minis for job acceptance require increasing manual input, which ultimately impacts costs. Hence, zero-touch provisioning becomes essential, as scalability is not merely a desirable feature but a necessity.&lt;/p&gt;

&lt;h3 id=&quot;plan-and-opt-in-for-a-power-cost-structure-best-suits-for-your-need&quot;&gt;Plan and opt in for a power cost structure best suits for your need&lt;/h3&gt;

&lt;h4 id=&quot;power-cost-structures&quot;&gt;Power cost structures&lt;/h4&gt;

&lt;p&gt;In a colocation setup power costs can be billed in several ways, each with pros and cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Flat rate per circuit&lt;/strong&gt;: A fixed monthly fee, predictable but limits flexibility (e.g., can’t exceed 80% without extra circuits).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Allocated kW&lt;/strong&gt;: Commit to a fixed power amount (e.g., 100 kW), potentially cheaper but with penalties for overages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metered usage&lt;/strong&gt;: Pay for actual consumption (kWh), good for variable loads but may still charge for space.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;All-in Sspace and power&lt;/strong&gt;: Single rate covering both, easy to compare but less flexible for upgrades.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We ultimately opted for an allocated kW commitment, a phased approach based on conservative equipment power ratings and historical usage. We structured this into phases of commitment increases for future capacity growth.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Mac Cloud Exit wasn’t just a technical migration; it was a strategic move that fundamentally enhanced our engineering efficiency. By onshoring our infrastructure into Southeast Asia, we have achieved $2.4 million USD in projected savings and supercharged our CI pipeline, delivering performance gains of 20-40%. This project proves that taking ownership of our core infrastructure can be a major competitive advantage, allowing us to deliver faster and more reliably for our users across the region.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmacos&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 06 Nov 2025 00:00:05 +0000</pubDate>
        <link>https://engineering.grab.com/mac-cloud-exit</link>
        <guid isPermaLink="true">https://engineering.grab.com/mac-cloud-exit</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we built a custom vision LLM to improve document processing at Grab</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the world of digital services, accurate extraction of information from user-submitted documents such as identification (ID) cards, driver’s licenses, and registration certificates is a critical first step for processes like electronic know-your-customer (eKYC). This task is especially challenging in Southeast Asia (SEA) due to the diversity of languages and document formats.&lt;/p&gt;

&lt;p&gt;We began this journey to address the limitations of traditional Optical Character Recognition (OCR) systems, which struggled with the variety of document templates it had to process. While powerful proprietary Large Language Models (LLMs) were an option, they often fell short in understanding SEA languages, produced errors, hallucinations, and had high latency. On the other hand, open-sourced Vision LLMs were more efficient but not accurate enough for production.&lt;/p&gt;

&lt;p&gt;This prompted us to fine-tune and ultimately develop a lightweight, specialized Vision LLM from the ground up. This blog is our account of the entire process.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Simplified overview of how Vision LLM works.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;what-is-a-vision-llm&quot;&gt;What is a Vision LLM?&lt;/h3&gt;

&lt;p&gt;You’ve likely heard of LLMs that process text. You give the LLM a text prompt, and it responds with a text output. A Vision LLM takes this a step further by allowing the model to understand images. The basic architecture involves three key components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Image encoder&lt;/strong&gt;: This component ‘looks’ at an image and converts it into a numerical (vectorized) format.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vision-language projector&lt;/strong&gt;: It acts as a translator, converting the image’s numerical format into a representation that the language model can understand.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Language model&lt;/strong&gt;: The familiar text-based model that processes the combined image and text input to generate a final text output.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Vision LLM basic  architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;choosing-our-base-vision-llm-model&quot;&gt;Choosing our base Vision LLM model&lt;/h3&gt;
&lt;p&gt;We evaluated a range of LLMs capable of performing OCR and Key Information Extraction (KIE). Our exploration of open-source options—including Qwen2VL, miniCPM, Llama3.2 Vision, Pixtral 12B, GOT-OCR2.0, and NVLM 1.0—led us to select Qwen2-VL 2B as our base multimodal LLM. This decision was driven by several critical factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Efficient size&lt;/strong&gt;: It is small enough for full fine-tuning on GPUs with limited VRAM resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SEA language support&lt;/strong&gt;: Its tokenizer is efficient for languages like Thai and Vietnamese, indicating decent native vocabulary coverage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic resolution&lt;/strong&gt;: Unlike models that require fixed-size image inputs, Qwen2-VL can process images in their native resolution. This is crucial for OCR tasks as it prevents the distortion of text characters that can happen when images are resized or cropped.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We benchmarked Qwen2VL and miniCPM on Grab’s dataset. Our initial findings showed low accuracy, mainly due to the limited coverage of SEA languages. This motivated us to fine-tune the model to improve OCR and KIE accuracy. Training the LLM can be a very data-intensive and GPU resource-intensive process. Due to this, we had to address these two concerns before progressing further:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: How do we use open source and internal data effectively to train the model?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: How do we customize the model to reduce latency but keep high accuracy?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-dataset-generation&quot;&gt;Training dataset generation&lt;/h2&gt;

&lt;h3 id=&quot;synthetic-ocr-dataset&quot;&gt;Synthetic OCR dataset&lt;/h3&gt;

&lt;p&gt;We extracted the SEA languages text content from a large online text corpus—&lt;a href=&quot;https://commoncrawl.org/&quot;&gt;Common Crawl&lt;/a&gt; (internet dataset). Then, we used an in-house synthetic data pipeline to generate text images by rendering SEA text contents in various fonts, backgrounds and augmentations.&lt;/p&gt;

&lt;p&gt;The dataset contains text in Bahasa Indonesia, Thai, Vietnamese, and English. Each image has a paragraph of random sentences extracted from the dataset as shown in Figure 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Two synthetic sample images in Thai language used for model training.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;documint-ai-powered-auto-labelling-framework&quot;&gt;Documint: AI-powered, auto-labelling framework&lt;/h3&gt;

&lt;p&gt;Our experiments showed that applying document detection and orientation correction significantly improves OCR and information extraction. Now that we have an OCR dataset, we needed to generate a pre-processing dataset to further improve model training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Documint&lt;/strong&gt; is an internal platform developed by our team that creates an auto‑labelling and pre‑processing framework for document understanding. It prepares high‑quality, labelled datasets. Documint utilizes various submodules to effectively execute the full OCR and KIE task. We then used a pipeline with the large amount of Grab collected cards and documents to extract training labels. The data was further refined by a human reviewer to achieve high label accuracy.&lt;/p&gt;

&lt;p&gt;Documint has four main modules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Detection module&lt;/strong&gt;: Detect the region from the full picture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Orientation module&lt;/strong&gt;: Gives correction angle (e.g. if document is upside down, 180 degrees).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;OCR module&lt;/strong&gt;: Returns text values in unstructured format.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;KIE module&lt;/strong&gt;: Returns JSON values from unstructured text.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:85%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Pipeline overview of Documint.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;experimentation&quot;&gt;Experimentation&lt;/h2&gt;

&lt;h3 id=&quot;phase-1-the-lora-experiment&quot;&gt;Phase 1: The LoRA experiment&lt;/h3&gt;

&lt;p&gt;Our first attempt in fine-tuning a Vision LLM involved fine-tuning an open-source model Qwen2VL, using a technique called Low-Rank Adaptation (LoRA). LoRA is efficient because it allows lightweight updates to the model’s parameters, minimizing the need for extensive computational resources.&lt;/p&gt;

&lt;p&gt;We trained the model on our curated document data, which included various document templates in multiple languages. The performance was promising for documents with Latin scripts. Our experiment of LoRA fine-tuned Qwen2VL-2B achieved high field-level of accuracy for Indonesian documents.&lt;/p&gt;

&lt;p&gt;However, the fine-tuned model still struggled with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Documents containing non-Latin scripts like Thai and Vietnamese.&lt;/li&gt;
  &lt;li&gt;Unstructured layouts with small, dense text.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-2-the-power-of-full-fine-tuning&quot;&gt;Phase 2: The power of full fine-tuning&lt;/h3&gt;

&lt;p&gt;Our experiments revealed a key limitation. While open-source Vision LLMs often have extensive multi-lingual corpus coverage for the LLM decoder’s pre-training, they lack visual text in SEA languages during vision encoder and joint training. This insight drove our decision to pursue full parameter fine-tuning for optimal results.&lt;/p&gt;

&lt;p&gt;Drawing from the &lt;a href=&quot;https://arxiv.org/abs/2304.08485&quot;&gt;Large Language and Vision Assistant (LLAVA)&lt;/a&gt; methodology, we implemented a two-stage training approach illustrated in Figure 5.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: From left to right—two-stage training process.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1 - Continual pre-training&lt;/strong&gt;: We first trained the vision components of the model using synthetic OCR datasets that we created for Bahasa Indonesia, Thai, Vietnamese, and English. This helps the model to learn the unique visual patterns of SEA scripts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2 - Full-parameter fine-tuning&lt;/strong&gt;: We then fine-tuned the entire model—vision encoder, projector, and language model—using our task-specific document data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 1: OCR Field level accuracy between the baseline and Qwen2-VL 2B model. (pp: percentage points).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The fully fine-tuned Qwen2-VL 2B model delivered significant improvement, especially on documents that the LoRA model struggled with.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Thai document accuracy increased &lt;strong&gt;+70pp&lt;/strong&gt; from baseline.&lt;/li&gt;
  &lt;li&gt;Vietnamese document accuracy rose &lt;strong&gt;+40pp&lt;/strong&gt; from baseline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-3-building-a-lightweight-1b-model-from-scratch&quot;&gt;Phase 3: Building a lightweight 1B model from scratch&lt;/h3&gt;

&lt;p&gt;While the Qwen2VL-2B model was a success, the full fine-tuning pushed the limits of GPUs. To optimize resources used and to create a model perfectly tailored to our needs, we decided to build a lightweight Vision LLM (~1B parameters) from scratch.&lt;/p&gt;

&lt;p&gt;Our strategy was to combine the best parts of all models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We took the powerful &lt;strong&gt;vision encoder&lt;/strong&gt; from the larger Qwen2-VL 2B model.&lt;/li&gt;
  &lt;li&gt;We paired it with the compact and efficient &lt;strong&gt;language decoder&lt;/strong&gt; from the Qwen2.5 0.5B model.&lt;/li&gt;
  &lt;li&gt;We connected them with an &lt;strong&gt;adjusted projector layer&lt;/strong&gt; to ensure they could work together seamlessly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This created a custom ~1B parameter Vision LLM optimized for training and deployment.&lt;/p&gt;

&lt;h4 id=&quot;four-stages-in-training-our-custom-model&quot;&gt;Four stages in training our custom model&lt;/h4&gt;

&lt;p&gt;We trained our new model using a comprehensive four-stage process as shown in Figure 6.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6: From left to right— four stages of model training.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1 - Projector alignment&lt;/strong&gt;: The first step was to train the new projector layer to ensure the vision encoder and language decoder could communicate effectively.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2 - Vision tower enhancement&lt;/strong&gt;: We then trained the vision encoder on a vast and diverse set of public multimodal datasets, covering tasks like visual Q&amp;amp;A, general OCR, and image captioning to improve its foundational visual understanding.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 3 - Language-specific visual training&lt;/strong&gt;: We trained the model on two types of synthetic OCR data. Without this stage, performance on non-Latin documents dropped by as much as 10%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 4 - Task-centric fine-tuning&lt;/strong&gt;: Lastly, we performed full-parameter fine-tuning on our custom 1B model using our curated document dataset.&lt;/p&gt;

&lt;h4 id=&quot;the-final-results-are-as-follow&quot;&gt;The final results are as follow:&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It achieved performance comparable to the larger 2B model, &lt;strong&gt;staying within a 3pp accuracy gap across most document types.&lt;/strong&gt;  The model also maintained strong generalization when trained on quality-augmented datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The latency of our model far outperforms the 2B model, as well as traditional OCR models, as well as external APIs like chatGPT or Gemini. One of the biggest weaknesses we identified with external APIs was the P99 latency, which can easily be 3 to 4x the P50 latency, which would not be acceptable for Grab’s large scale rollouts.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 2: Performance comparison between Qwen2-VL 2B and 1B sized Vision LLM.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;

&lt;p&gt;Our work demonstrates that strategic training with high-quality data enables smaller, specialized models to achieve remarkable efficiency and effectiveness. Here are the critical insights from our extensive experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Full fine-tuning is superior&lt;/strong&gt;: For specialized, non-Latin script domains, full-parameter fine-tuning dramatically outperforms LoRA.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lightweight models are effective&lt;/strong&gt;: A smaller model (~1B) built from scratch and trained comprehensively can achieve near state-of-the-art results, validating the custom architecture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Base model matters&lt;/strong&gt;: Starting with a base model that has native support for your target languages is crucial for success.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data is king&lt;/strong&gt;: Meticulous dataset preprocessing and augmentation plays a critical role in achieving consistent and accurate results.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Native resolution is a game changer&lt;/strong&gt;: A model that can handle dynamic image resolutions preserves text integrity, dramatically improves OCR capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our journey demonstrates that specialized Vision LLMs can effectively replace traditional OCR pipelines with a single, unified, highly accurate model—opening new possibilities for document processing at scale.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 3: Comparison of model types .&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;As we continue to enhance our Vision LLM capabilities, exciting developments are underway:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Smarter, more adaptable models&lt;/strong&gt;: We’re developing Chain of Thought-based OCR and KIE models to strengthen generalisation capabilities and tackle even more diverse document scenarios.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Expanding across Southeast Asia&lt;/strong&gt;: We’re extending support to all Grab markets, bringing our advanced document processing to Myanmar, Cambodia, and beyond.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2409.12191&quot;&gt;https://doi.org/10.48550/arXiv.2409.12&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Improved Baselines with Visual Instruction Tuning: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2310.03744&quot;&gt;https://doi.org/10.48550/arXiv.2310.03744&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2107.09313&quot;&gt;https://doi.org/10.48550/arXiv.2107.09313&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2403.13372&quot;&gt;https://doi.org/10.48550/arXiv.2403.13372&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmodel&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 04 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/custom-vision-llm-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/custom-vision-llm-at-grab</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
  </channel>
</rss>
