<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 30 Oct 2024 02:12:06 +0000</pubDate>
    <lastBuildDate>Wed, 30 Oct 2024 02:12:06 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>How we reduced peak memory and CPU usage of the product configuration management SDK</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;GrabX is Grab’s central platform for product configuration management. It has the capacity to control any component within Grab’s backend systems through configurations that are hosted directly on GrabX.&lt;/p&gt;

&lt;p&gt;GrabX clients read these configurations through an SDK, which reads the configurations in a way that’s asynchronous and eventually consistent. As a result, it takes about a minute for any updates to the configurations to reach the client SDKs.&lt;/p&gt;

&lt;p&gt;In this article, we discuss our analysis and the steps we took to reduce the peak memory and CPU usage of the SDK.&lt;/p&gt;

&lt;h2 id=&quot;observations-on-potential-sdk-improvements&quot;&gt;Observations on potential SDK improvements&lt;/h2&gt;

&lt;p&gt;Our GrabX clients noticed that the GrabX SDK tended to require high memory and CPU usage. From this, we saw opportunities for further improvements that could:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimise the tail latencies of client services.&lt;/li&gt;
  &lt;li&gt;Enable our clients to use their resources more effectively.&lt;/li&gt;
  &lt;li&gt;Reduce operation costs and improve the efficiency of using the GrabX SDK.&lt;/li&gt;
  &lt;li&gt;Accelerate the adoption of GrabX by Grab’s internal services.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sdk-design&quot;&gt;SDK design&lt;/h2&gt;

&lt;p&gt;At a high-level, creating, updating, and serving configuration values via the GrabX SDK involved the following process:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/previous-grabx-sdk-design.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Previous GrabX SDK design.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The process begins when GrabX clients either create or update configurations. This is done through the GrabX web portal or by making an API call.&lt;/li&gt;
  &lt;li&gt;Once the configurations are created or updated, the GrabX backend module takes over. It stores the new configuration into an SQL database table.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The GrabX backend ensures that the latest configuration data is available to client SDKs.&lt;/p&gt;

    &lt;p&gt;a. The GrabX backend checks every minute for any newly created or updated configurations.&lt;/p&gt;

    &lt;p&gt;b. If there are new or updated configurations, GrabX backend creates a new JSON file. This file contains all existing and newly created configurations. It’s important to note that all configurations across all services are stored in a single JSON file.&lt;/p&gt;

    &lt;p&gt;c. The backend module uploads this newly created JSON file to an AWS S3 bucket.&lt;/p&gt;

    &lt;p&gt;d. The backend module assigns a version number to the new JSON file and updates a text file in the AWS S3 bucket. This text file stores the latest JSON file version number. The client SDK refers to this version file to check if a newer version of the configuration data is available.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The client SDK performs a check on the version file every minute to determine if a newer version is available. This mechanism is crucial to maintain data consistency across all instances of a service. If any instance fell out of sync, it would be brought back in sync within a minute.&lt;/li&gt;
  &lt;li&gt;If a new version of the configuration JSON file is available, the client SDK downloads this new file. Following the download, it loads the configuration data into memory. Storing the configuration data in memory reduces the read latency for the configurations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;areas-of-improvement-for-existing-sdk-design&quot;&gt;Areas of improvement for existing SDK design&lt;/h2&gt;

&lt;p&gt;In this section we outline the areas of improvement we identified within the SDK design.&lt;/p&gt;

&lt;h3 id=&quot;service-based-data-partitioning&quot;&gt;Service-based data partitioning&lt;/h3&gt;

&lt;p&gt;We saw an opportunity for service-based data partitioning. The configuration data for all services was consolidated into a single JSON file. Upon studying the data read patterns of client services, we observed that most services primarily needed to access configuration data specific to their own service. However, the present design required storing configuration data for all other services. This resulted in unnecessary memory consumption.&lt;/p&gt;

&lt;h3 id=&quot;retaining-only-new-version-of-configuration-in-the-same-file&quot;&gt;Retaining only new version of configuration in the same file&lt;/h3&gt;

&lt;p&gt;By using a single JSON file for storing old and new configuration data, we saw a significant increase in the size of the JSON file.&lt;/p&gt;

&lt;p&gt;The SDK only needs the full data when it starts; the more common case is that it needs to stay updated with the latest configuration. Even in that scenario, the SDK needed to fetch a complete new JSON file every minute no matter the size of the updates. Consequently, the process of downloading, decoding, and loading high volumes of data at a high frequency (every minute) caused the client SDK to spike in memory and CPU usage.&lt;/p&gt;

&lt;h3 id=&quot;more-efficient-json-decoding&quot;&gt;More efficient JSON decoding&lt;/h3&gt;

&lt;p&gt;An additional factor which contributed to memory and CPU usage during the decoding phase was the inefficiency of the default JSON decode library to decode this large (&amp;gt;100MB) JSON file. Decoding this JSON file was heavy on available CPU resources, which tended to starve the service of its ability to handle incoming requests. This manifested as increasing the P99 latency of the service.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/graph-increased-p99-latency.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Graph illustrating the increased P99 latency due to CPU throttling for a service.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;implemented-solution&quot;&gt;Implemented solution&lt;/h2&gt;

&lt;p&gt;We proposed modifications to the existing SDK design, which we discuss in this section.&lt;/p&gt;

&lt;h3 id=&quot;partition-data-by-service&quot;&gt;Partition data by service&lt;/h3&gt;

&lt;p&gt;The proposed solution involved partitioning the data based on services. We chose this approach because a single configuration typically belonged to a single service, and most services primarily needed to read configurations that pertained to their own service.&lt;/p&gt;

&lt;p&gt;Upon analysing the distribution of service-configuration, we discovered that 98% of client services required less than 1% of the total configuration data. Despite this, they were required to maintain and reload 100% of the configuration data. Furthermore, the service with the largest number of configurations only required 20% of the total configuration data.&lt;/p&gt;

&lt;p&gt;Therefore, we proposed a shift towards service-based partitioning of configuration data. This allowed individual client services to access only the data they needed to read.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/service-config.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Graph showing the number of services with varying amounts of configurations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;create-separate-json-files-for-each-configuration&quot;&gt;Create separate JSON files for each configuration&lt;/h3&gt;

&lt;p&gt;Our proposal also included creating a separate JSON file for each configuration in a service. Previously, all data was stored in a single JSON file housed in an AWS S3 bucket, which supported a maximum of 3,500 write/update requests and 5,500 read requests per second.&lt;/p&gt;

&lt;p&gt;By storing each configuration in a separate JSON file, we were able to create a different S3 prefix for each configuration file. These S3 prefixes helped us to maximise S3 throughput by enhancing the read/write performance for each configuration. AWS S3 can handle at least 3,500 PUT/COPY/POST/DELETE requests or 5,500 GET/HEAD requests per second for each partitioned Amazon S3 prefix.&lt;/p&gt;

&lt;p&gt;Therefore, with each configuration’s data stored in a separate S3 file with a different prefix, the GrabX platform could achieve a throughput of 5,500 read requests and 3,500 write/update requests per second per configuration. This was beneficial for boosting read/write capacity when needed.&lt;/p&gt;

&lt;h3 id=&quot;implement-a-service-level-changelog&quot;&gt;Implement a service-level changelog&lt;/h3&gt;

&lt;p&gt;We proposed to create a changelog file at the service level. In other words, a changelog file was created for each service. This file was used to keep track of the latest update version, as well as previous service configuration update versions. This file also recorded the configurations which were created or updated in each version. This enables the SDK to accurately identify the configurations that were created or updated in each update version. This was useful to update the specific configurations belonging to a service on the client side.&lt;/p&gt;

&lt;h3 id=&quot;implement-service-based-sdk&quot;&gt;Implement service-based SDK&lt;/h3&gt;

&lt;p&gt;We proposed that SDK client services should be allowed to subscribe to a list of services for which they need to read configuration data. The SDK was initialised with data of the subscribed services and received updates only for configurations corresponding to the subscribed services.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/sdk-lifecycle-flowchart.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. This flowchart shows our proposed service-based SDK implementation.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The SDK only sought updates for the subscribed services. The client SDK needed to read the changelog file for each of the subscribed services, comparing the latest changelog version against the SDK version number. Whenever a newer changelog version was available, the SDK updated the variables with the latest version.&lt;/p&gt;

&lt;p&gt;This approach significantly reduced the volume of data that the SDK needed to download, decode, and load into memory during both initialisation and each subsequent update.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In summary, we identified ways to optimise CPU and memory usage in the GrabX SDK. Our analysis revealed that frequent high resource consumption hindered the wider adoption of GrabX. We proposed a series of modifications, including partitioning data by service and creating separate JSON files for each configuration.&lt;/p&gt;

&lt;p&gt;After benchmarking the proposed solution with a variety of configuration data sizes, we found that the solution has the potential to reduce memory utilisation by up to 70% and decrease the maximum CPU utilisation by more than 50%. These improvements significantly enhance the performance and scalability of the GrabX SDK.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reduced-memory-cpu-usage-grabx-sdk/bar-charts-before-after.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Bar charts showcasing memory(MB) &amp;amp; CPU(%) utilisation for Service A before and after using the discussed solution.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Moving forward, we plan to continue optimising the GrabX SDK by exploring additional improvements, such as reducing its initialisation time. These efforts aim to make GrabX an even more robust and reliable solution for product configuration management within Grab’s ecosystem.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/reduced-memory-cpu-usage-grabx-sdk</link>
        <guid isPermaLink="true">https://engineering.grab.com/reduced-memory-cpu-usage-grabx-sdk</guid>
        
        <category>Engineering</category>
        
        <category>Optimisation</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>LLM-assisted vector similarity search</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As the complexity of data retrieval requirements continue to grow, traditional search methods often struggle to provide relevant and accurate results, especially for nuanced or conceptual queries. Vector similarity search has emerged as a powerful technique for finding semantically similar information. It refers to finding vectors in a large dataset that are most similar to a given query vector, typically using some distance or similarity measure. The concept originated in the 1960s with the work by Minsky and Papert on nearest neighbour search &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Since then, the idea has evolved substantially with modern approaches often using approximate methods to enable fast search in high-dimensional spaces, such as locality-sensitive hashing &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and graph-based indexing &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Recently, vector similarity search has become a crucial component in many machine learning and information retrieval applications. It is one of the key technologies that popularised the idea of Retrieval Augmented Generation (RAG) &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; which increased the applicability of Transformer &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; based Generative Large Language Models (LLMs) &lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; in domain-specific tasks without requiring any further training or fine-tuning. However, the effectiveness of the vector search can be limited when dealing with intricate queries or contextual nuances. For example, from a typical vector similarity search perspective, “I like fishing” and “I do not like fishing” may be quite close to each other, while in reality, they are the exact opposite. In this blog post, we discuss an approach that we experimented with that combines vector similarity search with LLMs to enhance the relevance and accuracy of search results for such complex and nuanced queries. We leverage the strengths of both techniques: vector similarity search for efficient shortlisting of potential matches, and LLMs for their ability to understand natural language queries and rank the shortlisted results based on their contextual relevance.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h2&gt;
&lt;p&gt;The proposed solution involves a two-step process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Vector similarity search: We first perform a vector similarity search on the dataset to obtain a shortlist of potential matches (e.g., top 10-50 results) for the given query. This step leverages the efficiency of vector similarity search to quickly narrow down the search space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LLM-assisted ranking: The shortlisted results from the vector similarity search are then fed into an LLM, which ranks the results based on their relevance to the original query. The LLM’s ability to understand natural language queries and contextual information helps in identifying the most relevant results from the shortlist.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By combining these two steps, we aim to achieve the best of both worlds: the efficiency of vector similarity search for initial shortlisting, and the contextual understanding and ranking capabilities of LLMs for refining the final results.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-assisted-vector-similarity-search/similarity-search.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Similarity search and the proposed LLM-assisted similarity search.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;To evaluate the effectiveness of our proposed solution, we conducted experiments on two small synthetic datasets in CSV format that we curated using GPT-4o &lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Food dataset&lt;/strong&gt;: A collection of 100 dishes with their titles and descriptions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tourist spots dataset&lt;/strong&gt;: A collection of 100 tourist spots in Asia, including their names, cities, countries, and descriptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to note that we primarily focus on performing similarity search on structured data such as description of various entities in a relational database.&lt;/p&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;Our experimental setup included a Python script for vector similarity search leveraging Facebook AI Similarity Search (FAISS) &lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, a library developed by Facebook that offers efficient similarity search, and OpenAI’s embeddings (i.e., text-embedding-ada-002) &lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; to generate the vector embeddings needed for facilitating the vector search. For our proposed solution, an LLM component (i.e., GPT-4o) was included in the setup in addition to the FAISS-based similarity search component.&lt;/p&gt;

&lt;h3 id=&quot;observations&quot;&gt;Observations&lt;/h3&gt;

&lt;p&gt;To compare the performance of the proposed approach of LLM-assisted vector similarity search as outlined in the “Proposed solution” section with the raw vector similarity search, we conducted both techniques on our two synthetic datasets. With the raw vector search, we get the top three matches for a given query. For our proposed technique, we first get a shortlist of 15 entity matches from FAISS for the same query, and supply the shortlist and the original query to LLM with some descriptive instructions in the prompt to find the top three matches from the provided shortlist.&lt;/p&gt;

&lt;p&gt;From the experiments, in simpler cases where the queries were straightforward and directly aligned with the textual content of the data, both the raw similarity search and the LLM-assisted similarity search demonstrated comparable performance. However, as the queries became more complex, involving additional constraints, negations, or conceptual requirements, the LLM-assisted search exhibited a clear advantage over the raw similarity search. The LLM’s ability to understand context and capture subtleties in the queries allowed it to filter out irrelevant results and rank the most appropriate ones higher, leading to improved accuracy.&lt;/p&gt;

&lt;p&gt;Here are a few examples where the LLM-assisted similarity search performed better:&lt;/p&gt;

&lt;h4 id=&quot;food-dataset&quot;&gt;Food dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: “food with no fish or shrimp”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- title: Tempura, description: A Japanese dish of seafood or vegetables that have been battered and deep fried.
- title: Ceviche, description: A seafood dish popular in Latin America, made from fresh raw fish cured in citrus juices.
- title: Sushi, description: A Japanese dish consisting of vinegared rice accompanied by various ingredients such as seafood and vegetables.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;LLM-assisted similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- title: Chicken Piccata, description: Chicken breasts cooked in a sauce of lemon, butter, and capers.
- title: Chicken Alfredo, description: An Italian-American dish of pasta in a creamy sauce made from butter and Parmesan cheese.
- title: Chicken Satay, description: Grilled chicken skewers served with peanut sauce.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;: The LLM correctly filtered out dishes containing fish or shrimp, while the raw similarity search failed to do so, presumably due to the presence of negation in the query.&lt;/p&gt;

&lt;h4 id=&quot;tourist-spots-dataset&quot;&gt;Tourist spots dataset&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: “exposure to wildlife”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Ocean Park, city: Hong Kong, country: Hong Kong, description: Marine mammal park and oceanarium.
- name: Merlion Park, city: Singapore, country: Singapore, description: Iconic statue with the head of a lion and body of a fish.
- name: Manila Bay, city: Manila, country: Philippines, description: A natural harbor known for its sunset views.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;LLM-assisted similarity search result&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Ocean Park, city: Hong Kong, country: Hong Kong, description: Marine mammal park and oceanarium.
- name: Chengdu Research Base, city: Chengdu, country: China, description: A research center for giant panda breeding.
- name: Mount Hua, city: Shaanxi, country: China, description: Mountain known for its dangerous hiking trails.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;: Two out of the top three matches by the LLM-assisted technique seem relevant to the query while only one result from the raw similarity search is relevant and the other two being somewhat irrelevant to the query. The LLM identified the relevance of a research base for giant panda breeding to the “exposure to wildlife”, which the raw similarity search ignored in its ranking.&lt;/p&gt;

&lt;p&gt;These examples provide a glimpse into the utility of LLMs in finding more relevant matches in scenarios where the queries involved additional context, constraints, or conceptual requirements beyond simple keyword matching. On the other hand, when the queries were more straightforward and focused on specific keywords or phrases present in the data, both approaches demonstrated comparable performance. For instance, queries like “Japanese food” or “beautiful mountains” yielded similar results from both the raw similarity search and the proposed LLM-assisted approach.&lt;/p&gt;

&lt;p&gt;Overall, the LLM-assisted vector search exhibited a clear advantage in handling complex queries, leveraging its ability to understand natural language and contextual information. However, for simpler queries, the raw similarity search remained a viable option, especially when computational efficiency is a concern.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The experiments demonstrated the potential of combining vector similarity search with LLMs to enhance the relevance and accuracy of search results, particularly for complex and nuanced queries. While vector similarity search alone can provide reasonable results for straightforward queries, the LLM-assisted approach shines when dealing with queries that require a deeper understanding of context, nuances, and conceptual relationships. By leveraging the natural language understanding capabilities of LLMs, this approach can better capture the intent behind complex queries and provide more relevant search results.&lt;/p&gt;

&lt;p&gt;Our experiment was limited to using a small volume of structured data (100 data points in each dataset) with a limited number of queries. However, we have witnessed similar enhancement in search result relevance when we deployed this solution internally within Grab for larger datasets, for example, 4500+ rows of data stored in a relational database.&lt;/p&gt;

&lt;p&gt;Nevertheless, it is important to note that the effectiveness of this approach may still depend on the quality and complexity of the data, as well as the specific use case and query patterns. We believe it is still worthwhile to evaluate the proposed approach for more diverse (e.g., beyond CSV) and larger datasets. An interesting future work can be varying the size of the shortlist from the similarity search and observing how it impacts the overall search relevance when using the proposed approach. In addition, for real world applications, the performance implications in terms of additional latency introduced by the additional LLM query must also be considered.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;M. Minsky and S. Papert, Perceptrons: An Introduction to Computational Geometry. MIT Press, 1969. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;P. Indyk and R. Motwani, “Approximate nearest neighbors: Towards removing the curse of dimensionality,” in Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, 1998. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Y. Malkov and D. Yashunin, “Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in Advances in Neural Information Processing Systems, 2020. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A. Vaswani, “Attention is all you need,” in Advances in Neural Information Processing Systems, 2017. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A. Radford, “Improving language understanding by generative pre-training,” 2018. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“Hello GPT-4o,” OpenAI, May 2024. [Online]. Available: https://openai.com/index/hello-gpt-4o/. [Accessed: Oct. 6, 2024]. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P. E. Mazaré, and H. Jégou, “The faiss library,” arXiv preprint arXiv:2401.08281, 2024. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“Embeddings,” OpenAI API. [Online]. Available: https://platform.openai.com/docs/guides/embeddings. [Accessed: Oct. 6, 2024]. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 23 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-assisted-vector-similarity-search</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-assisted-vector-similarity-search</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        <category>Experiment</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Leveraging RAG-powered LLMs for Analytical Tasks</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Retrieval-Augmented Generation (RAG) is a powerful process that is designed to integrate direct function calling to answer queries more efficiently by retrieving relevant information from a broad database. In the rapidly evolving business landscape, Data Analysts (DAs) are struggling with the growing number of data queries from stakeholders. The conventional method of manually writing and running similar queries repeatedly is time-consuming and inefficient. This is where RAG-powered Large Language Models (LLMs) step in, offering a transformative solution to streamline the analytics process and empower DAs to focus on higher value tasks.&lt;/p&gt;

&lt;p&gt;In this article, we will share how the Integrity Analytics team has built out a data solution using LLMs to help automate tedious analytical tasks like generating regular metric reports and performing fraud investigations.&lt;/p&gt;

&lt;p&gt;While LLMs are known for their proficiency in data interpretation and insight generation, they represent just a fragment of the entire solution. For a comprehensive solution, LLMs must be integrated with other essential tools. The following is required in assembling a solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Internally facing LLM tool -&lt;/strong&gt; Spellvault is a platform within Grab that stores, shares, and refines LLM prompts. It features low/no-code RAG capabilities that lower the barrier of entry for people to create LLM applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data -&lt;/strong&gt; with real time or close to real-time latency to ensure accuracy. It has to be in a standardised format to ensure that all LLM data inputs are accurate.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduler -&lt;/strong&gt;  runs LLM applications at regular intervals. Useful for automating routine tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Messaging Tool -&lt;/strong&gt; a user interface where users can interact with LLM by entering a command to receive reports and insights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introducing-data-arks-the-data-middleware-serving-up-relevant-data-to-the-llm-agents&quot;&gt;Introducing Data-Arks, the data middleware serving up relevant data to the LLM agents&lt;/h2&gt;

&lt;p&gt;For most data use cases, DAs are usually running the same set of SQL queries with minor changes to parameters like dates, age or other filter conditions. In most instances, we already have a clear understanding of the required data and format to accomplish a task. Therefore, we need a tool that can execute the &lt;strong&gt;exact SQL query&lt;/strong&gt; and channel the data output to the LLM.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Data-Arks hosts various APIs which can be called to serve data to applications like SpellVault.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-data-arks&quot;&gt;What is Data-Arks?&lt;/h3&gt;

&lt;p&gt;Data-Arks is an in-house Python-based API platform housing several frequently used SQL queries and python functions packaged into individual APIs. Data-Arks is also integrated with Slack, Wiki, and JIRA APIs, allowing users to parse and fetch information and data from these tools as well. The benefits of Data-Arks are summarised as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration:&lt;/strong&gt; Data-Arks service allows users to upload any SQL query or Python script on the platform. These queries are then surfaced as APIs, which can be called to serve data to the LLM agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Versatility: Data-Arks&lt;/strong&gt; can be extended to everyone. Employees from various teams and functions at Grab can self-serve to upload any SQL query that they want onto the platform, allowing this tool to be used for different teams’ use cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;automating-regular-report-generation-and-summarisation-using-data-arks-and-spellvault&quot;&gt;Automating regular report generation and summarisation using Data-Arks and Spellvault&lt;/h2&gt;

&lt;p&gt;LLMs are just one piece of the puzzle, to build a comprehensive solution, they must be integrated with other tools. Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;p&gt;Figure 2 shows how different tools are used in executing report summaries in Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Report Summarizer uses various tools to summarise queries and deliver a summarised report through Slack.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 is an example of a summarised report generated by the Report Summarizer using dummy data.  Report Summarizer calls a Data-Arks API to generate the data in a tabular format and LLM helps summarise and generate a short paragraph of key insights. This automated report generation has helped save an estimated 3-4 hours per report.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Sample of a report generated using dummy data extracted from [https://data.gov.my/](https://data.gov.my/). &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;llm-bots-for-fraud-investigations&quot;&gt;LLM bots for fraud investigations&lt;/h2&gt;

&lt;p&gt;LLMs also excel in helping to streamline fraud investigations, as LLMs are able to contextualise several different data points and information and derive useful insights from them.&lt;/p&gt;

&lt;p&gt;Introducing &lt;strong&gt;A* bot&lt;/strong&gt;, the team’s very own LLM fraud investigation helper.&lt;/p&gt;

&lt;p&gt;A set of frequently used queries for fraud investigation is made available as Data-Arks APIs. Upon a user prompt or query, SpellVault selects the most relevant queries using RAG, executes them and provides a summary of the results to users through Slack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. A* bot uses Data-Arks and Spellvault to get information for fraud investigations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 shows a sample of fraud investigation responses from A* bot. Scaling to multiple queries for a fraud investigation process, what was once a time-consuming fraud investigation can now be reduced to a matter of minutes, as the A* bot is capable of providing all the necessary information simultaneously.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Sample of fraud investigation responses.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;rag-vs-fine-tuning&quot;&gt;RAG vs fine-tuning&lt;/h2&gt;

&lt;p&gt;On deciding between RAG or fine-tuning to improve LLM accuracy, three key factors tipped the scales in favour of the RAG approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Effort and cost considerations&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires significant computational cost as it involves taking a base model and further training it with smaller, domain specific data and context. RAG is computationally less expensive as it relies on retrieving only relevant data and context to augment a model’s response. As the same base model can be used for different use cases, RAG is the preferred choice due to its flexibility and cost efficiency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ability to respond with the latest information&lt;/strong&gt;&lt;br /&gt;
Fine-tuning requires model re-training with each new information update, whereas RAG simply retrieves required context and data from a knowledge base to enhance its response. Thus, by using RAG, LLM is able to answer questions using the most current information from our production database, eliminating the need for model re-training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Speed and scalability&lt;/strong&gt;&lt;br /&gt;
Without the burden of model re-training, the team can rapidly scale and build out new LLM applications with a well managed knowledge base.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;The potential of using RAG-powered LLM can be limitless as the ability of GPT is correlated with the tools it equips. Hence, the process does not stop here and we will try to onboard more tools or integration to GPT. In the near future, we plan to utilise Data-Arks to provide images to GPT as GPT-4o is a multimodal model that has vision capabilities. We are committed to pushing the boundaries of what’s possible with RAG-powered LLM, and we look forward to unveiling the exciting advancements that lie ahead.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rag-llm/rag-llm-what-next.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. What’s next?&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to express our sincere gratitude to the following individuals and teams whose invaluable support and contributions have made this project a reality: &lt;br /&gt;- Meichen Lu, a senior data scientist at Grab, for her guidance and assistance in building the MVP and testing the concept.&lt;br /&gt;- The data engineering team, particularly Jia Long Loh and Pu Li, for setting up the necessary services and infrastructure. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Wed, 09 Oct 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</link>
        <guid isPermaLink="true">https://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Analytics</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Evolution of Catwalk: Model serving platform at Grab</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As Southeast Asia’s leading super app, Grab serves millions of users across multiple countries every day. Our services range from ride-hailing and food delivery to digital payments and much more. The backbone of our operations? Machine Learning (ML) models. They power our real-time decision-making capabilities, enabling us to provide a seamless and personalised experience to our users. Whether it’s determining the most efficient route for a ride, suggesting a food outlet based on a user’s preference, or detecting fraudulent transactions, ML models are at the forefront.&lt;/p&gt;

&lt;p&gt;However, serving these ML models at Grab’s scale is no small feat. It requires a robust, efficient, and scalable model serving platform, which is where our ML model serving platform, Catwalk, comes in.&lt;/p&gt;

&lt;p&gt;Catwalk has evolved over time, adapting to the growing needs of our business and the ever-changing tech landscape. It has been a journey of continuous learning and improvement, with each step bringing new challenges and opportunities.&lt;/p&gt;

&lt;h1 id=&quot;evolution-of-the-platform&quot;&gt;Evolution of the platform&lt;/h1&gt;

&lt;h2 id=&quot;phase-0-the-need-for-a-model-serving-platform&quot;&gt;Phase 0: The need for a model serving platform&lt;/h2&gt;

&lt;p&gt;Before Catwalk’s debut as our dedicated model serving platform, data scientists across the company employed various ad-hoc approaches to serve ML models. These included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shipping models online using custom solutions.&lt;/li&gt;
  &lt;li&gt;Relying on backend engineering teams to deploy and manage trained ML models.&lt;/li&gt;
  &lt;li&gt;Embedding ML logic within Go backend services.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These methods, however, led to several challenges, undercovering the need for a unified, company-wide platform for serving machine learning models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Operational overhead&lt;/strong&gt;: Data scientists often lacked the necessary expertise to handle the operational aspects of their models, leading to service outages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource wastage&lt;/strong&gt;: There was frequently low resource utilisation (e.g., 1%) for data science services, leading to inefficient use of resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Friction with engineering teams&lt;/strong&gt;: Differences in release cycles and unclear ownership when code was embedded into backend systems resulted in tension between data scientists and engineers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinventing the wheel&lt;/strong&gt;: Multiple teams independently attempted to solve model serving problems, leading to a duplication of effort.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​​These challenges highlighted the need for a company-wide, centralised platform for serving machine learning models.&lt;/p&gt;

&lt;h2 id=&quot;phase-1-no-code-managed-platform-for-tensorflow-serving-models&quot;&gt;Phase 1: No-code, managed platform for TensorFlow Serving models&lt;/h2&gt;

&lt;p&gt;Our initial foray into model serving was centred around creating a managed platform for deploying TensorFlow Serving models. The process involved data scientists submitting their models to the platform’s engineering admin, who could then deploy the model with an endpoint. Infrastructure and networking were managed using Amazon Elastic Kubernetes Service (EKS) and Helm Charts as illustrated below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This phase of our platform, which we also detailed in our &lt;a href=&quot;https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale&quot;&gt;previous article&lt;/a&gt;, was beneficial for some users. However, we quickly encountered scalability challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Codebase maintenance&lt;/strong&gt;: Applying changes to every TensorFlow Serving (TFS) version was cumbersome and difficult to maintain.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited scalability&lt;/strong&gt;: The fully managed nature of the platform made it difficult to scale.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Admin bottleneck&lt;/strong&gt;: The engineering admin’s limited bandwidth became a bottleneck for onboarding new models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited serving types&lt;/strong&gt;: The platform only supported TensorFlow, limiting its usefulness for data scientists using other frameworks like LightGBM, XGBoost, or PyTorch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After a year of operation, only eight models were onboarded to the platform, highlighting the need for a more scalable and flexible solution.&lt;/p&gt;

&lt;h2 id=&quot;phase-2-from-models-to-model-serving-applications&quot;&gt;Phase 2: From models to model serving applications&lt;/h2&gt;

&lt;p&gt;To address the limitations of Phase 1, we transitioned from deploying individual models to self-contained model serving applications. This “low-code, self-serving” strategy introduced several new components and changes as illustrated in the points and diagram below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Support for multiple serving types&lt;/strong&gt;: Users gained the ability to deploy models trained with a variety of frameworks like Open Neural Network Exchange (ONNX), PyTorch, and TensorFlow.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Self-served platform through CI/CD pipelines&lt;/strong&gt;: Data scientists could self-serve and independently manage their model serving applications through CI/CD pipelines.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;New components&lt;/strong&gt;: We introduced these new components to support the self-serving approach:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Catwalk proxy&lt;/strong&gt;, a managed reverse proxy to various serving types.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Catwalk transformer&lt;/strong&gt;, a low-code component to transform input and output data.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amphawa&lt;/strong&gt;, a feature fetching component to augment model inputs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;api-request-flow&quot;&gt;API request flow&lt;/h4&gt;

&lt;p&gt;The Catwalk proxy acts as the orchestration layer. Clients send requests to Catwalk proxy then it orchestrates calls to different components like transformers, feature-store, and so on. A typical end to end request flow is illustrated below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase2-api-request-flow.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Within a year of implementing these changes, the number of models on the platform increased from 8 to 300, demonstrating the success of this approach. However, new challenges emerged:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Complexity of maintaining Helm chart&lt;/strong&gt;: As the platform continued to grow with new components and functionalities, maintaining the Helm chart became increasingly complex. The readability and flow control became more challenging, making the helm chart updating process prone to errors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Process-level mistakes&lt;/strong&gt;: The self-serving approach led to errors such as pushing empty or incompatible models to production, setting too few replicas, or allocating insufficient resources, which resulted in service crashes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We knew that our work was nowhere near done. We had to keep iterating and explore ways to address the new challenges.&lt;/p&gt;

&lt;h2 id=&quot;phase-3-replacing-helm-charts-with-kubernetes-crds&quot;&gt;Phase 3: Replacing Helm charts with Kubernetes CRDs&lt;/h2&gt;

&lt;p&gt;To tackle the deployment challenges and gain more control, we made the significant decision to replace Helm charts with Kubernetes Custom Resource Definitions (CRDs). This required substantial engineering effort, but the outcomes have been rewarding. This transition gave us improved control over deployment pipelines, enabling customisations such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Smart defaults for AutoML&lt;/li&gt;
  &lt;li&gt;Blue-green deployments&lt;/li&gt;
  &lt;li&gt;Capacity management&lt;/li&gt;
  &lt;li&gt;Advanced scaling&lt;/li&gt;
  &lt;li&gt;Application set groupings&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is an example of a simple model serving CRD manifest:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: ml.catwalk.kubebuilder.io/v1
kind: ModelServing
spec:
  hpa:
    desired: 1
    max: 1
    min: 1
  modelMeta:
    modelName: &quot;my-model&quot;
    modelOwner: john.doe
  proxyLayer:
    enableLogging: true
    logHTTPBody: true
  servingLayer:
    servingType: &quot;tensorflow-serving&quot;
    version: &quot;20&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;model-serving-crd-deployment-state-machine&quot;&gt;Model serving CRD deployment state machine&lt;/h4&gt;

&lt;p&gt;Every model serving CRD submission follows a sequence of steps. If there are failures at any step, the controller keeps retrying after small intervals. The major steps on the deployment cycle are described below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Validate whether the new CRD specs are acceptable. Along with sanity checks, we also enforce a lot of platform constraints through this step.&lt;/li&gt;
  &lt;li&gt;Clean up previous non-ready deployment resources. Sometimes a deployment submission might keep crashing and hence it doesn’t proceed to a ready state. On every submission, it’s important to check and clean up such previous deployments.&lt;/li&gt;
  &lt;li&gt;Create resources for the new deployment and ensure that the new deployment is ready.&lt;/li&gt;
  &lt;li&gt;Switch traffic from old deployment to the new deployment.&lt;/li&gt;
  &lt;li&gt;Clean up resources for old deployment. At this point, traffic is already being served by the new deployment resources. So, we can clean up the old deployment.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;phase-4-transition-to-a-high-code-self-served-process-managed-platform&quot;&gt;Phase 4: Transition to a high-code, self-served, process-managed platform&lt;/h2&gt;

&lt;p&gt;As the number of model serving applications and use cases multiplied, clients sought greater control over orchestrations between different models, experiment executions, traffic shadowing, and responses archiving. To cater to these needs, we introduced several changes and components with the Catwalk Orchestrator, a high code orchestration solution, leading the pack.&lt;/p&gt;

&lt;h4 id=&quot;catwalk-orchestrator&quot;&gt;Catwalk orchestrator&lt;/h4&gt;

&lt;p&gt;The &lt;strong&gt;Catwalk Orchestrator&lt;/strong&gt; is a highly abstracted framework for building ML applications that replaced the catwalk-proxy from previous phases. The key difference is that users can now write their own business/orchestration logic. The orchestrator offers a range of utilities, reducing the need for users to write extensive boilerplate code. Key components of the Catwalk Orchestrator include HTTP server, gRPC server, clients for different model serving flavours (TensorFlow, ONNX, PyTorch, etc), client for fetching features from the feature bank, and utilities for logging, metrics, and data lake ingestion.&lt;/p&gt;

&lt;p&gt;The Catwalk Orchestrator is designed to streamline the deployment of machine learning models. Here’s a typical user journey:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Scaffold a model serving application&lt;/strong&gt;: Users begin by scaffolding a model serving application using a command-line tool.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Write business logic&lt;/strong&gt;: Users then write the business logic for the application.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy to staging&lt;/strong&gt;: The application is then deployed to a staging environment for testing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Complete load testing&lt;/strong&gt;: Users test the application in the staging environment and complete load testing to ensure it can handle the expected traffic.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy to production&lt;/strong&gt;: Once testing is completed, the application is deployed to the production environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;bundled-deployments&quot;&gt;Bundled deployments&lt;/h4&gt;

&lt;p&gt;To support multiple ML models as part of a single model serving application, we introduced the concept of &lt;strong&gt;bundled deployments&lt;/strong&gt;. Multiple Kubernetes deployments are bundled together as a single model serving application deployment, allowing each component (e.g., models, catwalk-orchestrator, etc) to have its own Kubernetes deployment and to scale independently.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/catwalk-evolution/phase4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to the major developments, we implemented other changes to enhance our platform’s efficiency. We made &lt;strong&gt;load testing&lt;/strong&gt; mandatory for all ML application updates to ensure robust performance. This testing process was streamlined with a single command that runs the load test in the staging environment, with the results directly shared with the user.&lt;/p&gt;

&lt;p&gt;Furthermore, we boosted &lt;strong&gt;deployment transparency&lt;/strong&gt; by sharing deployment details through Slack and Datadog. This empowered users to diagnose issues independently, reducing the dependency on on-call support. This transparency not only improved our issue resolution times but also enhanced user confidence in our platform.&lt;/p&gt;

&lt;p&gt;The results of these changes speak for themselves. The Catwalk Orchestrator has evolved into our flagship product. In just two years, we have deployed 200 Catwalk Orchestrators serving approximately 1,400 ML models.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h1&gt;

&lt;p&gt;As we continue to innovate and enhance our model serving platform, we are venturing into new territories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Catwalk serverless&lt;/strong&gt;: We aim to further abstract the model serving experience, making it even more user-friendly and efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Catwalk data serving&lt;/strong&gt;: We are looking to extend Catwalk’s capabilities to serve data online, providing a more comprehensive service.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LLM serving&lt;/strong&gt;: In line with the trend towards generative AI and large language models (LLMs), we’re pivoting Catwalk to support these developments, ensuring we stay at the forefront of the AI and machine learning field.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stay tuned as we continue to advance our technology and bring these exciting developments to life.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Oct 2024 00:00:50 +0000</pubDate>
        <link>https://engineering.grab.com/catwalk-evolution</link>
        <guid isPermaLink="true">https://engineering.grab.com/catwalk-evolution</guid>
        
        <category>Machine Learning</category>
        
        <category>Models</category>
        
        <category>Data Science</category>
        
        <category>TensorFlow</category>
        
        <category>Kubernetes</category>
        
        <category>Docker</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Enabling conversational data discovery with LLMs at Grab</title>
        <description>&lt;p&gt;Imagine a world where finding the right data is like searching for a needle in a haystack. In today’s data-driven landscape, companies are drowning in a sea of information, struggling to navigate through countless datasets to uncover valuable insights. At Grab, we faced a similar challenge. With over 200,000 tables in our data lake, along with numerous Kafka streams, production databases, and ML features, locating the most suitable dataset for our Grabber’s use cases promptly has historically been a significant hurdle.&lt;/p&gt;

&lt;h2 id=&quot;problem-space&quot;&gt;Problem Space&lt;/h2&gt;

&lt;p&gt;Our internal data discovery tool, Hubble, built on top of the popular open-source platform Datahub, was primarily used as a reference tool. While it excelled at providing metadata for known datasets, it struggled with true data discovery due to its reliance on Elasticsearch, which performs well for keyword searches but cannot accept and use user-provided context (i.e., it can’t perform semantic search, at least in its vanilla form). The Elasticsearch parameters provided by Datahub out of the box also had limitations: our monthly average click-through rate was only 82%, meaning that in 18% of sessions, users abandoned their searches without clicking on any dataset. This suggested that the search results were not meeting their needs.&lt;/p&gt;

&lt;p&gt;Another indispensable requirement for efficient data discovery that was missing at Grab was documentation. Documentation coverage for our data lake tables was low, with only 20% of the most frequently queried tables (colloquially referred to as P80 tables) having existing documentation. This made it difficult for users to understand the purpose and contents of different tables, even when browsing through them on the Hubble UI.&lt;/p&gt;

&lt;p&gt;Consequently, data consumers heavily relied on tribal knowledge, often turning to their colleagues via Slack to find the datasets they needed. A survey conducted last year revealed that 51% of data consumers at Grab took multiple days to find the dataset they required, highlighting the inefficiencies in our data discovery process.&lt;/p&gt;

&lt;p&gt;To address these challenges and align with Grab’s ongoing journey towards a data mesh architecture, the Hubble team recognised the importance of improving data discovery. We embarked on a journey to revolutionise the way our employees find and access the data they need, leveraging the power of AI and Large Language Models (LLMs).&lt;/p&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;

&lt;p&gt;Given the historical context, our vision was clear: to remove humans in the data discovery loop by automating the entire process using LLM-powered products. We aimed to reduce the time taken for data discovery from multiple days to mere seconds, eliminating the need for anyone to ask their colleagues data discovery questions ever again.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;goals&quot;&gt;Goals&lt;/h2&gt;

&lt;p&gt;To achieve our vision, we set the following goals for ourselves for the first half of 2024:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Build HubbleIQ:&lt;/strong&gt; An LLM-based chatbot that could serve as the equivalent of a Lead Data Analyst for data discovery. Just as a lead is an expert in their domain and can guide data consumers to the right dataset, we wanted HubbleIQ to do the same across all domains at Grab. We also wanted HubbleIQ to be accessible where data consumers hang out the most: Slack.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improve documentation coverage:&lt;/strong&gt; A new Lead Analyst joining the team would require extensive documentation coverage of very high quality. Without this, they wouldn’t know what data exists and where. Thus, it was important for us to improve documentation coverage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhance Elasticsearch:&lt;/strong&gt; We aimed to tune our Elasticsearch implementation to better meet the requirements of Grab’s data consumers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-systematic-path-to-success&quot;&gt;A Systematic Path to Success&lt;/h2&gt;

&lt;h3 id=&quot;step-1-enhance-elasticsearch&quot;&gt;Step 1: Enhance Elasticsearch&lt;/h3&gt;

&lt;p&gt;Through clickstream analysis and user interviews, the Hubble team identified four categories of data search queries that were seen either on the Hubble UI or in Slack channels:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Exact search:&lt;/strong&gt; Queries belonging to this category were a substring of an existing dataset’s name at Grab, with the query length being at least 40% of the dataset’s name.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Partial search:&lt;/strong&gt; The Levenshtein distance between a query in this category and any existing dataset’s name was greater than 80. This category usually comprised queries that closely resembled an existing dataset name but likely contained spelling mistakes or were shorter than the actual name.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Exact and partial searches accounted for 75% of searches on Hubble (and were non-existent on Slack: as a human, receiving a message that just had the name of a dataset would feel rather odd). Given the effectiveness of vanilla Elasticsearch for these categories, the click rank was close to 0.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inexact search:&lt;/strong&gt; This category comprised queries that were usually colloquial keywords or phrases that may be semantically related to a given table, column, or piece of documentation (e.g., “city” or “taxi type”). Inexact searches accounted for the remaining 25% of searches on Hubble. Vanilla Elasticsearch did not perform well in this category since it relied on pure keyword matching and did not consider any additional context.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Semantic search:&lt;/strong&gt; These were free text queries with abundant contextual information supplied by the user. Hubble did not see any such queries as users rightly expected that Hubble would not be able to fulfil their search needs. Instead, these queries were sent by data consumers to data producers via Slack. Such queries were numerous, but usually resulted in data hunting journeys that spanned multiple days - the root of frustration amongst data consumers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two search types can be seen as “reference” queries, where the data consumer already knows what they are looking for. Inexact and contextual searches are considered “discovery” queries. The Hubble team noticed drop-offs in inexact searches because users learned that Hubble could not fulfil their discovery needs, forcing them to search for alternatives.&lt;/p&gt;

&lt;p&gt;Through user interviews, the team discovered how Elasticsearch should be tuned to better fit the Grab context. They implemented the following optimisations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tagging and boosting P80 tables&lt;/li&gt;
  &lt;li&gt;Boosting the most relevant schemas&lt;/li&gt;
  &lt;li&gt;Hiding irrelevant datasets like PowerBI dataset tables&lt;/li&gt;
  &lt;li&gt;Deboosting deprecated tables&lt;/li&gt;
  &lt;li&gt;Improving the search UI by simplifying and reducing clutter&lt;/li&gt;
  &lt;li&gt;Adding relevant tags&lt;/li&gt;
  &lt;li&gt;Boosting certified tables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result of these enhancements, the click-through rate rose steadily over the course of the half to 94%, a 12 percentage point increase.&lt;/p&gt;

&lt;p&gt;While this helped us make significant improvements to the first three search categories, we knew we had to build HubbleIQ to truly automate the last category - semantic search.&lt;/p&gt;

&lt;h3 id=&quot;step-2-build-a-context-store-for-hubbleiq&quot;&gt;Step 2: Build a Context Store for HubbleIQ&lt;/h3&gt;

&lt;p&gt;To support HubbleIQ, we built a documentation generation engine that used GPT-4 to generate documentation based on table schemas and sample data. We refined the prompt through multiple iterations of feedback from data producers.&lt;/p&gt;

&lt;p&gt;We added a “generate” button on the Hubble UI, allowing data producers to easily generate documentation for their tables. This feature also supported the ongoing Grab-wide initiative to certify tables.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image7.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In conjunction, we took the initiative to pre-populate docs for the most critical tables, while notifying data producers to review the generated documentation. Such docs were visible to data consumers with an “AI-generated” tag as a precaution. When data producers accepted or edited the documentation, the tag was removed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image3.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As a result, documentation coverage for P80 tables increased by 70 percentage points to ~90%. User feedback showed that ~95% of users found the generated docs useful.&lt;/p&gt;

&lt;h3 id=&quot;step-3-build-and-launch-hubbleiq&quot;&gt;Step 3: Build and Launch HubbleIQ&lt;/h3&gt;

&lt;p&gt;With high documentation coverage in place, we were ready to harness the power of LLMs for data discovery. To speed up go-to-market, we decided to leverage &lt;a href=&quot;https://www.glean.com/&quot;&gt;Glean&lt;/a&gt;, an enterprise search tool used by Grab.&lt;/p&gt;

&lt;p&gt;First, we integrated Hubble with Glean, making all data lake tables with documentation available on the Glean platform. Next, we used &lt;a href=&quot;https://www.glean.com/product/apps&quot;&gt;Glean Apps&lt;/a&gt; to create the HubbleIQ bot, which was essentially an LLM with a custom system prompt that could access all Hubble datasets that were catalogued on Glean. Finally, we integrated this bot into Hubble search, such that for any search that is likely to be a semantic search, HubbleIQ results are shown on top, followed by regular search results.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image5.png&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Recently, we integrated HubbleIQ with Slack, allowing data consumers to discover datasets without breaking their flow. Currently, we are working with analytics teams to add the bot to their “ask” channels (where data consumers come to ask contextual search queries for their domains). After integration, HubbleIQ will act as the first line of defence for answering questions in these channels, reducing the need for human intervention.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/hubble-data-discovery/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The impact of these improvements was significant. A follow-up survey revealed that 73% of respondents found it easy to discover datasets, marking a substantial 17 percentage point increase from the previous survey. Moreover, Hubble reached an all-time high in monthly active users, demonstrating the effectiveness of the enhancements made to the platform.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We’ve made significant progress towards our vision, but there’s still work to be done. Looking ahead, we have several exciting initiatives planned to further enhance data discovery at Grab.&lt;/p&gt;

&lt;p&gt;On the documentation generation front, we aim to enrich the generator with more context, enabling it to produce even more accurate and relevant documentation. We also plan to streamline the process by allowing analysts to auto-update data docs based on Slack threads directly from Slack. To ensure the highest quality of documentation, we will develop an evaluator model that leverages LLMs to assess the quality of both human and AI-written docs. Additionally, we will implement Reflexion, an agentic workflow that utilises the outputs from the doc evaluator to iteratively regenerate docs until a quality benchmark is met or a maximum try-limit is reached.&lt;/p&gt;

&lt;p&gt;As for HubbleIQ, our focus will be on continuous improvement. We’ve already added support for metric datasets and are actively working on incorporating other types of datasets as well. To provide a more seamless user experience, we will enable users to ask follow-up questions to HubbleIQ directly on the HubbleUI, with the system intelligently pulling additional metadata when a user mentions a specific dataset.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;By harnessing the power of AI and LLMs, the Hubble team has made significant strides in improving documentation coverage, enhancing search capabilities, and drastically reducing the time taken for data discovery. While our efforts so far have been successful, there are still steps to be taken before we fully achieve our vision of completely replacing the reliance on data producers for data discovery. Nonetheless, with our upcoming initiatives and the groundwork we have laid, we are confident that we will continue to make substantial progress in the right direction over the next few production cycles.&lt;/p&gt;

&lt;p&gt;As we forge ahead, we remain dedicated to refining and expanding our AI-powered data discovery tools, ensuring that Grabbers have every dataset they need to drive Grab’s success at their fingertips. The future of data discovery at Grab is brimming with possibilities, and the Hubble team is thrilled to be at the forefront of this exciting journey.&lt;/p&gt;

&lt;p&gt;To our readers, we hope that our journey has inspired you to explore how you can leverage the power of AI to transform data discovery within your own organisations. The challenges you face may be unique, but the principles and strategies we have shared can serve as a foundation for your own data discovery revolution. By embracing innovation, focusing on user needs, and harnessing the potential of cutting-edge technologies, you too can unlock the full potential of your data and propel your organisation to new heights. The future of data-driven innovation is here, and we invite you to join us on this exhilarating journey.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Sep 2024 00:00:40 +0000</pubDate>
        <link>https://engineering.grab.com/hubble-data-discovery</link>
        <guid isPermaLink="true">https://engineering.grab.com/hubble-data-discovery</guid>
        
        <category>Data Discovery</category>
        
        <category>AI</category>
        
        <category>LLM</category>
        
        <category>Documentation</category>
        
        <category>Elasticsearch</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Bringing Grab’s Live Activity to Android: Enhancing user experience through custom notifications</title>
        <description>&lt;p&gt;In May 2023, Grab unveiled the Live Activity feature for iOS, which received positive feedback from users. Live Activity is a feature that enhances user experience by displaying a user interface (UI) outside of the app, delivering real-time updates and interactive content. At Grab, we leverage this feature to keep users informed about their order updates without requiring them to manually open the app.&lt;/p&gt;

&lt;p&gt;While Live Activity is a native iOS feature provided by Apple, there is currently no official Android equivalent. However, we are determined to bring this immersive experience to Android users. Inspired by the success of Live Activity on iOS, we have embarked on design explorations and feasibility studies to ensure the seamless integration of Live Activity into the Android platform. Our ultimate goal is to provide Android users with the same level of convenience and real-time updates, elevating their Grab experience.&lt;/p&gt;

&lt;h2 id=&quot;product-exploration&quot;&gt;Product Exploration&lt;/h2&gt;

&lt;p&gt;In July 2023, we took a proactive step by forming a dedicated working group with the specific goal of exploring Live Activity on the Android platform. Our mindset was focused on quickly enabling the MVP (Minimum Viable Product) of this feature for Android users. We focused on enabling Grab users to track food and mart orders on Live Activity as our first use-case. We also designed the Live Activity module as an extendable platform, allowing easy adoption by other Grab internal verticals such as the Express and Transport teams.&lt;/p&gt;

&lt;p&gt;The team kicked off by analysing the current solution and end-to-end flow of Live Activity on iOS. The objective was to uncover opportunities on how we could leverage the existing platform approach.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure1.png&quot; alt=&quot;&quot; style=&quot;width:30%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Grab iOS Live Activity flow.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The first thing that caught our attention was that there is no Live Activity Token (also known as Push Token) concept on Android. Push Token is a token generated from the ActivityKit framework and used to remotely start, update, and end Live Activity notifications on iOS.&lt;/p&gt;

&lt;p&gt;Our goal was to match the Live Activity set-up of iOS in Android, which was a challenge due to the missing Push Token. This required us to think outside the box and develop an innovative workaround. After multiple brainstorming sessions, the team developed two potential solutions, Solution 1 and Solution 2, as illustrated below:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Proposed solutions for Live Activity for Android.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We evaluated the two solutions. The first solution is to substitute the Push Token with a placeholder value, serving as a distinctive notification identifier. Whereas, the second solution involves the Hedwig service, our in-house message delivery service. We proposed to bypass the Live Activity token check process specifically for Android devices. Following extensive discussions, we decided to proceed with the first solution, which ensures consistency in the technical approach between Android and iOS platforms. Additionally, this solution allows us to ensure that notifications are only pushed to the devices that support the Live Activity feature. This decision strikes a good balance between efficiency and compatibility.&lt;/p&gt;

&lt;h3 id=&quot;ui-components&quot;&gt;UI Components&lt;/h3&gt;

&lt;p&gt;Starting with a kick-off project meeting where we showcased our plans and proposed solutions to our stakeholders, the engineering team presented two native Android UI components that could be utilised to replicate Live Activity: the Notification View and the Floating View.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Notification View&lt;/strong&gt; is a component located in the notification drawer (and potentially on the Lock Screen) that fulfils the most basic use-case of the Live Activity feature. It enables Android users to access information without the need to open the app. Since the standard notification template only allows developers to display a single content title, a content subtitle, and one image, it falls short of meeting our Live Activity UI requirements. To overcome this limitation, custom notifications with custom layouts are needed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure3.gif&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Early design spec of Grab’s LA using custom notification.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One of the key advantages of custom notifications is that they do not require any additional new permissions, ensuring a smooth user experience. Additionally, Android users are accustomed to checking their notifications from the notification tray, making it a familiar and intuitive interaction. However, it is important to acknowledge that custom notifications rely on a remote view, which can pose restrictions on rendering only specific views. On top of that, custom notifications provide a limited space for content – limited to 48dp when collapsed and 252dp when expanded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Floating View&lt;/strong&gt; is a component that will appear above all the applications in Android. It adds the convenience of accessing the information when the device is unlocked or when the user is on another app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Early design spec of Grab’s LA using floating view.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The use of a Floating View offers greater flexibility to the view by eliminating the reliance on a remote view. However, it’s important to be aware of the potential limitations associated with this approach. These limitations include the requirement for screen space, which can potentially impact other app functionalities and cause frustration for users. Additionally, if we intend to display multiple order updates, we may require even more space, taking into account that Grab allows users to place multiple orders. Furthermore, the Floating View feature requires an extra “Draw over other apps” permission, a setting that allows an app to display information on top of other apps on your screen.&lt;/p&gt;

&lt;p&gt;After thoughtful deliberation, we concluded that custom notifications provide a more consistent and user-friendly solution for implementing Grab’s Live Activity feature on Android. They offer compatibility, non-intrusiveness, no extra permissions, and the flexibility of silent notifications, ensuring an optimised user experience.&lt;/p&gt;

&lt;h2 id=&quot;building-grab-androids-live-activity&quot;&gt;Building Grab Android’s “Live Activity”&lt;/h2&gt;

&lt;p&gt;We began developing the Live Activity feature by focusing on Food and Mart for the MVP. However, we prioritised potential future use cases for other verticals by examining the existing functionality of the Grab iOS Live Activity feature. By considering these factors from the start, we need to make sure that we build an extendable and flexible solution that caters to different verticals and their various use-cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure5.gif&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Grab’s Android Live Activity.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As we set out to design Grab’s Android Live Activity module, we broke down the task into three key components:&lt;/p&gt;

&lt;h3 id=&quot;registering-live-activity-token&quot;&gt;&lt;strong&gt;Registering Live Activity Token&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In order to enable Hedwig services to send Live Activity notifications to devices, it is necessary to register a Live Activity Token for a specific order to Grab Devices services (refer to figure 1 for the iOS flow). As this use-case is applicable across various verticals in iOS, we have designed a LiveActivityIntegrationManager class specifically to handle this functionality.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;interface LiveActivityIntegrationManager {  
    /\*\*  
     \* To start live activity journey  
     \* @param vertical refers to vertical name  
     \* @param id refers to unique id which is used to differentiate live activity UI instances  
     \* eg: Food will use orderID as id, transport can pass rideID  
     \*\*/  
    fun startLiveActivity(vertical: Vertical, id: String): Completable

    fun updateLiveActivity(id: String, attributes: LiveActivityAttributes)

    fun cancelLiveActivity(id: String)  
}  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our goal is to provide developers with an easy implementation of Live Activity in the Grab app. Developers can simply utilize the startLiveActivity() function to register the token to Grab Devices by passing the vertical name and unique ID as parameters.&lt;/p&gt;

&lt;h3 id=&quot;notification-listener-and-payload-mapping&quot;&gt;&lt;strong&gt;Notification Listener and Payload Mapping&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;To handle Live Activity notifications in Android, it is necessary to listen to the Live Activity notification payload and map it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt;. Taking into consideration the initial Live Activity design (refer to figure 3), we need to analyse the variables necessary for this process. As a result, we break down the Live Activity UI into different UI elements and layouts, as follows:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img//live-activity-2/figure6.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Android Live Activity view breakdown.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;App Icon&lt;/strong&gt; – labeled as 1 in Figure 6. &lt;br /&gt;
This view always shows the Grab app icon.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Header Icon&lt;/strong&gt; – labeled as 2 in Figure 6.&lt;br /&gt;
This view is an image view that could be set with icon resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content Title View&lt;/strong&gt; – labeled as 3 in Figure 6. &lt;br /&gt;
This view is a placeholder that could be set with a text or custom remote view.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content Text View&lt;/strong&gt; – labeled as 4 in Figure 6. &lt;br /&gt;
This view is a placeholder that could be set with a text or custom remote view.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Footer View&lt;/strong&gt; – labeled as 5 in Figure 6.&lt;br /&gt;
This view is a placeholder that could be set with icon resources, bitmap, or custom remote view.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Decomposing the UI into different parts allows us to clearly understand of the UI components that need to maintain consistency across different use-cases, as well as the elements that can be easily customised and configured based on specific requirements. As a result, we have designed the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt; class that serves as a container that encompasses all the necessary configurations required for rendering the Live Activity.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 
class LiveActivityAttributes private constructor(  
    val iconRes: Int?,  
    val headerIconRes: Int?,  
    val contentTitle: CharSequence?,  
    val contentTitleStyle: ContentStyle.TitleStyle?,  
    val customContentTitleView: LiveActivityCustomView?,  
    val contentText: CharSequence?,  
    val contentTextStyle: ContentStyle.TextStyle?,  
    val customContentTextView: LiveActivityCustomView?,  
    val footerIconRes: Int?,  
    val footerBitmap: Bitmap?,  
    val footerProgressBarProgress: Float?,  
    val footerProgressBarStyle: ProgressBarStyle?,  
    val footerRatingBarAttributes: RatingBarAttributes?,  
    val customFooterView: LiveActivityCustomView?,  
    val contentIntent: PendingIntent?,  
    …  
)  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;payload-rendering&quot;&gt;&lt;strong&gt;Payload Rendering&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;To ensure a clear separation of responsibilities, we have designed a separate class called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityManager&lt;/code&gt;. This dedicated class is responsible for the mapping of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LiveActivityAttributes&lt;/code&gt; to Notifications. The generated notifications are then utilised by Android’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NotificationManager&lt;/code&gt; class to be posted and displayed accordingly.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
interface LiveActivityManager {  
    /\*\*  
     \* Post a Live Activity to be shown in the status bar, stream, etc.  
     \*  
     \* @param id           the ID of the Live Activity  
     \* @param attributes the LiveActivity to post to the system  
     \*/  
    fun notify(id: Int, attributes: LiveActivityAttributes)

    fun cancel(id: Int)  
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;We are delighted to announce that we have successfully implemented Grab’s Android version of the Live Activity feature for Express and Transport products. Furthermore, we plan to extend this feature to the Driver and Merchant applications as well. We understand the value this feature brings to our users and are committed to enhancing it further. Stay tuned for upcoming updates and enhancements to the Live Activity feature as we continue to improve and expand its capabilities across various verticals.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Mon, 23 Sep 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/live-activity-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/live-activity-2</guid>
        
        <category>Engineering</category>
        
        <category>Android</category>
        
        <category>Exploration</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Unveiling the process: The creation of our powerful campaign builder</title>
        <description>&lt;p&gt;In a previous &lt;a href=&quot;https://engineering.grab.com/trident-real-time-event-processing-at-scale&quot;&gt;blog&lt;/a&gt;, we introduced Trident, Grab’s internal marketing campaign platform. Trident empowers our marketing team to configure If This, Then That (IFTTT) logic and processes real-time events based on that.&lt;/p&gt;

&lt;p&gt;While we mainly covered how we scaled up the system to handle large volumes of real-time events, we did not explain the implementation of the event processing mechanism. This blog will fill up this missing piece. We will walk you through the various processing mechanisms supported in Trident and how they were built.&lt;/p&gt;

&lt;h2 id=&quot;base-building-block-treatment&quot;&gt;Base building block: Treatment&lt;/h2&gt;

&lt;p&gt;In our system, we use the term “treatment” to refer to the core unit of a full IFTTT data structure. A treatment is an amalgamation of three key elements - an event, conditions (which are optional), and actions. For example, consider a promotional campaign that offers “100 GrabPoints for completing a ride paid with GrabPay Credit”. This campaign can be transformed into a treatment in which the event is “ride completion”, the condition is “payment made using GrabPay Credit”, and the action is “awarding 100 GrabPoints”.&lt;/p&gt;

&lt;p&gt;Data generated across various Kafka streams by multiple services within Grab forms the crux of events and conditions for a treatment. Trident processes these Kafka streams, treating each data object as an event for the treatments. It evaluates the set conditions against the data received from these events. If all conditions are met, Trident then executes the actions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/processing-treatments.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Trident processes Kafka streams as events for treatments.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When the Trident user interface (UI) was first established, campaign creators had to grasp the treatment concept and configure the treatments accordingly. As we improved the UI, it became more user-friendly.&lt;/p&gt;

&lt;h2 id=&quot;building-on-top-of-treatment&quot;&gt;Building on top of treatment&lt;/h2&gt;

&lt;p&gt;Campaigns can be more complex than the example we provided earlier. In such scenarios, a single campaign may need transformation into several treatments. All these individual treatments are categorised under what we refer to as a “treatment group”. In this section, we discuss features that we have developed to manage such intricate campaigns.&lt;/p&gt;

&lt;h3 id=&quot;counter&quot;&gt;Counter&lt;/h3&gt;

&lt;p&gt;Let’s say we have a marketing campaign that “rewards users after they complete 4 rides”. For this requirement, it’s necessary for us to keep track of the number of rides each user has completed. To make this possible, we developed a capability known as &lt;strong&gt;counter&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;On the backend, a single counter setup translates into two treatments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treatment 1&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: N/A&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStats&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Treatment 2&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ride Count == 4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this feature, we introduce a new event, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStats&lt;/code&gt; action in &lt;strong&gt;Treatment 1&lt;/strong&gt; triggers the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; event following the update of the user counter. This allows &lt;strong&gt;Treatment 2&lt;/strong&gt; to consume the event and perform subsequent evaluations.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. The end-to-end evaluation process when using the Counter feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt; event is consumed, &lt;strong&gt;Treatment 1&lt;/strong&gt; is evaluated which then executes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incrementUserStat&lt;/code&gt; action. This action increments the user’s ride counter in the database, gets the latest counter value, and publishes an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; event to Kafka.&lt;/p&gt;

&lt;p&gt;There are also other consumers that listen to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onProfileUpdate&lt;/code&gt; events. When this event is consumed, &lt;strong&gt;Treatment 2&lt;/strong&gt; is evaluated. This process involves verifying whether the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ride Count&lt;/code&gt; equals to 4. If the condition is satisfied, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt; action is triggered.&lt;/p&gt;

&lt;p&gt;This feature is not limited to counting the number of event occurrences only. It’s also capable of tallying the total amount of transactions, among other things.&lt;/p&gt;

&lt;h3 id=&quot;delay&quot;&gt;Delay&lt;/h3&gt;

&lt;p&gt;Another feature available on Trident is a delay function. This feature is particularly beneficial in situations where we want to time our actions based on user behaviour. For example, we might want to give a ride voucher to a user three hours after they’ve ordered a ride to a theme park. The intention for this is to offer them a voucher they can use for their return trip.&lt;/p&gt;

&lt;p&gt;On the backend, a delay setup translates into two treatments. Given the above scenario, the treatments are as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treatment 1&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dropoff Location == Universal Studio&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduleDelayedEvent&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Treatment 2&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onDelayedEvent&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: N/A&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We introduce a new event, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onDelayedEvent&lt;/code&gt;, which &lt;strong&gt;Treatment 1&lt;/strong&gt; triggers during the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduleDelayedEvent&lt;/code&gt; action. This is made possible by using Simple Queue Service (SQS), given its built-in capability to publish an event with a delay.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process-delay-feature.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. The end-to-end evaluation process when using the Delay feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The maximum delay that SQS supports is 15 minutes; meanwhile, our platform allows for a delay of up to x hours. To address this limitation, we publish the event multiple times upon receiving the message, extending the delay by another 15 minutes each time, until it reaches the desired delay of x hours.&lt;/p&gt;

&lt;h3 id=&quot;limit&quot;&gt;Limit&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;Limit&lt;/strong&gt; feature is used to restrict the number of actions for a specific campaign or user within that campaign. This feature can be applied on a daily basis or for the full duration of the campaign.&lt;/p&gt;

&lt;p&gt;For instance, we can use the &lt;strong&gt;Limit&lt;/strong&gt; feature to distribute 1000 vouchers to users who have completed a ride and restrict it to only one voucher for one user per day. This ensures a controlled distribution of rewards and prevents a user from excessively using the benefits of a campaign.&lt;/p&gt;

&lt;p&gt;In the backend, a limit setup translates into conditions within a single treatment. Given the above scenario, the treatment would be as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;onRideCompleted&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Condition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TotalUsageCount &amp;lt;= 1000 AND DailyUserUsageCount &amp;lt;= 1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Action: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awardReward&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similar to the &lt;strong&gt;Counter&lt;/strong&gt; feature, it’s necessary for us to keep track of the number of completed rides for each user in the database.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/end-to-end-evaluation-process-limit-feature.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. The end-to-end evaluation process when using the Limit feature.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;a-better-campaign-builder&quot;&gt;A better campaign builder&lt;/h2&gt;

&lt;p&gt;As our campaigns grew more and more complex, the treatment creation quickly became overwhelming. A complex logic flow often required the creation of many treatments, which was cumbersome and error-prone. The need for a more visual and simpler campaign builder UI became evident.&lt;/p&gt;

&lt;p&gt;Our design team came up with a flow-chart-like UI. Figure 5, 6, and 7 show examples of how certain imaginary campaign setup would look like in the new UI.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. When users complete a food order, if they are a gold user, award them with A. However, if they are a silver user, award them with  B.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. When users complete a food or mart order, increment a counter. When the counter reaches 5, send them a message. Once the counter reaches 10, award them with points.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/better-campaign-builder-example-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. When a user confirms a ride booking, wait for 1 minute, and then conduct A/B testing by sending a message 50% of the time.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The campaign setup in the new UI can be naturally stored as a node tree structure. The following is how the example in figure 5 would look like in JSON format. We assign each node a unique number ID, and store a map of the ID to node content.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;1&quot;: {
    &quot;type&quot;: &quot;scenario&quot;,
    &quot;data&quot;: { &quot;eventType&quot;: &quot;foodOrderComplete&quot;  },
    &quot;children&quot;: [&quot;2&quot;, &quot;3&quot;]
  },
  &quot;2&quot;: {
    &quot;type&quot;: &quot;condition&quot;,
    &quot;data&quot;: { &quot;lhs&quot;: &quot;var.user.tier&quot;, &quot;operator&quot;: &quot;eq&quot;, &quot;rhs&quot;: &quot;gold&quot; },
    &quot;children&quot;: [&quot;4&quot;]
  },
  &quot;3&quot;: {
    &quot;type&quot;: &quot;condition&quot;,
    &quot;data&quot;: { &quot;lhs&quot;: &quot;var.user.tier&quot;, &quot;operator&quot;: &quot;eq&quot;, &quot;rhs&quot;: &quot;silver&quot; },
    &quot;children&quot;: [&quot;5&quot;]
  },
  &quot;4&quot;: {
    &quot;type&quot;: &quot;action&quot;,
    &quot;data&quot;: {
      &quot;type&quot;: &quot;awardReward&quot;,
      &quot;payload&quot;: { &quot;rewardID&quot;: &quot;ID-of-A&quot;  }
    }
  },
  &quot;5&quot;: {
    &quot;type&quot;: &quot;action&quot;,
    &quot;data&quot;: {
      &quot;type&quot;: &quot;awardReward&quot;,
      &quot;payload&quot;: { &quot;rewardID&quot;: &quot;ID-of-B&quot;  }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conversion-to-treatments&quot;&gt;Conversion to treatments&lt;/h3&gt;

&lt;p&gt;The question then arises, how do we execute this node tree as treatments? This requires a conversion process. We then developed the following algorithm for converting the node tree into equivalent treatments:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// convertToTreatments is the main function
func convertToTreatments(rootNode) -&amp;gt; []Treatment:
  output = []

  for each scenario in rootNode.scenarios:
    // traverse down each branch
    context = createConversionContext(scenario)
    for child in rootNode.children:
      treatments = convertHelper(context, child)
      output.append(treatments)

  return output

// convertHelper is a recursive helper function
func convertHelper(context, node) -&amp;gt; []Treatment:
  output = []
  f = getNodeConverterFunc(node.type)
  treatments, updatedContext = f(context, node)

  output.append(treatments)

  for child in rootNode.children:
    treatments = convertHelper(updatedContext, child)
    output.append(treatments)

  return output
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getNodeConverterFunc&lt;/code&gt; will return different handler functions according to the node type. Each handler function will either update the conversion context, create treatments, or both.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;caption style=&quot;caption-side:bottom&quot;&gt;Table 1. The handler logic mapping for each node type.&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;width:20%&quot;&gt;Node type&lt;/th&gt;
      &lt;th&gt;Logic&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;condition&lt;/td&gt;
      &lt;td&gt;Add conditions into the context and return the updated context.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;action&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and the action itself.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;delay&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and a scheduleDelayedEvent action.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;count&lt;/td&gt;
      &lt;td&gt;Return a treatment with the event type, condition from the context, and an incrementUserStats action.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;count condition&lt;/td&gt;
      &lt;td&gt;Form a condition with the count key from the context, and return an updated context with the condition.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It is important to note that treatments cannot always be reverted to their original node tree structure. This is because different node trees might be converted into the same set of treatments.&lt;/p&gt;

&lt;p&gt;The following is an example where two different node trees setups correspond to the same set of treatments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Food order complete -&amp;gt; if gold user -&amp;gt; then award A&lt;/li&gt;
  &lt;li&gt;Food order complete -&amp;gt; if silver user -&amp;gt; then award B&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/two-node-tree-setup.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. An example of two node tree setups corresponding to the the same set of treatments.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Therefore, we need to store both the campaign node tree JSON and treatments, along with the mapping between the nodes and the treatments. Campaigns are executed using treatments, but displayed using the node tree JSON.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/storing-campaigns.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. For each campaign, we store both the node tree JSON and treatments, along with their mapping.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;how-we-handle-campaign-updates&quot;&gt;How we handle campaign updates&lt;/h3&gt;

&lt;p&gt;There are instances where a marketing user updates a campaign after its creation. For such cases we need to identify:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Which existing treatments should be removed.&lt;/li&gt;
  &lt;li&gt;Which existing treatments should be updated.&lt;/li&gt;
  &lt;li&gt;What new treatments should be added.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can do this by using the node-treatment mapping information we stored. The following is the pseudocode for this process:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func howToUpdateTreatments(oldTreatments []Treatment, newTreatments []Treatment):
  treatmentsUpdate = map[int]Treatment // treatment ID -&amp;gt; updated treatment
  treatmentsRemove = []int // list of treatment IDs
  treatmentsAdd = []Treatment // list of new treatments to be created

  matchedOldTreamentIDs = set()

  for newTreatment in newTreatments:
    matched = false

    // see whether the nodes match any old treatment
    for oldTreatment in oldTreatments:
      // two treatments are considered matched if their linked node IDs are identical
      if isSame(oldTreatment.nodeIDs, newTreatment.nodeIDs):
        matched = true
        treatmentsUpdate[oldTreament.ID] = newTreatment
        matchedOldTreamentIDs.Add(oldTreatment.ID)
        break

    // if no match, that means it is a new treatment we need to create
    if not matched:
      treatmentsAdd.Append(newTreatment)

  // all the non-matched old treatments should be deleted
  for oldTreatment in oldTreatments:
    if not matchedOldTreamentIDs.contains(oldTreatment.ID):
      treatmentsRemove.Append(oldTreatment.ID)

  return treatmentsAdd, treatmentsUpdate, treatmentsRemove
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For a visual illustration, let’s consider a campaign that initially resembles the one shown in figure 10. The node IDs are highlighted in red.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/campaign-node-tree-structure.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10. A campaign in node tree structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This campaign will generate two treatments.&lt;/p&gt;

&lt;table style=&quot;width: 70%&quot; class=&quot;table&quot;&gt;
  &lt;caption style=&quot;caption-side:bottom&quot;&gt;Table 2. The campaign shown in the figure 10 will generated two treatments.&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;width:10%&quot;&gt;ID&lt;/th&gt;
      &lt;th style=&quot;width:50%&quot;&gt;Treatment&lt;/th&gt;
      &lt;th&gt;Linked node IDs&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Event: food order complete &lt;br /&gt; Condition: gold user &lt;br /&gt; Action: award A&lt;/td&gt;
      &lt;td&gt;1, 2, 3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Event: food order complete &lt;br /&gt; Condition: silver user &lt;br /&gt; Action: award B&lt;/td&gt;
      &lt;td&gt;1, 4, 5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;After creation, the campaign creator updates the upper condition branch, deletes the lower branch, and creates a new branch. Note that after node deletion, the deleted node ID will not be reused.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/the-creation-of-our-powerful-campaign-builder/updated-campaign-node-tree-structure.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 11. An updated campaign in node tree structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;According to our logic in figure 11, the following update will be performed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Update action for treatment 1 to “award C”.&lt;/li&gt;
  &lt;li&gt;Delete treatment 2&lt;/li&gt;
  &lt;li&gt;Create a new treatment: food -&amp;gt; is promo used -&amp;gt; send push&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article reveals the workings of Trident, our bespoke marketing campaign platform. By exploring the core concept of a “treatment” and additional features like Counter, Delay and Limit, we illustrated the flexibility and sophistication of our system.&lt;/p&gt;

&lt;p&gt;We’ve explained changes to the Trident UI that have made campaign creation more intuitive. Transforming campaign setups into executable treatments while preserving the visual representation ensures seamless campaign execution and adaptation.&lt;/p&gt;

&lt;p&gt;Our devotion to improving Trident aims to empower our marketing team to design engaging and dynamic campaigns, ultimately providing excellent experiences to our users.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Sep 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/the-creation-of-our-powerful-campaign-builder</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-creation-of-our-powerful-campaign-builder</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Chimera Sandbox: A scalable experimentation and development platform for Notebook services</title>
        <description>&lt;p&gt;Key to innovation and improvement in machine learning (ML) models is the ability for rapid iteration. Our team, Chimera, part of the Artificial Intelligence (AI) Platform team, provides the essential compute infrastructure, ML pipeline components, and backend services. This support enables our ML engineers, data scientists, and data analysts to efficiently experiment and develop ML solutions at scale.&lt;/p&gt;

&lt;p&gt;With a commitment to leveraging the latest Generative AI (GenAI) technologies, Grab is enhancing productivity tools for all Grabbers. Our Chimera Sandbox, a scalable Notebook platform, facilitates swift experimentation and development of ML solutions, offering deep integration with our AI Gateway. This enables easy access to various Large Language Models (LLMs) (both proprietary and open source), ensuring scalability, compliance, and access control are managed seamlessly.&lt;/p&gt;

&lt;h2 id=&quot;what-is-chimera-sandbox&quot;&gt;What is Chimera Sandbox?&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox is a Notebook service platform. It allows users to launch multiple notebook and visualisation services for experimentation and development. The platform offers an extremely quick onboarding process enabling any Grabber to start learning, exploring and experimenting in just a few minutes. This inclusivity and ease of use have been key in driving the adoption of the platform across different teams within Grab and empowering all Grabbers to be GenAI-ready.&lt;/p&gt;

&lt;p&gt;One significant challenge in harnessing ML for innovation, whether for technical experts or non-technical enthusiasts, has been the accessibility of resources. This includes GPU instances and specialised services for developing LLM-powered applications. Chimera Sandbox addresses this head-on by offering an extensive array of compute instances, both with and without GPU support, thus removing barriers to experimentation. Its deep integration with Grab’s suite of internal ML tools transforms the way users approach ML projects. Users benefit from features like hyperparameter tuning, tracking ML training metadata, accessing diverse LLMs through Grab’s AI Gateway, and experimenting with rich datasets from Grab’s data lake. Chimera Sandbox ensures that users have everything they need at their fingertips. This ecosystem not only accelerates the development process but also encourages innovative approaches to solving complex problems.&lt;/p&gt;

&lt;p&gt;The underlying compute infrastructure of the Chimera Sandbox platform is Grab’s very own battle-tested, highly scalable ML compute infrastructure running on multiple Kubernetes clusters. Each cluster can scale up to thousands of nodes at peak times gracefully. This scalability ensures that the platform can handle the high computational demands of ML tasks. The robustness of Kubernetes ensures that the platform remains stable, reliable, and highly available even under heavy load. At any point in time, there can be hundreds of data scientists, ML engineers and developers experimenting and developing on the Chimera Sandbox platform.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/chimera-sandbox-platform.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Chimera Sandbox Platform.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/ui-starting-chimera.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. UI for Starting Chimera Sandbox.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;best-of-both-worlds&quot;&gt;Best of both worlds&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox is suitable for both new users who want to explore and experiment ML solutions and advanced users who want to have full control over the Notebook services they run. Users can launch Notebook services using default Docker images provided by the Chimera Sandbox platform. These images come pre-loaded with popular data science and ML libraries and various Grab internal systems integrations. Chimera also provides basic Docker images from which the users can use as base images to build their own customised Notebook service Docker images. Once the images are built, the users can configure their Notebook services to use their custom Docker images. This ensures their Notebook environment can be exactly the way they want them to be.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/customise-notebook-packages.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Users are able to customise their Notebook service with additional packages.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;real-time-collaboration&quot;&gt;Real-time collaboration&lt;/h2&gt;
&lt;p&gt;The Chimera Sandbox platform also features a real-time collaboration feature. This feature fosters a collaborative environment where users can exchange ideas and work together on projects.&lt;/p&gt;

&lt;h2 id=&quot;cpu-and-gpu-choices&quot;&gt;CPU and GPU choices&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox offers a wide variety of CPU and GPU choices to cater to specific needs, whether it is a CPU, memory, or GPU intensive experimentation. This flexibility allows users to choose the most suitable computational resources for their tasks, ensuring optimal performance and efficiency.&lt;/p&gt;

&lt;h2 id=&quot;deep-integration-with-spark&quot;&gt;Deep integration with Spark&lt;/h2&gt;

&lt;p&gt;The platform is deeply integrated with internal Spark engines, enabling users to experiment building  extract, transform, and load (ETL) jobs with data from Grab’s data lake. Integrated helpers such as SparkConnect Kernel and %%spark_sql magic cell, provide a faster developer experience, which can execute Spark SQL queries without needing to write additional code to start a Spark session and query.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/magic-cell.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. %%spark_sql magic cell enables users to quickly explore data with Spark.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to Magic Cell, the Chimera Sandbox offers advanced Spark functionalities. Users can write PySpark code using pre-configured and configurable Spark clients in the runtime environment. The underlying computation engine leverages Grab’s custom Spark-on-Kubernetes operator, enabling support for large-scale Spark workloads. This high-code capability complements the low-code Magic Cell feature, providing users with a versatile data processing environment.&lt;/p&gt;

&lt;h2 id=&quot;ai-gallery&quot;&gt;AI Gallery&lt;/h2&gt;

&lt;p&gt;Chimera Sandbox features an AI Gallery to guide and accelerate users to start experimenting with ML solutions or building GenAI-powered applications. This is especially useful for new or novice users who are keen to explore what they can do on the Chimera Sandbox platform. With Chimera Sandbox, users are not just presented with a bare bones compute solution but rather are provided with ways to do ML tasks right from Chimera Sandbox Notebooks. This approach saves users from the hassle of having to piece together the examples from the public internet, which may not work on the platform. These ready-to-run and comprehensive notebooks in the AI Gallery assure users that they can run end-to-end examples without a hitch. Based on these examples, the users can only extend their experimentations and development for their specific needs. Not only that, these tutorials and notebooks exhibit the platform capabilities and integrations available on the platform in an interactive manner rather than having the users refer to a separate documentation.&lt;/p&gt;

&lt;p&gt;Lastly, the AI Gallery encourages contributions from other Grabbers, fostering a collaborative environment. Users who are enthusiastic about creating educational contents on Chimera Sandbox can effectively share their work with other Grabbers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/ai-gallery.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Including AI Gallery in user specified sandbox images.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;integration-with-various-llm-services&quot;&gt;Integration with various LLM services&lt;/h2&gt;

&lt;p&gt;Notebook users on Chimera Sandbox can easily tap into a plethora of LLMs, both open source and proprietary models, without any additional setup via our AI Gateway. The platform takes care of access mechanisms and endpoints for various LLM services so that the users can easily use their favourite libraries to create LLM-powered applications and conduct experimentations. This seamless integration with LLMs enables users to focus on their GAI-powered ideas rather than having to worry about underlying logistics and technicalities of using different LLMs.&lt;/p&gt;

&lt;h2 id=&quot;more-than-a-notebook-service&quot;&gt;More than a notebook service&lt;/h2&gt;

&lt;p&gt;While Notebook is the most popular service on the platform, Chimera Sandbox offers much more than just notebook capabilities. It serves as a comprehensive namespace workspace equipped with a suite of ML/AI tools. Alongside notebooks, users can access essential ML tools such as Optuna for hyperparameter tuning, MLflow for experiment tracking, and other tools including Zeppelin, RStudio, Spark history, Polynote, and LabelStudio. All these services use a shared storage system, creating a tailored workspace for ML and AI tasks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/sandbox-namespace.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. A Sandbox namespace with its out-of-the-box services.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, the Sandbox framework allows for the seamless integration of more services into personal workspaces. This high level of flexibility significantly enhances the capabilities of the Sandbox platform, making it an ideal environment for diverse ML and AI applications.&lt;/p&gt;

&lt;h2 id=&quot;cost-attribution&quot;&gt;Cost attribution&lt;/h2&gt;

&lt;p&gt;For a multi-tenanted platform such as Chimera Sandbox, it is crucial to provide users information on how much they have spent with their experimentations. Cost showback and chargeback capabilities are of utmost importance for a platform on which users can launch Notebook services that use accelerated instances with GPUs. The platform provides cost attribution to individual users, so each user knows exactly how much they are spending on their experimentations and can make budget-conscious decisions. This transparency in cost attribution encourages responsible usage of resources and helps users manage their budgets effectively.&lt;/p&gt;

&lt;h2 id=&quot;growth-and-future-plans&quot;&gt;Growth and future plans&lt;/h2&gt;

&lt;p&gt;In essence, Chimera Sandbox is more than just a tool; it’s a catalyst for innovation and growth, empowering Grabbers to explore the frontiers of ML and AI. By providing an inclusive, flexible, and powerful platform, Chimera Sandbox is helping shape the future of Grab, making every Grabber not just ready but excited to contribute to the AI-driven transformation of our products and services.&lt;/p&gt;

&lt;p&gt;In July and August of this year, teams were given the opportunity to intensively learn and experiment with AI. Since then, we have observed hockey stick growth on the Chimera Sandbox platform. We are enabling massive experimentation across different teams at Grab to experiment and work on different GAI-powered applications.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/chimera-sandbox/daily-active-users.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Chimera Sandbox daily active users.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our future plans include mechanisms for better notebook discovery, collaboration and usability, and the ability to enable users to schedule their notebooks right from Chimera Sandbox. These enhancements aim to improve the user experience and make the platform even more versatile and powerful.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 700 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Aug 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/chimera-sandbox</link>
        <guid isPermaLink="true">https://engineering.grab.com/chimera-sandbox</guid>
        
        <category>Engineering</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        <category>Experiment</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>How we improved translation experience with cost efficiency</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As COVID restrictions were fully lifted in 2023, the number of tourists grew dramatically. People began to explore the world again, frequently using the Grab app to make bookings outside of their home country. However, we noticed that communication posed a challenge for some users. Despite our efforts to integrate an auto-translation feature in the booking chat, we received feedback about occasional missed or inaccurate translations. You can refer to this &lt;a href=&quot;https://engineering.grab.com/message-center&quot;&gt;blog&lt;/a&gt; for a better understanding of Grab’s chat system.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/improved-translation-experience-with-cost-efficiency/translation-example.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;An example of a bad translation. The correct translation is: &apos;ok sir&apos;.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In an effort to enhance the user experience for travellers using the Grab app, we formed an engineering squad to tackle this problem. The objectives are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure translation is provided when it’s needed.&lt;/li&gt;
  &lt;li&gt;Improve the quality of translation.&lt;/li&gt;
  &lt;li&gt;Maintain the cost of this service within a reasonable range.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ensure-translation-is-provided-when-its-needed&quot;&gt;Ensure translation is provided when it’s needed&lt;/h2&gt;

&lt;p&gt;Originally, we relied on users’ device language settings to determine if translation is needed. For example, if both the passenger and driver’s language setting is set to English, translation is not needed. Interestingly, it turned out that the device language setting did not reliably indicate the language in which a user would send their messages. There were numerous cases where despite having their device language set to English, drivers sent messages in another language.&lt;/p&gt;

&lt;p&gt;Therefore, we needed to detect the language of user messages on the fly to make sure we trigger translation when it’s needed.&lt;/p&gt;

&lt;h3 id=&quot;language-detection&quot;&gt;Language detection&lt;/h3&gt;

&lt;p&gt;Simple as it may seem, language detection is not that straightforward a task. We were unable to find an open-source language detector library that covered all Southeast Asian languages. We looked for Golang libraries as our service was written in Golang. The closest we could find were the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Whatlang: unable to detect Malay&lt;/li&gt;
  &lt;li&gt;Lingua: unable to detect Burmese and Khmer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We decided to choose Lingua over Whatlang as the base detector due to the following factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overall higher accuracy.&lt;/li&gt;
  &lt;li&gt;Capability to provide detection confidence level.&lt;/li&gt;
  &lt;li&gt;We have more users using Malay than those using Burmese or Khmer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a translation request comes in, our first step is to use Lingua for language detection. If the detection confidence level falls below a predefined threshold, we fall back to call the third-party translation service as it can detect all Southeast Asian languages.&lt;/p&gt;

&lt;p&gt;You may ask, why don’t we simply use the third-party service in the first place. It’s because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The third-party service only has a translate API that also does language detection, but it does not provide a standalone language detection API.&lt;/li&gt;
  &lt;li&gt;Using the translate API is costly, so we need to avoid calling it when it’s unnecessary. We will cover more on this in a later section.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another challenge we’ve encountered is the difficulty of distinguishing between Malay and Indonesian languages due to their strong similarities and shared vocabulary. The identical text might convey different meanings in these two languages, which the third-party translation service struggles to accurately detect and translate.&lt;/p&gt;

&lt;p&gt;Differentiating Malay and Indonesian is a tough problem in general. However, in our case, the detection has a very specific context, and we can make use of the context to enhance our detection accuracy.&lt;/p&gt;

&lt;h3 id=&quot;making-use-of-translation-context&quot;&gt;Making use of translation context&lt;/h3&gt;

&lt;p&gt;All our translations are for the messages sent in the context of a booking or order, predominantly between passenger and driver. There are two simple facts that can aid in our language detection:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Booking/order happens in one single country.&lt;/li&gt;
  &lt;li&gt;Drivers are almost always local to that country.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, for a booking that happens in an Indonesian city, if the driver’s message is detected as Malay, it’s highly likely that the message is actually in Bahasa Indonesia.&lt;/p&gt;

&lt;h2 id=&quot;improve-quality-of-translation&quot;&gt;Improve quality of translation&lt;/h2&gt;

&lt;p&gt;Initially, we were entirely dependent on a third-party service for translating our chat messages. While overall powerful, the third-party service is not perfect, and it does generate weird translations from time to time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/improved-translation-experience-with-cost-efficiency/weird-translation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;An example of a weird translation from a third-party service recorded on 19 Dec 2023.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Then, it came to us that we might be able to build an in-house translation model that could translate chat messages better than the third-party service. The reasons being:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The scope of our chat content is highly specific. All the chats are related to bookings or orders. There would not be conversations about life or work in the chat. Maybe a small Machine Learning (ML) model would suffice to do the job.&lt;/li&gt;
  &lt;li&gt;The third-party service is a general translation service. It doesn’t know the context of our messages. We, however, know the whole context. Having the right context gives us a great edge on generating the right translation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training-steps&quot;&gt;Training steps&lt;/h3&gt;

&lt;p&gt;To create our own translation model, we took the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perform topic modelling on Grab chat conversations.&lt;/li&gt;
  &lt;li&gt;Worked with the localisation team to create a benchmark set of translations.&lt;/li&gt;
  &lt;li&gt;Measured existing translation solutions against benchmarks.&lt;/li&gt;
  &lt;li&gt;Used an open source Large Language Model (LLM) to produce synthetic training data.&lt;/li&gt;
  &lt;li&gt;Used synthetic data to train our lightweight translation model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;topic-modelling&quot;&gt;Topic modelling&lt;/h4&gt;

&lt;p&gt;In this step, our aim was to generate a dataset which is both representative of the chat messages sent by our users and diverse enough to capture all of the nuances of the conversations. To achieve this, we took a stratified sampling approach. This involved a random sample of past chat conversation messages stratified by various topics to ensure a comprehensive and balanced representation.&lt;/p&gt;

&lt;h4 id=&quot;developing-a-benchmark&quot;&gt;Developing a benchmark&lt;/h4&gt;

&lt;p&gt;For this step we engaged Grab’s localisation team to create a benchmark for translations. The intention behind this step wasn’t to create enough translation examples to fully train or even finetune a model, but rather, it was to act as a benchmark for translation quality, and also as a set of few-shot learning examples for when we generate our synthetic data.&lt;/p&gt;

&lt;p&gt;This second point was critical! Although LLMs can generate good quality translations, LLMs are highly susceptible to their training examples. Thus, by using a set of handcrafted translation examples, we hoped to produce a set of examples that would teach the model the exact style, level of formality, and correct tone for the context in which we plan to deploy the final model.&lt;/p&gt;

&lt;h4 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h4&gt;

&lt;p&gt;From a theoretical perspective there are two ways that one can measure the performance of a machine translation system. The first is through the computation of some sort of translation quality score such as a BLEU or CHRF++ score. The second method is via subjective evaluation. For example, you could give each translation a score from 1 to 5 or pit two translations against each other and ask someone to assess which they prefer.&lt;/p&gt;

&lt;p&gt;Both methods have their relative strengths and weaknesses. The advantage of a subjective method is that it corresponds better with what we want, a high quality translation experience for our users. The disadvantage of this method is that it is quite laborious. The opposite is true for the computed translation quality scores, that is to say that they correspond less well to a human’s subjective experience of our translation quality, but that they are easier and faster to compute.&lt;/p&gt;

&lt;p&gt;To overcome the inherent limitations of each method, we decided to do the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Set a benchmark score for the translation quality of various translation services using a CHRF++ score.&lt;/li&gt;
  &lt;li&gt;Train our model until its CHRF++ score is significantly better than the benchmark score.&lt;/li&gt;
  &lt;li&gt;Perform a manual A/B test between the newly trained model and the existing translation service.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;synthetic-data-generation&quot;&gt;Synthetic data generation&lt;/h4&gt;

&lt;p&gt;To generate the training data needed to create our model, we had to rely on an open source LLM to generate the synthetic translation data. For this task, we spent considerable effort looking for a model which had both a large enough parameter count to ensure high quality outputs, but also a model which had the correct tokenizer to handle the diverse sets of languages which Grab’s customers speak. This is particularly important for languages which use non-standard character sets such as Vietnamese and Thai. We settled on using a public model from &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; for this task.&lt;/p&gt;

&lt;p&gt;We then used a subset of the previously mentioned benchmark translations to input as few-shot learning examples to our prompt. After many rounds of iteration, we were able to generate translations which were superior to the benchmark CHRF++ scores which we had attained in the previous section.&lt;/p&gt;

&lt;h4 id=&quot;model-fine-tuning&quot;&gt;Model fine tuning&lt;/h4&gt;
&lt;p&gt;We now had one last step before we had something that was production ready! Although we had successfully engineered a prompt capable of generating high quality translations from the public Hugging Face model, there was no way we’d be able to deploy such a model. The model was far too big for us to deploy it in a cost efficient manner and within an acceptable latency. Our solution to this was to fine-tune a smaller bespoke model using the synthetic training data which was derived from the larger model.&lt;/p&gt;

&lt;p&gt;These models were language specific (e.g. English to Indonesian) and built solely for the purpose of language translation. They are 99% smaller than the public model. With approximately 10 Million synthetic training examples, we were able to achieve performance which was 98% as effective as our larger model.&lt;/p&gt;

&lt;p&gt;We deployed our model and ran several A/B tests with it. Our model performed pretty well overall, but we noticed a critical problem: sometimes, numbers got mutated in the translation. These numbers can be part of an address, phone number, price etc. Showing the wrong number in a translation can cause great confusion to the users. Unfortunately, an ML model’s output can never be fully controlled; therefore, we added an additional layer of programmatic check to mitigate this issue.&lt;/p&gt;

&lt;h3 id=&quot;post-translation-quality-check&quot;&gt;Post-translation quality check&lt;/h3&gt;
&lt;p&gt;Our goal is to ensure non-translatable content such as numbers, special symbols, and emojis  in the original message doesn’t get mutated in the translation produced by our in-house model. We extract all the non-translatable content from the original message, count the occurrences of each, and then try to match the same in the translation. If it fails to match, we discard the in-house translation and fall back to using the third-party translation service.&lt;/p&gt;

&lt;h2 id=&quot;keep-cost-low&quot;&gt;Keep cost low&lt;/h2&gt;

&lt;p&gt;At Grab, we try to be as cost efficient as possible in all aspects. In the case of translation, we tried to minimise cost by avoiding unnecessary on-the-fly translations.&lt;/p&gt;

&lt;p&gt;As you would have guessed, the first thing we did was to implement caching. A cache layer is placed before both the in-house translation model and the third-party translation. We try to serve translation from the cache first before hitting the underlying translation service. However, given that translation requests are in free text and can be quite dynamic, the impact of caching is limited. There’s more we need to do.&lt;/p&gt;

&lt;p&gt;For context, in a booking chat, other than the users, Grab’s internal services can also send messages to the chat room. These messages are called system messages. For example,our food service always sends a message with information on the food order when an order is confirmed.&lt;/p&gt;

&lt;p&gt;System messages are all fairly static in nature, however, we saw a very high amount of translation cost attributed to system messages. Taking a deeper look, we noticed the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Many system messages were not sent in the recipient’s language, thus requiring on-the-fly translation.&lt;/li&gt;
  &lt;li&gt;Many system messages, though having the same static structure, contain quite a few variants such as passenger’s name and  food order item name. This makes it challenging to utilise our translation cache effectively as each message is different.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since all system messages are manually prepared, we should be able to get them all manually translated into all the required languages, and avoid on-the-fly translations altogether.&lt;/p&gt;

&lt;p&gt;Therefore, we launched an internal campaign, mandating all internal services that send system messages to chat rooms to get manual translations prepared, and pass in the translated contents. This alone helped us save roughly US$255K a year!&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;At Grab, we firmly believe that our proprietary in-house translation models are not only more cost-effective but cater more accurately to our unique use cases compared to third-party services. We will focus on expanding these models to more languages and countries across our operating regions.&lt;/p&gt;

&lt;p&gt;Additionally, we are exploring opportunities to apply learnings of our chat translations to other Grab content. This strategy aims to guarantee a seamless language experience for our rapidly expanding user base, especially travellers. We are enthusiastically looking forward to the opportunities this journey brings!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Aug 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/improved-translation-experience-with-cost-efficiency</link>
        <guid isPermaLink="true">https://engineering.grab.com/improved-translation-experience-with-cost-efficiency</guid>
        
        <category>Chat</category>
        
        <category>Chat support</category>
        
        <category>Engineering</category>
        
        <category>GrabChat</category>
        
        <category>Messaging</category>
        
        <category>Translation</category>
        
        <category>Generative AI</category>
        
        <category>LLM</category>
        
        
        <category>Engineering</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>LLM-powered data classification for data entities at scale</title>
        <description>&lt;p&gt;&lt;small class=&quot;credits&quot;&gt; Editor’s note: This post was originally published in October 2023 and has been updated to reflect Grab’s partnership with the Infocomm Media Development Authority as part of its Privacy Enhancing Technology Sandbox that concluded in March 2024.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we deal with PetaByte-level data and manage countless data entities ranging from database tables to Kafka message schemas. Understanding the data inside is crucial for us, as it not only streamlines the data access management to safeguard the data of our users, drivers and merchant-partners, but also improves the data discovery process for data analysts and scientists to easily find what they need.&lt;/p&gt;

&lt;p&gt;The Caspian team (Data Engineering team) collaborated closely with the Data Governance team on automating governance-related metadata generation. We started with Personal Identifiable Information (PII) detection and built an orchestration service using a third-party classification service. With the advent of the Large Language Model (LLM), new possibilities dawned for metadata generation and sensitive data identification at Grab. This prompted the inception of the project, which aimed to integrate LLM classification into our existing service. In this blog, we share insights into the transformation from what used to be a tedious and painstaking process to a highly efficient system, and how it has empowered the teams across the organisation.&lt;/p&gt;

&lt;p&gt;For ease of reference, here’s a list of terms we’ve used and their definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Entity&lt;/strong&gt;: An entity representing a schema that contains rows/streams of data, for example, database tables, stream messages, data lake tables.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Refers to the model’s output given a data entity, unverified manually.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Classification&lt;/strong&gt;: The process of classifying a given data entity, which in the context of this blog, involves generating tags that represent sensitive data or Grab-specific types of data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metadata Generation&lt;/strong&gt;: The process of generating the metadata for a given data entity. In this blog, since we limit the metadata to the form of tags, we often use this term and data classification interchangeably.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;: Refers to the level of confidentiality of data. High sensitivity means that the data is highly confidential. The lowest level of sensitivity often refers to public-facing or publicly-available data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;When we first approached the data classification problem, we aimed to solve something more specific - Personal Identifiable Information (PII) detection. Initially, to protect sensitive data from accidental leaks or misuse, Grab implemented manual processes and campaigns targeting data producers to tag schemas with sensitivity tiers. These tiers ranged from Tier 1, representing schemas with highly sensitive information, to Tier 4, indicating no sensitive information at all. As a result, half of all schemas were marked as Tier 1, enforcing the strictest access control measures.&lt;/p&gt;

&lt;p&gt;The presence of a single Tier 1 table in a schema with hundreds of tables justifies classifying the entire schema as Tier 1. However, since Tier 1 data is rare, this implies that a large volume of non-Tier 1 tables, which ideally should be more accessible, have strict access controls.&lt;/p&gt;

&lt;p&gt;Shifting access controls from the schema-level to the table-level could not be done safely due to the lack of table classification in the data lake. We could have conducted more manual classification campaigns for tables, however this was not feasible for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The volume, velocity, and variety of data had skyrocketed within the organisation, so it took significantly more time to classify at table level compared to schema level. Hence, a programmatic solution was needed to streamline the classification process, reducing the need for manual effort.&lt;/li&gt;
  &lt;li&gt;App developers, despite being familiar with the business scope of their data, interpreted internal data classification policies and external data regulations differently, leading to inconsistencies in understanding.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A service called Gemini &lt;em&gt;(named before Google announced the Gemini model!)&lt;/em&gt; was built internally to automate the tag generation process using a third party data classification service. Its purpose was to scan the data entities in batches and generate column/field level tags. These tags would then go through a review process by the data producers. The data governance team provided classification rules and used regex classifiers, alongside the third-party tool’s own machine learning classifiers, to discover sensitive information.&lt;/p&gt;

&lt;p&gt;After the implementation of the initial version of Gemini, a few challenges remained.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The third-party tool did not allow customisations of its machine learning classifiers, and the regex patterns produced too many false positives during our evaluation.&lt;/li&gt;
  &lt;li&gt;Building in-house classifiers would require a dedicated data science team to train a customised model. They would need to invest a significant amount of time to understand data governance rules thoroughly and prepare datasets with manually labelled training data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;LLM came up on our radar following its recent &lt;em&gt;“iPhone moment”&lt;/em&gt; with ChatGPT’s explosion onto the scene. It is trained using an extremely large corpus of text and contains trillions of parameters. It is capable of conducting natural language understanding tasks, writing code, and even analysing data based on requirements. The LLM naturally solves the mentioned pain points as it provides a natural language interface for data governance personnel. They can express governance requirements through text prompts, and the LLM can be customised effortlessly without code or model training.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;In this section, we dive into the implementation details of the data classification workflow. Please refer to the diagram below for a high-level overview:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/data_classification_workflow.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Overview of data classification workflow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This diagram illustrates how data platforms, the metadata generation service (Gemini), and data owners work together to classify and verify metadata. Data platforms trigger scan requests to the Gemini service to initiate the tag classification process. After the tags are predicted, data platforms consume the predictions, and the data owners are notified to verify these tags.&lt;/p&gt;

&lt;h3 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/arch_diagram_orchestration.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Architecture diagram of the orchestration service Gemini&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Our orchestration service, Gemini, manages the data classification requests from data platforms. From the diagram, the architecture contains the following components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data platforms: These platforms are responsible for managing data entities and initiating data classification requests.&lt;/li&gt;
  &lt;li&gt;Gemini: This orchestration service communicates with data platforms, schedules and groups data classification requests.&lt;/li&gt;
  &lt;li&gt;Classification engines: There are two available engines (a third-party classification service and GPT3.5) for executing the classification jobs and return results. Since we are still in the process of evaluating two engines, both of the engines are working concurrently.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the orchestration service receives requests, it helps aggregate the requests into reasonable mini-batches. Aggregation is achievable through the message queue at fixed intervals. In addition, a rate limiter is attached at the workflow level. It allows the service to call the Cloud Provider APIs with respective rates to prevent the potential throttling from the service providers.&lt;/p&gt;

&lt;p&gt;Specific to LLM orchestration, there are two limits to be mindful of. The first one is the context length. The input length cannot surpass the context length, which was 4000 tokens for GPT3.5 at the time of development (or around 3000 words). The second one is the overall token limit (since both the input and output share the same token limit for a single request). Currently, all Azure OpenAI model deployments share the same quota under one account, which is set at 240K tokens per minute.&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;In this section, we focus on LLM-powered column-level tag classification. The tag classification process is defined as follows:&lt;/p&gt;

&lt;p&gt;Given a data entity with a defined schema, we want to tag each field of the schema with metadata classifications that follow an internal classification scheme from the data governance team. For example, the field can be tagged as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular kind of business metric&amp;gt;&lt;/code&gt; or a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;particular type of personally identifiable information (PII)&lt;/code&gt;. These tags indicate that the field contains a business metric or PII.&lt;/p&gt;

&lt;p&gt;We ask the language model to be a column tag generator and to assign the most appropriate tag to each column. Here we showcase an excerpt of the prompt we use:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;You are a database column tag classifier, your job is to assign the most appropriate tag based on table name and column name. The database columns are from a company that provides ride-hailing, delivery, and financial services. Assign one tag per column. However not all columns can be tagged and these columns should be assigned &amp;lt;None&amp;gt;. You are precise, careful and do your best to make sure the tag assigned is the most appropriate.

The following is the list of tags to be assigned to a column. For each line, left hand side of the : is the tag and right hand side is the tag definition

…
&amp;lt;Personal.ID&amp;gt; : refers to government-provided identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar. This tag should absolutely not be assigned to columns named &quot;id&quot;, &quot;merchant id&quot;, &quot;passenger id&quot;, “driver id&quot; or similar since these are not government-provided identification numbers. This tag should be very rarely assigned.

&amp;lt;None&amp;gt; : should be used when none of the above can be assigned to a column.
…

Output Format is a valid json string, for example:

[{
        &quot;column_name&quot;: &quot;&quot;,
        &quot;assigned_tag&quot;: &quot;&quot;
}]

Example question

`These columns belong to the &quot;deliveries&quot; table

        1. merchant_id
        2. status
        3. delivery_time`

Example response

[{
        &quot;column_name&quot;: &quot;merchant_id&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;Personal.ID&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;status&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
},{
        &quot;column_name&quot;: &quot;delivery_time&quot;,
        &quot;assigned_tag&quot;: &quot;&amp;lt;None&amp;gt;&quot;
}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also curated a tag library for LLM to classify. Here is an example:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column-level Tag&lt;/th&gt;
      &lt;th&gt;Definition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.ID&lt;/td&gt;
      &lt;td&gt;Refers to external identification numbers that can be used to uniquely identify a person and should be assigned to columns containing &quot;NRIC&quot;, &quot;Passport&quot;, &quot;FIN&quot;, &quot;License Plate&quot;, &quot;Social Security&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Name &lt;/td&gt;
      &lt;td&gt;Refers to the name or username of a person and should be assigned to columns containing &quot;name&quot;, &quot;username&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Personal.Contact_Info&lt;/td&gt;
      &lt;td&gt;Refers to the contact information of a person and should be assigned to columns containing &quot;email&quot;, &quot;phone&quot;, &quot;address&quot;, &quot;social media&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Geo.Geohash&lt;/td&gt;
      &lt;td&gt;Refers to a geohash and should be assigned to columns containing &quot;geohash&quot; or similar.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;Should be used when none of the above can be assigned to a column.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The output of the language model is typically in free text format, however, we want the output in a fixed format for downstream processing. Due to this nature, prompt engineering is a crucial component to make sure downstream workflows can process the LLM’s output.&lt;/p&gt;

&lt;p&gt;Here are some of the techniques we found useful during our development:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Articulate the requirements: The requirement of the task should be as clear as possible, LLM is only instructed to do what you ask it to do.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/en-gb/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#few-shot-learning&quot;&gt;Few-shot learning&lt;/a&gt;: By showing the example of interaction, models understand how they should respond.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/TypeChat&quot;&gt;Schema Enforcement&lt;/a&gt;: Leveraging its ability of understanding code, we explicitly provide the DTO (Data Transfer Object) schema to the model so that it understands that its output must conform to it.&lt;/li&gt;
  &lt;li&gt;Allow for confusion: In our prompt we specifically added a default tag – the LLM is instructed to output the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;None&amp;gt;&lt;/code&gt; tag when it cannot make a decision or is confused.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Regarding classification accuracy, we found that it is surprisingly accurate with its great semantic understanding. For acknowledged tables, users on average change less than one tag. Also, during an internal survey done among data owners at Grab in September 2023, 80% reported that this new tagging process helped them in tagging their data entities.&lt;/p&gt;

&lt;h3 id=&quot;publish-and-verification&quot;&gt;Publish and verification&lt;/h3&gt;

&lt;p&gt;The predictions are published to the Kafka queue to downstream data platforms. The platforms inform respective users weekly to verify the classified tags to improve the model’s correctness and to enable iterative prompt improvement. Meanwhile, we plan to remove the verification mandate for users once the accuracy reaches a certain level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/llm-powered-data-classification/verification_message.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Verification message shown in the data platform for user to verify the tags&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since the new system was rolled out, we have successfully integrated this with Grab’s metadata management platform and production database management platform. Within a month since its rollout, we have scanned more than 20,000 data entities, averaging around 300-400 entities per day.&lt;/p&gt;

&lt;p&gt;Using a quick back-of-the-envelope calculation, we can see the significant time savings achieved through automated tagging. Assuming it takes a data owner approximately 2 minutes to classify each entity, we are saving approximately 360 man-days per year for the company. This allows our engineers and analysts to focus more on their core tasks of engineering and analysis rather than spending excessive time on data governance.&lt;/p&gt;

&lt;p&gt;The classified tags pave the way for more use cases downstream. These tags, in combination with rules provided by data privacy office in Grab, enable us to determine the sensitivity tier of data entities, which in turn will be leveraged for enforcing the Attribute-based Access Control (ABAC) policies and enforcing Dynamic Data Masking for downstream queries. To learn more about the benefits of ABAC, readers can refer to another engineering &lt;a href=&quot;https://engineering.grab.com/migrating-to-abac&quot;&gt;blog&lt;/a&gt; posted earlier.&lt;/p&gt;

&lt;p&gt;Cost wise, for the current load, it is extremely affordable contrary to common intuition. This affordability enables us to scale the solution to cover more data entities in the company.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;h3 id=&quot;prompt-improvement&quot;&gt;Prompt improvement&lt;/h3&gt;

&lt;p&gt;We are currently exploring feeding sample data and user feedback to greatly increase accuracy. Meanwhile, we are experimenting on outputting the confidence level from LLM for its own classification. With confidence level output, we would only trouble users when the LLM is uncertain of its answers. Hopefully this can remove even more manual processes in the current workflow.&lt;/p&gt;

&lt;h3 id=&quot;prompt-evaluation&quot;&gt;Prompt evaluation&lt;/h3&gt;

&lt;p&gt;To track the performance of the prompt given, we are building analytical pipelines to calculate the metrics of each version of the prompt. This will help the team better quantify the effectiveness of prompts and iterate better and faster.&lt;/p&gt;

&lt;h3 id=&quot;scaling-out&quot;&gt;Scaling out&lt;/h3&gt;

&lt;p&gt;We are also planning to scale out this solution to more data platforms to streamline governance-related metadata generation to more teams. The development of downstream applications using our metadata is also on the way. These exciting applications are from various domains such as security, data discovery, etc.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Grab recently participated in the Singapore government’s regulatory &lt;a href=&quot;https://www.imda.gov.sg/how-we-can-help/data-innovation/privacy-enhancing-technology-sandboxes&quot;&gt;sandbox&lt;/a&gt;, where we successfully demonstrated how LLMs can efficiently and effectively perform data classification, allowing Grab to compound the value of its data for innovative use cases while safeguarding sensitive information such as PII.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Jul 2024 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/llm-powered-data-classification</link>
        <guid isPermaLink="true">https://engineering.grab.com/llm-powered-data-classification</guid>
        
        <category>Data</category>
        
        <category>Machine Learning</category>
        
        <category>Generative AI</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
  </channel>
</rss>
