<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 30 Aug 2023 01:57:03 +0000</pubDate>
    <lastBuildDate>Wed, 30 Aug 2023 01:57:03 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Building hyperlocal GrabMaps</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Southeast Asia (SEA) is a dynamic market, very different from other parts of the world. When travelling on the road, you may experience fast-changing road restrictions, new roads appearing overnight, and high traffic congestion. To address these challenges, GrabMaps has adapted to the SEA market by leveraging big data solutions. One of the solutions is the integration of hyperlocal data in GrabMaps.&lt;/p&gt;

&lt;p&gt;Hyperlocal information is oriented around very small geographical communities and obtained from the local knowledge that our map team gathers. The map team is spread across SEA, enabling us to define clear specifications (e.g. legal speed limits), and validate that our solutions are viable.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/map-detection.gif&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Map showing detections from images and probe data, and hyperlocal data.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Hyperlocal inputs make our mapping data even more robust, adding to the details collected from our image and probe detection pipelines. Figure 1 shows how data from our detection pipeline is overlaid with hyperlocal data, and then mapped across the SEA region. If you are curious and would like to check out the data yourself, you can download it &lt;a href=&quot;https://dumps.improveosm.org/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;processing-hyperlocal-data&quot;&gt;Processing hyperlocal data&lt;/h2&gt;

&lt;p&gt;Now let’s go through the process of detecting hyperlocal data.&lt;/p&gt;

&lt;h3 id=&quot;download-data&quot;&gt;Download data&lt;/h3&gt;

&lt;p&gt;GrabMaps is based on &lt;a href=&quot;https://www.openstreetmap.org/&quot;&gt;OpenStreetMap&lt;/a&gt; (OSM). The first step in the process is to download the .pbf file for Asia from &lt;a href=&quot;https://www.geofabrik.de/&quot;&gt;geofabrick.de&lt;/a&gt;. This .pbf file contains all the data that is available on OSM, such as details of places, trees, and roads. Take for example a park, the .pbf file would contain data on the park name, wheelchair accessibility, and many more.&lt;/p&gt;

&lt;p&gt;For this article, we will focus on hyperlocal data related to the road network. For each road, you can obtain data such as the type of road (residential or motorway), direction of traffic (one-way or more), and road name.&lt;/p&gt;

&lt;h3 id=&quot;convert-data&quot;&gt;Convert data&lt;/h3&gt;
&lt;p&gt;To take advantage of big data computing, the next step in the process is to convert the .pbf file into Parquet format using a Parquetizer. This will convert the binary data in the .pbf file into a table format. Each road in SEA is now displayed as a row in a table as shown in Figure 2.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/data-parquet.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Road data in Parquet format.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;identify-hyperlocal-data&quot;&gt;Identify hyperlocal data&lt;/h3&gt;

&lt;p&gt;After the data is prepared, GrabMaps then identifies and inputs all of our hyperlocal data, and delivers a consolidated view to our downstream services. Our hyperlocal data is obtained from various sources, either by looking at geometry, or other attributes in OSM such as the direction of travel and speed limit. We also apply customised rules defined by our local map team, all in a fully automated manner. This enhances the map together with data obtained from our rides and deliveries GPS pings and from KartaView, Grab’s product for imagery collection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/architecture.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Architecture diagram showing how hyperlocal data is integrated into GrabMaps.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;benefit-of-our-hyperlocal-grabmaps&quot;&gt;Benefit of our hyperlocal GrabMaps&lt;/h2&gt;

&lt;p&gt;GrabNav, a turn-by-turn navigation tool available on the Grab driver app, is one of our products that benefits from having hyperlocal data. Here are some hyperlocal data that are made available through our approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Localisation of roads: The country, state/county, or city the road is in&lt;/li&gt;
  &lt;li&gt;Language spoken, driving side, and speed limit&lt;/li&gt;
  &lt;li&gt;Region-specific default speed regulations&lt;/li&gt;
  &lt;li&gt;Consistent name usage using language inference&lt;/li&gt;
  &lt;li&gt;Complex attributes like intersection links&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To further explain the benefits of this hyperlocal feature, we will use intersection links as an example. In the next section, we will explain how intersection links data is used and how it impacts our driver-partners and passengers.&lt;/p&gt;

&lt;h3 id=&quot;identifying-hyperlocal-data---intersection-links&quot;&gt;Identifying hyperlocal data - intersection links&lt;/h3&gt;
&lt;p&gt;An intersection link is when two or more roads meet. Figure 4 and 5 illustrates what an intersection link looks like in a GrabMaps mock and in OSM.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/intersection-link-mock.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Mock of an intersection link. 
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/intersection-link-illustration.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Intersection link illustration from a real road network in OSM.  
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To locate intersection links in a road network, there are computations involved. We would first combine big data processing (which we do using Spark) with graphs. We use geohash as the unit of processing, and for each geohash, a bi-directional graph is created.&lt;/p&gt;

&lt;p&gt;From such resulting graphs, we can determine intersection links if:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Road segments are parallel&lt;/li&gt;
  &lt;li&gt;The roads have the same name&lt;/li&gt;
  &lt;li&gt;The roads are one way roads&lt;/li&gt;
  &lt;li&gt;Angles and the shape of the road are in the intervals or requirements we seek&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each intersection link we identify is tagged in the map as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intersection_links&lt;/code&gt;. Our downstream service teams can then identify them by searching for the tag.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;The impact we create with our intersection link can be explained through the following example.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/impact1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Longer route, without GrabMaps intersection link feature. The arrow indicates where the route should have suggested a U-turn.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/building-hyperlocal-grabmaps/impact2.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Shorter route using GrabMaps by taking a closer link between two main roads.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 6 and Figure 7 show two different routes for the same origin and destination. However, you can see that Figure 7 has a shorter route and this is made available by taking an intersection link early on in the route. The highlighted road segment in Figure 7 is an intersection link, tagged by the process we described earlier. The route is now much shorter making GrabNav more efficient in its route suggestion.&lt;/p&gt;

&lt;p&gt;There are numerous factors that can impact a driver-partner’s trip, and intersection links are just one example. There are many more features that GrabMaps offers across Grab’s services that allow us to “outserve” our partners.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;GrabMaps and GrabNav deliver enriched experiences to our driver-partners. By integrating certain hyperlocal data features, we are also able to provide more accurate pricing for both our driver-partners and passengers. In our mission towards sustainable growth, this is an area that we will keep on improving by leveraging scalable tech solutions.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Aug 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/building-hyperlocal-grabmaps</link>
        <guid isPermaLink="true">https://engineering.grab.com/building-hyperlocal-grabmaps</guid>
        
        <category>Maps</category>
        
        <category>Data</category>
        
        <category>Big Data</category>
        
        <category>Data processing</category>
        
        <category>hyperlocalisation</category>
        
        <category>GrabMaps</category>
        
        <category>navigation</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Streamlining Grab's Segmentation Platform with faster creation and lower latency</title>
        <description>&lt;p&gt;Launched in 2019, Segmentation Platform has been Grab’s one-stop platform for user segmentation and audience creation across all business verticals. User segmentation is the process of dividing passengers, driver-partners, or merchant-partners (users) into sub-groups (segments) based on certain attributes. Segmentation Platform empowers Grab’s teams to create segments using attributes available within our data ecosystem and provides APIs for downstream teams to retrieve them.&lt;/p&gt;

&lt;p&gt;Checking whether a user belongs to a segment (Membership Check) influences many critical flows on the Grab app:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;When a passenger launches the Grab app, our in-house experimentation platform will tailor the app experience based on the segments the passenger belongs to.&lt;/li&gt;
  &lt;li&gt;When a driver-partner goes online on the Grab app, the Drivers service calls Segmentation Platform to ensure that the driver-partner is not blacklisted.&lt;/li&gt;
  &lt;li&gt;When launching marketing campaigns, Grab’s communications platform relies on Segmentation Platform to determine which passengers, driver-partners, or merchant-partners to send communication to.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This article peeks into the current design of Segmentation Platform and how the team optimised the way segments are stored to reduce read latency thus unlocking new segmentation use cases.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;Segmentation Platform comprises two major subsystems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Segment creation&lt;/li&gt;
  &lt;li&gt;Segment serving&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image1.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. Segmentation Platform architecture
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;segment-creation&quot;&gt;Segment creation&lt;/h3&gt;

&lt;p&gt;Segment creation is powered by Spark jobs. When a Grab team creates a segment, a Spark job is started to retrieve data from our data lake. After the data is retrieved, cleaned, and validated, the Spark job calls the serving sub-system to populate the segment with users.&lt;/p&gt;

&lt;h3 id=&quot;segment-serving&quot;&gt;Segment serving&lt;/h3&gt;

&lt;p&gt;Segment serving is powered by a set of Go services. For persistence and serving, we use ScyllaDB as our primary storage layer. We chose to use ScyllaDB as our NoSQL store due to its ability to scale horizontally and meet our &amp;lt;80ms p99 SLA. Users in a segment are stored as rows indexed by the user ID. The table is partitioned by the user ID ensuring that segment data is evenly distributed across the ScyllaDB clusters.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;User ID&lt;/th&gt;
    &lt;th&gt;Segment Name&lt;/th&gt;
    &lt;th&gt;Other metadata columns&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td&gt;1221&lt;/td&gt;
    &lt;td&gt;Segment_A&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;3421&lt;/td&gt;
    &lt;td&gt;Segment_A&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;5632&lt;/td&gt;
    &lt;td&gt;Segment_B&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;7889&lt;/td&gt;
    &lt;td&gt;Segment_B&lt;/td&gt;
    &lt;td&gt;…&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With this design, Segmentation Platform handles up to 12K read and 36K write QPS, with a p99 latency of 40ms.&lt;/p&gt;

&lt;h2 id=&quot;problems&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;The existing system has supported Grab, empowering internal teams to create rich and personalised experiences. However, with the increased adoption and use, certain challenges began to emerge:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As more and larger segments are being created, the write QPS became a bottleneck leading to longer wait times for segment creation.&lt;/li&gt;
  &lt;li&gt;Grab services requested even lower latency for membership checks.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;long-segment-creation-times&quot;&gt;Long segment creation times&lt;/h3&gt;

&lt;p&gt;As more segments were created by different teams within Grab, the write QPS was no longer able to keep up with the teams’ demands. Teams would have to wait for hours for their segments to be created, reducing their operational velocity.&lt;/p&gt;

&lt;h3 id=&quot;read-latency&quot;&gt;Read latency&lt;/h3&gt;

&lt;p&gt;Further, while the platform already offers sub-40ms p99 latency for reads, this was still too slow for certain services and their use cases. For example, Grab’s communications platform needed to check whether a user belongs to a set of segments before sending out communication and incurring increased latency for every communication request was not acceptable. Another use case was for Experimentation Platform, where checks must have low latency to not impact the user experience. 
Thus, the team explored alternative ways of storing the segment data with the goals of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reducing segment creation time&lt;/li&gt;
  &lt;li&gt;Reducing segment read latency&lt;/li&gt;
  &lt;li&gt;Maintaining or reducing cost&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;h3 id=&quot;segments-as-bitmaps&quot;&gt;Segments as bitmaps&lt;/h3&gt;

&lt;p&gt;One of the main engineering challenges was scaling the write throughput of the system to keep pace with the number of segments being created. As a segment is stored across multiple rows in ScyllaDB, creating a large segment incurs a huge number of writes to the database. What we needed was a better way to store a large set of user IDs. Since user IDs are represented as integers in our system, a natural solution to storing a set of integers was a bitmap.&lt;/p&gt;

&lt;p&gt;For example, a segment containing the following user IDs: 1, 6, 25, 26, 89 could be represented with a bitmap as follows:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. Bitmap representation of a segment
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To perform a membership check, a bitwise operation can be used to check if the bit at the user ID’s index is 0 or 1. As a bitmap, the segment can also be stored as a single Blob in object storage instead of inside ScyllaDB.&lt;/p&gt;

&lt;p&gt;However, as the number of user IDs in the system is large, a small and sparse segment would lead to prohibitively large bitmaps. For example, if a segment contains 2 user IDs 100 and 200,000,000, it will require a bitmap containing 200 million bits (25MB) where all but 2 of the bits are just 0. Thus, the team needed an encoding to handle sparse segments more efficiently.&lt;/p&gt;

&lt;h4 id=&quot;roaring-bitmaps&quot;&gt;Roaring Bitmaps&lt;/h4&gt;

&lt;p&gt;After some research, we landed on Roaring Bitmaps, which are compressed uint32 bitmaps. With roaring bitmaps, we are able to store a segment with 1 million members in a Blob smaller than 1 megabyte, compared to 4 megabytes required by a naive encoding.&lt;/p&gt;

&lt;p&gt;Roaring Bitmaps achieve good compression ratios by splitting the set into fixed-size (216) integer chunks and using three different data structures (containers) based on the data distribution within the chunk. The most significant 16 bits of the integer are used as the index of the chunk, and the least significant 16 bits are stored in the containers.&lt;/p&gt;

&lt;h5 id=&quot;array-containers&quot;&gt;Array containers&lt;/h5&gt;

&lt;p&gt;Array containers are used when data is sparse (&amp;lt;= 4096 values). An array container is a sorted array of 16-bit integers. It is memory-efficient for sparse data and provides logarithmic-time access.&lt;/p&gt;

&lt;h5 id=&quot;bitmap-containers&quot;&gt;Bitmap containers&lt;/h5&gt;

&lt;p&gt;Bitmap containers are used when data is dense. A bitmap container is a 216 bit container where each bit represents the presence or absence of a value. It is memory-efficient for dense data and provides constant-time access.&lt;/p&gt;

&lt;h5 id=&quot;run-containers&quot;&gt;Run containers&lt;/h5&gt;

&lt;p&gt;Finally, run containers are used when a chunk has long consecutive values. Run containers use run-length encoding (RLE) to reduce the storage required for dense bitmaps. Run containers store a pair of values representing the start and the length of the run. It provides good memory efficiency and fast lookups.&lt;/p&gt;

&lt;p&gt;The diagram below shows how a dense bitmap container that would have required 91 bits can be compressed into a run container by storing only the start (0) and the length (90). It should be noted that run containers are used only if it reduces the number of bytes required compared to a bitmap.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. A dense bitmap container compressed into a run container
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;By using different containers, Roaring Bitmaps are able to achieve good compression across various data distributions, while maintaining excellent lookup performance. Additionally, as segments are represented as Roaring Bitmaps, service teams are able to perform set operations (union, interaction, and difference, etc) on the segments on the fly, which previously required re-materialising the combined segment into the database.&lt;/p&gt;

&lt;h3 id=&quot;caching-with-an-sdk&quot;&gt;Caching with an SDK&lt;/h3&gt;

&lt;p&gt;Even though the segments are now compressed, retrieving a segment from the Blob store for each membership check would incur an unacceptable latency penalty. To mitigate the overhead of retrieving a segment, we developed an SDK that handles the retrieval and caching of segments.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/streamlining-grabs-segmentation-platform/image2.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. How the SDK caches segments
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The SDK takes care of the retrieval, decoding, caching, and watching of segments. Users of the  SDK are only required to specify the maximum size of the cache to prevent exhausting the service’s memory. The SDK provides a cache with a least-recently-used eviction policy to ensure that hot segments are kept in the cache. They are also able to watch for updates on a segment and the SDK will automatically refresh the cached segment when it is updated.&lt;/p&gt;

&lt;h2 id=&quot;hero-teams&quot;&gt;Hero teams&lt;/h2&gt;

&lt;h3 id=&quot;communications-platform&quot;&gt;Communications Platform&lt;/h3&gt;

&lt;p&gt;Communications Platform has adopted the SDK to implement a new feature to control the communication frequency based on which segments a user belongs to. Using the SDK, the team is able to perform membership checks on multiple multi-million member segments, achieving peak QPS 15K/s with a p99 latency of &amp;lt;1ms. With the new feature, they have been able to increase communication engagement and reduce the communication unsubscribe rate.&lt;/p&gt;

&lt;h3 id=&quot;experimentation-platform&quot;&gt;Experimentation Platform&lt;/h3&gt;

&lt;p&gt;Experimentation Platform powers experimentation across all Grab services. Segments are used heavily in experiments to determine a user’s experience. Prior to using the SDK, Experimentation Platform limited the maximum size of the segments that could be used to prevent exhausting a service’s memory.&lt;/p&gt;

&lt;p&gt;After migrating to the new SDK, they were able to lift this restriction due to the compression efficiency of Roaring Bitmaps. Users are now able to use any segments as part of their experiment without worrying that it would require too much memory.&lt;/p&gt;

&lt;h2 id=&quot;closing&quot;&gt;Closing&lt;/h2&gt;

&lt;p&gt;This blog post discussed the challenges that Segmentation Platform faced when scaling and how the team explored alternative storage and encoding techniques to improve segment creation time, while also achieving low latency reads. The SDK allows our teams to easily make use of segments without having to handle the details of caching, eviction, and updating of segments.&lt;/p&gt;

&lt;p&gt;Moving forward, there are still existing use cases that are not able to use the Roaring Bitmap segments and thus continue to rely on segments from ScyllaDB. Therefore, the team is also taking steps to optimise and improve the scalability of our service and database.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Axel, the wider Segmentation Platform team, and Data Technology team for reviewing the post. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Aug 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/streamlining-grabs-segmentation-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/streamlining-grabs-segmentation-platform</guid>
        
        <category>Back End</category>
        
        <category>Performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Unsupervised graph anomaly detection - Catching new fraudulent behaviours</title>
        <description>&lt;p&gt;Earlier in this series, we covered the &lt;a href=&quot;/graph-networks&quot;&gt;importance of graph networks&lt;/a&gt;, &lt;a href=&quot;/graph-concepts&quot;&gt;graph concepts&lt;/a&gt;, &lt;a href=&quot;/graph-visualisation&quot;&gt;graph visualisation&lt;/a&gt;, and &lt;a href=&quot;/graph-for-fraud-detection&quot;&gt;graph-based fraud detection methods&lt;/a&gt;. In this article, we will discuss how to automatically detect new types of fraudulent behaviour and swiftly take action on them.&lt;/p&gt;

&lt;p&gt;One of the challenges in fraud detection is that fraudsters are incentivised to always adversarially innovate their way of conducting frauds, i.e., their modus operandi (MO in short). Machine learning models trained using historical data may not be able to pick up new MOs, as they are new patterns that are not available in existing training data. To enhance Grab’s existing security defences and protect our users from these new MOs, we needed a machine learning model that is able to detect them quickly without the need for any label supervision, i.e., an unsupervised learning model rather than the regular supervised learning model.&lt;/p&gt;

&lt;p&gt;To address this, we developed an in-house machine learning model for detecting anomalous patterns in graphs, which has led to the discovery of new fraud MOs. Our focus was initially on GrabFood and GrabMart verticals, where we monitored the interactions between consumers and merchants. We modelled these interactions as a bipartite graph (a type of graph for modelling interactions between two groups) and then performed anomaly detection on the graph. Our in-house anomaly detection model was also presented at the International Joint Conference on Neural Networks (IJCNN) 2023, a premier academic conference in the area of neural networks, machine learning, and artificial intelligence.&lt;/p&gt;

&lt;p&gt;In this blog, we discuss the model and its application within Grab. For avid audiences that want to read the details of our model, you can access our paper below. Note that even though we implemented our model for anomaly detection in GrabFood and GrabMart, the model is designed for general purposes and is applicable to interaction graphs between any two groups.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;/img/graph-anomaly-model/image1.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;/files/Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs.pdf&quot; download=&quot;&quot;&gt;Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs&lt;/a&gt;&lt;br /&gt;By Rizal Fathony, Jenn Ng, Jia Chen&lt;br /&gt;Presented at the International Joint Conference on Neural Networks (IJCNN) 2023&lt;br /&gt;DOI: &lt;a href=&quot;https://ieeexplore.ieee.org/document/10191331&quot; target=&quot;_blank&quot;&gt;10.1109/IJCNN54540.2023.10191331&lt;/a&gt; (&lt;a href=&quot;#citation-info&quot;&gt;citation&lt;/a&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
Before we dive into how our model works, it is important to understand the process of graph construction in our application as the model assumes the availability of the graphs in a standardised format.&lt;/p&gt;

&lt;h2 id=&quot;graph-construction&quot;&gt;Graph construction &lt;/h2&gt;

&lt;p&gt;We modelled the interactions between consumers and merchants in GrabFood and GrabMart platforms as bipartite graphs (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;G&lt;/code&gt;), where the first group of nodes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U&lt;/code&gt;) represents the consumers, the second group of nodes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;) represents the merchants, and the edges (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;E&lt;/code&gt;) connecting them means that the consumers have placed some food/mart orders to the merchants. The graph is also supplied with rich transactional information about the consumers and the merchants in the form of node features (&lt;code&gt;X&lt;sub&gt;u&lt;/sub&gt;&lt;/code&gt; and &lt;code&gt;X&lt;sub&gt;v&lt;/sub&gt;&lt;/code&gt;), as well as order information in the form of edge features (&lt;code&gt;X&lt;sub&gt;e&lt;/sub&gt;&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 1. Graph construction process&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The goal of our anomaly model is to detect anomalous and suspicious behaviours from the consumers or merchants (node-level anomaly detection), as well as anomalous order interactions (edge-level anomaly detection). As mentioned, this detection needs to be done without any label supervision.&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model architecture&lt;/h2&gt;

&lt;p&gt;We designed our graph anomaly model as a type of autoencoder, with an encoder and two decoders – a feature decoder and a structure decoder. The key feature of our model is that it accepts a bipartite graph with both node and edge attributes as the input. This is important as both node and edge attributes encode essential information for determining if certain behaviours are suspicious. Many previous works on graph anomaly detection only support node attributes. In addition, our model can produce both node and edge level anomaly scores, unlike most of the previous works that produce node-level scores only. We named our model GraphBEAN, which is short for &lt;strong&gt;B&lt;/strong&gt;ipartite Node-and-&lt;strong&gt;E&lt;/strong&gt;dge-&lt;strong&gt;A&lt;/strong&gt;ttributed &lt;strong&gt;N&lt;/strong&gt;etworks.&lt;/p&gt;

&lt;p&gt;From the input, the encoder then processes the attributed bipartite graph into a series of graph convolution layers to produce latent representations for both node groups. Our graph convolution layers produce new representations for each node in both node groups (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;), as well as for each edge in the graph. Note that the last convolution layer in the encoder only produces the latent representations for nodes, without producing edge representations. The reason for this design is that we only put the latent representations for the active actors, the nodes representing consumers and merchants, but not their interactions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 2. GraphBEAN architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;From the nodes’ latent representations, the feature decoder is tasked to reconstruct the original graph with both node and edge attributes via a series of graph convolution layers. As the graph structure is provided by the feature decoder, we task the structure decoder to learn the graph structure by predicting if there exists an edge connecting two nodes. This edge prediction, as well as the graph reconstructed by the feature decoder, are then compared to the original input graph via a reconstruction loss function.&lt;/p&gt;

&lt;p&gt;The model is then trained using the bipartite graph constructed from GrabFood and GrabMart transactions. We use a reconstruction-based loss function as the training objective of the model. After the training is completed, we compute the anomaly score of each node and edge in the graph using the trained model.&lt;/p&gt;

&lt;h2 id=&quot;anomaly-score-computation&quot;&gt;Anomaly score computation&lt;/h2&gt;
&lt;p&gt;Our anomaly scores are reconstruction-based. The score design assumes that normal behaviours are common in the dataset and thus, can be easily reconstructed by the model. On the other hand, anomalous behaviours are rare. Therefore the model will have a hard time reconstructing them, hence producing high errors.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 3. Edge-level and node-level anomaly scores computation
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The model produces two types of anomaly scores. First, the edge-level anomaly scores, which are calculated from the edge reconstruction error. Second, the node-level anomaly scores, which are calculated from node reconstruction error plus an aggregate over the edge scores from the edges connected to the node. This aggregate could be a mean or max aggregate.&lt;/p&gt;

&lt;h2 id=&quot;actioning-system&quot;&gt;Actioning system&lt;/h2&gt;

&lt;p&gt;In our implementation of GraphBEAN within Grab, we designed a full pipeline of anomaly detection and actioning systems. It is a fully-automated system for constructing a bipartite graph from GrabFood and GrabMart transactions, training a GraphBEAN model using the graph, and computing anomaly scores. After computing anomaly scores for all consumers and merchants (node-level), as well as all of their interactions (edge-level), it automatically passes the scores to our actioning system. But before that, it also passes them through a system we call &lt;em&gt;fraud type tagger&lt;/em&gt;. This is also a fully-automated heuristic-based system that tags some of the detected anomalies with some fraud tags. The purpose of this tagging is to provide some context in general, like the types of detected anomalies. Some examples of these tags are &lt;em&gt;promo abuse&lt;/em&gt; or &lt;em&gt;possible collusion&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-anomaly-model/image3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig 4. Pipeline in our actioning system&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Both the anomaly scores and the fraud type tags are then forwarded to our actioning system. The system consists of two subsystems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Human expert actioning system&lt;/strong&gt;: Our fraud experts analyse the detected anomalies and perform certain actioning on them, like suspending certain transaction features from suspicious merchants.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automatic actioning system&lt;/strong&gt;: Combines the anomaly scores and fraud type tags with other external signals to automatically do actioning on the detected anomalies, like preventing promos from being used by fraudsters or preventing fraudulent transactions from occurring. These actions vary depending on the type of fraud and the scores.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;The GraphBEAN model enables the detection of suspicious behaviour on graph data without the need for label supervision. By implementing the model on GrabFood and GrabMart platforms, we learnt that having such a system enables us to quickly identify new types of fraudulent behaviours and then swiftly perform action on them. This also allows us to enhance Grab’s defence against fraudulent activity and actively protect our users.&lt;/p&gt;

&lt;p&gt;We are currently working on extending the model into more generic heterogeneous (multi-entity) graphs. In addition, we are also working on implementing it to more use cases within Grab.&lt;/p&gt;

&lt;h2 id=&quot;citation-info&quot;&gt;Citation information&lt;/h2&gt;
&lt;p&gt;(*) If you use the paper for academic purposes, please cite the following publication:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R. Fathony, J. Ng and J. Chen, “Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs,” 2023 International Joint Conference on Neural Networks (IJCNN), Gold Coast, Australia, 2023, pp. 1-10, doi: 10.1109/IJCNN54540.2023.10191331.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;IEEE copyright notice:&lt;/p&gt;

&lt;p&gt;© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Aug 2023 01:23:05 +0000</pubDate>
        <link>https://engineering.grab.com/graph-anomaly-model</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-anomaly-model</guid>
        
        <category>Data science</category>
        
        <category>Graph networks</category>
        
        <category>Graphs</category>
        
        <category>Graph visualisation</category>
        
        <category>Security</category>
        
        <category>Fraud detection</category>
        
        <category>Anomaly detection</category>
        
        <category>Machine learning</category>
        
        
        <category>Data Science</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Zero traffic cost for Kafka consumers</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Coban, Grab’s real-time data streaming platform team, has been building an ecosystem around &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt;, serving all Grab verticals. Along with stability and performance, one of our priorities is also cost efficiency.&lt;/p&gt;

&lt;p&gt;In this article, we explain how the Coban team has substantially reduced Grab’s annual cost for data streaming by enabling Kafka consumers to fetch from the closest replica.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;The Grab platform is primarily hosted on AWS cloud, located in one region, spanning over three Availability Zones (AZs). When it comes to data streaming, both the Kafka brokers and Kafka clients run across these three AZs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Initial design, consumers fetching from the partition leader&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 1 shows the initial design of our data streaming platform. To ensure high availability and resilience, we configured each Kafka partition to have three replicas. We have also set up our Kafka clusters to be rack-aware (i.e. 1 “rack” = 1 AZ) so that all three replicas reside in three different AZs.&lt;/p&gt;

&lt;p&gt;The problem with this design is that it generates staggering cross-AZ network traffic. This is because, by default, Kafka clients communicate only with the partition leader, which has a 67% probability of residing in a different AZ.&lt;/p&gt;

&lt;p&gt;This is a concern as we are charged for cross-AZ traffic as per &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer_within_the_same_AWS_Region&quot;&gt;AWS’s network traffic pricing model&lt;/a&gt;. With this design, our cross-AZ traffic amounted to half of the total cost of our Kafka platform.&lt;/p&gt;

&lt;p&gt;The Kafka cross-AZ traffic for this design can be broken down into three components as shown in Figure 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Producing&lt;/strong&gt; (step 1): Typically, a single service produces data to a given Kafka topic. Cross-AZ traffic occurs when the producer does not reside in the same AZ as the partition leader it is producing data to. This cross-AZ traffic cost is minimal, because the data is transferred to a different AZ at most once (excluding retries).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Replicating&lt;/strong&gt; (step 2): The ingested data is replicated from the partition leader to the two partition followers, which reside in two other AZs. The cost of this is also relatively small, because the data is only transferred to a different AZ twice.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consuming&lt;/strong&gt; (step 3): Most of the cross-AZ traffic occurs here because there are many consumers for a single Kafka topic. Similar to the producers, the consumers incur cross-AZ traffic when they do not reside in the same AZ as the partition leader. However, on the consuming side, cross-AZ traffic can occur as many times as there are consumers (on average, two-thirds of the number of consumers). The solution described in this article addresses this particular component of the cross-AZ traffic in the initial design.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://archive.apache.org/dist/kafka/2.3.0/RELEASE_NOTES.html&quot;&gt;Kafka 2.3&lt;/a&gt; introduced the ability for consumers to fetch from partition replicas. This opens the door to a more cost-efficient design.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - Target design, consumers fetching from the closest replica&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Step 3 of Figure 2 shows how consumers can now consume data from the replica that resides in their own AZ. Implementing this feature requires rack-awareness and extra configurations for both the Kafka brokers and consumers. We will describe this in the following sections.&lt;/p&gt;

&lt;h2 id=&quot;the-coban-journey&quot;&gt;The Coban journey&lt;/h2&gt;

&lt;h3 id=&quot;kafka-upgrade&quot;&gt;Kafka upgrade&lt;/h3&gt;

&lt;p&gt;Our journey started with the upgrade of our legacy Kafka clusters. We decided to upgrade them directly to version 3.1, in favour of capturing bug fixes and optimisations over version 2.3. This was a safe move as version 3.1 was deemed stable for almost a year and we projected no additional operational cost for this upgrade.&lt;/p&gt;

&lt;p&gt;To perform an online upgrade with no disruptions for our users, we broke down the process into three stages.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stage 1&lt;/strong&gt;: Upgrading Zookeeper. All versions of Kafka are tested by the community with a specific version of Zookeeper. To ensure stability, we followed this same process. The upgraded Zookeeper would be backward compatible with the pre-upgrade version of Kafka which was still in use at this early stage of the operation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 2&lt;/strong&gt;: Rolling out the upgrade of Kafka to version 3.1 with an explicit backward-compatible inter-broker protocol version (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inter.broker.protocol.version&lt;/code&gt;). During this progressive rollout, the Kafka cluster is temporarily composed of brokers with heterogeneous Kafka versions, but they can communicate with one another because they are explicitly set up to use the same inter-broker protocol version. At this stage, we also upgraded Cruise Control to a compatible version, and we configured Kafka to import the updated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cruise-control-metrics-reporter&lt;/code&gt; JAR file on startup.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 3&lt;/strong&gt;: Upgrading the inter-broker protocol version. This last stage makes all brokers use the most recent version of the inter-broker protocol. During the progressive rollout of this change, brokers with the new protocol version can still communicate with brokers on the old protocol version.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;/h3&gt;
&lt;p&gt;Enabling Kafka consumers to fetch from the closest replica requires a configuration change on both Kafka brokers and Kafka consumers. They also need to be aware of their AZ, which is done by leveraging Kafka rack-awareness (1 “rack” = 1 AZ).&lt;/p&gt;

&lt;h4 id=&quot;brokers&quot;&gt;Brokers&lt;/h4&gt;
&lt;p&gt;In our Kafka brokers’ configuration, we already had &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.rack&lt;/code&gt; set up to distribute the replicas across different AZs for resiliency. Our Ansible role for Kafka automatically sets it with the AZ ID that is dynamically retrieved from the EC2 instance’s metadata at deployment time.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: Get availability zone ID
  uri:
    url: http://169.254.169.254/latest/meta-data/placement/availability-zone-id
    method: GET
    return_content: yes
  register: ec2_instance_az_id
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that we use AWS AZ IDs (suffixed &lt;em&gt;az1, az2, az3&lt;/em&gt;) instead of the typical AWS AZ names (suffixed &lt;em&gt;1a, 1b, 1c&lt;/em&gt;) because the latter’s mapping is &lt;a href=&quot;https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html&quot;&gt;not consistent across AWS accounts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, we added the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replica.selector.class&lt;/code&gt; parameter, set with value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;org.apache.kafka.common.replica.RackAwareReplicaSelector&lt;/code&gt;, to enable the new feature on the server side.&lt;/p&gt;

&lt;h4 id=&quot;consumers&quot;&gt;Consumers&lt;/h4&gt;
&lt;p&gt;On the Kafka consumer side, we mostly rely on Coban’s internal Kafka SDK in Golang, which streamlines how service teams across all Grab verticals utilise Coban Kafka clusters. We have updated the SDK to support fetching from the closest replica.&lt;/p&gt;

&lt;p&gt;Our users only have to export an environment variable to enable this new feature. The SDK then dynamically retrieves the underlying host’s AZ ID from the host’s metadata on startup, and sets a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client.rack&lt;/code&gt; parameter with that information. This is similar to what the Kafka brokers do at deployment time.&lt;/p&gt;

&lt;p&gt;We have also implemented the same logic for our non-SDK consumers, namely &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; pipelines and &lt;a href=&quot;https://developer.confluent.io/learn-kafka/kafka-connect/intro/&quot;&gt;Kafka Connect&lt;/a&gt; connectors.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;
&lt;p&gt;We rolled out fetching from the closest replica at the turn of the year and the feature has been progressively rolled out on more and more Kafka consumers since then.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Variation of our cross-AZ traffic before and after enabling fetching from the closest replica&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 shows the relative impact of this change on our cross-AZ traffic, as reported by AWS Cost Explorer. AWS charges cross-AZ traffic on both ends of the data transfer, thus the two data series. On the Kafka brokers’ side, less cross-AZ traffic is sent out, thereby causing the steep drop in the dark green line. On the Kafka consumers’ side, less cross-AZ traffic is received, causing the steep drop in the light green line. Hence, both ends benefit by fetching from the closest replica.&lt;/p&gt;

&lt;p&gt;Throughout the observeration period, we maintained a relatively stable volume of data consumption. However, after three months, we observed a substantial 25% drop in our cross-AZ traffic compared to December’s average. This reduction had a direct impact on our cross-AZ costs as it directly correlates with the cross-AZ traffic volume in a linear manner.&lt;/p&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;

&lt;h4 id=&quot;increased-end-to-end-latency&quot;&gt;Increased end-to-end latency&lt;/h4&gt;

&lt;p&gt;After enabling fetching from the closest replica, we have observed an increase of up to 500ms in end-to-end latency, that comes from the producer to the consumers. Though this is expected by design, it makes this new feature unsuitable for Grab’s most latency-sensitive use cases. For these use cases, we retained the traditional design whereby consumers fetch directly from the partition leaders, even when they reside in different AZs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - End-to-end latency (99th percentile) of one of our streams, before and after enabling fetching from the closest replica&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;inability-to-gracefully-isolate-a-broker&quot;&gt;Inability to gracefully isolate a broker&lt;/h4&gt;

&lt;p&gt;We have also verified the behaviour of Kafka clients during a broker rotation; a common maintenance operation for Kafka. One of the early steps of our corresponding runbook is to demote the broker that is to be rotated, so that all of its partition leaders are drained and moved to other brokers.&lt;/p&gt;

&lt;p&gt;In the traditional architecture design, Kafka clients only communicate with the partition leaders, so demoting a broker gracefully isolates it from all of the Kafka clients. This ensures that the maintenance is seamless for them. However, by fetching from the closest replica, Kafka consumers still consume from the demoted broker, as it keeps serving partition followers. When the broker effectively goes down for maintenance, those consumers are suddenly disconnected. To work around this, they must handle connection errors properly and implement a retry mechanism.&lt;/p&gt;

&lt;h4 id=&quot;potentially-skewed-load&quot;&gt;Potentially skewed load&lt;/h4&gt;

&lt;p&gt;Another caveat we have observed is that the load on the brokers is directly determined by the location of the consumers. If they are not well balanced across all of the three AZs, then the load on the brokers is similarly skewed. At times, new brokers can be added to support an increasing load on an AZ. However, it is undesirable to remove any brokers from the less loaded AZs as more consumers can suddenly relocate there at any time. Having these additional brokers and underutilisation of existing brokers on other AZs can also impact cost efficiency.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/zero-traffic-cost/fig-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Average CPU utilisation by AZ of one of our critical Kafka clusters&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 shows the CPU utilisation by AZ for one of our critical Kafka clusters. The skewage is visible after 01/03/2023. To better manage this skewage in load across AZs, we have updated our SDK to expose the AZ as a new metric. This allows us to monitor the skewness of the consumers and take measures proactively, for example, moving some of them to different AZs.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;We have implemented the feature to fetch from the closest replica on all our Kafka clusters and all Kafka consumers that we control. This includes internal Coban pipelines as well as the managed pipelines that our users can self-serve as part of our data streaming offering.&lt;/p&gt;

&lt;p&gt;We are now evangelising and advocating for more of our users to adopt this feature.&lt;/p&gt;

&lt;p&gt;Beyond Coban, other teams at Grab are also working to reduce their cross-AZ traffic, notably, Sentry, the team that is in charge of Grab’s service mesh.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 07 Jul 2023 00:23:05 +0000</pubDate>
        <link>https://engineering.grab.com/zero-traffic-cost</link>
        <guid isPermaLink="true">https://engineering.grab.com/zero-traffic-cost</guid>
        
        <category>Engineering</category>
        
        <category>Kafka</category>
        
        <category>Performance</category>
        
        <category>Access control</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Go module proxy at Grab</title>
        <description>&lt;p&gt;At Grab, we rely heavily on &lt;a href=&quot;/go-module-a-guide-for-monorepos-part-1&quot;&gt;a large Go monorepo&lt;/a&gt; for backend development, which offers benefits like code reusability and discoverability. However, as we continue to grow, managing a large monorepo brings about its own set of unique challenges.&lt;/p&gt;

&lt;p&gt;As an example, using Go commands such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go list&lt;/code&gt; can be incredibly slow when fetching Go modules residing in a large &lt;a href=&quot;https://github.com/golang/go/wiki/Modules#what-are-multi-module-repositories&quot;&gt;multi-module repository&lt;/a&gt;. This sluggishness takes a toll on developer productivity, burdens our Continuous Integration (CI) systems, and strains our Version Control System host (VCS), GitLab.&lt;/p&gt;

&lt;p&gt;In this blog post, we look at how &lt;a href=&quot;https://github.com/gomods/athens&quot;&gt;Athens&lt;/a&gt;, a Go module proxy, helps to improve the overall developer experience of engineers working with a large Go monorepo at Grab.&lt;/p&gt;

&lt;h2 id=&quot;key-highlights&quot;&gt;Key highlights&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;We reduced the time of executing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; command from &lt;strong&gt;~18 minutes&lt;/strong&gt; to &lt;strong&gt;~12 seconds&lt;/strong&gt; when fetching monorepo Go modules.&lt;/li&gt;
  &lt;li&gt;We scaled in and &lt;strong&gt;scaled down our entire Athens cluster by 70%&lt;/strong&gt; by utilising the fallback network mode in Athens along with Golang’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOVCS&lt;/code&gt; mode, resulting in cost savings and enhanced efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-statements-and-solutions&quot;&gt;Problem statements and solutions&lt;/h2&gt;

&lt;h3 id=&quot;1-painfully-slow-performance-of-go-commands&quot;&gt;1. Painfully slow performance of Go commands&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Problem summary: Running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; command in our monorepo takes a considerable amount of time and can lead to performance degradation in our VCS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When working with the Go programming language, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; is one of the most common commands that you’ll use every day. Besides developers, this command is also used by CI systems.&lt;/p&gt;

&lt;h4 id=&quot;what-does-go-getdo&quot;&gt;What does &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; do?&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; command is used to download and install packages and their dependencies in Go. Note that it operates differently depending on whether it is run in &lt;a href=&quot;https://pkg.go.dev/cmd/go#hdr-Legacy_GOPATH_go_get&quot;&gt;legacy GOPATH mode&lt;/a&gt; or module-aware mode. In Grab, we’re using the &lt;a href=&quot;https://go.dev/ref/mod#mod-commands&quot;&gt;module-aware mode&lt;/a&gt; in a &lt;a href=&quot;https://github.com/golang/go/wiki/Modules#faqs--multi-module-repositories&quot;&gt;multi-module repository&lt;/a&gt; setup.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/go-module-proxy/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Every time &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; is run, it uses Git commands, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git ls-remote&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git tag&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git fetch&lt;/code&gt;, etc, to search and download the entire worktree. The excessive use of these Git commands on our monorepo contributes to the long processing time and can be strenuous to our VCS.&lt;/p&gt;

&lt;h4 id=&quot;how-big-is-our-monorepo&quot;&gt;How big is our monorepo?&lt;/h4&gt;

&lt;p&gt;To fully grasp the challenges faced by our engineering teams, it’s crucial to understand the vast scale of the monorepo that we work with daily. For this, we use &lt;a href=&quot;https://github.com/github/git-sizer&quot;&gt;git-sizer&lt;/a&gt; to analyse our monorepo.&lt;/p&gt;

&lt;p&gt;Here’s what we found:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Overall repository size&lt;/strong&gt;: The monorepo has a total uncompressed size of &lt;strong&gt;69.3 GiB&lt;/strong&gt;, a fairly substantial figure. To put things into perspective, the &lt;a href=&quot;https://github.com/torvalds/linux&quot;&gt;Linux kernel repository&lt;/a&gt;, known for its vastness, currently stands at 55.8 GiB.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Trees&lt;/strong&gt;: The total number of trees is 3.21M and tree entries are 99.8M, which consume 3.65 GiB. This may cause performance issues during some Git operations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;References&lt;/strong&gt;: Totalling 10.7k references.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Biggest checkouts&lt;/strong&gt;: There are 64.7k directories in our monorepo. This affects operations like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git status&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git checkout&lt;/code&gt;. Moreover, our monorepo has a maximum path depth of 20. This contributes to a slow processing time on Git and negatively impacts developer experience. The number of files (354k) and the total size of files (5.08 GiB) are also concerns due to their potential impact on the repository’s performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To draw a comparison, refer to &lt;a href=&quot;https://github.com/github/git-sizer/blob/0b6d3a21c6ccbd49463534a19cc1b3f71526c077/README.md#usage&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-sizer&lt;/code&gt; output of the Linux repository&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;how-slow-is-slow&quot;&gt;How slow is “slow”?&lt;/h4&gt;

&lt;p&gt;To illustrate the issue further, we will compare the time taken for various Go commands to fetch a single module in our monorepo at a 10 MBps download speed.&lt;/p&gt;

&lt;p&gt;This is an example of how a module is structured in our monorepo:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gitlab.company.com/monorepo/go
  |-- go.mod
  |-- commons/util/gk
        |-- go.mod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table class=&quot;table&quot; border=&quot;1&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Go commands&lt;/th&gt;
      &lt;th&gt;GOPROXY&lt;/th&gt;
      &lt;th&gt;Previously cached?&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Result (time taken)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go get -x gitlab.company.com/monorepo/go/commons/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;proxy.golang.org,direct&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Download and install the latest version of the module. This is a common scenario that developers often encounter.&lt;/td&gt;
      &lt;td&gt;18:50.71 minutes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go get -x gitlab.company.com/monorepo/go/commons/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;proxy.golang.org,direct&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Download and install the latest version of the module &lt;strong&gt;without any module cache&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1:11:54.56 hour&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;proxy.golang.org,direct&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;List information about the module&lt;/td&gt;
      &lt;td&gt;3.873 seconds&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;proxy.golang.org,direct&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;List information about the module &lt;strong&gt;without any module cache&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;3:18.58 minutes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In this example, using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; to fetch a module took over &lt;strong&gt;18 minutes&lt;/strong&gt; to complete. If we needed to retrieve more than one module in our monorepo, it can be incredibly time-consuming.&lt;/p&gt;

&lt;h4 id=&quot;why-is-it-slow-in-a-monorepo&quot;&gt;Why is it slow in a monorepo?&lt;/h4&gt;

&lt;p&gt;In a large Go monorepo, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; commands can be slow due to several factors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Large number of files and directories&lt;/strong&gt;: When running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt;, the command needs to search and download the entire worktree. In a large multi-module monorepo, the vast number of files and directories make this search process very expensive and time-consuming.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of refs&lt;/strong&gt;: A large number of refs (branches or tags) in our monorepo can affect performance. Ref advertisements (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git ls-remote&lt;/code&gt;), which contain every ref in our monorepo, are the first phase in any remote Git operation, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git clone&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git fetch&lt;/code&gt;. With a large number of refs, performance takes a hit when performing these operations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Commit history traversal&lt;/strong&gt;: Operations that need to traverse a repository’s commit history and consider each ref will be slow in a monorepo. The larger the monorepo, the more time-consuming these operations become.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;the-consequences-stifled-productivity-and-strained-systems&quot;&gt;The consequences: Stifled productivity and strained systems&lt;/h4&gt;

&lt;h5 id=&quot;developers-and-ci&quot;&gt;Developers and CI&lt;/h5&gt;

&lt;p&gt;When Go command operations like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; are slow, they contribute to significant delays and inefficiencies in software development workflows. This leads to reduced productivity and demotivated developers.&lt;/p&gt;

&lt;p&gt;Optimising Go command operations’ speed is crucial to ensure efficient software development workflows and high-quality software products.&lt;/p&gt;

&lt;h5 id=&quot;version-control-system&quot;&gt;Version Control System&lt;/h5&gt;

&lt;p&gt;It’s also worth noting that overusing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; commands can also lead to performance issues for VCS. When Go packages are frequently downloaded using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt;, we saw that it caused a bottleneck in our VCS cluster, which can lead to performance degradation or even cause rate-limiting queue issues.&lt;/p&gt;

&lt;p&gt;This negatively impacts the performance of our VCS infrastructure, causing delays or sometimes unavailability for some users and CI.&lt;/p&gt;

&lt;h4 id=&quot;solution-athens--fallbacknetwork-mode--govcs-custom-cache-refresh-solution&quot;&gt;Solution: Athens + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fallback&lt;/code&gt; Network Mode + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOVCS&lt;/code&gt; + Custom Cache Refresh Solution&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Problem summary: Speed up &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; command by not fetching from our VCS&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We addressed the speed issue by using Athens, &lt;a href=&quot;https://www.practical-go-lessons.com/chap-18-go-module-proxies#what-is-a-proxy-server&quot;&gt;a proxy server for Go modules&lt;/a&gt; (read more about the &lt;a href=&quot;https://go.dev/ref/mod#goproxy-protocol&quot;&gt;GOPROXY protocol&lt;/a&gt;).&lt;/p&gt;

&lt;h5 id=&quot;how-does-athens-work&quot;&gt;How does Athens work?&lt;/h5&gt;

&lt;p&gt;The following sequence diagram describes the default flow of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; command with Athens.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/go-module-proxy/image1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Athens uses a &lt;a href=&quot;https://docs.gomods.io/configuration/storage/&quot;&gt;storage system&lt;/a&gt; for Go module packages, which can also be configured to use various storage systems such as Amazon S3, and Google Cloud Storage, among others.&lt;/p&gt;

&lt;p&gt;By caching these module packages in storage, Athens can serve the packages directly from storage rather than requesting them from an upstream VCS while serving Go commands such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go mod download&lt;/code&gt; and &lt;a href=&quot;https://go.dev/ref/mod#build-commands&quot;&gt;certain go build modes&lt;/a&gt;. However, just using a Go module proxy didn’t fully resolve our issue since the &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go list&lt;/code&gt;&lt;/strong&gt; commands still hit our VCS through the proxy.&lt;/p&gt;

&lt;p&gt;With this in mind, we thought “what if we could just serve the Go modules directly from Athens’ storage for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt;?” This question led us to discover Athens network mode.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is Athens network mode?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Athens &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NetworkMode&lt;/code&gt; configures how Athens will return the results of the Go commands. It can be assembled from both its own storage and the upstream VCS. As of &lt;a href=&quot;https://github.com/gomods/athens/releases/tag/v0.12.1&quot;&gt;Athens v0.12.1&lt;/a&gt;, it currently supports these 3 modes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;strict&lt;/strong&gt;: merge VCS versions with storage versions, but fail if either of them fails.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;offline&lt;/strong&gt;: only get storage versions, &lt;strong&gt;never reach out to VCS&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;fallback&lt;/strong&gt;: only return storage versions, if VCS fails. Fallback mode does the best effort of giving you what’s available at the time of requesting versions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our Athens clusters were initially set to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;strict&lt;/code&gt; network mode, but this was not ideal for us. So we explored the other network modes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exploring &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offline&lt;/code&gt; mode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We initially sought to explore the idea of putting Athens in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offline&lt;/code&gt; network mode, which would allow Athens to serve Go requests only from its storage. This concept aligned with our aim of reducing VCS hits while also leading to significant performance improvement in Go workflows.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/go-module-proxy/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However in practice, it’s not an ideal approach. The default Athens setup (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;strict&lt;/code&gt; mode) automatically updates the module version when a user requests a new module version. Nevertheless, switching Athens to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offline&lt;/code&gt; mode would disable the automatic updates as it wouldn’t connect to the VCS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Custom cache refresh solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To solve this, we implemented a CI pipeline that refreshes Athens’ module cache whenever a new module is released in our monorepo. Employing this with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offline&lt;/code&gt; mode made Athens effective for the monorepo but it resulted in the loss of automatic updates for other repositories&lt;/p&gt;

&lt;p&gt;Restoring this feature requires applying our custom cache refresh solution to all other Go repositories. However, implementing this workaround can be quite cumbersome and significant additional time and effort. We decided to look for another solution that would be easier to maintain in the long run.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A balanced approach: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fallback&lt;/code&gt; Mode and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOVCS&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This approach builds upon our aforementioned custom cache refresh which is specifically designed for the monorepo.&lt;/p&gt;

&lt;p&gt;We came across the &lt;a href=&quot;https://go.dev/ref/mod#vcs-govcs&quot;&gt;GOVCS environment variable&lt;/a&gt;, which we use in combination with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fallback&lt;/code&gt; network mode to effectively put only the monorepo in “offline” mode.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOVCS&lt;/code&gt; is set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab.company.com/monorepo/go:off&lt;/code&gt;, Athens encounters an error whenever it tries to fetch modules from VCS:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gitlab.company.com/monorepo/go/commons/util/gk@v1.1.44: unrecognized import path &quot;gitlab.company.com/monorepo/go/commons/util/gk&quot;: GOVCS disallows using git for private gitlab.company.com/monorepo/go; see 'go help vcs'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If Athens network mode is set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;strict&lt;/code&gt;, Athens returns 404 errors to the user. By switching to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fallback&lt;/code&gt; mode, Athens tries to retrieve the module from its storage if a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOVCS&lt;/code&gt; failure occurs.&lt;/p&gt;

&lt;p&gt;Here’s the updated Athens configuration (&lt;a href=&quot;https://github.com/gomods/athens/blob/8e1581e10b0d3a70a30f45b10c24c3f992464d7a/config.dev.toml#L46&quot;&gt;example default config&lt;/a&gt;):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GoBinaryEnvVars = [&quot;GOPROXY=direct&quot;, 
&quot;GOPRIVATE=gitlab.company.com&quot;, 
&quot;GOVCS=gitlab.company.com/monorepo/go:off&quot;]

NetworkMode = &quot;fallback&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With the custom cache refresh solution coupled with this approach, we not only accelerate the retrieval of Go modules within the monorepo but also allow for automatic updates for non-monorepo Go modules.&lt;/p&gt;

&lt;h4 id=&quot;final-results&quot;&gt;Final results&lt;/h4&gt;

&lt;p&gt;This solution resulted in a significant improvement in the performance of Go commands for our developers. With Athens, the same command is completed in just &lt;strong&gt;~12 seconds (down from ~18 minutes)&lt;/strong&gt;, which is impressively fast.&lt;/p&gt;

&lt;table class=&quot;table&quot; border=&quot;1&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Go commands&lt;/th&gt;
      &lt;th&gt;GOPROXY&lt;/th&gt;
      &lt;th&gt;Previously cached?&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Result (time taken)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go get -x gitlab.company.com/monorepo/go/commons/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;goproxy.company.com&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Download and install the latest version of the module. This is a common scenario that developers often encounter.&lt;/td&gt;
      &lt;td&gt;11.556 seconds&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go get -x gitlab.company.com/monorepo/go/commons/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;goproxy.company.com&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Download and install the latest version of the module &lt;strong&gt;without any module cache&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1:05.60 minutes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;goproxy.company.com&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;List information about the monorepo module&lt;/td&gt;
      &lt;td&gt;0.592 seconds&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;goproxy.company.com&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;List information about the monorepo module &lt;strong&gt;without any module cache&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1.023 seconds&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/go-module-proxy/image5.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Average cluster CPU utlisation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/go-module-proxy/image3.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Average cluster memory utlisation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In addition, this change to our Athens cluster also leads to substantial reduction in average cluster CPU and memory utilisation. This also enabled us to scale in and &lt;strong&gt;scale down our entire Athens cluster by 70%&lt;/strong&gt;, resulting in cost savings and enhanced efficiency. On top of that, we were also able to effectively eliminate VCS’s rate-limiting issues while making the monorepo’s command operation considerably faster.&lt;/p&gt;

&lt;h3 id=&quot;2-go-modules-in-gitlab-subgroups&quot;&gt;2. Go modules in GitLab subgroups&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Problem summary: Go modules are unable to work natively with private or internal repositories under GitLab subgroups.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When it comes to managing code repositories and packages, &lt;a href=&quot;https://docs.gitlab.com/ee/user/group/subgroups/&quot;&gt;GitLab subgroups&lt;/a&gt; and Go modules have become an integral part of the development process at Grab. Go modules help to organise and manage dependencies, and GitLab subgroups provide an additional layer of structure to group related repositories together.&lt;/p&gt;

&lt;p&gt;However, a common issue when using Go modules is that they &lt;strong&gt;do not work natively&lt;/strong&gt; with private or internal repositories under a GitLab subgroup (see this &lt;a href=&quot;https://github.com/golang/go/issues/29953&quot;&gt;GitHub issue&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;For example, using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get&lt;/code&gt; to retrieve a module from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitlab.company.com/gitlab-org/subgroup/repo&lt;/code&gt; will result in a failure. This problem is not specific to Go modules, all repositories under the subgroup will face the same issue.&lt;/p&gt;

&lt;h4 id=&quot;a-cumbersome-workaround&quot;&gt;A cumbersome workaround&lt;/h4&gt;

&lt;p&gt;To overcome this issue, we had to use workarounds. One workaround is to authenticate the HTTPS calls to GitLab by adding authentication details to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.netrc&lt;/code&gt; file on your machine.&lt;/p&gt;

&lt;p&gt;The following lines can be added to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.netrc&lt;/code&gt; file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;machine gitlab.company.com
    login user@company.com
    password &amp;lt;personal-access-token&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In our case, we are using a Personal Access Token (PAT) since we have 2FA enabled. If 2FA is not enabled, the GitLab password can be used instead. However, this approach would mean configuring the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.netrc&lt;/code&gt; file in the CI environments as well as on the machine of &lt;strong&gt;every&lt;/strong&gt; Go developer.&lt;/p&gt;

&lt;h4 id=&quot;solution-athens--netrc&quot;&gt;Solution: Athens + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.netrc&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;A feasible solution is to set up the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.netrc&lt;/code&gt; file in the Go proxy server. This method eliminates the need for N number of developers to configure their own &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.netrc&lt;/code&gt; files. Instead, the responsibility for this task is delegated to the Go proxy server.&lt;/p&gt;

&lt;h3 id=&quot;3-sharing-common-libraries&quot;&gt;3. Sharing common libraries&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Problem summary: Distributing internal common libraries within a monorepo without granting direct repository access can be challenging.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;At Grab, we work with various cross-functional teams, and some could have distinct network access like different VPNs. This adds complexity to sharing our monorepo’s internal common libraries with them. To maintain the security and integrity of our monorepo, we use a Go proxy for controlled access to necessary libraries.&lt;/p&gt;

&lt;p&gt;The key difference between granting direct access to the monorepo via VCS and using a Go proxy is that the former allows users to read everything in the repository, while the latter enables us to grant access only to the specific libraries users need within the monorepo. This approach ensures secure and efficient collaboration across diverse network configurations.&lt;/p&gt;

&lt;h4 id=&quot;without-go-module-proxy&quot;&gt;Without Go module proxy&lt;/h4&gt;

&lt;p&gt;Without Athens, we would need to create a separate repository to store the code we want to share and then use a build system to automatically mirror the code from the monorepo to the public repository.&lt;/p&gt;

&lt;p&gt;This process can be cumbersome and lead to inconsistencies in code versions between the two repositories, ultimately making it challenging to maintain the shared libraries.&lt;/p&gt;

&lt;p&gt;Furthermore, copying code can lead to errors and increase the risk of security breaches by exposing confidential or sensitive information.&lt;/p&gt;

&lt;h4 id=&quot;solution-athens--download-mode-file&quot;&gt;Solution: Athens + Download Mode File&lt;/h4&gt;

&lt;p&gt;To tackle this problem statement, we utilise Athens’ &lt;a href=&quot;https://docs.gomods.io/configuration/download/&quot;&gt;download mode file&lt;/a&gt; feature using an allowlist approach to specify which repositories can be downloaded by users.&lt;/p&gt;

&lt;p&gt;Here’s an example of the Athens download mode config file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;downloadURL = &quot;https://proxy.golang.org&quot;

mode = &quot;sync&quot;

download &quot;gitlab.company.com/repo/a&quot; {
    mode = &quot;sync&quot;
}

download &quot;gitlab.company.com/repo/b&quot; {
    mode = &quot;sync&quot;
}

download &quot;gitlab.company.com/*&quot; {
    mode = &quot;none&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the configuration file, we specify allowlist entries for each desired repo, including their respective download modes. For example, in the snippet above, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;repo/a&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;repo/b&lt;/code&gt; are allowed (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mode = “sync”&lt;/code&gt;), while everything else is blocked using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mode = “none”&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;final-results-1&quot;&gt;Final results&lt;/h4&gt;

&lt;p&gt;By using Athens’ download mode feature in this case, the benefits are clear. Athens provides a secure, centralised place to store Go modules. This approach not only provides consistency but also improves maintainability, as all code versions are managed in one single location.&lt;/p&gt;

&lt;h2 id=&quot;additional-benefits-of-go-proxy&quot;&gt;Additional benefits of Go proxy&lt;/h2&gt;

&lt;p&gt;As we’ve touched upon the impressive results achieved by implementing Athens Go proxy at Grab, it’s crucial to explore the supplementary advantages that accompany this powerful solution.&lt;/p&gt;

&lt;p&gt;These unsung benefits, though possibly overlooked, play a vital role in enriching the overall developer experience at Grab and promoting more robust software development practices:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Module immutability&lt;/strong&gt;: ​​As the software world continues to face issues around changing or &lt;a href=&quot;https://qz.com/646467/how-one-programmer-broke-the-internet-by-deleting-a-tiny-piece-of-code&quot;&gt;disappearing libraries&lt;/a&gt;, Athens serves as a useful tool in mitigating build disruptions by providing immutable storage for copied VCS code. The use of a Go proxy also ensures that builds remain deterministic, improving consistency across our software.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Uninterrupted development&lt;/strong&gt;: Athens allows users to fetch dependencies even when VCS is down, ensuring continuous and seamless development workflows.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhanced security&lt;/strong&gt;: Athens offers access control by enabling the blocking of specific packages within Grab. This added layer of security protects our work against potential risks from malicious third-party packages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vendor directory removal&lt;/strong&gt;: Athens prepares us for the eventual removal of the &lt;a href=&quot;https://docs.gomods.io/faq/#when-should-i-use-a-vendor-directory-and-when-should-i-use-athens&quot;&gt;vendor directory&lt;/a&gt;, fostering faster workflows in the future.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Since adopting Athens as a Go module proxy, we have observed considerable benefits, such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Accelerated Go command operations&lt;/li&gt;
  &lt;li&gt;Reduced infrastructure costs&lt;/li&gt;
  &lt;li&gt;Mitigated VCS load issues&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Moreover, its lesser-known advantages like module immutability, uninterrupted development, enhanced security, and vendor directory transition have also contributed to improved development practices and an enriched developer experience for Grab engineers.&lt;/p&gt;

&lt;p&gt;Today, the straightforward process of exporting three environment variables has greatly influenced our developers’ experience at Grab.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export GOPROXY=&quot;goproxy.company.com|proxy.golang.org,direct&quot;

export GONOSUMDB=&quot;gitlab.company.com&quot;

export GONOPROXY=&quot;none&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At Grab, we are always looking for ways to improve and optimise the way we work, so we contribute to open-sourced projects like Athens, where we help with bug fixes. If you are interested in setting up a Go module proxy, do give Athens (&lt;a href=&quot;https://github.com/gomods/athens&quot;&gt;github.com/gomods/athens&lt;/a&gt;) a try!&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Swaminathan Venkatraman, En Wei Soh, Anuj More, Darius Tan, and Fernando Christyanto for contributing to this project and this article.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Jun 2023 01:18:00 +0000</pubDate>
        <link>https://engineering.grab.com/go-module-proxy</link>
        <guid isPermaLink="true">https://engineering.grab.com/go-module-proxy</guid>
        
        <category>Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>PII masking for privacy-grade machine learning</title>
        <description>&lt;p&gt;At Grab, data engineers work with large sets of data on a daily basis. They design and build advanced machine learning models that provide strategic insights using all of the data that flow through the Grab Platform. This enables us to provide a better experience to our users, for example by increasing the supply of drivers in areas where our predictive models indicate a surge in demand in a timely fashion.&lt;/p&gt;

&lt;p&gt;Grab has a mature privacy programme that complies with applicable privacy laws and regulations and we use tools to help identify, assess, and appropriately manage our privacy risks. To ensure that our users’ data are well-protected and avoid any human-related errors, we always take extra measures to secure this data.&lt;/p&gt;

&lt;p&gt;However, data engineers &lt;strong&gt;will still require&lt;/strong&gt; access to actual production data in order to tune effective machine learning models and ensure the models work as intended in production.&lt;/p&gt;

&lt;p&gt;In this article, we will describe how the Grab’s data streaming team (Coban), along with the data platform and user teams, have enforced Personally Identifiable Information (PII) masking on machine learning data streaming pipelines. This ensures that we uphold a high standard and embody a &lt;em&gt;privacy by design&lt;/em&gt; culture, while enabling data engineers to refine their models with sanitised production data.&lt;/p&gt;

&lt;h2 id=&quot;pii-tagging&quot;&gt;PII tagging&lt;/h2&gt;

&lt;p&gt;Data streaming at Grab leverages the Protocol Buffers (&lt;a href=&quot;https://protobuf.dev/&quot;&gt;protobuf&lt;/a&gt;) data format to structure in-transit data. When creating a new stream, developers &lt;strong&gt;must&lt;/strong&gt; describe its fields in a protobuf schema that is then used for serialising the data wherever it is sent over the wire, and deserialising it wherever it is consumed.&lt;/p&gt;

&lt;p&gt;A fictional example schema looks like this (the indexes are arbitrary, but commonly created in sequence):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;message Booking {
  string bookingID = 1;
  int64 creationTime = 2;
  int64 passengerID = 3;
  string passengerName = 4;
  ... truncated output ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Over here, the fourth field &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;passengerName&lt;/code&gt; involves a PII and the data pertaining to that field should never be accessible by any data engineer. Therefore, developers owning the stream must tag that field with a PII label like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import &quot;streams/coban/options/v1/pii.proto&quot;;

message Booking {
  string bookingID = 1;
  int64 creationTime = 2;
  int64 passengerID = 3;
  string passengerName = 4 [(streams.coban.options.v1.pii_type) = PII_TYPE_NAME];
  ... truncated output ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The imported &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pii.proto&lt;/code&gt; library defines the tags for all possible types of PII. In the example above, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;passengerName&lt;/code&gt; field has not only been flagged as PII, but is also marked as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PII_TYPE_NAME&lt;/code&gt; – a specific type of PII that conveys the names of individuals. This high-level typing enables more flexible PII masking methods, which we will explain later.&lt;/p&gt;

&lt;p&gt;Once the PII fields have been properly identified and tagged, developers need to publish the schema of their new stream into Coban’s Git repository. A Continuous Integration (CI) pipeline described below ensures that all fields describing PII are correctly tagged.&lt;/p&gt;

&lt;p&gt;The following diagram shows this CI pipeline in action.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/pii-masking/image1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 1 CI pipeline failure due to untagged PII fields&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;When a developer creates a Merge Request (MR) or pushes a new commit to create or update a schema (step 1), the CI pipeline is triggered. It runs an in-house Python script that scans each variable name of the committed schema and tests it against an extensive list of PII keywords that is regularly updated, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;name&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;address&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;email&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;phone&lt;/code&gt;, etc (step 2). If there is a match and the variable is not tagged with the expected PII label, the pipeline fails (step 3) with an explicit error message in the CI pipeline’s output, similar to this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Field name [Booking.passengerName] should have been marked with type streams.coban.options.v1.pii_type = PII_TYPE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are cases where a variable name in the schema is a partial match against a PII keyword but is legitimately not a PII – for example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;carModelName&lt;/code&gt; is a partial match against &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;name&lt;/code&gt; but does not contain PII data. In this case, the developer can choose to add it to a whitelist to pass the CI.&lt;/p&gt;

&lt;p&gt;However, modifying the whitelist requires approval from the Coban team for verification purposes. Apart from this particular case, the requesting team can autonomously approve their MR in a self-service fashion.&lt;/p&gt;

&lt;p&gt;Now let us look at an example of a successful CI pipeline execution.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/pii-masking/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 2 CI pipeline success and schema publishing&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Fig. 2, the committed schema (step 1) is properly tagged so our in-house Python script is unable to find any untagged PII fields (step 2). The MR is approved by a code owner (step 3), then merged to the master branch of the repository (step 4).&lt;/p&gt;

&lt;p&gt;Upon merging, another CI pipeline is triggered to package the protobuf schema in a Java Archive (JAR) of &lt;a href=&quot;https://docs.scala-lang.org/tour/classes.html&quot;&gt;Scala classes&lt;/a&gt; (step 5), which in turn is stored into a package registry (step 6). We will explain the reason for this in a later section.&lt;/p&gt;

&lt;h2 id=&quot;production-environment&quot;&gt;Production environment&lt;/h2&gt;

&lt;p&gt;With the schemas published and all of their PII fields properly tagged, we can now take a look at the data streaming pipelines.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/pii-masking/image3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 3 PII flow in the production environment&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In this example, the user generates data by interacting with the Grab superapp and making a booking (step 1). The booking service, compiled with the stream’s schema definition, generates and produces Kafka records for other services to consume (step 2). Among those consuming services are the production machine learning pipelines that are of interest to this article (step 3).&lt;/p&gt;

&lt;p&gt;PII is not masked in this process because it is actually required by the consuming services. For example, the driver app needs to display the passenger’s actual name, so the driver can confirm their identity easily.&lt;/p&gt;

&lt;p&gt;At this part of the process, this is not much of a concern because access to the sacrosanct production environment is highly restricted and monitored by Grab.&lt;/p&gt;

&lt;h2 id=&quot;pii-masking&quot;&gt;PII masking&lt;/h2&gt;

&lt;p&gt;To ensure the security, stability, and privacy of our users, data engineers who need to tune their new machine learning models based on production data are &lt;strong&gt;not granted access&lt;/strong&gt; to the production environment. Instead, they have access to the staging environment, where production data is mirrored and PII is masked.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/pii-masking/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 4 PII masking pipeline from the production environment to the staging environment&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The actual PII masking is performed by an in-house &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; application that resides in the production environment. Flink is a reference framework for data streaming that we use extensively. It is also fault tolerant, with the ability to restart from a checkpoint.&lt;/p&gt;

&lt;p&gt;The Flink application is compiled along with the JAR containing the schema as Scala classes previously mentioned. Therefore, it is able to consume the original data as a regular Kafka consumer (step 1). It then dynamically masks the PII of the consumed data stream, based on the PII tags of the schema (step 2). Ultimately, it produces the sanitised data to the Kafka cluster in the staging environment as a normal Kafka producer (step 3).&lt;/p&gt;

&lt;p&gt;Depending on the kind of PII, there are several methods of masking such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Names and strings of characters&lt;/strong&gt;: They are replaced by consistent &lt;a href=&quot;https://csrc.nist.gov/glossary/term/hash_based_message_authentication_code&quot;&gt;HMAC&lt;/a&gt; (Hash-based message authentication code). A HMAC is a digest produced by a one-way cryptographic hash function that takes a secret key as a parameter. Leveraging a secret key here is a defence against chosen plaintext attacks, i.e. computing the digest of a particular plaintext, like a targeted individual’s name.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Numbers and dates&lt;/strong&gt;: Similarly, they are transformed in a consistent manner, by leveraging a random generator that takes the unmasked value as a seed, so that the same PII input consistently produces the same masked output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that consistency is a recurring pattern. This is because it is a key requirement for certain machine learning models.&lt;/p&gt;

&lt;p&gt;This sanitised data produced to the Kafka cluster in the staging environment is then consumed by the staging machine learning pipelines (step 4). There, it is used by data engineers to tune their models effectively with near real-time production data (step 5).&lt;/p&gt;

&lt;p&gt;The Kafka cluster in the staging environment is secured with authorisation and authentication (see &lt;a href=&quot;/zero-trust-with-kafka&quot;&gt;Zero Trust with Kafka&lt;/a&gt;). This is an extra layer of security in case some PII data inadvertently fall through the cracks of PII tagging, following the defence in depth principle.&lt;/p&gt;

&lt;p&gt;Finally, whenever a new PII-tagged field is added to a schema, the PII masking Flink application needs to be compiled and deployed again. If the schema is not updated, the Flink pipeline is unable to decode this new field when deserialising the stream. Thus, the added field is just dropped and the new PII data does not make it to the staging environment.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;For the immediate next steps, we are going to enhance this design with an in-house product based on &lt;a href=&quot;https://aws.amazon.com/macie/&quot;&gt;AWS Macie&lt;/a&gt; to automatically detect the PII that would have fallen through the cracks. Caspian, Grab’s data lake team and one of Coban’s sister teams, has built a service that is already able to detect PII data in relational databases and data lake tables. It is currently being adapted for data streaming.&lt;/p&gt;

&lt;p&gt;In the longer run, we are committed to taking our privacy by design posture to the next level. Indeed, the PII masking described in this article does not prevent a bad actor from retrieving the consistent hash of a particular individual based on their non-PII data. For example, the target might be identifiable by a signature in the masked data set, such as unique food or transportation habits.&lt;/p&gt;

&lt;p&gt;A possible counter-measure could be one or a combination of the following techniques, ordered by difficulty of implementation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data minimisation&lt;/strong&gt;: Non-essential fields in the data stream should not be mirrored at all. E.g. fields of the data stream that are not required by the data engineers to tune their models. We can introduce a dedicated tag in the schema to flag those fields and instruct the mirroring pipeline to drop them. This is the most straightforward approach.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Differential privacy&lt;/strong&gt;: The mirroring pipeline could introduce some noise in the mirrored data, in a way that would obfuscate the signatures of particular individuals while still preserving the essential statistical properties of the dataset required for machine learning. It happens that Flink is a suitable framework to do so, as it can split a stream into multiple windows and apply computation over those windows. Designing and generalising a logic that meets the objective is challenging though.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PII encryption at source&lt;/strong&gt;: PII could be encrypted by the producing services (like the booking service), and dynamically decrypted where plaintext values are required. However, key management and performance are two tremendous challenges of this approach.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will explore these techniques further to find the solution that works best for Grab and ensures the highest level of privacy for our users.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Jun 2023 01:18:00 +0000</pubDate>
        <link>https://engineering.grab.com/pii-masking</link>
        <guid isPermaLink="true">https://engineering.grab.com/pii-masking</guid>
        
        <category>Engineering</category>
        
        <category>Privacy</category>
        
        <category>Data masking</category>
        
        <category>Machine learning</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Performance bottlenecks of Go application on Kubernetes with non-integer (floating) CPU allocation</title>
        <description>&lt;p&gt;Grab’s real-time data platform team, Coban, has been running its stream processing framework on Kubernetes, as detailed in &lt;a href=&quot;/plumbing-at-scale&quot;&gt;Plumbing at scale&lt;/a&gt;. We’ve also written another article (&lt;a href=&quot;/optimally-scaling-kafka-consumer-applications&quot;&gt;Scaling Kafka consumers&lt;/a&gt;) about vertical pod autoscaling (VPA) and the benefits of using it.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In this article, we cover the performance bottlenecks and other issues we came across for Go applications on Kubernetes.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;We noticed CPU throttling issues on some pipelines leading to consumption lag, which meant there was a delay between data production and consumption. This was an issue because the data might no longer be relevant or accurate when it gets consumed. This led to incorrect data-driven conclusions, costly mistakes, and more.&lt;/p&gt;

&lt;p&gt;While debugging this issue, we focused primarily on the SinktoS3 pipeline. It is essentially used for sinking data from Kafka topics to AWS S3. Depending on your requirements, data sinking is primarily for archival purposes and can be used for analytical purposes.&lt;/p&gt;

&lt;h2 id=&quot;investigation&quot;&gt;Investigation&lt;/h2&gt;

&lt;p&gt;After conducting a thorough investigation, we found two main issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Resource throttling&lt;/li&gt;
  &lt;li&gt;Issue with VPA&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resource-throttling&quot;&gt;Resource throttling&lt;/h3&gt;

&lt;p&gt;We redesigned our SinktoS3 pipeline architecture to concurrently perform the most CPU intensive operations using parallel goroutines (workers). This improved performance and considerably reduced consumer lag.&lt;/p&gt;

&lt;p&gt;But the high-performance architecture needed more intensive resource configuration. As mentioned in &lt;a href=&quot;/optimally-scaling-kafka-consumer-applications&quot;&gt;Scaling kafka consumers&lt;/a&gt;, VPA helps remove manual resource configuration. So, we decided to let the SinktoS3 pipeline run on VPA, but this exposed a new set of problems.&lt;/p&gt;

&lt;p&gt;We tested our hypothesis on one of the highest traffic pipelines with parallel goroutines (workers). When the pipeline was left running on VPA, it tried optimising the resources by slowly reducing from &lt;strong&gt;2.5 cores&lt;/strong&gt; to &lt;strong&gt;2.05 cores&lt;/strong&gt;, and then to &lt;strong&gt;1.94 cores&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;CPU requests dropped from 2.05 cores to 1.94 cores, since the maximum performance can be seen at ~1.7 cores.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As you can see from the image above, CPU usage and performance reduced significantly after VPA changed the CPU cores to less than 2. The pipeline ended up with a huge backlog to clear and although it had resources on pod (around 1.94 cores), it did not process any faster and instead, slowed down significantly, resulting in throttling.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image7.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;From the image above, we can see that after VPA scaled the limits of CPU down to 1.94 cores per pod, there was a sudden drop in CPU usage in each of the pods.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Stream production rate&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;You can see that at 21:00, CPU usage reached a maximum of 80%. This value dropped to around 50% between 10:00 to 12:00, which is our consecutive peak production rate.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image8.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Significant drop in consumption rate from Day_Before&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image6.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Consumer lag in terms of records pending to be consumed and in terms of minutes&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In the image above, we compared this data with trends from previous data, where the purple line indicates the day before. We noticed a significant drop in consumption rate compared to the day before, which resulted in consumer lag. This drop was surprising since we didn’t tweak the application configuration. The only change was done by VPA, which brought the CPU request and limit down to less than 2 cores.&lt;/p&gt;

&lt;p&gt;To revert this change, we redeployed the pipeline by retaining the same application setting but adjusting the minimum VPA limit to 2 cores. This helps to prevent VPA from bringing down the CPU cores below 2. With this simple change, performance and CPU utilisation improved almost instantly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;CPU usage percentage jumped back up to ~95%&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Pipeline consumption rate compared to Day_Before&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In the image above, we compared the data with trends from the day before (indicated in purple), where the pipeline was lagging and had a large backlog. You can see that the improved consumption rate was even better than the day before and the application consumed even more records. This is because it was catching up on the backlog from the previous consumer lag.&lt;/p&gt;

&lt;h4 id=&quot;deep-dive-into-the-root-cause&quot;&gt;Deep dive into the root cause&lt;/h4&gt;

&lt;p&gt;This significant improvement just from increasing CPU allocation from 1.94 to 2 cores was unexpected as we had &lt;a href=&quot;http://go.uber.org/automaxprocs&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AUTO-GOMAXPROCS&lt;/code&gt;&lt;/a&gt; enabled in our SPF pipelines and this only uses integer values for CPU.&lt;/p&gt;

&lt;p&gt;Upon &lt;a href=&quot;https://blog.devgenius.io/know-gomaxprocs-before-deploying-your-go-app-to-kubernetes-7a458fb63af1&quot;&gt;further investigation&lt;/a&gt;, we found that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOMAXPROCS&lt;/code&gt; is useful to control the CPU that golang uses on a kubernetes node when kubernetes Cgroup masks the actual CPU cores of the nodes. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOMAXPROCS&lt;/code&gt; only allocates the requested resources of the pod, hence configuring this value correctly helps the runtime to preallocate the correct CPU resources.&lt;/p&gt;

&lt;p&gt;Without configuring &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOMAXPROCS&lt;/code&gt;, go runtime assumes the node’s entire CPU capacity is available for its execution, which is sub-optimal when we run the Golang application on Kubernetes. Thus, it is important to configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOMAXPROCS&lt;/code&gt; correctly so your application pre-allocates the right number of threads based on CPU resources. More details can be found in &lt;a href=&quot;https://blog.devgenius.io/know-gomaxprocs-before-deploying-your-go-app-to-kubernetes-7a458fb63af1&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s look at how Kubernetes resources relate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GOMAXPROCS&lt;/code&gt; value in the following table:&lt;/p&gt;

&lt;table class=&quot;table&quot; border=&quot;1&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Kubernetes resources&lt;/th&gt;
      &lt;th&gt;GOMAXPROCS value&lt;/th&gt;
      &lt;th&gt;Remarks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;2.5 core&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Go runtime will just take and utilise 2 cores efficiently.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2 core&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Go runtime will take and utilise the maximum CPU of the pod efficiently if the workload requires it.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1.5 core&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;AUTO-GOMAXPROCS will set the value as &lt;strong&gt;1&lt;/strong&gt; since it rounds down the &lt;strong&gt;non-integer&lt;/strong&gt; CPU value to an integer number. Hence the performance will be the same as if you had 1 core CPU.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.5 core&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;AUTO-GOMAXPROCS will set the value as &lt;strong&gt;1&lt;/strong&gt; CPU as the minimum allowed value for GOMAXPROCS is &lt;strong&gt;1&lt;/strong&gt;. Here we will see some throttling as Kubernetes will only give 0.5 core but runtime configures itself as it would have 1  hence it will starve for a few CPU cycles.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;issue-with-vpa&quot;&gt;Issue with VPA&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler&quot;&gt;vertical pod autoscaler&lt;/a&gt; enables you to easily scale pods vertically so you don’t have to make manual adjustments. It automatically allocates resources based on usage and allows proper scheduling so that there will be appropriate resources available for each pod. However, in our case, the throttling and CPU starvation issue was because VPA brought resources down to less than 2 cores.&lt;/p&gt;

&lt;p&gt;To better visualise the issue, let’s use an example. Assume that this application needs roughly &lt;strong&gt;1.7 cores&lt;/strong&gt; to perform all its operations without any resource throttling. Let’s see how the VPA journey in this scenario looks like and where it will fail to correctly scale.&lt;/p&gt;

&lt;table class=&quot;table&quot; border=&quot;1&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Timeline&lt;/th&gt;
      &lt;th&gt;VPA recommendation&lt;/th&gt;
      &lt;th&gt;CPU Utilisation&lt;/th&gt;
      &lt;th&gt;AUTO-GOMAXPROCS&lt;/th&gt;
      &lt;th&gt;Remarks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;T0&lt;/td&gt;
      &lt;td&gt;0.5 core&lt;/td&gt;
      &lt;td&gt;&amp;gt;90%&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Throttled by Kubernetes Cgroup as it does give only 0.5 core.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;T1&lt;/td&gt;
      &lt;td&gt;1 core&lt;/td&gt;
      &lt;td&gt;&amp;gt;90%&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;CPU utilisation will still be &amp;gt;90% as GOMAXPROCS setting for the application remains the same. In reality, it will need even more.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;T2&lt;/td&gt;
      &lt;td&gt;1.2 core&lt;/td&gt;
      &lt;td&gt;&amp;lt;85%&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Since the application actually needs more resources, VPA sets a non-integer value but GOMAXPROCS never utilised that extra resource and continued to throttle. Now, VPA computes that the CPU is underutilised and it won't scale further.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;T3&lt;/td&gt;
      &lt;td&gt;2 core (manual override)&lt;/td&gt;
      &lt;td&gt;80-90%&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Since the application has enough resources, it will perform most optimally without throttling and will have maximum throughput.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;During our investigation, we saw that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AUTO-GOMAXPROCS&lt;/code&gt; sets an integer value (minimum 1). To avoid CPU throttling, we need VPA to propose integer values while scaling.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/kubernetes/autoscaler/releases/tag/vertical-pod-autoscaler-0.13.0&quot;&gt;v0.13 of VPA&lt;/a&gt;, this &lt;a href=&quot;https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md%23using-cpu-management-with-static-policy&quot;&gt;feature&lt;/a&gt; is available but only for Kubernetes versions &lt;strong&gt;≥1.25&lt;/strong&gt; – see #5313 in the image below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/performance-bottlenecks/image9.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We acknowledge that if we define a default minimum integer CPU value of 1 core for Coban’s stream processing pipelines, it might be excessive for those that only require less than 1 core. So we propose to &lt;strong&gt;only enable&lt;/strong&gt; this default setting for pipelines with heavy resource requirements and require more than 1 core.&lt;/p&gt;

&lt;p&gt;That said, you should make this decision by evaluating your application’s needs. For example, some Coban pipelines still run on VPA with less than one core but they do not experience any lag. As we mentioned earlier  AUTO-GOMAXPROCS would be configured to 1 in this case, still they can catch up with message production rates. However, technically these pipelines are actually throttled and do not perform optimally but these pipelines don’t have consumer lag.&lt;/p&gt;

&lt;p&gt;As we move from single to concurrent goroutine processing, we need more intensive CPU allocation. In the following table, we consider some scenarios where we have a few pipelines with heavy workloads that are not able to catch up with the production rate.&lt;/p&gt;

&lt;table class=&quot;table&quot; border=&quot;1&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Actual CPU requirement&lt;/th&gt;
      &lt;th&gt;VPA recommendation (after upgrade to v0.13)&lt;/th&gt;
      &lt;th&gt;GOMAXPROCS value&lt;/th&gt;
      &lt;th&gt;Remarks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;1 core&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Optimal setting for this pipeline. It should not lag and should utilise the CPU resources optimally via concurrent goroutines.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;No CPU throttling and no lag. But not very cost efficient.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1.8&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Optimal performance with no lag and cost efficiency.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;From this experience, we learnt several things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Incorrect GOMAXPROCS configuration can lead to significant throttling and CPU starvation issues.&lt;/li&gt;
  &lt;li&gt;Autoscaling solutions are important, but can only take you so far. Depending on your application needs, manual intervention might still be needed to ensure optimal performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 May 2023 01:18:00 +0000</pubDate>
        <link>https://engineering.grab.com/performance-bottlenecks-go-apps</link>
        <guid isPermaLink="true">https://engineering.grab.com/performance-bottlenecks-go-apps</guid>
        
        <category>Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we improved our iOS CI infrastructure with observability tools</title>
        <description>&lt;p&gt;&lt;small&gt;&lt;em&gt;Note: Timestamps used in this article are in UTC+8 Singapore time, unless stated otherwise.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;When we upgraded to Xcode 13.1 in April 2022, we noticed a few issues such as instability of the CI tests and other problems related to the switch to Xcode 13.1. &lt;/p&gt;

&lt;p&gt;After taking a step back, we investigated this issue by integrating some observability tools into our iOS CI development process. This gave us a comprehensive perspective of the entire process, from the beginning to the end of the UITest job. In this article, we share the improvements we made, the insights we gathered, and the impact of these improvements on the overall process and resource utilisation.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;In the following sections, we elaborate the various steps we took to investigate the issues, like unstable CI tests and high CPU utilisation, and the improvements we made to make our iOS CI infrastructure more reliable.&lt;/p&gt;

&lt;h3 id=&quot;analyse-xcode-131-cpu-utilisation&quot;&gt;Analyse Xcode 13.1 CPU utilisation&lt;/h3&gt;

&lt;p&gt;As an iOS developer, we are certain that you have also experienced Spotlight process-related CPU usage problems with Xcode 13.1, which have since been resolved in Xcode 13.2. After investigating, we found that the CPU usage issues were one of the root causes of UITest’s instability and it was something we needed to fix urgently. We decided not to wait for Apple’s update as it would cost us more time to perform another round of migration.&lt;/p&gt;

&lt;p&gt;Before we started UITest, we moved the spotlight.app into a new folder. When the test was complete, we restored the application to its original location. This significantly decreased CPU utilisation by more than 50%.&lt;/p&gt;

&lt;p&gt;This section helps you better visualise how the different versions of Xcode affected CPU utilisation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/iOS-CI-infrastructure-with-observability-tools/image6.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Xcode 12.1&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/iOS-CI-infrastructure-with-observability-tools/image1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Xcode 13.1 before fix&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/iOS-CI-infrastructure-with-observability-tools/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Xcode 13.1 after fix&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;remove-ios-safaris-dependency-during-deep-link-testing&quot;&gt;Remove iOS Safari’s dependency during deep link testing&lt;/h3&gt;

&lt;p&gt;As a superapp, there are countless scenarios that need to be thoroughly tested at Grab before the feature is released in production. One of these tests is deep link testing.&lt;/p&gt;

&lt;p&gt;More than 10% of the total number of tests are deep link tests. Typically, it is advised to mock the dependencies throughout the test to ensure that it runs quickly and reliably. However, this creates another reliance on iOS Safari.&lt;/p&gt;

&lt;p&gt;As a result, we created a mock browser in UITest. We used the URL to the mock browser as the launch argument, and the same URL is then called back. This method results in a 20% reduction in CI time and more stable tests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/iOS-CI-infrastructure-with-observability-tools/image4.gif&quot; alt=&quot;&quot; style=&quot;width:20% ;height:20% &quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;boot-the-ios-simulator-with-permission&quot;&gt;Boot the iOS simulator with permission&lt;/h3&gt;

&lt;p&gt;It is always a good idea to reset the simulator before running UITest so that there are no residual presets or simulated data from a different test. Additionally, using any of the simulator’s services (location, ATT, contacts, etc.) will prompt the simulator to request permission, which slows down execution. We used UIInterruptionHandler (a handler block for managing alerts and other dialogues) to manage asynchronous UI interruptions during the test.&lt;/p&gt;

&lt;p&gt;We wanted to reduce the time taken for test execution, which we knew includes many permissions. Therefore, in order to speed up execution, we boot the simulator with permissions. This removes the need for permissions during UITest, which speeds up performance by 5%.&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/iOS-CI-infrastructure-with-observability-tools/image7.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;monitor-http-traffic-during-the-uitest&quot;&gt;Monitor HTTP traffic during the UITest&lt;/h3&gt;

&lt;p&gt;When writing tests, it is important to mock all resources as this enables us to focus on the code that’s being tested and not how external dependencies interact or respond. However, with a large team working concurrently, it can be challenging to ensure that nothing is actually downloaded from the internet.&lt;/p&gt;

&lt;p&gt;Developers often make changes to code, and UITests are essential for ensuring that these modifications do not adversely affect existing functionality. It is advised to mock all dependencies while writing tests to simulate all possible behavior. We discovered that a significant number of resources were being downloaded each time we ran the tests, which was highly inefficient.&lt;/p&gt;

&lt;p&gt;In large teams working simultaneously, preventing downloads from the internet can be quite challenging. To tackle this issue, we devised a custom tool that tracks all URLs accessed throughout the UITest. This enabled us to identify resources being downloaded from the internet during the testing process.&lt;/p&gt;

&lt;p&gt;By using our custom tool to analyse network traffic, we were able to ensure that no resources were being downloaded during testing. Instead, we relied on mocked dependencies, resulting in reduced testing times and improved stability.&lt;/p&gt;

&lt;h3 id=&quot;gitlab-load-runner-analysis&quot;&gt;GitLab load runner analysis&lt;/h3&gt;

&lt;p&gt;At Grab, we have many teams of developers who maintain the app, make code changes, and raise merge requests (MRs) on a daily basis. To make sure that new changes don’t conflict with existing code, these MRs are integrated with CI.&lt;/p&gt;

&lt;p&gt;Additionally, to manage the number of MRs, we maintain a list of clusters that run test runners concurrently for better resource utilisation and performance. We frequently run these tests to determine how many parallel processors are required for stable results.&lt;/p&gt;

&lt;p&gt;####Return HTTP responses to the local mock server&lt;/p&gt;

&lt;p&gt;We have a tool that we use to mock API requests, which we improved to also support HTML responses. This increases the scope of testing and ensures the HTML response sequences work properly.&lt;/p&gt;

&lt;h3 id=&quot;use-explicit-waiting-commands&quot;&gt;Use explicit waiting commands&lt;/h3&gt;

&lt;p&gt;When running multiple tests, timing issues are inevitable and they cause tests to occasionally pass and fail. To mitigate this, most of the developers prefer to add a sleep command so there is time for the element to render properly before we verify it – but this slows down execution. In order to improve CI execution, we introduced a link that allows us to track sleep function usage and suggest developers use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;waitForExistence&lt;/code&gt; wrappers in UI tests.&lt;/p&gt;

&lt;h3 id=&quot;track-each-failure-state&quot;&gt;Track each failure state&lt;/h3&gt;

&lt;p&gt;With large codebases, it is quite common to see flakiness in UITests, where tests occasionally succeed and fail without any code changes. This means that test results can be inconsistent and in some cases, faulty. Faulty testing can be frustrating, and quite expensive. This is because engineers need to re-trigger entire builds, which ends up consuming more time.&lt;/p&gt;

&lt;p&gt;Initially, we used an internal tool that required all tests to pass on the first run, before merging was allowed. However, we realised that this significantly increased engineers’ manual retry time, hence, we modified the rules to allow merging as long as a subsequent retry passes the tests. This minor change improved our engineers’ CI overall experience and did not result in more flaky tests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;img/iOS-CI-infrastructure-with-observability-tools/image5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;Our journey to improve iOS CI infrastructure is still ongoing, but from this experience, we learnt several things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Focus on the feature being tested by ensuring all external responses are mocked.&lt;/li&gt;
  &lt;li&gt;A certain degree of test flakiness is expected, but you should monitor past trends. If flakiness increases, there’s probably a deeper lying issue within your code.&lt;/li&gt;
  &lt;li&gt;Regularly monitor resource utilisation and performance – detecting a sudden spike early could save you a lot of time and money.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 18 May 2023 04:39:00 +0000</pubDate>
        <link>https://engineering.grab.com/iOS-CI-infrastructure-with-observability-tools</link>
        <guid isPermaLink="true">https://engineering.grab.com/iOS-CI-infrastructure-with-observability-tools</guid>
        
        <category>iOS</category>
        
        <category>Mobile</category>
        
        <category>Engineering</category>
        
        <category>UITesting</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>2.3x faster using the Go plugin to replace Lua virtual machine</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We’re excited to share with you the latest update on our open-source project &lt;a href=&quot;https://github.com/kelindar/talaria&quot;&gt;Talaria&lt;/a&gt;. In our efforts to improve performance and overcome infrastructure limitations, we’ve made significant strides by implementing the &lt;a href=&quot;https://pkg.go.dev/plugin&quot;&gt;Go plugin&lt;/a&gt; to replace Lua VM.&lt;/p&gt;

&lt;p&gt;Our team has found that the &lt;a href=&quot;https://pkg.go.dev/plugin&quot;&gt;Go plugin&lt;/a&gt; is roughly 2.3x faster and uses 2.3x less memory than the Lua VM. This significant performance boost has helped us improve overall functionality, scalability, and speed.&lt;/p&gt;

&lt;p&gt;For those who aren’t familiar, Talaria is a distributed, highly available, and low-latency time-series database that’s designed for Big Data systems. &lt;a href=&quot;https://engineering.grab.com/big-data-real-time-presto-talariadb&quot;&gt;Originally developed and implemented at Grab&lt;/a&gt;, Talaria is a critical component in processing millions and millions of transactions and connections every day, which demands scalable, data-driven decision-making.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;One of the methods we previously used for processing ingested data was &lt;a href=&quot;https://github.com/talariadb/talaria/blob/51560d23faed1c0d8174531142ef3314cfdc86b1/internal/scripting/script_test.go#L14&quot;&gt;Lua script&lt;/a&gt;. This method allowed users to customise the ingestion process, providing a high degree of flexibility.&lt;/p&gt;

&lt;p&gt;The config below is an example of using Lua script to JSON encode the row as a data column:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;computed:
  - name: data
    type: json
    func: |
      local json = require(&quot;json&quot;)
      function main(row)
        return json.encode(row)
      end     
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;
&lt;p&gt;We found that loading a Lua script required launching a Lua virtual machine (VM) to execute the script, which had a significant impact on performance, especially when ingesting large amounts of events.&lt;/p&gt;

&lt;p&gt;This performance issue led us to reevaluate our approach to processing ingested data and make changes to improve Talaria’s performance.&lt;/p&gt;

&lt;p&gt;As a result, this is the code we used on Lua VM to run the trim, remove keys “key1”, “key2”, “key3”, “key4”, “key5”, in the ingested data:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import &quot;github.com/kelindar/lua&quot;

func luaTrim() string {
    s, err := lua.FromString(&quot;test.lua&quot;, `
    local json = require(&quot;json&quot;)
    local keys = {
    &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;, &quot;key4&quot;, &quot;key5&quot;,
    }
    function main(input)
        local data = json.decode(input)
        for i, key in ipairs(keys) do
            data[key] = nil
        end
        return json.encode(data)
    end
`)
    if err != nil {
        panic(err)
    }
    result, err := s.Run(context.Background(), jsonstr)
    if err != nil {
        panic(err)
    }
    return result.String()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the benchmark, using Lua VM is 1000 times slower and uses 1000 times more memory than Golang’s native function on a Trim function:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;BenchmarkTrim-12  &lt;/th&gt;
      &lt;th&gt;543541 &lt;/th&gt;
      &lt;th&gt;2258 ns/op&lt;/th&gt;
      &lt;th&gt;848 B/op&lt;/th&gt;
      &lt;th&gt;12 allocs/op&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;BenchmarkLuaTrim-12 &lt;/th&gt;
      &lt;th&gt;553&lt;/th&gt;
      &lt;th&gt;2105506 ns/op&lt;/th&gt;
      &lt;th&gt;5006319 B/op&lt;/th&gt;
      &lt;th&gt;10335 allocs/op&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;

&lt;p&gt;But, anything can be improved by adding a cache, what if we cache the Lua VM and reuse them? Here is the new improved benchmark:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;BenchmarkTrim-8&lt;/th&gt;
      &lt;th&gt;232105 &lt;/th&gt;
      &lt;th&gt;4995 ns/op &lt;/th&gt;
      &lt;th&gt;2192 B/op &lt;/th&gt;
      &lt;th&gt;53 allocs/op&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;BenchmarkLuaTrim-8&lt;/th&gt;
      &lt;th&gt;97536&lt;/th&gt;
      &lt;th&gt;12108 ns/op &lt;/th&gt;
      &lt;th&gt;4573 B/op &lt;/th&gt;
      &lt;th&gt;121 allocs/op&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;

&lt;p&gt;So we can conclude that Lua VMs are roughly 2.3x faster and use 2.3x less memory than Golang’s native function.&lt;/p&gt;

&lt;h2 id=&quot;use-the-go-plugin-as-lua-vm-to-execute-custom-code&quot;&gt;Use the Go plugin as Lua VM to execute custom code&lt;/h2&gt;
&lt;p&gt;We came up with the idea of using a &lt;a href=&quot;https://developer.ibm.com/tutorials/l-dynamic-libraries/&quot;&gt;Linux shared library&lt;/a&gt; to execute the custom function instead of using Lua VM to run the custom script. Maybe you will be more familiar with the files with suffixes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.so&lt;/code&gt;; they are shared libraries designed to package similar functionality in a single unit and shared with other developers so that they can call the function without writing it again.&lt;/p&gt;

&lt;p&gt;In Golang, a similar idea is called &lt;a href=&quot;https://pkg.go.dev/plugin&quot;&gt;Go plugin&lt;/a&gt;, which allows you to build Golang code as a shared library (Golang names it a plugin). Open this file and call the Go function inside this plugin.&lt;/p&gt;

&lt;h3 id=&quot;how-to-use-the-go-plugin&quot;&gt;How to use the Go plugin&lt;/h3&gt;
&lt;p&gt;Let’s say you have a function F that wants to be called via the plugin.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;package main
import &quot;fmt&quot;
func F() { fmt.Printf(&quot;Hello, world&quot;) }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After writing the function F, you can compile it as a Go plugin file f_plugin.so via Go build -buildmode=plugin -o f_plugin.so. And you can open the file and use the function F like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p, err := plugin.Open(&quot;f_plugin.so&quot;)
if err != nil {
    panic(err)
}
f, err := p.Lookup(&quot;F&quot;)
if err != nil {
    panic(err)
}
f.(func())() // prints &quot;Hello, world&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;go-plugin-benchmark&quot;&gt;Go plugin benchmark&lt;/h3&gt;
&lt;p&gt;Here is the result that compares Golang native function, Golang plugin call.&lt;/p&gt;

&lt;p&gt;Golang native function: 2.3x faster and 2.3x lesser memory than using the Lua VM.
Golang plugin call has almost the same performance as Golang native function.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;BenchmarkNativeFunctionCall-12&lt;/th&gt;
      &lt;th&gt;2917465 &lt;/th&gt;
      &lt;th&gt;401.7 ns/op &lt;/th&gt;
      &lt;th&gt;200 B/op&lt;/th&gt;
      &lt;th&gt;6 allocs/op&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;BenchmarkPluginCall-12&lt;/th&gt;
      &lt;th&gt;2778988 &lt;/th&gt;
      &lt;th&gt;447.1 ns/op  &lt;/th&gt;
      &lt;th&gt;200 B/op &lt;/th&gt;
      &lt;th&gt;6 allocs/op&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;

&lt;h3 id=&quot;integrated-into-talaria&quot;&gt;Integrated into Talaria&lt;/h3&gt;
&lt;p&gt;This is the MR we integrated the Go plugin into Talaria: &lt;a href=&quot;https://github.com/talariadb/talaria/pull/87&quot;&gt;https://github.com/talariadb/talaria/pull/87&lt;/a&gt;, adding it as a loader like LuaLoader.&lt;/p&gt;

&lt;p&gt;They both implemented the Handler interfaces.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;type Handler interface {
    Load(uriOrCode string) (Handler, error)
    String() string
    Value(map[string]interface{}) (interface{}, error)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The implementation of this interface is listed here:&lt;/p&gt;

&lt;h4 id=&quot;for-lua-loader&quot;&gt;For Lua loader&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Load&lt;/strong&gt;: Load the Lua code or Lua script file path (local file path or s3 path) as the loader.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;String&lt;/strong&gt;: Return “lua” so that we can call it to get what the loader is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt;: Run the Lua script, and take the arg as input.&lt;/p&gt;

&lt;h4 id=&quot;for-go-plugin-loader&quot;&gt;For Go plugin loader&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Load&lt;/strong&gt;: Read the plugin file path (local file path or s3 path) as the plugin, lookup the function name defined by the user, save the function for later use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;String&lt;/strong&gt;: Return “plugin” so that we can call it to get what the loader is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt;: Run the saved function, take the arg as input.&lt;/p&gt;

&lt;h2 id=&quot;things-you-need-to-notice&quot;&gt;Things you need to notice&lt;/h2&gt;
&lt;p&gt;The Go version you used to build the  Golang plugin must be the same as the service used in that plugin. We use Docker to build the service, so that we can ensure the Go version is the same.&lt;/p&gt;

&lt;h2 id=&quot;reference-benchmark-plugin-and-lua&quot;&gt;Reference (Benchmark plugin and LUA)&lt;/h2&gt;
&lt;p&gt;https://github.com/atlas-comstock/talaria_benchmark/tree/master/benchmark_plugin_and_lua&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 15 May 2023 01:23:05 +0000</pubDate>
        <link>https://engineering.grab.com/faster-using-the-go-plugin-to-replace-Lua-VM</link>
        <guid isPermaLink="true">https://engineering.grab.com/faster-using-the-go-plugin-to-replace-Lua-VM</guid>
        
        <category>Engineering</category>
        
        <category>Virtual machines</category>
        
        <category>Faster</category>
        
        <category>Go plugin</category>
        
        <category>Lua VM</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Safer deployment of streaming applications</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; framework has gained popularity as a real-time stateful stream processing solution for distributed stream and batch data processing. Flink also provides data distribution, communication, and fault tolerance for distributed computations over data streams. To fully leverage Flink’s features, Coban, Grab’s real-time data platform team, has adopted Flink as part of our service offerings.&lt;/p&gt;

&lt;p&gt;In this article, we explore how we ensure that deploying Flink applications remain safe as we incorporate the lessons learned through our &lt;a href=&quot;/our-journey-to-continuous-delivery-at-grab&quot;&gt;journey to continuous delivery&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/safer-flink-deployments/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Flink platform architecture within Coban&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Users interact with our systems to develop and deploy Flink applications in three different ways.&lt;/p&gt;

&lt;p&gt;Firstly, users create a Merge Request (MR) to develop their Flink applications on our Flink Scala repository, according to business requirements. After the MR is merged, GitOps Continuous Integration/Continuous Deployment (CI/CD) automatically runs and dockerises the application, allowing the containerised applications to be deployed easily.&lt;/p&gt;

&lt;p&gt;Secondly, users create another MR to our &lt;a href=&quot;/securing-gitops-pipeline&quot;&gt;infrastructure as a code&lt;/a&gt; repository. The GitOps CI/CD that is integrated with Terraform runs and configures the created Spinnaker application. This process configures the Flink application that will be deployed.&lt;/p&gt;

&lt;p&gt;Finally, users trigger the actual deployment of the Flink applications on Spinnaker, which orchestrates the deployment of the Docker image onto our Kubernetes cluster. Flink applications are deployed as standalone clusters in Grab to ensure resource isolation.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;The main issue we noticed with streaming pipelines like these, is that they are often interconnected, where application A depends on application B’s output. This makes it hard to find a solution that perfectly includes integration tests and ensures that propagated changes do not affect downstream applications.&lt;/p&gt;

&lt;p&gt;However, this problem statement is too large to solve with a single solution. As such, we are narrowing the problem statement to focus on ensuring safety of our applications, where engineers can deploy Flink applications that will be rolled back if they fail health checks. In our case, the definition of a Flink application’s health is limited to the uptime of the Flink application itself.&lt;/p&gt;

&lt;p&gt;It is worth noting that Flink applications are designed to be &lt;strong&gt;stateful streaming applications&lt;/strong&gt;, meaning a “state” is shared between events (stream entities) and thus, past events can influence the way current events are processed. This also implies that traditional deployment strategies do not apply to the deployment of Flink applications.&lt;/p&gt;

&lt;h3 id=&quot;current-strategy&quot;&gt;Current strategy&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/safer-flink-deployments/image3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Current deployment stages&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Figure 2, our current deployment stages are split into three parts:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Delete current deployment&lt;/strong&gt;: Remove current configurations (if applicable) to allow applications to pick up the new configurations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bake (Manifest)&lt;/strong&gt;: Bake the Helm charts with the provided configurations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deploy (Manifest)&lt;/strong&gt;: Deploy the charts onto Kubernetes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Over time, we learnt that this strategy can be risky. Part 2 can result in a loss of Flink application states due to how internal CI/CD processes are set up. There is also no easy way to rollback if an issue arises. Engineers will need to revert all config changes and rollback the deployment &lt;strong&gt;manually&lt;/strong&gt; by re-deploying the older Docker image – which results in slower operation recovery.&lt;/p&gt;

&lt;p&gt;Lastly, there are no in-built monitoring mechanisms that perform regular health probes. Engineers need to manually monitor their applications to see if their deployment was successful or if they need to perform a rollback.&lt;/p&gt;

&lt;p&gt;With all these issues, deploying Flink applications for engineers are often stressful and fraught with uncertainty. Common mitigation strategies are &lt;em&gt;canary&lt;/em&gt; and &lt;em&gt;blue-green deployments&lt;/em&gt;, which we cover in the next section.&lt;/p&gt;

&lt;h3 id=&quot;canary-deployments&quot;&gt;Canary deployments&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/safer-flink-deployments/image1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Canary deployment&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In canary deployments, you gradually roll out new versions of the application in parallel with the production version, while serving a percentage of total traffic before promoting it gradually.&lt;/p&gt;

&lt;p&gt;This does not work for Flink deployments due to the nature of stream processing. Applications are frequently required to do streaming operations like stream joining, which involves matching related events in different Kafka topics. So, if a Flink application is only receiving a portion of the total traffic, the data generated will be considered inaccurate due to incomplete data inputs.&lt;/p&gt;

&lt;h3 id=&quot;blue-greendeployments&quot;&gt;Blue-green deployments&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/safer-flink-deployments/image6.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Blue-green deployment&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Blue-green deployments work by running two versions of the application with a Load Balancer that acts as a traffic switch, which determines which version traffic is directed to.&lt;/p&gt;

&lt;p&gt;This method might work for Flink applications if we only allow one version of the application to consume Kafka messages at any point in time. However, we noticed some issues when switching traffic to another version. For example, the state of both versions will be inconsistent because of the different data traffic each version receives, which complicates the process of switching Kafka consumption traffic.&lt;/p&gt;

&lt;p&gt;So if there’s a failure and we need to rollback from Green to Blue deployment, or vice versa, we will need to take an extra step and ensure that before the failure, the data traffic received is &lt;strong&gt;exactly the same&lt;/strong&gt; for both deployments.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;As previously mentioned, it is crucial for streaming applications to ensure that at any point in time, only one application is receiving data traffic to ensure data completeness and accuracy. Although employing blue-green deployments can technically fulfil this requirement, the process must be modified to handle state consistency such that both versions have the same starting internal state and receive the &lt;strong&gt;same data traffic&lt;/strong&gt; as each other, if a rollback is needed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/safer-flink-deployments/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Visualised deployment flow&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This deployment flow will operate in the following way:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Collect metadata regarding current application&lt;/li&gt;
  &lt;li&gt;Take savepoint and stop the current application&lt;/li&gt;
  &lt;li&gt;Clear up high availability configurations&lt;/li&gt;
  &lt;li&gt;Bake and deploy the new application&lt;/li&gt;
  &lt;li&gt;Monitor application and rollback if the health check fails&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s elaborate on the key changes implemented in this new process.&lt;/p&gt;

&lt;h3 id=&quot;savepointing&quot;&gt;Savepointing&lt;/h3&gt;

&lt;p&gt;Flink’s savepointing feature helps address the issue of state consistency and ensures safer deployments.&lt;/p&gt;

&lt;p&gt;A savepoint in Flink is a snapshot of a Flink application’s state at the point in time. This savepoint allows us to pause the Flink application and restore the application to this snapshot state, if there’s an issue.&lt;/p&gt;

&lt;p&gt;Before deploying a Flink application, we perform a savepoint via the Flink API before killing the current application. This would enable us to save the current state of the Flink application and rollback if our deployment fails – just like how you would do a quick save before attempting a difficult level when playing games. This mechanism ensures that both deployment versions have the same internal state during deployment as they both start from the same savepoint.&lt;/p&gt;

&lt;p&gt;Additionally, this feature allows us to easily handle Kafka offsets since these consumed offsets are stored as part of the savepoint. As Flink manages their own state, they don’t need to rely on Kafka’s consumer offset management. With this savepoint feature, we can ensure that the application receives the same data traffic post rollback and that no messages are lost due to processing on the failed version.&lt;/p&gt;

&lt;h3 id=&quot;monitoring&quot;&gt;Monitoring&lt;/h3&gt;

&lt;p&gt;To consistently monitor Flink applications, we can conduct health probes to the respective API endpoints to check if the application is stuck in a restart state or if it is running healthily.&lt;/p&gt;

&lt;p&gt;We also configured our monitoring jobs to wait for a few minutes for the deployment to stabilise before probing it over a defined duration, to ensure that the application is in a stable running state.&lt;/p&gt;

&lt;h3 id=&quot;rollback&quot;&gt;Rollback&lt;/h3&gt;

&lt;p&gt;If the health checks fail, we then perform an automatic rollback. Typically, Flink applications are deployed as a standalone cluster and a rollback involves changes in one of the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Application and Flink configurations&lt;/li&gt;
  &lt;li&gt;Taskmanager or Jobmanager resource provision&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;application-and-flink-configuration-changes&quot;&gt;Application and Flink configuration changes&lt;/h4&gt;

&lt;p&gt;For configuration changes, we leverage the fact that Spinnaker performs versioned deployment of &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/configmap/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configmap&lt;/code&gt;&lt;/a&gt; resources. In this case, a rollback simply involves mounting the old &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configmap&lt;/code&gt; back onto the Kubernetes deployment.&lt;/p&gt;

&lt;p&gt;To retrieve the old version of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configmap&lt;/code&gt; mount, we can simply utilise Kubernetes’ rollback mechanisms – Kubernetes updates a deployment by creating a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replicaset&lt;/code&gt; with an incremental version before attaching it to the current deployment and scaling the previous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replicaset&lt;/code&gt; to 0. To retrieve previous deployment specs, we just need to list all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replicasets&lt;/code&gt; related to the deployment and find the previous deployed version, before updating the current deployment to mimic the previous template specifications.&lt;/p&gt;

&lt;p&gt;However, this deployment does not contain the number of replicas of previously configured task managers. Kubernetes does not register the number of replicas as part of deployment configuration as this is a dynamic configuration and might be changed during processing due to auto scaling operations.&lt;/p&gt;

&lt;p&gt;Our Flink applications are deployed as standalone clusters and do not use native or yarn resource providers. Coupled with the fact that Flink has strict resource provision, we realised that we do not have enough information to perform rollbacks, without the exact number of replicas created.&lt;/p&gt;

&lt;h4 id=&quot;taskmanager-or-jobmanager-resource-provision-changes&quot;&gt;Taskmanager or Jobmanager resource provision changes&lt;/h4&gt;

&lt;p&gt;To gather information about resource provision changes, we can simply include the previously configured number of replicas as part of our metadata annotation. This allows us to retrieve it in future during rollback.&lt;/p&gt;

&lt;p&gt;Making this change involves creating an additional step of metadata retrieval to retrieve and store previous deployment states as annotations of the new deployment.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;With this solution, the deployment flow on Spinnaker looks like this:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/safer-flink-deployments/image7.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. New deployment flow on Spinnaker&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Engineers no longer need to monitor the deployment pipeline as closely as they get notified of their application’s deployment status via Slack. They only need to interact or take action when they get notified that the different stages of the deployment pipeline are completed.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/safer-flink-deployments/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Slack notifications on deployment status&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;It is also easier to deploy Flink applications since failures and rollbacks are handled automatically. Furthermore, application state management is also automated, which reduces the amount of uncertainties.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;As we work to further improve our deployment pipeline, we will look into extending the capabilities at our monitoring stage to allow engineers to define and configure their own health probes, allowing our deployment configurations to be more extendable.&lt;/p&gt;

&lt;p&gt;Another interesting improvement will be to make this deployment flow seamlessly, ensuring as little downtime as possible by minimising cold start duration.&lt;/p&gt;

&lt;p&gt;Coban also looks forward to pushing more features on our Flink platform to enable our engineers to explore more use cases that utilises real-time data to allow our operations to become auto adaptive and make data-driven decisions.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/savepoints/&quot;&gt;Flink Savepointing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/&quot;&gt;Flink API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/%23updating-a-deployment&quot;&gt;Kubernetes rollback&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 02 May 2023 01:23:05 +0000</pubDate>
        <link>https://engineering.grab.com/safer-flink-deployments</link>
        <guid isPermaLink="true">https://engineering.grab.com/safer-flink-deployments</guid>
        
        <category>Engineering</category>
        
        <category>Deployments</category>
        
        <category>Streaming applications</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
