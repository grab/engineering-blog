<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 02 Dec 2025 01:50:01 +0000</pubDate>
    <lastBuildDate>Tue, 02 Dec 2025 01:50:01 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>A Decade of Defense: Celebrating Grab&apos;s 10th Year Bug Bounty Program</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Ten years ago, we launched our bug bounty program in partnership with &lt;a href=&quot;https://www.hackerone.com/blog&quot;&gt;HackerOne&lt;/a&gt;. Beyond a security initiative, it represented an open invitation to collaborative development.
As pioneers in Southeast Asia, we began the program with 23 initial researchers, and it has since evolved into a global community of security researchers.&lt;/p&gt;

&lt;p&gt;The strategic structure and scope of our Bug Bounty Program, combined with our continuous innovation and experimentation, have successfully captured the attention of the global security research community. Over the past decade, we have partnered with more than 850 active security researchers from HackerOne’s community of over 2 million cybersecurity professionals worldwide. These dedicated researchers work alongside us across borders and time zones, forming a collaborative defense network that helps protect over 187 million users throughout Southeast Asia. Their ongoing participation demonstrates both the maturity of our program and the trust we’ve built within the security research community.&lt;/p&gt;

&lt;p&gt;This milestone reflects the strength of shared purpose and our sustained partnership with the HackerOne platform. It demonstrates the value of human connection and the collective understanding that security is stronger through collaboration. Here’s to a decade of partnership and to many more years of building a safer future, one collaboration at a time!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/decade-of-defense/figure-1.jpg&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Ten years of achievements with our HackerOne partnership.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;evolution-and-growth-adapting-to-a-dynamic-threat-landscape&quot;&gt;Evolution and growth: Adapting to a dynamic threat landscape&lt;/h3&gt;

&lt;p&gt;Over the past ten years, our program has consistently adapted to the dynamic threat landscape and integrated invaluable feedback from our research community. We have grown from a private initiative to a program that consistently ranks among the top 20 worldwide and among the top 3 in Asia on HackerOne. Key milestones from our journey include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expanding our horizons:&lt;/strong&gt; Our scope significantly broadened in 2023-2024, continuously adding new assets and prominently including financial services in Indonesia and AI systems. This expansion provides researchers with more avenues to contribute to Grab’s security.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Focused mobile security:&lt;/strong&gt; We introduced a dedicated bounty table for mobile-specific issues, recognizing the unique challenges of mobile security.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incentivizing excellence:&lt;/strong&gt; We regularly experiment with campaigns of various types and targets, diversifying our reward methods to include both financial rewards and recognition.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evolving vulnerability focus:&lt;/strong&gt; We’ve observed a significant shift in the types of vulnerabilities reported over the decade, moving from foundational issues in early years to more sophisticated and emerging categories recently.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/decade-of-defense/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. The journey of our bug bounty program.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-global-stage-connecting-with-the-best&quot;&gt;The global stage: Connecting with the best&lt;/h3&gt;

&lt;p&gt;Our program’s success is deeply rooted in its vibrant global community, which we actively foster through continuous engagement. Our strategy extends beyond the platform to major live hacking events, including the &lt;strong&gt;ThreatCon Live Hacking Event 2023&lt;/strong&gt; &lt;strong&gt;in Nepal&lt;/strong&gt; and &lt;strong&gt;DEFCON 32’s Live Recon Village 2024 in Las Vegas.&lt;/strong&gt; These initiatives have been instrumental in connecting us with a diverse pool of new talent and strengthening relationships with researchers across different continents. By meeting hackers where they are, we’ve not only brought new expertise into our ecosystem but also demonstrated our commitment to being an accessible and collaborative partner on a global scale.&lt;/p&gt;

&lt;p&gt;The high participation and quality submissions from these events demonstrate the effectiveness of this approach. They’ve expanded our global security testing coverage and strengthened our standing within the worldwide cybersecurity community. Through ongoing interactions and submitted reports, we continue to see that security is a collaborative effort with no borders.&lt;/p&gt;

&lt;h3 id=&quot;exclusive-anniversary-celebrations-global-club-campaigns&quot;&gt;Exclusive anniversary celebrations: Global club campaigns&lt;/h3&gt;

&lt;p&gt;To commemorate our 10th anniversary, we launched three exclusive, invite-only campaigns with HackerOne’s regional clubs in &lt;strong&gt;Germany, Morocco, and India&lt;/strong&gt;. These campaigns served as cultural exchanges, bringing fresh perspectives from outside our core Southeast Asian consumer markets. By engaging with these clubs, we expanded our researcher community and connected with security experts who understand different threat landscapes and methodologies, bringing outside perspectives to our systems.&lt;/p&gt;

&lt;p&gt;In August, we also ran a broader anniversary campaign that drew significant participation from the researcher community, resulting in 461 submissions. &lt;a href=&quot;https://hackerone.com/xchopath?type=user&quot;&gt;xchopath&lt;/a&gt; was awarded the Best Hacker Bonus for their contributions during this campaign.&lt;/p&gt;

&lt;p&gt;These campaigns expanded our global security testing coverage and strengthened relationships with international researcher communities. Beyond vulnerability reports, they functioned as knowledge-sharing initiatives. We connected directly with researchers to learn from their experience and feedback, creating a continuous loop of improvement. This international collaboration also informed our global expansion security strategy by providing insights into how different regions approach digital payments and authentication.&lt;/p&gt;

&lt;p&gt;The anniversary campaigns allowed us to validate our security frameworks against diverse regulatory environments and advanced testing methodologies from established security markets, reinforcing our commitment to maintaining robust security standards.&lt;/p&gt;

&lt;h3 id=&quot;voices-from-our-community&quot;&gt;Voices from our community&lt;/h3&gt;

&lt;p&gt;Behind every vulnerability report is a researcher who chose to help make Grab safer. Their perspectives reveal the human side of our security evolution. These individuals are not just cybersecurity experts; they are partners in our mission to protect millions of users and ensure a safe digital environment. Here are a few testimonies from participants in our past campaigns:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“The triage was very fast despite the time difference, which I really appreciated. The triaging experience was better than other programs. The huge scope and business portal with different user roles made it especially interesting to explore.” – &lt;a href=&quot;https://hackerone.com/artsec?type=user&quot;&gt;&lt;em&gt;ArtSec&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Germany club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“I liked that different countries have different features—this gives me more attack surface to explore. Response time was great, triage was very fast, and I appreciated Grab’s effort in providing fast responses. The scope was huge with a lot of wildcards for reconnaissance.” – &lt;a href=&quot;https://hackerone.com/sicksec?type=user&quot;&gt;&lt;em&gt;Sicksec&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Morocco club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“More than 20 bugs were reported, and was particularly happy that bounties were being paid upon triage. The Germany team spent a lot of time on the educational part, especially for newcomers. Communication overall was very good, and the immediate response even outside working hours was really cool. SSO and authentication is my expertise and I liked that aspect of exploring the platform.” – &lt;a href=&quot;https://hackerone.com/lauritz?type=user&quot;&gt;&lt;em&gt;Lauritz&lt;/em&gt;&lt;/a&gt; &lt;em&gt;[H1 Germany club campaign participant]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-road-ahead-our-commitment-to-a-secure-future&quot;&gt;The road ahead: Our commitment to a secure future&lt;/h3&gt;

&lt;p&gt;With a strong community of security researchers across countries and a decade of collaboration, we’ve built meaningful partnerships. Every vulnerability report represents trust, and every discovery reflects dedication to our shared mission. The program demonstrates our choice to build together rather than work in isolation, to protect rather than exploit, and to collaborate rather than compete.&lt;/p&gt;

&lt;p&gt;While we celebrate our external community, the success of our program relies equally on our dedicated internal teams. Our cybersecurity teams form the operational foundation of this initiative. Their consistent responsiveness and researcher-focused approach have enabled vulnerability reporting to evolve into a genuine partnership, maintaining researcher trust and keeping Grab secure.&lt;/p&gt;

&lt;p&gt;The next ten years will bring challenges we can’t yet imagine, from emerging threats in artificial intelligence to novel cryptographic approaches in a quantum-powered world. We will face them together as a community that spans cultures, time zones, and expertise.&lt;/p&gt;

&lt;p&gt;Together, we’ll continue securing Southeast Asia’s digital future, one partnership, one discovery, one shared achievement at a time.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility, and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people every day to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://www.grab.careers/en/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Dec 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/a-decade-of-defense</link>
        <guid isPermaLink="true">https://engineering.grab.com/a-decade-of-defense</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Real-time data quality monitoring: Kafka stream contracts with syntactic and semantic test</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In today’s data-driven landscape, monitoring data quality has become a critical need for ensuring reliable and efficient data usage across domains. High-quality data is the backbone of AI innovation, driving efficiency and unlocking new opportunities. As decentralized data ownership grows, the ability to effectively monitor data quality is essential for maintaining reliability in data systems.&lt;/p&gt;

&lt;p&gt;Kafka streams, as a vital component of real-time data processing, play a significant role in this ecosystem. However, unreliable data within Kafka streams can lead to errors and inefficiencies for downstream users, and monitoring the quality of data within these streams has always been a challenge. This blog introduces a solution that empowers stream users to define a data contract, specifying the rules that Kafka stream data must adhere to. By leveraging this user-defined data contract, the solution performs automated real-time data quality checks, identifies problematic data as it occurs, and promptly notifies stream owners. This ensures timely action, enabling effective monitoring and management of Kafka stream data quality while supporting the broader goals of data mesh and AI-driven innovation.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;In the past, monitoring Kafka stream data processing lacked an effective solution for data quality validation. This limitation made it challenging to identify bad data, notify users in a timely manner, and prevent the cascading impact on downstream users from further escalating.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Challenges in syntactic and semantic issue identification&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Syntactic issues&lt;/strong&gt;: Refers to schema mismatches between producers and consumers, which can lead to deserialization errors. While schema backward compatibility can be validated upon schema evolution, there are scenarios where the actual data in the Kafka topic does not align with the defined schema. For example, this can occur when a rogue Kafka producer is not using the expected schema for a given Kafka topic. Identifying the specific fields causing these syntactic issues is a typical challenge.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semantic issues&lt;/strong&gt;: Refers to inconsistencies or misalignments between producers and consumers about the expected pattern or significance of each field. Unlike Kafka stream schemas, which act as a data structure contract between producers and consumers, there is no existing framework for stakeholders to define and enforce field-level semantic rules, for example, the expected length or pattern of an identifier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Timeliness challenge in data quality monitoring&lt;/strong&gt;: There is no real-time mechanism to automatically validate data against predefined rules, timely identify quality issues, and promptly alert stream stakeholders. Without real-time stream validation, data quality issues can sometimes persist for periods of time, impacting various online and offline downstream systems before being discovered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Observability challenge for troubleshooting bad data&lt;/strong&gt;: Even when problematic data is identified, stream users face difficulties in pinpointing the exact “poison data” and understanding which fields are incompatible with the schema or violate semantic rules. This lack of visibility complicates Root Cause Analysis and resolution efforts.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Our &lt;a href=&quot;https://engineering.grab.com/an-elegant-platform&quot;&gt;Coban platform&lt;/a&gt; offers a standardized data quality test and observability solution at the platform level, consisting of the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Contract Definition&lt;/strong&gt;: Enables Kafka stream stakeholders to define contracts that include schema agreements, semantic rules that Kafka topic data must comply with, and Kafka stream ownership details for alerting and notifications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automated Test Execution&lt;/strong&gt;: Provides a long running Test Runner to automatically execute real-time tests based on the defined contract.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time Data Quality Issue Identification&lt;/strong&gt;: Detects data issues at both syntactic and semantic levels in real-time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alerts and Result Observability&lt;/strong&gt;: Alerts users, simplifying observation of data quality issues via the platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;p&gt;The solution includes three components: &lt;em&gt;Data Contract Definition, Test Execution &amp;amp; Data Quality Issue Identification, and Result Observability as shown in the architecture diagram in figure 1&lt;/em&gt;. All mentions of “Flow” from here onwards refer to the corresponding processes illustrated in figure 1.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/coban-architecture.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Real-time Kafka Stream Data Quality Monitoring Architecture diagram.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;data-contract-definition&quot;&gt;Data Contract Definition&lt;/h4&gt;

&lt;p&gt;The Coban Platform streamlines the process of defining Kafka stream data contracts, serving as a formal agreement among Kafka stream stakeholders. This includes the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Schema&lt;/strong&gt;: Represents the schema used by the Kafka topic under test and helps the Test Runner to validate schema compatibility across data streams (Flow 1.1).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Configuration&lt;/strong&gt;: Encompasses essential configurations such as the endpoint and topic name, which the platform automatically populates (Flow 1.2).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability Metadata&lt;/strong&gt;: Provides contact information for notifying Kafka stream stakeholders about data quality issues and includes alert configurations for monitoring (Flow 1.3).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Semantic Test Rules&lt;/strong&gt;: Empowers users to define intuitive semantic test rules at the field level. These rules include checks for string patterns, number ranges, constant values, etc. (Flow 1.5).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LLM-Based Semantic Test Rules Recommendation&lt;/strong&gt;: Defining dozens if not hundreds of field-specific test rules can overwhelm users. To simplify this process, the Coban Platform uses LLM-based recommendations to predict semantic test rules using provided Kafka stream schemas and anonymized sample data (Flow 1.4). This feature helps users set up semantic rules efficiently, as demonstrated in the sample UI in figure 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/sample-ui.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Sample UI showcasing LLM-based Kafka stream schema field-level semantic test rules. Note that the data shown is entirely fictional.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;data-contract-transformation&quot;&gt;Data Contract Transformation&lt;/h4&gt;

&lt;p&gt;Once defined, the Coban Platform’s transformation engine converts the data contract into configurations that the Test Runner can interpret (Flow 2.1). This transformation process includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Schema&lt;/strong&gt;: Translates the schema defined in the data contract into a schema reference that the Test Runner can parse.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Configuration&lt;/strong&gt;: Sets up the Kafka stream as a source for the Test Runner.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Observability metadata&lt;/strong&gt;: Sets contact information as configurations of the Test Runner.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka Stream Semantic Test Rules&lt;/strong&gt;: Transforms human-readable semantic test rules into an inverse SQL query to capture the data that violates the defined rules.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/semantic-test-rules.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Illustration of semantic test rules being converted from human-readable formats into inverse SQL queries.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;test-execution--data-quality-issue-identification&quot;&gt;Test Execution &amp;amp; Data Quality Issue Identification&lt;/h3&gt;

&lt;p&gt;Once the Test Configuration Transformation Engine generates the Test Runner configuration (Flow 2.1), the platform automatically deploys the Test Runner.&lt;/p&gt;

&lt;h4 id=&quot;test-runner&quot;&gt;Test Runner&lt;/h4&gt;

&lt;p&gt;The Test Runner utilises FlinkSQL as the compute engine to execute the tests. FlinkSQL was selected for its flexibility in defining test rules as straightforward SQL statements, enabling our platform to efficiently convert data contracts into enforceable rules.&lt;/p&gt;

&lt;h4 id=&quot;test-execution-workflow-and-problematic-data-identification&quot;&gt;Test Execution Workflow And Problematic Data Identification&lt;/h4&gt;

&lt;p&gt;FlinkSQL consumes data from the Kafka topic under test (Flow 2.2) using its own consumer group, ensuring it doesn’t impact other consumers. It runs the inverse SQL query (Flow 2.3) to identify any data that violates the semantic rules or that is syntactically incorrect in the first place. Test Runner captures such data, packages it into a data quality issue event enriched with a test summary, the total count of bad records, and sample bad data, and publishes it to a dedicated Kafka topic (Flow 3.2). Additionally, the platform sinks all such data quality events to an AWS S3 bucket (Flow 3.1) to enable deeper observability and analysis.&lt;/p&gt;

&lt;h3 id=&quot;result-observability&quot;&gt;Result Observability&lt;/h3&gt;

&lt;p&gt;Grab’s in-house data quality observability platform, Genchi, consumes problematic data captured by the Test Runner (Flow 3.3).&lt;/p&gt;

&lt;h4 id=&quot;alerting&quot;&gt;Alerting&lt;/h4&gt;
&lt;p&gt;Genchi sends Slack notifications (Flow 3.5) to stream owners specified in the data contract observability metadata. These notifications include detailed information about stream issues, such as links to sample data in Coban UI, observed windows, counts of bad records, and other relevant details.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/slack-notification.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Sample Slack notifications
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;observability&quot;&gt;Observability&lt;/h4&gt;

&lt;p&gt;Users can access the Coban UI (Flow 3.4), displaying Kafka stream test rules and sample bad records, highlighting fields and values that violate rules.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/real-time-data-quality-monitoring/sample-test-result.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. In this Sample Test Result, the highlighted fields indicate violations of the semantic test rules.
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Since its deployment earlier this year, the solution has enabled Kafka stream users to define contracts with syntactic and semantic rules, automate test execution, and alert users when problematic data is detected, prompting timely action. It has been actively monitoring data quality across 100+ critical Kafka topics. The solution offers the capability to immediately identify and halt the propagation of invalid data across multiple streams.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We implemented and rolled out a solution to assist Grab engineers in effectively monitoring data quality in their Kafka streams. This solution empowers them to establish syntactic and semantic tests for their data. Our platform’s automatic testing feature enables real-time tracking of data quality, with instant alerts for any discrepancies. Additionally, we provide detailed visibility into test results, facilitating the easy identification of specific data fields that violate the rules. This accelerates the process of diagnosing and resolving issues, allowing users to swiftly address production data challenges.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;While our current solution emphasizes monitoring the quality of Kafka streaming data, further exploration will focus on tracing producers to pinpoint the origin of problematic data, as well as enabling more advanced semantic tests such as cross-field validations. Additionally, we aim to expand monitoring capabilities to cover broader aspects like data completeness and freshness, and integrate with &lt;a href=&quot;https://www.gable.ai/&quot;&gt;Gable AI&lt;/a&gt; to detect Data Transfer Object (DTO) changes and semantic regressions in Go producers upon committing code to the Git repository. These enhancements will pave the way for a more robust, multidimensional data quality testing solution across a wider range.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.oreilly.com/library/view/driving-data-quality/9781837635009/&quot;&gt;Driving Data Quality with Data Contracts: A Comprehensive Guide to Building Reliable, Trusted, and Effective Data Platforms&lt;/a&gt; by &lt;a href=&quot;https://www.oreilly.com/search?q=author:%22Andrew%20Jones%22&quot;&gt;Andrew Jones&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebdatam&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/real-time-data-quality-monitoring</link>
        <guid isPermaLink="true">https://engineering.grab.com/real-time-data-quality-monitoring</guid>
        
        <category>Engineering</category>
        
        <category>Kafka</category>
        
        <category>Performance</category>
        
        <category>Data science</category>
        
        <category>Data processing</category>
        
        <category>Real-time streaming</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>SpellVault’s evolution: Beyond LLM apps, towards the agentic future</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, innovation isn’t just about building new features; it’s about evolving our platforms to meet the changing needs of our users and the broader technological landscape. &lt;a href=&quot;https://www.grab.com/sg/inside-grab/stories/ai-llm-productivity-tool-apps-coding/&quot;&gt;SpellVault&lt;/a&gt;, our internal AI platform, exemplifies this philosophy. When SpellVault was first launched, our vision was straightforward: empower everyone at Grab to effortlessly build and manage AI-powered apps without the need for coding. Built on the principles of Retrieval-Augmented Generation (RAG) and enhanced by plugin support, SpellVault rapidly evolved into a powerful productivity engine for the organization, enabling the creation of thousands of apps that drive automation, foster experimentation, and support production use cases.&lt;/p&gt;

&lt;p&gt;As the AI landscape has evolved, SpellVault has grown alongside it. Initially launched as a straightforward no-code app builder for Large Language Models (LLMs), it has now evolved into a cutting-edge platform that embraces the agentic future—a future where AI goes beyond generating responses to reasoning, acting, and dynamically adapting through the use of tools and contextual understanding.&lt;/p&gt;

&lt;p&gt;This article outlines SpellVault’s journey towards an agentic future and how we empower users to build AI Agents that are smarter, more adaptable, and ready for the future.&lt;/p&gt;

&lt;h2 id=&quot;a-no-code-platform-for-building-llm-apps&quot;&gt;A no-code platform for building LLM apps&lt;/h2&gt;

&lt;p&gt;SpellVault was founded with a clear mission: to democratize access to AI for everyone at Grab, regardless of their technical expertise. Initially launched as a no-code LLM app builder, the platform was built on a foundation of RAG pipelines and basic plugin support.&lt;/p&gt;

&lt;p&gt;Early on, we recognized that the true potential of AI apps extends beyond the capabilities of language models alone. Their real value lies in the ability to seamlessly interact with external systems and diverse data sources. This insight drove our commitment to minimizing barriers and ensuring users could access data from various sources with ease. From the very beginning, we centered our efforts on three key focus areas:&lt;/p&gt;

&lt;h4 id=&quot;comprehensive-rag-solution-with-useful-integrations&quot;&gt;Comprehensive RAG solution with useful integrations&lt;/h4&gt;

&lt;p&gt;From the start, the SpellVault team prioritized enabling users to enhance their LLM apps with data through RAG. Rather than solely relying on the LLM’s internal information, we wanted the apps to ground their responses in up-to-date, contextually relevant, and factual information. SpellVault has built-in integrations with knowledge sources such as Wikis, Google Docs, as well as plain text and PDF uploads. These capabilities empower users to build assistants that reference relevant knowledge and provide more accurate, verifiable answers.&lt;/p&gt;

&lt;h4 id=&quot;plugins-to-fetch-information-on-demand&quot;&gt;Plugins to fetch information on demand&lt;/h4&gt;

&lt;p&gt;To move beyond static knowledge retrieval, we needed a way for apps to act dynamically. This was made possible through SpellVault plugins—modular components that allow apps to interact with internal systems (e.g. service dashboards, incident trackers) and external APIs (e.g. search engines, weather data). Rather than being confined to their initial prompt and data, these plugins can fetch fresh information at runtime. From the available plugin types, users can create their own instances of plugins with custom settings, enabling highly specialized functionality tailored to their specific workflows. For instance, with SpellVault’s HTTP plugin, users can define custom endpoints and credentials, enabling their AI apps to make tailored HTTP calls during runtime. These custom plugins have become the backbone of many of our most impactful apps, empowering teams to seamlessly integrate SpellVault with their existing systems and processes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. SpellVault’s early architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;making-spellvault-accessible-via-common-interfaces-web-slack-api&quot;&gt;Making SpellVault accessible via common interfaces: Web, Slack, API&lt;/h4&gt;

&lt;p&gt;One of our primary goals was to make AI seamlessly accessible and useful within the tools users already use—whether it’s a browser or Slack. With SpellVault, users can make their AI apps in minutes and start using them via browser or Slack messaging immediately and intuitively, without requiring any additional setup. We also exposed APIs that enabled other internal services to integrate with SpellVault apps for a variety of use cases. This multi-channel approach ensured that SpellVault wasn’t just a standalone sandbox but a platform woven into existing tools and processes.&lt;/p&gt;

&lt;p&gt;Users quickly adopted the platform, creating thousands of apps for internal productivity gains, automation, and even production use cases. The platform’s success validated our hypothesis that there was significant demand for democratized AI tools within the organization.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. SpellVault’s web interface for LLM App configuration and chat.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;evolution-over-time&quot;&gt;Evolution over time&lt;/h2&gt;

&lt;p&gt;The AI landscape over the past few years has been defined by relentless change. New frameworks, execution paradigms, and standards have emerged in quick succession, each promising to make AI systems more powerful, more reliable, or more extensible. At Grab, we recognized that for SpellVault to stay relevant, it could not remain static. It needed to evolve in tandem with the ever-changing ecosystem, continuously incorporating valuable advancements while ensuring a seamless experience for our users.&lt;/p&gt;

&lt;p&gt;This philosophy of continuous adaptation has guided SpellVault’s journey. From its early days as a simple RAG-powered app builder with a few plugins, the platform grew to support an extensive number of plugin types, richer execution models, and eventually a unified approach to tools. Each step was a response both to the needs of our users and to the shifting definition of what “building with AI” meant in practice. Rather than opting for a complete overhaul, SpellVault has embraced incremental advancements, ensuring that users can seamlessly benefit from new capabilities without disruption.&lt;/p&gt;

&lt;p&gt;This approach to evolution has naturally positioned SpellVault to transition from a platform for LLM apps to one designed for AI agents. The following section delves into this transition in greater detail.&lt;/p&gt;

&lt;h3 id=&quot;expanding-capabilities&quot;&gt;Expanding capabilities&lt;/h3&gt;

&lt;p&gt;Over time, we introduced numerous new capabilities to SpellVault, driven both by user feedback and our commitment to innovation and staying ahead of industry trends. For instance, we extended support for different plugin types, enabling integrations with tools like Slack and Kibana, and continuously added more integrations to enhance the platform’s versatility. We implemented auto-updates for users’ Knowledge Vaults, ensuring their data remained current. With more users building with the platform, ensuring the trustworthiness of responses generated by SpellVault apps became increasingly important. We included citation capability to mitigate some of that concern. Recognizing the need for more precise answers to mathematical problems, we developed a feature that enabled LLMs to solve such problems using Python runtime. Additionally, many users requested an automated way to trigger their LLM apps, which led to the creation of a Task Scheduler feature that allows LLMs to schedule actions based on natural language user input.&lt;/p&gt;

&lt;p&gt;A significant milestone in SpellVault’s evolution was the introduction of “Workflow,” a drag-and-drop interface within the platform that empowered users to design deterministic workflows. These workflows enabled users to seamlessly combine various components from the SpellVault ecosystem—such as LLM calls, Python code execution, and Knowledge Vault lookups—in a predefined and structured manner. This enabled advanced use cases for many users.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Evolving tools landscape of SpellVault with increasing integrations.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;shifting-the-execution-model&quot;&gt;Shifting the execution model&lt;/h3&gt;

&lt;p&gt;As SpellVault evolved, a fundamental shift took place in the way its apps were executed internally. We transitioned from our &lt;a href=&quot;https://python.langchain.com/docs/how_to/agent_executor/&quot;&gt;legacy executor system&lt;/a&gt;, which facilitated one-off information retrieval from the Knowledge Vault or user plugins, to a more advanced &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/low_level/&quot;&gt;graph based executor&lt;/a&gt;. This empowered SpellVault’s app execution with nodes, edges, and states that supported branching, looping, and modularity. This laid the groundwork for more sophisticated agent behaviors, moving beyond the linear input-output paradigm.&lt;/p&gt;

&lt;p&gt;This transformed all existing SpellVault apps into ‘Reasoning and Acting’ agents, better known as &lt;a href=&quot;https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html&quot;&gt;ReAct agents&lt;/a&gt; - a “one size fits many” solution that significantly enhanced the capabilities of these apps. By enabling them to leverage the Knowledge Vault and plugins in a more agentic and dynamic manner, the ReAct agent framework allowed apps to perform more complex tasks while seamlessly preserving their existing functionality, ensuring no disruption to their behavior.&lt;/p&gt;

&lt;p&gt;In addition, the internal decoupling of the executor and prompt engineering components enabled us to design multiple execution pathways with ease. This allowed us to provide generic Deep Research capability to any SpellVault app via a simple UI checkbox, as well as sophisticated internal workflows that cater to high-ROI complex use cases like on-call alert analysis. The Deep Research capability came with SpellVault’s ability to search across internal information repositories (e.g., Slack messages, Wiki, Jira) within Grab, as well as searching online for relevant information.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. SpellVault’s evolved architecture with more dynamic context gathering and advanced interaction modes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;towards-an-agentic-framework&quot;&gt;Towards an agentic framework&lt;/h3&gt;

&lt;p&gt;Over time, several capabilities were added to SpellVault, including features like Python code execution and internal repository search. Initially, these functionalities were integrated directly into the core PromptBuilder class. For users, these features were primarily accessible through simple checkboxes in the user interface. As SpellVault gradually transitioned towards giving more agency to user-crafted apps, we recognized that these capabilities should instead be positioned as “Tools” for LLMs to use with greater autonomy, similar to how ReAct agent–backed apps have been using SpellVault’s user plugins. We also understood that this shift could bring a clearer mental model for users where they were no longer simply toggling features but creating AI agents with access to a defined set of tools. The agents could then decide when and how to use those tools intelligently to accomplish tasks, making the overall experience more natural and intuitive.&lt;/p&gt;

&lt;p&gt;This recognition led to the consolidation of these scattered capabilities into a unified framework called “Native Tools.” These Native Tools, along with SpellVault’s existing user plugins—rebranded as “Community Built Tools”—formed a comprehensive collection of tools that LLMs could dynamically invoke at runtime. Despite being grouped under the same umbrella, a key distinction was maintained: Native Tools required no user-specific configuration (e.g., performing internet searches), whereas Community Built Tools were custom, user-configured entities (e.g., invoking specific HTTP endpoints) created from available plugin types, often requiring credentials or other personalized settings.&lt;/p&gt;

&lt;p&gt;This consolidation of capabilities under a unified Tools abstraction and enabling SpellVault apps to invoke them with greater autonomy marked a pivotal milestone in the platform’s evolution. It meaningfully shifted SpellVault toward making agentic behavior more natural, discoverable, and extensible for every app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/spellvault-img/image-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. SpellVault’s Unified Tools housing both Native Tools and Community Built Tools.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;spellvault-as-an-mcp-service&quot;&gt;SpellVault as an MCP service&lt;/h3&gt;

&lt;p&gt;As we streamlined SpellVault’s internal capabilities into a unified tools framework, we also turned our focus outward to align with industry standards. The growing adoption of the &lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;Model Context Protocol&lt;/a&gt; (MCP) presented an opportunity for agents and clients to seamlessly interact without requiring custom integrations. To remain at the forefront of innovation, we adapted SpellVault to function as an MCP service, enabling it to actively participate in this evolving ecosystem. This extension brought two key advancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SpellVault apps as MCP tools&lt;/strong&gt;: Each app created in SpellVault can now be exposed through the MCP protocol. This allows other agents or MCP-compatible clients, such as IDEs or external orchestration frameworks, to treat a SpellVault app as a callable tool. Instead of living only inside our web user interface or Slack interface, these apps become accessible building blocks that other systems can invoke dynamically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;RAG as an MCP tool&lt;/strong&gt;: We extended the same idea to our Knowledge Vaults. Through MCP, external clients can search, retrieve, and even add information to Vaults. This effectively turns SpellVault’s RAG pipeline into an MCP-native service, making contextual grounding available to agents beyond SpellVault itself.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While building the SpellVault MCP Server, we also created &lt;a href=&quot;https://github.com/grab/tinymcp&quot;&gt;TinyMCP&lt;/a&gt; - a lightweight open-source Python library that adds MCP capabilities to an existing FastAPI app as just another router, instead of mounting a separate app.&lt;/p&gt;

&lt;p&gt;By exposing both apps and RAG through MCP, we shifted SpellVault from being a self-contained platform to becoming an interoperable service provider in the agentic ecosystem. Users still benefit from the no-code simplicity inside SpellVault. However, the output of their work, apps, and knowledge, are now usable by other agents and tools outside of it.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;SpellVault’s evolution shows how a platform can adapt with the AI landscape while staying true to its original mission of making powerful technology accessible to everyone. What began as a no-code builder for LLM apps has steadily expanded into an agentic platform - one where apps can act with more intelligence, agency, and context and interact with the systems around them.&lt;/p&gt;

&lt;p&gt;This progress wasn’t the result of a single breakthrough, but of steady, incremental improvements that introduced new capabilities while preserving ease of use. By layering in these advancements thoughtfully but boldly, SpellVault has managed to support more sophisticated agentic behaviors without compromising its original goal of democratizing AI at Grab.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebspell&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/spellvault-evolution-beyond-llm</link>
        <guid isPermaLink="true">https://engineering.grab.com/spellvault-evolution-beyond-llm</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Grab&apos;s Mac Cloud Exit supercharges macOS CI/CD</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In our mission to optimize continuous integration and delivery (CI/CD), we have taken a bold step by relocating our infrastructure from a cloud vendor in the US to a colocation cluster within Southeast Asia, closer to our Git server infrastructure. This change has dramatically improved the performance of our macOS builds, primarily by reducing the network traffic delays associated with distant data centers. By bringing our infrastructure closer to home, we have not only accelerated CI/CD job completion times but also massively slashed operational costs.&lt;/p&gt;

&lt;p&gt;Join us as we delve into the Mac Cloud Exit journey and the significant improvements it has brought to our workflows.&lt;/p&gt;

&lt;p&gt;Our macOS CI/CD infrastructure has evolved from 1 Physical Mac Pro running in our office to a cluster of 250 Mac minis fully occupied during peak hours of the day. There were multiple stages in the journey to transition to the current state. The following diagram shows the focus area for this blog post.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image1.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Infrastructure transition path&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;before-and-after-visualizing-the-evolution&quot;&gt;Before and after: Visualizing the evolution&lt;/h3&gt;

&lt;p&gt;We began our journey with a much simpler setup.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image2.png&quot; alt=&quot;&quot; style=&quot;width:40%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Photo of the setup when we started&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Today, that infrastructure has scaled significantly to meet the growing demands of Grab&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image3.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Mac mini cluster today&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;economy-at-scale-the-rent-vs-own-equation&quot;&gt;Economy at scale: The rent vs. own equation&lt;/h2&gt;

&lt;p&gt;At the beginning, it was a no-brainer to rent when our demand for macOS hardware increased from 1 MacPro to 20 times that size. However, when that grew to over 200 machines, the total cost became significant, prompting us to consider:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the desired reliability for this cluster?&lt;/li&gt;
  &lt;li&gt;What would be the total cost of ownership for us to build this cluster ourselves compared to cloud-based options?&lt;/li&gt;
  &lt;li&gt;What kind of operational leverage would it bring us by controlling end-to-end stack by ourselves?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-is-grabs-scale&quot;&gt;What is Grab’s scale&lt;/h3&gt;

&lt;p&gt;At Grab, our iOS build needs have scaled quite significantly, so we went from running some builds on a single Mac Pro to running them on an army of 250+ Mac minis. And so did the cost.&lt;/p&gt;

&lt;h4 id=&quot;active-jobs-trend&quot;&gt;Active jobs trend&lt;/h4&gt;

&lt;p&gt;The total number of jobs trend is one of the data points to understand the demand situation. The following chart is a snapshot from our demand curve in 2022. Peak demand often started to exceed the available supply, creating queues for the jobs.&lt;/p&gt;

&lt;p&gt;We estimated we would need 200+ machines to comfortably supply for the peak demand and projected a demand for 400+ machines in 2025.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Active macOS CI/CD jobs&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-our-workload&quot;&gt;What is our workload&lt;/h3&gt;

&lt;p&gt;We have several iOS apps that share a common macOS compute cluster for their CI/CD workloads. &lt;br /&gt;
This includes, but is not limited to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grab-taxi-ride-food-delivery/id647268330&quot;&gt;Grab app&lt;/a&gt; (Largest iOS code base with approximately 2.5M+ total lines of code)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grab-driver-app-for-partners/id1257641454&quot;&gt;Grab Driver app&lt;/a&gt; (Second largest iOS code base with approximately 0.7M+ total lines of code)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/kartalink/id6450411148&quot;&gt;KartaLink&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/grabmerchant/id1282271764&quot;&gt;GrabMerchant&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/sg/app/kartaview/id1089548849&quot;&gt;KartaView&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/us/app/ovo/id1142114207&quot;&gt;OVO&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/ph/app/move-it-fast-moto-taxi-ride/id1481198245&quot;&gt;Move It: Fast Moto Taxi Ride&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://apps.apple.com/ph/app/move-it-driver-app/id6446633186&quot;&gt;Move It Driver App&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The workload primarily involves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Building apps&lt;/li&gt;
  &lt;li&gt;Execution of tests&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-evaluation-cloud-vs-colocation-vs-on-premises&quot;&gt;The evaluation: Cloud vs colocation vs on-premises&lt;/h2&gt;

&lt;p&gt;We did a comprehensive comparison and total cost of ownership (TCO) estimation to compare many different options, including cloud vendors and colocation in different places.&lt;/p&gt;

&lt;h3 id=&quot;cost-of-macos-compute&quot;&gt;Cost of macOS compute&lt;/h3&gt;

&lt;p&gt;The expense of macOS compute is notably higher, particularly in continuous integration (CI) setups, posing challenges for optimal configuration. Several factors contribute to these increased costs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apple’s restrictive EULA mandates a minimum lease period of 24 hours for macOS instances, which alters the utilization equation.&lt;/li&gt;
  &lt;li&gt;Economies of scale are not favorable for available macOS hardware configurations compared to alternatives. Optimized server hardware designed for racking offers various configurations that reduce operational costs, unlike macOS options such as Mac Mini and Mac Pro.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, although not a direct comparison, the &lt;a href=&quot;https://docs.github.com/en/billing/managing-billing-for-your-products/managing-billing-for-github-actions/about-billing-for-github-actions#per-minute-rates-for-standard-runners&quot;&gt;pricing for GitHub Actions build minutes&lt;/a&gt; shows macOS is ten times more costly than Linux. This reflects the pricing GitHub can offer after &lt;a href=&quot;https://www.youtube.com/watch?v=I2J2MzKjcqY&quot;&gt;implementing racking optimizations.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Initially, we conducted rough estimations to assess the total cost of ownership differences between cloud, colocation, and on-premises setups. Even with conservative estimates for manpower and engineering costs, colocation or on-premises setups proved more cost-effective at our scale. This cost disparity became even more pronounced when focusing on cloud vendors providing macOS compute physically located in Southeast Asia.&lt;/p&gt;

&lt;p&gt;We opted to conduct an in-depth evaluation of the following options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Establishing a macOS cluster at our headquarters in Singapore, which was swiftly dismissed due to scalability and cost concerns making it an unsuitable long-term solution.&lt;/li&gt;
  &lt;li&gt;Colocating in a Southeast Asian country where we have operational presence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;choice-of-location&quot;&gt;Choice of location&lt;/h3&gt;

&lt;p&gt;As a Southeast Asian company, we maintain offices in each country where we operate, some of which boast advanced data center infrastructures. We focused our location choices on Singapore and Malaysia, assessing them based on several criteria, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The maturity of existing data center infrastructure.&lt;/li&gt;
  &lt;li&gt;The proximity of the data centers to our offices, ensuring staff availability for infrastructure setup.&lt;/li&gt;
  &lt;li&gt;The cost and reliability of power.&lt;/li&gt;
  &lt;li&gt;The proximity to our Git servers and the expense of establishing direct network connections.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Eventually we concluded to go ahead with a decision to colocate in a data center in Malaysia &lt;a href=&quot;https://www.edgeconnex.com/news/edge-blog/southeast-asias-data-center-powerhouse-malaysia/&quot;&gt;which is one of the emerging data center powerhouses in the region&lt;/a&gt; with relatively low energy cost compared to Singapore.&lt;/p&gt;

&lt;h3 id=&quot;choice-of-mac-hardware&quot;&gt;Choice of Mac hardware&lt;/h3&gt;

&lt;p&gt;Our choice of hardware model for our build and test workload was guided by a cost-benefit analysis. We decided to use bare-metal setups without virtualization, simplifying migration processes, which may be revisited in the future. We ensured we neither over-specified nor under-specified the bare-metal hardware. We had a clear understanding of the resource consumption of our most demanding workload on a few reference models, as illustrated in the following graphs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. User and system CPU usage during build&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Memory usage&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;virtualization-vs-bare-metal&quot;&gt;Virtualization vs bare-metal&lt;/h3&gt;

&lt;p&gt;Virtualization offers significant advantages in managing and provisioning clusters, including the flexibility to create ephemeral builds. However, our experience with macOS virtualization has been mixed. While off-the-shelf virtualization solutions provide maintenance benefits, they often come at the cost of performance or stability.&lt;/p&gt;

&lt;p&gt;Key points:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Improved utilization&lt;/strong&gt;: Virtualization can improve resource utilization by consolidating multiple workloads on fewer physical servers, thereby reducing the overall cost.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Performance penalty&lt;/strong&gt;: However, the performance penalty associated with virtualization can sometimes negate these cost benefits. This is particularly true for macOS virtualization, where we have observed trade-offs in performance or stability.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evolution of virtualization&lt;/strong&gt;: The virtualization space has been evolving and making good progress. We may re-evaluate these solutions in the future as they continue to mature and potentially address current performance and stability issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our conclusion was to stick to bare-metal for the time-being as the benefits didn’t justify the downside and cost.&lt;/p&gt;

&lt;h2 id=&quot;execution&quot;&gt;Execution&lt;/h2&gt;

&lt;h3 id=&quot;progressive-migration&quot;&gt;Progressive migration&lt;/h3&gt;

&lt;p&gt;Any disruption to the macOS CI/CD cluster would be hugely disruptive to the company given our scale highlighted above. So, we enabled new cluster partially for part of the workload for a reasonably long period of time and monitored and compared:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Job failure rate&lt;/li&gt;
  &lt;li&gt;Jobs performance&lt;/li&gt;
  &lt;li&gt;Reliability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once we were confident, we made the full switch and terminated vendor contracts at due.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image7.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7. Total active jobs trend&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;The migration yielded better results overall than our initial conservative estimates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cost savings: Estimated over 2.4 million USD over three years&lt;/li&gt;
  &lt;li&gt;Performance improvement: Between 20-40% depending on the use case&lt;/li&gt;
  &lt;li&gt;Stability: No compromise&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A strategic investment in our mission to drive Southeast Asia forward by onshoring critical Mac infrastructure into the region.&lt;/p&gt;

&lt;h3 id=&quot;cost&quot;&gt;Cost&lt;/h3&gt;

&lt;p&gt;We anticipate a three-year replacement cycle for our hardware. While some equipment may be utilized beyond this period, it provides a reasonable lifespan for cost estimation purposes.&lt;/p&gt;

&lt;p&gt;The lifecycle of networking equipment involves both physical reliability, following the bathtub curve, and technological obsolescence, often necessitating replacement every 3 to 5 years. Mac minis could become outdated after approximately three years, making the opportunity cost of extended use potentially higher than the net replacement cost after benefits.&lt;/p&gt;

&lt;p&gt;Importantly, the experience gained during this cycle could significantly reduce the engineering costs associated with future replacements.&lt;/p&gt;

&lt;p&gt;Overall, we project total cost of ownership savings of approximately 2.4 million USD over a three-year period compared to our last cloud-based setup rented from a vendor.&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;We measured the performance gains in two of our largest iOS apps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Grab app&lt;/li&gt;
  &lt;li&gt;Grab Driver app&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;overall-gains&quot;&gt;Overall gains&lt;/h4&gt;

&lt;p&gt;The following table summarizes the total time measured before and after the migration for total CI pipeline time and building the app codebase. Measurements are presented in 3 percentiles (p50, p75, p95).&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th rowspan=&quot;2&quot;&gt;App / Metric&lt;/th&gt;
            &lt;th rowspan=&quot;2&quot;&gt;&lt;/th&gt;
            &lt;th colspan=&quot;3&quot;&gt;Time (Minutes)&lt;/th&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;th&gt;p50&lt;/th&gt;
            &lt;th&gt;p75&lt;/th&gt;
            &lt;th&gt;p95&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;CI pipeline time trend for the Grab app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;43&lt;/td&gt;
            &lt;td&gt;54&lt;/td&gt;
            &lt;td&gt;67&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;33&lt;/td&gt;
            &lt;td&gt;42&lt;/td&gt;
            &lt;td&gt;49&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;23.26%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;22.22%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;26.87%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;App build time trend for the Grab app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;10.7&lt;/td&gt;
            &lt;td&gt;13.2&lt;/td&gt;
            &lt;td&gt;17.6&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;6.45&lt;/td&gt;
            &lt;td&gt;9&lt;/td&gt;
            &lt;td&gt;10.8&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;39.72%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;31.82%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.64%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;Pipeline time trend for the Grab Driver app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;47&lt;/td&gt;
            &lt;td&gt;50&lt;/td&gt;
            &lt;td&gt;52&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;26&lt;/td&gt;
            &lt;td&gt;31&lt;/td&gt;
            &lt;td&gt;32&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;44.68%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.00%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.46%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td rowspan=&quot;3&quot;&gt;App build time trend for the Grab Driver app&lt;/td&gt;
            &lt;td&gt;Before&lt;/td&gt;
            &lt;td&gt;10&lt;/td&gt;
            &lt;td&gt;13&lt;/td&gt;
            &lt;td&gt;14&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;After&lt;/td&gt;
            &lt;td&gt;6&lt;/td&gt;
            &lt;td&gt;8&lt;/td&gt;
            &lt;td&gt;8.5&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;b&gt;Gain&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;40.00%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;38.46%&lt;/b&gt;&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;39.29%&lt;/b&gt;&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;a-different-perspective-trends&quot;&gt;A different perspective: Trends&lt;/h4&gt;

&lt;p&gt;The following trend illustrations show how the performance of various tasks has improved while we progressively migrated to the new colocation setup.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image8.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8. 14 day aggregate percentiles of p50, p75 and p95 for total CI pipeline times for the Grab app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image9.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9. Pipeline time pulse for the Grab app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mac-cloud-exit/image10.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10. 14 day aggregate percentiles of p50, p75 and p95 for total CI pipeline times for the Grab Driver app codebase&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;stability&quot;&gt;Stability&lt;/h3&gt;

&lt;p&gt;We measured overall job failure rates between both clusters for extended periods as a guardrail metric and ensured the stability of the new cluster before shutting down the old one.&lt;/p&gt;

&lt;h2 id=&quot;colocation-setup-and-rack-configuration&quot;&gt;Colocation setup and rack configuration&lt;/h2&gt;

&lt;p&gt;The following table provides an overview of the layout of our new Mac mini cluster.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;Component&lt;/th&gt;
            &lt;th&gt;Description&lt;/th&gt;
            &lt;th&gt;Redundancy&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;Rack&lt;/td&gt;
            &lt;td&gt;We have got four 42RU (600x1200x42RU) racks housing 200+ Mac minis, plus some spare racks to house upcoming scheduled capacity upgrades.&lt;/td&gt;
            &lt;td&gt;Racks have shared resources which have their own redundancy. Generally rack separation does provide some level of redundancy for total compute.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Power&lt;/td&gt;
            &lt;td&gt;2 power sources power the cluster. Each rack is powered by these 2 power sources. It is 1U, 2-post rack mount.&lt;/td&gt;
                &lt;td&gt;Losing 1 power source will reduce 50% of capacity.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Mac Mini&lt;/td&gt;
            &lt;td&gt;We rack 2 Mac minis in a row on a mounting tray, typically racking 70 minis in one rack in total. Except for the first rack which requires extra rack units (RUs) for core switches and firewalls.&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;KVM&lt;/td&gt;
            &lt;td&gt;KVM switches with adaptor for keyboard and mouse emulation when required.&lt;/td&gt;
            &lt;td&gt;N/A&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Networking Setup&lt;/td&gt;
            &lt;td&gt;Networking consists of Core Switches, Access Switches, Firewalls, Internet and Direct Connect Links.&lt;/td&gt;
            &lt;td&gt;Mostly active/active redundancy.&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;provisioning-and-configuration&quot;&gt;Provisioning and configuration&lt;/h2&gt;

&lt;h3 id=&quot;zero-touch-provisioning&quot;&gt;Zero-touch provisioning&lt;/h3&gt;

&lt;p&gt;Zero-touch provisioning is a streamlined method for setting up and configuring devices with minimal manual intervention. This section outlines the process and benefits of zero-touch provisioning using Jamf for Mac minis.&lt;/p&gt;

&lt;p&gt;We have a setup that enables these machines to start accepting jobs once they are racked up and connected (Power and network cables). Here is how it works:&lt;/p&gt;

&lt;h4 id=&quot;mobile-device-management-mdm-configuration-and-automated-device-enrollment-ade&quot;&gt;Mobile Device Management (MDM) configuration and Automated Device Enrollment (ADE)&lt;/h4&gt;

&lt;p&gt;ADE, previously known as Device Enrollment Program (DEP), is an Apple service that facilitates automatic enrollment. When a new Mac Mini is acquired and registered in the organization’s ADE account, it is primed for automatic enrollment. Administrators create a PreStage enrollment configuration within Jamf Pro, encompassing account settings (e.g., creating a local admin account, hiding it in Users &amp;amp; Groups, skipping account creation for the user), configuration profiles (defining device settings, security policies, and restrictions), and enrollment packages (including necessary software and scripts).&lt;/p&gt;

&lt;h4 id=&quot;device-setup-activation-and-redirection&quot;&gt;Device setup: Activation and redirection&lt;/h4&gt;

&lt;p&gt;Upon powering on and connecting to the internet, the Mac Mini communicates with Apple’s activation servers. The activation servers identify the device as part of the organization’s ADE and redirect it to the Jamf MDM server, ensuring automatic enrollment without user input.&lt;/p&gt;

&lt;h4 id=&quot;enrollment-and-configuration&quot;&gt;Enrollment and configuration&lt;/h4&gt;

&lt;p&gt;The Mac Mini enrolls into the Jamf MDM system automatically. Jamf applies predefined configuration profiles to set up the device’s settings, installs required applications based on configured policies, and enforces security policies such as encryption and authentication settings to ensure compliance.&lt;/p&gt;

&lt;h4 id=&quot;key-benefits-of-zero-touch-provisioning&quot;&gt;Key benefits of zero-touch provisioning&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Devices are ready to use right out of the box, reducing the time and effort required by IT staff.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Ensures that all devices are configured uniformly according to organizational policies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Enforces security policies from the moment the device is first powered on, reducing vulnerabilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Easily manage and configure a large number of devices without manual intervention.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learnings-and-insights&quot;&gt;Learnings and insights&lt;/h2&gt;

&lt;h3 id=&quot;supply-chain-is-as-fast-as-the-last-essential-component-you-need&quot;&gt;Supply chain is as fast as the last essential component you need&lt;/h3&gt;

&lt;p&gt;The efficiency of a supply chain hinges on the delivery of its final essential component. Despite being a fundamental principle, it’s worth reiterating. Our timely launch was facilitated by a buffer period for unexpected delays. Interestingly, one of the last critical items to arrive was the rack mounting trays. The brief delay underscored the importance of prioritizing and planning for on-time delivery of every essential component, irrespective of its manufacturing simplicity.&lt;/p&gt;

&lt;h3 id=&quot;consistently-address-the-question-how-will-this-scale&quot;&gt;Consistently address the question: How will this scale?&lt;/h3&gt;

&lt;p&gt;From the outset, our goal was to develop a scalable infrastructure. As the cluster expands, tasks such as preparing Mac minis for job acceptance require increasing manual input, which ultimately impacts costs. Hence, zero-touch provisioning becomes essential, as scalability is not merely a desirable feature but a necessity.&lt;/p&gt;

&lt;h3 id=&quot;plan-and-opt-in-for-a-power-cost-structure-best-suits-for-your-need&quot;&gt;Plan and opt in for a power cost structure best suits for your need&lt;/h3&gt;

&lt;h4 id=&quot;power-cost-structures&quot;&gt;Power cost structures&lt;/h4&gt;

&lt;p&gt;In a colocation setup power costs can be billed in several ways, each with pros and cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Flat rate per circuit&lt;/strong&gt;: A fixed monthly fee, predictable but limits flexibility (e.g., can’t exceed 80% without extra circuits).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Allocated kW&lt;/strong&gt;: Commit to a fixed power amount (e.g., 100 kW), potentially cheaper but with penalties for overages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metered usage&lt;/strong&gt;: Pay for actual consumption (kWh), good for variable loads but may still charge for space.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;All-in Sspace and power&lt;/strong&gt;: Single rate covering both, easy to compare but less flexible for upgrades.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We ultimately opted for an allocated kW commitment, a phased approach based on conservative equipment power ratings and historical usage. We structured this into phases of commitment increases for future capacity growth.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Mac Cloud Exit wasn’t just a technical migration; it was a strategic move that fundamentally enhanced our engineering efficiency. By onshoring our infrastructure into Southeast Asia, we have achieved $2.4 million USD in projected savings and supercharged our CI pipeline, delivering performance gains of 20-40%. This project proves that taking ownership of our core infrastructure can be a major competitive advantage, allowing us to deliver faster and more reliably for our users across the region.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmacos&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 06 Nov 2025 00:00:05 +0000</pubDate>
        <link>https://engineering.grab.com/mac-cloud-exit</link>
        <guid isPermaLink="true">https://engineering.grab.com/mac-cloud-exit</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we built a custom vision LLM to improve document processing at Grab</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the world of digital services, accurate extraction of information from user-submitted documents such as identification (ID) cards, driver’s licenses, and registration certificates is a critical first step for processes like electronic know-your-customer (eKYC). This task is especially challenging in Southeast Asia (SEA) due to the diversity of languages and document formats.&lt;/p&gt;

&lt;p&gt;We began this journey to address the limitations of traditional Optical Character Recognition (OCR) systems, which struggled with the variety of document templates it had to process. While powerful proprietary Large Language Models (LLMs) were an option, they often fell short in understanding SEA languages, produced errors, hallucinations, and had high latency. On the other hand, open-sourced Vision LLMs were more efficient but not accurate enough for production.&lt;/p&gt;

&lt;p&gt;This prompted us to fine-tune and ultimately develop a lightweight, specialized Vision LLM from the ground up. This blog is our account of the entire process.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: Simplified overview of how Vision LLM works.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;what-is-a-vision-llm&quot;&gt;What is a Vision LLM?&lt;/h3&gt;

&lt;p&gt;You’ve likely heard of LLMs that process text. You give the LLM a text prompt, and it responds with a text output. A Vision LLM takes this a step further by allowing the model to understand images. The basic architecture involves three key components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Image encoder&lt;/strong&gt;: This component ‘looks’ at an image and converts it into a numerical (vectorized) format.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vision-language projector&lt;/strong&gt;: It acts as a translator, converting the image’s numerical format into a representation that the language model can understand.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Language model&lt;/strong&gt;: The familiar text-based model that processes the combined image and text input to generate a final text output.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Vision LLM basic  architecture.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;choosing-our-base-vision-llm-model&quot;&gt;Choosing our base Vision LLM model&lt;/h3&gt;
&lt;p&gt;We evaluated a range of LLMs capable of performing OCR and Key Information Extraction (KIE). Our exploration of open-source options—including Qwen2VL, miniCPM, Llama3.2 Vision, Pixtral 12B, GOT-OCR2.0, and NVLM 1.0—led us to select Qwen2-VL 2B as our base multimodal LLM. This decision was driven by several critical factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Efficient size&lt;/strong&gt;: It is small enough for full fine-tuning on GPUs with limited VRAM resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SEA language support&lt;/strong&gt;: Its tokenizer is efficient for languages like Thai and Vietnamese, indicating decent native vocabulary coverage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic resolution&lt;/strong&gt;: Unlike models that require fixed-size image inputs, Qwen2-VL can process images in their native resolution. This is crucial for OCR tasks as it prevents the distortion of text characters that can happen when images are resized or cropped.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We benchmarked Qwen2VL and miniCPM on Grab’s dataset. Our initial findings showed low accuracy, mainly due to the limited coverage of SEA languages. This motivated us to fine-tune the model to improve OCR and KIE accuracy. Training the LLM can be a very data-intensive and GPU resource-intensive process. Due to this, we had to address these two concerns before progressing further:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: How do we use open source and internal data effectively to train the model?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: How do we customize the model to reduce latency but keep high accuracy?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-dataset-generation&quot;&gt;Training dataset generation&lt;/h2&gt;

&lt;h3 id=&quot;synthetic-ocr-dataset&quot;&gt;Synthetic OCR dataset&lt;/h3&gt;

&lt;p&gt;We extracted the SEA languages text content from a large online text corpus—&lt;a href=&quot;https://commoncrawl.org/&quot;&gt;Common Crawl&lt;/a&gt; (internet dataset). Then, we used an in-house synthetic data pipeline to generate text images by rendering SEA text contents in various fonts, backgrounds and augmentations.&lt;/p&gt;

&lt;p&gt;The dataset contains text in Bahasa Indonesia, Thai, Vietnamese, and English. Each image has a paragraph of random sentences extracted from the dataset as shown in Figure 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Two synthetic sample images in Thai language used for model training.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;documint-ai-powered-auto-labelling-framework&quot;&gt;Documint: AI-powered, auto-labelling framework&lt;/h3&gt;

&lt;p&gt;Our experiments showed that applying document detection and orientation correction significantly improves OCR and information extraction. Now that we have an OCR dataset, we needed to generate a pre-processing dataset to further improve model training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Documint&lt;/strong&gt; is an internal platform developed by our team that creates an auto‑labelling and pre‑processing framework for document understanding. It prepares high‑quality, labelled datasets. Documint utilizes various submodules to effectively execute the full OCR and KIE task. We then used a pipeline with the large amount of Grab collected cards and documents to extract training labels. The data was further refined by a human reviewer to achieve high label accuracy.&lt;/p&gt;

&lt;p&gt;Documint has four main modules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Detection module&lt;/strong&gt;: Detect the region from the full picture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Orientation module&lt;/strong&gt;: Gives correction angle (e.g. if document is upside down, 180 degrees).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;OCR module&lt;/strong&gt;: Returns text values in unstructured format.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;KIE module&lt;/strong&gt;: Returns JSON values from unstructured text.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:85%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Pipeline overview of Documint.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;experimentation&quot;&gt;Experimentation&lt;/h2&gt;

&lt;h3 id=&quot;phase-1-the-lora-experiment&quot;&gt;Phase 1: The LoRA experiment&lt;/h3&gt;

&lt;p&gt;Our first attempt in fine-tuning a Vision LLM involved fine-tuning an open-source model Qwen2VL, using a technique called Low-Rank Adaptation (LoRA). LoRA is efficient because it allows lightweight updates to the model’s parameters, minimizing the need for extensive computational resources.&lt;/p&gt;

&lt;p&gt;We trained the model on our curated document data, which included various document templates in multiple languages. The performance was promising for documents with Latin scripts. Our experiment of LoRA fine-tuned Qwen2VL-2B achieved high field-level of accuracy for Indonesian documents.&lt;/p&gt;

&lt;p&gt;However, the fine-tuned model still struggled with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Documents containing non-Latin scripts like Thai and Vietnamese.&lt;/li&gt;
  &lt;li&gt;Unstructured layouts with small, dense text.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-2-the-power-of-full-fine-tuning&quot;&gt;Phase 2: The power of full fine-tuning&lt;/h3&gt;

&lt;p&gt;Our experiments revealed a key limitation. While open-source Vision LLMs often have extensive multi-lingual corpus coverage for the LLM decoder’s pre-training, they lack visual text in SEA languages during vision encoder and joint training. This insight drove our decision to pursue full parameter fine-tuning for optimal results.&lt;/p&gt;

&lt;p&gt;Drawing from the &lt;a href=&quot;https://arxiv.org/abs/2304.08485&quot;&gt;Large Language and Vision Assistant (LLAVA)&lt;/a&gt; methodology, we implemented a two-stage training approach illustrated in Figure 5.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: From left to right—two-stage training process.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1 - Continual pre-training&lt;/strong&gt;: We first trained the vision components of the model using synthetic OCR datasets that we created for Bahasa Indonesia, Thai, Vietnamese, and English. This helps the model to learn the unique visual patterns of SEA scripts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2 - Full-parameter fine-tuning&lt;/strong&gt;: We then fine-tuned the entire model—vision encoder, projector, and language model—using our task-specific document data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 1: OCR Field level accuracy between the baseline and Qwen2-VL 2B model. (pp: percentage points).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The fully fine-tuned Qwen2-VL 2B model delivered significant improvement, especially on documents that the LoRA model struggled with.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Thai document accuracy increased &lt;strong&gt;+70pp&lt;/strong&gt; from baseline.&lt;/li&gt;
  &lt;li&gt;Vietnamese document accuracy rose &lt;strong&gt;+40pp&lt;/strong&gt; from baseline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-3-building-a-lightweight-1b-model-from-scratch&quot;&gt;Phase 3: Building a lightweight 1B model from scratch&lt;/h3&gt;

&lt;p&gt;While the Qwen2VL-2B model was a success, the full fine-tuning pushed the limits of GPUs. To optimize resources used and to create a model perfectly tailored to our needs, we decided to build a lightweight Vision LLM (~1B parameters) from scratch.&lt;/p&gt;

&lt;p&gt;Our strategy was to combine the best parts of all models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We took the powerful &lt;strong&gt;vision encoder&lt;/strong&gt; from the larger Qwen2-VL 2B model.&lt;/li&gt;
  &lt;li&gt;We paired it with the compact and efficient &lt;strong&gt;language decoder&lt;/strong&gt; from the Qwen2.5 0.5B model.&lt;/li&gt;
  &lt;li&gt;We connected them with an &lt;strong&gt;adjusted projector layer&lt;/strong&gt; to ensure they could work together seamlessly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This created a custom ~1B parameter Vision LLM optimized for training and deployment.&lt;/p&gt;

&lt;h4 id=&quot;four-stages-in-training-our-custom-model&quot;&gt;Four stages in training our custom model&lt;/h4&gt;

&lt;p&gt;We trained our new model using a comprehensive four-stage process as shown in Figure 6.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6: From left to right— four stages of model training.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1 - Projector alignment&lt;/strong&gt;: The first step was to train the new projector layer to ensure the vision encoder and language decoder could communicate effectively.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2 - Vision tower enhancement&lt;/strong&gt;: We then trained the vision encoder on a vast and diverse set of public multimodal datasets, covering tasks like visual Q&amp;amp;A, general OCR, and image captioning to improve its foundational visual understanding.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 3 - Language-specific visual training&lt;/strong&gt;: We trained the model on two types of synthetic OCR data. Without this stage, performance on non-Latin documents dropped by as much as 10%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 4 - Task-centric fine-tuning&lt;/strong&gt;: Lastly, we performed full-parameter fine-tuning on our custom 1B model using our curated document dataset.&lt;/p&gt;

&lt;h4 id=&quot;the-final-results-are-as-follow&quot;&gt;The final results are as follow:&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It achieved performance comparable to the larger 2B model, &lt;strong&gt;staying within a 3pp accuracy gap across most document types.&lt;/strong&gt;  The model also maintained strong generalization when trained on quality-augmented datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The latency of our model far outperforms the 2B model, as well as traditional OCR models, as well as external APIs like chatGPT or Gemini. One of the biggest weaknesses we identified with external APIs was the P99 latency, which can easily be 3 to 4x the P50 latency, which would not be acceptable for Grab’s large scale rollouts.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 2: Performance comparison between Qwen2-VL 2B and 1B sized Vision LLM.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;

&lt;p&gt;Our work demonstrates that strategic training with high-quality data enables smaller, specialized models to achieve remarkable efficiency and effectiveness. Here are the critical insights from our extensive experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Full fine-tuning is superior&lt;/strong&gt;: For specialized, non-Latin script domains, full-parameter fine-tuning dramatically outperforms LoRA.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lightweight models are effective&lt;/strong&gt;: A smaller model (~1B) built from scratch and trained comprehensively can achieve near state-of-the-art results, validating the custom architecture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Base model matters&lt;/strong&gt;: Starting with a base model that has native support for your target languages is crucial for success.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data is king&lt;/strong&gt;: Meticulous dataset preprocessing and augmentation plays a critical role in achieving consistent and accurate results.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Native resolution is a game changer&lt;/strong&gt;: A model that can handle dynamic image resolutions preserves text integrity, dramatically improves OCR capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our journey demonstrates that specialized Vision LLMs can effectively replace traditional OCR pipelines with a single, unified, highly accurate model—opening new possibilities for document processing at scale.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/custom-vision-llm-at-grab/table-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 3: Comparison of model types .&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;As we continue to enhance our Vision LLM capabilities, exciting developments are underway:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Smarter, more adaptable models&lt;/strong&gt;: We’re developing Chain of Thought-based OCR and KIE models to strengthen generalisation capabilities and tackle even more diverse document scenarios.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Expanding across Southeast Asia&lt;/strong&gt;: We’re extending support to all Grab markets, bringing our advanced document processing to Myanmar, Cambodia, and beyond.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2409.12191&quot;&gt;https://doi.org/10.48550/arXiv.2409.12&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Improved Baselines with Visual Instruction Tuning: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2310.03744&quot;&gt;https://doi.org/10.48550/arXiv.2310.03744&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2107.09313&quot;&gt;https://doi.org/10.48550/arXiv.2107.09313&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models: &lt;a href=&quot;https://doi.org/10.48550/arXiv.2403.13372&quot;&gt;https://doi.org/10.48550/arXiv.2403.13372&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmodel&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 04 Nov 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/custom-vision-llm-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/custom-vision-llm-at-grab</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Machine-learning predictive autoscaling for Flink</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As Grab transitions to derive more valuable insights from our wealth of operational data, we are witnessing a steep increase in stream-processing applications. Over the past year, the number of Flink applications grew 2.5 times, driven by interest in real-time stream processing and the improved accessibility of developing such applications with Flink SQL. At this scale, it has become crucial for the internal Flink platform team to provide a &lt;strong&gt;cost-effective&lt;/strong&gt; and &lt;strong&gt;self-service&lt;/strong&gt; offering that supports users of diverse backgrounds.&lt;/p&gt;

&lt;h2 id=&quot;background-flink-at-grab&quot;&gt;Background: Flink at Grab&lt;/h2&gt;

&lt;p&gt;Flink at Grab is deployed in application mode, each pipeline has its own isolated resources for JobManager and TaskManager. Flink pipeline creators control both application logic and deployment configuration that affect throughput and performance, including OSS configurations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of TaskManagers and task slots per TaskManager&lt;/li&gt;
  &lt;li&gt;CPU cores per TaskManager&lt;/li&gt;
  &lt;li&gt;Memory per TaskManager&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As pipeline creation has become more accessible, users of different backgrounds (analyst, data scientist, engineers, etc.) often struggle to choose a set of configurations that work for their applications. Many go through a long process of trial and error and still end up over-provisioning their applications, leading to huge resource waste. Moreover, pipeline behavior changes over time due to changes in application logic or data pattern, invalidating previous efforts in tuning and causing users to repeat the exercise.&lt;/p&gt;

&lt;p&gt;In this article, we focus on addressing the challenge of efficient CPU provisioning for TaskManagers, as CPU constraints are a common bottleneck in our clusters. Our solution specifically targets Flink applications sourcing data from our message bus system (eg. Kafka, Change Data Capture Streams, DynamoDB Streams) , which represents the majority of our use cases. These workloads offer significant opportunities for cost savings due to their clear seasonal patterns, making them an ideal starting point for optimising autoscaling strategies.&lt;/p&gt;

&lt;h2 id=&quot;limits-of-reactive-autoscaling&quot;&gt;Limits of reactive autoscaling&lt;/h2&gt;

&lt;h3 id=&quot;our-initial-reactive-setup&quot;&gt;Our initial reactive setup&lt;/h3&gt;

&lt;p&gt;Our first automated solution relied on Flink’s Adaptive Scheduler in Reactive Mode. In this mode, each Flink application is deployed as its own individual Flink cluster running a dedicated job. The cluster greedily uses all available TaskManagers and scales its job parallelism accordingly. Running on Kubernetes, the cluster relies on Horizon Pod Autoscaler (HPA) to scale the number of TaskManager pods based on metrics such as CPU usage or custom metrics such as the pipeline’s consumer latency. While this solution was helpful initially, we quickly observed multiple issues with it.
It is important to note that while the below issues can be solved by fine-tuning, it is a tedious trial and error effort that only works for specific applications, requiring users to repeat the process for every pipeline they own.&lt;/p&gt;

&lt;h3 id=&quot;restart-spike-root-cause-of-many-issues&quot;&gt;Restart spike: root cause of many issues&lt;/h3&gt;

&lt;p&gt;When autoscaling a Flink pipeline, the job restarts from the last checkpoint. This triggers an immediate spike in load, as the pipeline must reprocess records from the period between the last checkpoint and job restart, along with any new records that were backlogged at the source during the downtime. As a result, CPU usage and P99 consumer latency  typically spikes after scaling events, for example, at 00:05 and 00:55, as shown in Figure 1. These spikes occur even though there is no change in source topic throughput. In this case, CPU usage surges from 0.5 cores to near provision limit of 2.5 cores,  while consumer latency temporarily spiked from sub-second levels to as high as three minutes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/cpu-usage-con-latency-after-restart.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: CPU usage and consumer latency spike after a pipeline restart.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;reactive-spiral-and-fluctuation&quot;&gt;Reactive spiral and fluctuation&lt;/h3&gt;

&lt;p&gt;Typically, HPA scales on metrics such as CPU usage, consumer latency, or backpressure crossing a defined threshold. The challenge arises if these thresholds are misconfigured. The HPA’s reactive nature, when combined with restart spikes, can become detrimental to your Flink application. It piles additional load onto a system that’s already degrading, further amplifying the problem.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/reactive-scaling-fluctuation.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: A reactive scaling incident that demonstrates scaling fluctuations and restarts.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 2 provides us a case study of reactive spiral and fluctuation, assuming we are having a pipeline that consumes a Kafka topic of 300 partitions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;07:00: As the source topic throughput increases, the P99 consumer latency rises due to insufficient processing power.&lt;/li&gt;
  &lt;li&gt;07:15: Reactive scaling is triggered, resulting in a scale out event. This is reflected in the increased TaskManager and task slot count. The pipeline continues to operate, as there is no increase in restart count.&lt;/li&gt;
  &lt;li&gt;07:30: As the P99 consumer latency remains high, reactive scaling continues to scale out incrementally. The records in rate by task rises rapidly as the pipeline reprocesses data from the checkpoint. During this period, the pipeline repeatedly restarts CPU usage drops significantly, and P99 consumer latency spikes to nearly one hour. This marks the onset of a spiral failure.&lt;/li&gt;
  &lt;li&gt;08:00: Reactive scaling reaches its upper limit of 300 slots, corresponding to the number of partitions in the source topic. This halts the spiral effect as it cannot scale out any further. Without disruption from autoscaling restart, the pipeline begins to process the backlog since the last successful checkpoint, as observed by the significant increase in records in rate by task. As the pipeline catches up, it eventually stabilizes, and the P99 consumer latency returns to normal levels.&lt;/li&gt;
  &lt;li&gt;08:30 - 10:15: The P99 consumer latency returns to normal levels, below the threshold. Reactive scaling triggers scale-in events despite the source topic throughput continuing to trend upward. During these scale-in events, P99 latency fluctuates, occasionally spiking up to 15 minutes. However, these fluctuations are not severe enough to prevent the repeated scale in process.&lt;/li&gt;
  &lt;li&gt;10:15: The P99 consumer latency rises again, triggering a scale-out event back to the upper limit of 300 slots.&lt;/li&gt;
  &lt;li&gt;11:15-11:45: Despite the source topic throughput maintaining an upward trend, the pipeline undergoes multiple scale-in events in quick succession, encounters latency issues due to reprocessing data from checkpoints, and scales out again shortly after. This is an example of fluctuation after scaling in, resulting in 6 restarts within a 30 minutes window.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;limited-parallelism-constraints&quot;&gt;Limited parallelism constraints&lt;/h3&gt;

&lt;p&gt;Even with HPA, we frequently encounter a bottleneck when trying to scale our applications’ throughput. This is primarily because some of our connectors, most notably the Kafka connector, don’t inherently support dynamic parallelism changes.
Kafka topics, by design, have a fixed number of partitions. This directly limits the number of parallel consumers we can run. Consequently, once we reach this maximum parallelism for our consumers, we often have to scale up resources, for example, increase memory/CPU per instance instead of scaling out (adding more instances).&lt;/p&gt;

&lt;h2 id=&quot;predictive-resource-advisor&quot;&gt;Predictive Resource Advisor&lt;/h2&gt;

&lt;h3 id=&quot;assumptions-and-hypothesis&quot;&gt;Assumptions and hypothesis&lt;/h3&gt;

&lt;p&gt;To tackle the issue of reactive spirals and fluctuations, the new solution should have the following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vertical scaling: To tackle the issue of limited parallelism with our dependencies, we should be looking at vertical instead of horizontal scaling.&lt;/li&gt;
  &lt;li&gt;Predictive: Adjust CPU to scale up or down before demand spikes or dips occur, ensuring the system is prepared for changes in workload. This prevents artificial workload increases caused by processing backlogs on top of actual workload increase, further straining the system.&lt;/li&gt;
  &lt;li&gt;Deterministic: The CPU configuration must be precisely calculated based on the workload demand, ensuring predictable and consistent resource allocation. For a given workload, the calculated CPU value should remain the same every time, eliminating variability and uncertainty in scaling decisions.&lt;/li&gt;
  &lt;li&gt;Accurate: Determine the optimal CPU configuration required to handle workload demand in a single, precise calculation, avoiding the inefficiencies of multi-step, trial-and-error tuning.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;key-observations&quot;&gt;Key observations&lt;/h3&gt;

&lt;p&gt;Our solution is conceptualized based on key observations of our Flink applications:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The CPU usage of Flink applications is primarily driven by the input load.&lt;/li&gt;
  &lt;li&gt;The input load of our Flink applications can be accurately forecasted using time-series forecasting techniques.&lt;/li&gt;
  &lt;li&gt;Time-based autoscaling that relies solely on historical CPU usage is not robust enough to adapt to evolving workloads. This approach also carries the risk of a negative self-amplifying feedback loop: each autoscaling restart causes a CPU usage spike (as illustrated in Figure 1), which, if anomalies are not properly handled, inflates subsequent CPU calculations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;model-formulation&quot;&gt;Model formulation&lt;/h3&gt;

&lt;p&gt;We then formulate the relationship between CPU usage and input load using a regression model to provide a mathematical framework for predicting CPU requirements based on workload patterns, expressed as:&lt;/p&gt;

&lt;p style=&quot;text-align:center; font-weight:bold;&quot;&gt;C&lt;sub&gt;t&lt;/sub&gt; = f(x&lt;sub&gt;t&lt;/sub&gt;)&lt;/p&gt;

&lt;p&gt;In this equation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;C&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; represents the CPU required at a specific point in time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; represents the input workload at the corresponding point in time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;f()&lt;/strong&gt; represents the regression function that maps the input load to the required CPU capacity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Input load, represented by Kafka source topic throughput in our case, is chosen as the independent variable &lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; because it reflects true business demand and is entirely independent of Flink consumers. This metric is influenced solely by the business logic of upstream producers and remains unaffected by any changes or behaviors in the Flink consumer pipeline.&lt;/p&gt;

&lt;h3 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h3&gt;

&lt;p&gt;Our predictive autoscaler operates through four key stages as shown in Figure 3.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/predictive-autosclaing-flow.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: The predictive autoscaling system operates through four key stages.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stage 1: Workload forecast model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The workload forecast model is a time-series forecasting model trained on actual workload data, specifically source topic throughput from our Kafka cluster (1). This approach is particularly effective as our workload exhibits seasonal patterns. While historical data could be directly used as input for CPU prediction, time-series forecasting offers a more robust solution by enabling the model to account for organic traffic growth over time. Through periodic retraining, the model adapts to evolving workload trends, ensuring more accurate and reliable predictions for resource provisioning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2: Resource prediction model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This follows the regression-based model &lt;strong&gt;C&lt;sub&gt;t&lt;/sub&gt; = f(x&lt;sub&gt;t&lt;/sub&gt;)&lt;/strong&gt; defined earlier. We use the same source topic throughput from our Kafka cluster (2a) as  input feature &lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;, and  the Flink application’s Kubernetes CPU usage metric (2b) as output label &lt;strong&gt;C&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; for model training. To ensure clean and representative data for model training, we collect CPU usage metrics under conditions that simulate infinite resource availability. We include data  exclusively from periods of continuous and stable operation, as determined by latency, uptime, and restart metrics (2b), eliminating biases caused by hardware limitations or disruptions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 3: Workload forecasting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To prepare for autoscaling, we forecast the workload for the future t-hour window (3) using our trained time-series forecast model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 4: Predict CPU usage&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The forecasted workload (3) is fed into the resource prediction model to estimate the CPU usage required to handle that workload. The predicted value is then refined using custom safety feature adjustments to account for variability and ensure stability. This adjusted prediction is passed to the custom autoscaler controller, which evaluates the current CPU configuration of the TaskManager deployment. If the adjusted predicted value differs from the existing CPU configuration, the controller initiates vertical scaling to update the TaskManager deployment accordingly.&lt;/p&gt;

&lt;h2 id=&quot;proof-of-concept-and-results&quot;&gt;Proof of concept and results&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;Experiment setup&lt;/h3&gt;

&lt;p&gt;To validate our hypothesis, we present a deep dive into one of our experiments. This pipeline features complex business logic, aggregates from multiple Kafka sources, with a checkpoint interval of one minute and a maximum consumer latency of five minutes.&lt;/p&gt;

&lt;p&gt;We set up an experimental pipeline with configurations identical to the production pipeline (the control). Both applications sourced data from the same Kafka topics but sank data to alternative topics to maintain isolation. The Predictive Resource Advisor was enabled on the experimental pipeline, while the control pipeline operated with fixed CPU provisioning.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Figure 4 demonstrates a strong correlation between CPU usage (yellow, green) and the total Kafka topics throughput. The variable CPU provisioning (blue) for the experimental pipeline is calculated by our autoscaler models, which were trained exclusively on data collected from the experiment pipeline. The CPU usage trend of the experimental pipeline closely mirrors that of the control pipeline and remains aligned with the Kafka throughput trend. However, the experimental pipeline’s CPU provisioning is dynamically adjusted to more closely match its actual CPU usage, whereas the control pipeline maintains a static CPU allocation (purple). This illustrates the model’s effectiveness in dynamically adjusting CPU allocation to meet variable workload demands.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/cpu-usage-source-throughput.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: CPU usage closely correlates with source throughput for both the experimental and control pipelines.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Without autoscaler enabled, the control pipeline experienced no disruptions and maintained latency (blue) consistently below one second, which is not visible in Figure 5. On the other hand, the experiment pipeline latency (red) experienced a highest recorded peak latency of just over four minutes during a single disruption window. Other latency spikes observed were comparable to or lower than the three minutes peak latency previously identified as part of the restart spike issue analysis. The varied durations and amplitudes of these spikes showed some correlation with the heavy Kafka topic throughput during those periods. Importantly, there were only nine autoscaling events throughout the day, resulting in nine restarts for the experiment pipeline.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/ml-predictive-autoscaling-for-flink/impact-autoscaling-sla.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: Autoscaling impacts service-level agreement requirements through latency spikes during scaling events.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;outcome&quot;&gt;Outcome&lt;/h3&gt;

&lt;p&gt;The Predictive Resource Advisor solution has been successfully deployed across more than 50% of applicable production applications, specifically those consuming from Kafka topics and exhibiting seasonal workload patterns with some tolerance for disruptions. This implementation has delivered significant results across three key areas, stability, efficiency, and user experience.&lt;/p&gt;

&lt;h4 id=&quot;stability&quot;&gt;Stability&lt;/h4&gt;

&lt;p&gt;With autoscaling becoming more predictable and controllable, our Flink applications experience fewer disruptions caused by autoscaling fluctuations. The machine learning and predictive capabilities of the solution also ensure that applications remain operational during periods of increased workload by automatically learning and adapting to organic growth trends and workload surges.&lt;/p&gt;

&lt;h4 id=&quot;efficiency&quot;&gt;Efficiency&lt;/h4&gt;

&lt;p&gt;Applications powered by the Predictive Resource Advisor demonstrated significant improvements in CPU provisioning, aligning CPU configuration more closely with actual requirements, particularly during low traffic periods. As a result of this optimization, on average, these applications made approximately &amp;gt;35% savings in cloud infrastructure cost.&lt;/p&gt;

&lt;h4 id=&quot;user-experience&quot;&gt;User experience&lt;/h4&gt;

&lt;p&gt;The solution has simplified the deployment process for users, allowing them to simply deploy Flink applications with default configurations. The Predictive Resource Advisor automatically collects data, trains autoscaling models, and applies configuration changes, thus eliminating the need for manual fine-tuning. This significantly enhances the user experience by streamlining pipeline maintenance and enabling self-service capabilities, such as effortless onboarding. It empowers users to explore and derive value from real-time features with minimal effort.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Our journey doesn’t stop here. We’re continuously working to enhance our predictive autoscaler, with the following key areas of focus:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Tackling memory configuration (Predictive Resource Advisor’s next frontier)&lt;/strong&gt; &lt;br /&gt;
Memory is critical yet often misconfigured that can lead to unrecoverable failures for example, OOMKilled. Our next major goal for the Predictive Resource Advisor is to take on memory tuning, completely removing the burden of complex memory configuration from our users and further empowering them.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhancing model accuracy&lt;/strong&gt; &lt;br /&gt;
To further improve the robustness of our predictions, we are actively exploring advanced techniques in input feature engineering and anomaly detection, especially for workloads exhibiting frequent bursting patterns. By refining these aspects, we aim to extend the applicability of our solution to a broader range of Flink applications, including those connected to diverse sources such as change data capture systems or batch-like, spiky workloads, such as the Flink applications powering our real-time data lake.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Streamlining model training&lt;/strong&gt; &lt;br /&gt;
We’re developing a more efficient model training workflow. A particularly exciting avenue we’re investigating is the use of pretrained time-series forecasting models based on large language model architectures.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/resource-providers/standalone/kubernetes/#application-mode&quot;&gt;Flink deployed in Application Mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/elastic_scaling/#reactive-mode&quot;&gt;Flink Elastic Scaling in Reactive Mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/system/files/osdi18-kalavri.pdf&quot;&gt;Three steps is all you need: fast, accurate, automatic scaling decisions for distributed streaming dataflows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmlflink&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Oct 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/ml-predictive-autoscaling-for-flink</link>
        <guid isPermaLink="true">https://engineering.grab.com/ml-predictive-autoscaling-for-flink</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        <category>data-science</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Modernising Grab’s model serving platform with NVIDIA Triton Inference Server</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/catwalk-evolution&quot;&gt;Catwalk&lt;/a&gt; is Grab’s machine learning (ML) model serving platform, designed to enable data scientists and engineers in deploying production-ready inference APIs. Currently, Catwalk powers hundreds of ML models and online deployments. To accommodate this growth, the platform has adapted to the rapidly evolving machine learning technology landscape. This involved progressively integrating support for multiple frameworks such as ONNX, PyTorch, TensorFlow, and vLLM. While this approach initially worked for a limited number of frameworks, it soon became unsustainable as maintaining various inference engines, ensuring backward compatibility, and managing deprecated legacy components (such as the ONNX server) introduced significant technical debt. Over time, this resulted in degraded platform performance: with increased latency, reduced throughput, and escalating costs. These issues began to impact users, as larger models could no longer be served efficiently or cost-effectively by legacy components. Recognising the need for change, the team revisited the platform’s design to address these challenges.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-and-implementation&quot;&gt;Evaluation and implementation&lt;/h2&gt;

&lt;p&gt;After evaluating other industry-leading model serving platforms and studying best practices, we decided to conduct an in-depth analysis of NVIDIA Triton. Triton offers significant advantages as an inference engine, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Multi-framework support&lt;/strong&gt;: Compatibility with major ML frameworks, including ONNX, PyTorch, and TensorFlow, ensuring versatility and broad applicability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unified inference interface&lt;/strong&gt;: Provides a single, consistent API for various ML frameworks, simplifying user interaction and reducing overhead when switching between models or frameworks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Hardware optimisation&lt;/strong&gt;: Optimised for NVIDIA GPUs, Triton delivers strong performance on CPU-only environments and specialised instances like AWS Inferentia.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Up-to-date support&lt;/strong&gt;: Continuously updated by upstream to support the latest optimisation and features from upstream ML frameworks, ensuring access to cutting-edge capabilities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Advanced inference features&lt;/strong&gt;: Includes capabilities like dynamic batching and model ensembling (model pipelining), which enhances throughput and efficiency for complex ML workflows.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our extensive benchmarking demonstrated that NVIDIA Triton delivers substantial enhancements in both performance and service stability compared to our existing solutions.&lt;/p&gt;

&lt;p&gt;We are now working towards consolidating the various inference engines we manage into a unified, all-in-one Triton engine, beginning with ONNX adoption as the first phase of implementation.&lt;/p&gt;

&lt;p&gt;In this blog, we aim to share our journey of adopting Triton. From initial benchmarking results on one of Grab’s core models facing performance challenges, to the development of the “Triton manager”, a component designed to integrate Triton into our platform seamlessly and with minimal user disruption. Ultimately, more than 50% of online deployments were successfully migrated to Triton, with some of our critical systems achieving a 50% improvement in tail latency.&lt;/p&gt;

&lt;h2 id=&quot;exploratory-benchmark-results&quot;&gt;Exploratory benchmark results&lt;/h2&gt;

&lt;p&gt;We conducted rigorous testing of Triton against our existing ONNX server under varying levels of request traffic.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/table-1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 1: Benchmark results of Triton against Catwalk ONNX server.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;During testing with a transformer-based model, Triton demonstrated the ability to handle at least 5 times the traffic while maintaining excellent latency. Additionally, its performance was further enhanced with features like batching enabled, and there is potential for even greater optimisation by converting the model to TensorRT, leveraging GPU support.&lt;/p&gt;

&lt;p&gt;Through profiling, we learned that a handful of ONNX Runtime knobs have an outsized impact on throughput. One low-effort, high-return tweak is to set the intra-op thread count to match the number of physical CPU cores. In most cases, this single change yields a healthy performance lift, sparing us from time-consuming, model-by-model micro-optimisation.&lt;/p&gt;

&lt;h2 id=&quot;adopting-triton-at-scale&quot;&gt;Adopting Triton at scale&lt;/h2&gt;

&lt;p&gt;While the benchmark results clearly demonstrate Triton’s advantages, the primary challenge was ensuring a seamless migration, ideally with minimal user reactions. Given the high frequency of migrations within our company, even exceptional performance improvements are often insufficient to fully motivate internal users to adopt new systems. From our point of view, a successful migration required:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Maintaining API compatibility with existing systems.&lt;/li&gt;
  &lt;li&gt;Ensuring zero-downtime.&lt;/li&gt;
  &lt;li&gt;Preserving all existing functionality while adding new capabilities.&lt;/li&gt;
  &lt;li&gt;Minimising disruption to downstream services and users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To streamline the migration process, we opted to manage it centrally within our platform, rather than relying on individual users to address the details themselves.&lt;/p&gt;

&lt;p&gt;We landed on the idea of offering Triton to our users as a drop-in replacement for the old server, with the help of a new component, “Triton manager”. The Triton manager is a critical component that glues Triton to the Catwalk ecosystem. It consists of two major components: Triton server manager and Triton proxy.&lt;/p&gt;

&lt;p&gt;Triton server manager is designed as the entry point of our Catwalk Triton. It downloads the model from remote storage, runs verification on the model files, prepares per-model configurations based on users’ customisation, and lastly it launches the Triton server. It also periodically checks the server’s health and provides observability overlooking the server’s status.&lt;/p&gt;

&lt;p&gt;Triton proxy provides backward compatibility to the existing clients. It hosts endpoints that translate requests from the older API and forward them to the Triton server. The proxy layer plays a crucial role in facilitating a seamless transition from our legacy servers, eliminating the need for user code changes. The conversion logic is designed to prioritise performance, ensuring minimal overhead. Extensive benchmarks were conducted during development to validate and optimise its efficiency.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: High-level architecture for Triton Inference Server (TIS) deployment at Catwalk.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Finally, a special mode in the Triton server manager is implemented to allow the Triton Inference Server (TIS) to be backward compatible with the command line interface of the existing ONNX runtime server used in Catwalk.&lt;/p&gt;

&lt;p&gt;We plan to enhance the Triton Manager to ensure backward compatibility with other ML frameworks, as part of our efforts to onboard additional frameworks seamlessly.&lt;/p&gt;

&lt;h2 id=&quot;rollout-result&quot;&gt;Rollout result&lt;/h2&gt;

&lt;p&gt;Within just 10 days of Triton’s availability, we successfully rolled it out to over 50% of our online model deployments. Thanks to rigorous testing for backward compatibility, the rollout was seamless, with most users unaware of the transition while benefiting from the improved performance.&lt;/p&gt;

&lt;h2 id=&quot;tritons-impacts-on-critical-models&quot;&gt;Triton’s impacts on critical models&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Latency before and after rollout in ms. Blue line: XGBoost-based model. Orange line: transformer-based model. Solid line: average. Dashed line: p99&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We’ve observed significant performance improvements in our business-critical models that have high demands for stability. Latency improvements were consistently observed in all models, especially in the models that suffered from highly volatile request traffic. For some larger transformer models, the p90 latency decreased dramatically from 120ms to 20ms, and the average latency remained steady at 4ms. Smaller XGBoost models maintained their average latency at 2ms across regions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/nvida-triton/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt; Figure 3: Number of pods, before (blue line) and after (purple line) rollout in another model.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Triton has delivered significant cost savings for certain models, with some achieving over 90% reductions due to its advanced optimisations. These improvements have come alongside enhanced performance and reliability.&lt;/p&gt;

&lt;p&gt;It is worth noting that Triton was initially rolled out with limited capabilities to prioritise backward compatibility and ensure a seamless migration. However, we’ve noticed that higher tail latency still remains an issue when facing request spikes for larger models in production. To address this, we are working on enabling batching through Triton to minimise tail latency during traffic surges. This effort will involve close collaboration with model owners to optimise the capacity of each Triton instance further.&lt;/p&gt;

&lt;h2 id=&quot;early-cost-impact-of-the-migration&quot;&gt;Early cost impact of the migration&lt;/h2&gt;

&lt;p&gt;To gauge the financial upside of migrating to Triton, we took a snapshot of 11 production ML services that had already completed the migration. For every ML service, we compared its infrastructure spend over the 14 days before the cut-over with the 14 days after.&lt;/p&gt;

&lt;p&gt;Despite the staggered migration dates, the trend was uniform: average spend fell by ~ 20% across this small cohort within 14 days. As more models and applications migrate, we expect the absolute dollar savings to scale proportionally.&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;Initial results are aligned with our benchmarks for the Triton migration. With improved performance and cost reduction, we expect model owners to either upgrade their model sizes or allow for higher Queries Per Second (QPS). While making further progress with the overall Triton migration, the model serving platform team will continue to monitor cost differences and provide consultation to model owners who seek further optimisation for their deployments.&lt;/p&gt;

&lt;p&gt;Another key takeaway is the painless migration of Triton for our internal users. Rather than asking internal users to make necessary code changes, our team dedicated significant time to providing Triton as a drop-in inference engine to minimise any inconvenience of migration.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Big appreciation to Shengwei Pang from the Geo team, Khai Hung Do, Nhat Minh Nguyen, and Siddharth Pandey from the Catwalk team, along with Richard Ryu from the PM team and Padarn George Wilson for the sponsorship. &lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebmodel&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 21 Oct 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/modernising-grab-model-serving-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/modernising-grab-model-serving-platform</guid>
        
        <category>engineering</category>
        
        <category>performance</category>
        
        <category>data-science</category>
        
        
        <category>Engineering</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Highly concurrent in-memory counter in GoLang</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Ah, the familiar &lt;em&gt;beep beep beep&lt;/em&gt; but don’t worry, it’s not your alarm coaxing you out of bed. No, this is far worse: the dreaded PagerDuty on-call alert! What’s the crisis this time? There appears to be an issue with high database CPU utilisation, overwhelmed by a flood of heavy traffic. If you’re a developer, chances are you’ve faced this scenario at least once. The very moment when you question every life decision while desperately searching for answers at 3 AM.&lt;/p&gt;

&lt;p&gt;This article was born of one such heart-pounding, adrenaline-fuelled incident. Picture this: the database was struggling, the traffic was relentless, and the team was caught in the crossfire. The seemingly obvious solution was to migrate from SQL to NoSQL—a straightforward fix, or so it seemed. Instead of taking the easy way out, we stepped back, rolled up our sleeves, and tackled the problem head-on, embarking on a bold journey of optimisation.&lt;/p&gt;

&lt;p&gt;What followed was a rollercoaster of trial, error, and a few “why did we even try this” moments. Yet, isn’t that the beauty of being a developer? Embracing the chaos, thriving in the madness, and eventually emerging victorious with a story worth sharing.&lt;/p&gt;

&lt;p&gt;Real-time usage count tracking is a common use case that can be found across many applications, like Instagram’s post like count, YouTube’s watch count, or a marketing campaign usage count, which is used in monitoring and measuring the performance of marketing campaigns to assess effectiveness. These counts don’t have to be highly accurate, but rather an approximation in most use cases. This meant that in an occurrence of an event, instead of immediately updating the count in the database, the count is cached in the application server and later updated in batches to reduce the database Queries Per Second (QPS) and Central Processing Unit (CPU) utilisation.&lt;/p&gt;

&lt;p&gt;This article shares one such use case where we optimised the campaign usage count tracking with highly concurrent in-memory caching that flushes to the database at periodic intervals.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Marketing campaigns are configured to deliver push notifications, emails, and award rewards and points to Grab users. Total usage as well as daily usage needs to be tracked for display purposes to give a sense of how the campaign is performing. In this use case, accuracy is not a top priority. This release in constraint helps us to reduce write traffic by incrementing the counter in-memory and flushing the disk at periodic intervals for persistence.&lt;/p&gt;

&lt;p&gt;In this section, let’s break down the process of designing a highly concurrent in-memory counter with data persistence.&lt;/p&gt;

&lt;h3 id=&quot;functional-requirements&quot;&gt;Functional requirements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Upsert the counter value for the given key.&lt;/li&gt;
  &lt;li&gt;Periodically flush the counter value to the storage layer for persistence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;non-functional-requirements&quot;&gt;Non-functional requirements&lt;/h3&gt;

&lt;p&gt;Do note that although consistency is not critical for this use case, we will build a generic in-memory counter with the following guarantees, which can be reused for other use cases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Highly consistent updates of the counter values in memory during high concurrency.&lt;/li&gt;
  &lt;li&gt;Consistent flushing of the counter values to the storage layer for persistence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simple GoLang code for writing an in-memory counter may look like the code sample shown in &lt;strong&gt;Figure 1&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/memory-counter-golang/code-base-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. In-memory counter code snippet.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The code has a map declared globally, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do&lt;/code&gt; function increments the counter value against the key. However, this code fails to work when multiple Goroutines (GoLang version of threads) try to access this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do&lt;/code&gt; function concurrently. This will result in the following error, as shown in &lt;strong&gt;Figure 2&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/memory-counter-golang/code-base-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Code error sample.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Maps in GoLang are not thread safe and need to be locked when being accessed concurrently. The GoLang sync package has Mutex, which serves this locking purpose. The code changes are shown in &lt;strong&gt;Figure 3&lt;/strong&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sync.RWMutex&lt;/code&gt; object is declared globally and every time the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do&lt;/code&gt; function is called, the lock is obtained first. Then the map is mutated, followed by releasing the lock at the end. This code works as intended even when multiple go routines try to access it concurrently.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/memory-counter-golang/code-base-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Implementing sync.RWMutex for locking purpose.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The code for the functional requirement of periodically flushing the counter value to the storage layer is shown in &lt;strong&gt;Figure 4&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/memory-counter-golang/code-base-4.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Code snippet of flushing counter value to storage function. &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Assuming that this design is a success, every 200 milliseconds, a background job acquires a global map lock, iterates over all keys, writes each entry to the storage layer asynchronously, then deletes it from the map. After that, a flush is executed where counter increments are blocked until the lock is released. We can further optimise the flushing process by acquiring the lock, handing the old map to the flusher, swapping in a fresh map to handle new traffic, and then releasing the lock.&lt;/p&gt;

&lt;h2 id=&quot;can-we-do-something-better&quot;&gt;Can we do something better?&lt;/h2&gt;

&lt;p&gt;Yes, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sync.Map&lt;/code&gt; is the synchronised version of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;map&lt;/code&gt; in GoLang. This can be used to get rid of the explicit locking overheads. This works well when the keys accessed are disjoint and the map has a finite set of keys accessed frequently.&lt;/p&gt;

&lt;p&gt;Powerful features of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sync.Map&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;LoadOrStore&lt;/strong&gt;: Retrieves the existing value for a key if present, or stores and returns a new value if the key is absent. Ensuring atomic operation and preventing race conditions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CompareAndSwap&lt;/strong&gt;: Atomically compares a variable’s current value to an expected value. If they match, it is swapped with a new value, ensuring thread-safe updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;LoadAndDelete&lt;/strong&gt;: Atomically retrieves and removes the value of a given key, returning the value and a boolean indicating if the key was present.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When combined, these &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sync.Map&lt;/code&gt; features produce the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do&lt;/code&gt; function shown in &lt;strong&gt;Figure 5&lt;/strong&gt;. When the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do&lt;/code&gt; function is called, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LoadOrStore&lt;/code&gt; function tries to atomically store the key in the map if the key is absent. Otherwise, it returns the current value for the key with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;isLoaded&lt;/code&gt; variable set to true. If the key is already present, a new value is created by summing up the increment value with the current value and setting it as the new value in the map using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CompareAndSwap&lt;/code&gt; function. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compareAndSwap&lt;/code&gt; function successfully sets the new value to the key only if the existing value in the map matches the current value. During high concurrency, this can fail, so we recursively retry until the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CompareAndSwap&lt;/code&gt; replaces the current value with the new value.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/memory-counter-golang/code-base-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Sync.Map features in do function. &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The code example for periodically flushing the counter value to the storage layer is shown in &lt;strong&gt;Figure 6&lt;/strong&gt;. In the previous version of the code, it obtained the lock on the entire map and flushed the counter to the storage layer before releasing the lock. However, there is no locking during this flushing operation. Instead, we rely on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LoadAndDelete&lt;/code&gt; function to atomically remove a key from the map. This also returns the latest value for the key, which is updated into the storage layer async.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/memory-counter-golang/code-base-6.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Code snippet of LoadAndDelete function. &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Note: In production, we run flushKeys less frequently, trading a risk of data loss for lower database usage and, consequently, improved database performance.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;p&gt;In an experiment conducted with an Apple M4 24 GB RAM machine, using the test case of spawning up to millions of concurrent Go routines to increment the counter of 2000 keys.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go test -bench . -benchmem -benchtime=80s -v inmemory_counter_test.go
goos: darwin
goarch: arm64
cpu: Apple M4 Pro
Benchmark1_Mutex_2000Keys
Benchmark1_Mutex_2000Keys-14            588503698              161.5 ns/op             0 B/op          0 allocs/op
Benchmark2_SyncMap_2000Keys
Benchmark2_SyncMap_2000Keys-14          1000000000              59.89 ns/op           88 B/op          4 allocs/op
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Metric&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Mutex&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Sync.Map&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Average latency&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;159 ns/op&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;53 ns/op&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Speed advantage&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.0x faster&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Throughput (60s)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;466M ops&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1B ops&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Memory/op&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0 B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;88 B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Allocations/op&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In summary, getting rid of explicit locking with Sync.Map is 3 times faster than using map with Mutex.&lt;/p&gt;

&lt;h2 id=&quot;why-is-syncmap-is-faster-than-map-with-mutex-in-our-case&quot;&gt;Why is Sync.Map is faster than map with Mutex in our case?&lt;/h2&gt;

&lt;p&gt;Acknowledging the factual information on Sync.Map:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sync.Map is optimised for the ready heavy workloads.&lt;/li&gt;
  &lt;li&gt;Sync.Map maintains two maps internally: read map and dirty map and has additional overheads in internal management.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the actual production environment, there are roughly a few thousand keys in the map (campaignIDs) and at high QPS. Go routines update the same keys concurrently (tracking the usage by incrementing internal value).&lt;/p&gt;

&lt;p&gt;Looking into the internal implementation of Sync.Map. Each key holds a pointer to a struct named entry, which holds the unsafe pointer to the actual value. Whenever a key is accessed and the key is present in the internal read map, the pointer to the struct is returned, and CompareAndSwap (CAS) is then used to atomically replace the pointer to the new value present in the struct in case of update. This strategy is lock free, but there is contention by CAS operation.&lt;/p&gt;

&lt;p&gt;When a key already exists in the internal read map, the value pointer is updated atomically without acquiring a lock—the fast path. The dirty map is only involved when the key is missing from the read map or has been marked for deletion—the slow path. In our case, the same finite set of keys is accessed across Go routines over time, so we hit the fast path about 99% of the time. Compared with using a regular map protected by a Mutex that locks on every update, Sync.Map is typically faster for this access pattern.&lt;/p&gt;

&lt;p&gt;As we have an estimate of 2000 entries in the map, the total memory occupied by Sync.Map will be roughly around 150KB~200KB in memory, including the overheads.&lt;/p&gt;

&lt;p&gt;Performance varies by case. Benchmarking is necessary to determine the optimal strategy for our specific use case. It is important to take note that Sync.Map performance may degrade if there are frequent insertions or deletions. In our use case, insertion of keys happens occasionally when a new campaign starts. The deletion of keys is triggered by the flush keys job.&lt;/p&gt;

&lt;h2 id=&quot;approach-comparison&quot;&gt;Approach comparison&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Map with Mutex&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Synchronised map (Sync.Map)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Locks are explicitly taken.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Implicit locks.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Locks serialises every update operation though different keys are accessed.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Atomic operations on different keys happen in parallel when keys are present in the read map.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Benchmark running over 80 seconds with 2000 keys in map, each operation took 160ns on avg with total throughput of 466million operations.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Benchmark running over 80 seconds with 2000 keys in map, each operation took 53ns on avg with total throughput of 1billion operations.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We implemented the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sync.Map&lt;/code&gt; approach for our in-memory counter that periodically flushes the campaign usage count in the database. This implementation resulted in the following efficiency improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;68% decrease in usage tracking update queries, nose-diving from 140 QPS to just 45 QPS!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Master database experienced a significant reduction in CPU utilisation, decreasing by 48.5%—from 35% to just 18%, alleviating considerable strain on its resources.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replica databases benefited from a 37% decrease in CPU utilisation, dropping from 19% to a more manageable 12%.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Through this optimisation journey, we successfully overcame the challenging database CPU bottlenecks while avoiding the substantial effort and complexity of migrating from SQL to NoSQL. Who would have thought that a calculated leap of faith could save us so much time, effort, and countless sleepless nights? At times, the most effective solutions arise from taking a step back and approaching the problem with a fresh perspective, rather than rushing towards an immediate fix.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;grb.to/gebgolang&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Oct 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/highly-concurrent-in-memory-counter-in-go-lang</link>
        <guid isPermaLink="true">https://engineering.grab.com/highly-concurrent-in-memory-counter-in-go-lang</guid>
        
        <category>Database</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>User foundation models for Grab</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Artificial intelligence (AI) is central to Grab’s mission of delivering valuable, personalised experiences to millions of users across Southeast Asia. Achieving this requires a deep understanding of individual preferences, such as their favorite foods, relevant advertisements, spending habits, and more. This personalisation is driven by recommender models, which depend heavily on high-quality representations of the user.&lt;/p&gt;

&lt;p&gt;Traditionally, these models have relied on hundreds to thousands of manually engineered features. Examples include the types of food ordered in the past week, the frequency of rides taken, or the average spending per transaction. However, these features were often highly specific to individual tasks, siloed within teams, and required substantial manual effort to create. Furthermore, they struggled to effectively capture time-series data, such as the sequence of user interactions with the app.&lt;/p&gt;

&lt;p&gt;With advancements in learning from tabular and sequential data, Grab has developed a foundation model that addresses these limitations. By simultaneously learning from user interactions (clickstream data) and tabular data (e.g. transaction data), the model generates user embeddings that capture app behavior in a more holistic and generalised manner. These embeddings, represented as numerical values, serve as input features for downstream recommender models, enabling higher levels of personalisation and improved performance. Unlike manually engineered features, they generalise effectively across a wide range of tasks, including advertisement optimisation, dual app prediction, fraud detection, and churn probability, among others.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. The process of building a foundation model involves three steps.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We build foundation models by first constructing a diverse training corpus encompassing user, merchant, and driver interactions. The pre-trained model can then be used in two ways. Based on &lt;strong&gt;Figure 1&lt;/strong&gt;, in 2a we extract user embeddings from the model to serve downstream tasks to improve user understanding. The other path is 2b, where we fine-tune the model to make predictions directly.&lt;/p&gt;

&lt;h2 id=&quot;crafting-a-foundation-model-for-grabs-users&quot;&gt;Crafting a foundation model for Grab’s users&lt;/h2&gt;

&lt;p&gt;Grab’s journey towards building its own foundation model began with a clear recognition: existing models are not well-suited to our data. A general-purpose Large Language Model (LLM), for example, lacks the contextual understanding required to interpret why a specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;geohash&lt;/code&gt; represents a bustling mall rather than a quiet residential area. Yet, this level of insight is precisely what we need for effective personalisation. This challenge extends beyond IDs, encompassing our entire ecosystem of text, numerical values, locations, and transactions.&lt;/p&gt;

&lt;p&gt;Moreover, this rich data exists in two distinct forms: tabular data that captures a user’s long-term profile, and sequential time-series data that reflects their immediate intent. To truly understand our users, we needed a model capable of mastering both forms simultaneously. It became evident that off-the-shelf solutions would not suffice, prompting us to develop a custom foundation model tailored specifically to our users and their unique data.&lt;/p&gt;

&lt;h2 id=&quot;the-importance-of-data&quot;&gt;The importance of data&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. We use tabular and time-series data to build user embeddings.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The success of foundation models hinges on the quality and diversity of the datasets used for training. Grab identified two essential sources of data for building user embeddings as shown in &lt;strong&gt;Figure 2&lt;/strong&gt;. Tabular data provides general attributes and long-term behavior. Time-series data reflects how the user uses the app and captures the evolution of user preferences.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tabular data&lt;/strong&gt;: This classic data source provides general user attributes and insights into long-term behavior. For example, this includes attributes like a user’s age and saved locations, along with aggregated behavioral data such as their average monthly spending or most frequently used service.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Time-series clickstream data&lt;/strong&gt;: Sequential data captures the dynamic nature of user decision-making and trends. Grab tracks every interaction on its app, including what users view, click, consider, and ultimately transact. Additionally, metrics like the duration between events reveal insights into user decisiveness. Time-series data provides a valuable perspective on evolving user preferences.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A successful user foundation model must be capable of integrating both tabular and time-series data. Adding to the complexity is the diversity of data modalities, including categorical/text, numerical, user IDs, images, and location data. Each modality carries unique information, often specific to Grab’s business, underscoring the need for a bespoke architecture.&lt;/p&gt;

&lt;p&gt;This inherent diversity in data modalities distinguishes Grab from many other platforms. For example, a video recommendation platform primarily deals with a single modality: videos, supplemented by user interaction data such as watch history and ratings. Similarly, social media platforms are largely centred around posts, images, and videos. In contrast, Grab’s identity as a “superapp” generates a far broader spectrum of user actions and data types. As users navigate between ordering food, booking taxis, utilising courier services, and more, their interactions produce a rich and varied data trail that a successful model must be able to comprehend. Moreover, an effective foundation model for Grab must not only create embeddings for our users but also for our merchant-partners and driver-partners, each of whom brings their own distinctive sets of data modalities.&lt;/p&gt;

&lt;h3 id=&quot;examples-of-data-modalities-at-grab&quot;&gt;Examples of data modalities at Grab&lt;/h3&gt;

&lt;p&gt;To illustrate the breadth of data, consider these examples across different modalities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Text:&lt;/strong&gt; This includes user-provided information such as search queries within GrabFood or GrabMart (“chicken rice,” “fresh milk”) and reviews or ratings for drivers and restaurants. For merchants, this could encompass the restaurant’s name, menu descriptions, and promotional texts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Numerical:&lt;/strong&gt; This modality is rich with data points such as the price of a food order, the fare for a ride, the distance of a delivery, the waiting time for a driver, and the commission earned by a driver-partner. User behavior can also be quantified through numerical data, such as the frequency of app usage or average spending over a month.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Merchant/User/Driver ID:&lt;/strong&gt; These categorical identifiers are central to the platform. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt; tracks an individual’s activity across all of Grab’s services. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merchant_id&lt;/code&gt; represents a specific restaurant or store, linking to its menu, location, and order history. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;driver_id&lt;/code&gt; corresponds to a driver-partner, associated with their vehicle type, service area, and performance metrics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Location data:&lt;/strong&gt; Geographic information is fundamental to Grab’s operations. This includes airport locations, malls, pickup and drop-off points for a ride (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(lat_A, lon_A)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(lat_B, lon_B)&lt;/code&gt;), the delivery address for a food order, and the real-time location of drivers. This data helps in understanding user routines (e.g., commuting patterns) and logistical flows.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-challenges-and-opportunities-of-diverse-modalities&quot;&gt;The challenges and opportunities of diverse modalities&lt;/h3&gt;

&lt;p&gt;The sheer variety of these data modalities presents several significant challenges and opportunities for building a unified user foundation model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data heterogeneity:&lt;/strong&gt; The different data types—text, numbers, geographical coordinates, and categorical IDs do not naturally lend themselves to being combined. Each modality has its own unique structure and requires specialised processing techniques before it can be integrated into a single model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Complex interactions as an opportunity:&lt;/strong&gt; The relationships between different modalities are often intricate, revealing a user’s context and intent. A model that only sees one data type at a time will miss the full picture.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, consider a single user’s evening out. The journey begins when they book a ride (involving their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt; and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;driver_id&lt;/code&gt;) to a specific drop-off point, such as a popular shopping mall (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location data&lt;/code&gt;). Two hours later, from that same mall location, they open the app again and perform a search for “Japanese food” (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text data&lt;/code&gt;). They then browse several restaurant profiles (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merchant_ids&lt;/code&gt;) before placing an order, which includes a price (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numerical data&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;A traditional, siloed model would treat the ride and the food search as two independent events. However, the real opportunity lies in capturing the interactions within a single user’s journey. This is precisely what our unified foundation model is designed to achieve: to identify the connections and recognise that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;drop-off location&lt;/code&gt; of a ride provides valuable context for a subsequent &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text search&lt;/code&gt;. A model that understands a location is not merely a coordinate, but a place that influences a user’s next action, can develop a far deeper understanding of user context. Unlocking this capability is the key to achieving superior performance in downstream tasks, such as personalisation.&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model architecture&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Transformer architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt; displays Grab’s transformer architecture, enabling joint pre-training on tabular and time-series data with different modalities. Grab’s foundation model is built on a transformer architecture specifically designed to tackle four fundamental challenges inherent to Grab’s superapp ecosystem:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Jointly training on tabular and time-series data:&lt;/strong&gt; A core requirement is to unify column order invariant tabular data (e.g. user attributes) with order-dependent time-series data (e.g. a sequence of user actions) within a single, coherent model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Handling a wide variety of data modalities:&lt;/strong&gt; The model must process and integrate diverse data types, including text, numerical values, categorical IDs, and geographic locations, each requiring its own specialised encoding techniques.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generalising beyond a single task:&lt;/strong&gt; The model must learn a universal representation from the entire ecosystem to power a wide array of downstream applications (e.g., recommendations, churn prediction, logistics) across all of Grab’s verticals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scaling to massive entity vocabularies:&lt;/strong&gt; The architecture must efficiently handle predictions across vocabularies containing hundreds of millions of unique entities (users, merchants, drivers), a scale that makes standard classification techniques computationally prohibitive.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the following section, we highlight how we tackled each challenge.&lt;/p&gt;

&lt;h2 id=&quot;1-unifying-tabular-and-time-series-data&quot;&gt;1. Unifying tabular and time-series data&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Differences between tabular data and time-series data&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;A key architectural challenge lies in jointly training on both tabular and time-series data. Tabular data, which contains user attributes, is inherently order-agnostic — the sequence of columns does not matter. In contrast, time-series data is order-dependent, as the sequence of user actions is critical for understanding intent and behavior.&lt;/p&gt;

&lt;p&gt;Traditional approaches often process these data types separately or attempt to force tabular data into a sequential format. However, this can result in suboptimal representations, as the model may incorrectly infer meaning from the arbitrary order of columns.&lt;/p&gt;

&lt;p&gt;Our solution begins with a novel tokenisation strategy. We define a universal token structure as a &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt;&lt;/strong&gt; pair.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For &lt;strong&gt;tabular data&lt;/strong&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt; is the column name (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;online_hours&lt;/code&gt;) and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;value&lt;/code&gt; is the user’s attribute (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4&lt;/code&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For &lt;strong&gt;time-series data&lt;/strong&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt; is the event type (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;view_merchant&lt;/code&gt;) and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;value&lt;/code&gt; is the specific entity involved (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merchant_id_114&lt;/code&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; format creates a common language for all input data. To preserve the distinct nature of each data source, we employ custom positional embeddings and attention masks. These components instruct the model to treat &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; pairs from tabular data as an unordered set while treating tokens from time-series data as an ordered sequence. This allows the model to benefit from both data structures simultaneously within a single, coherent framework.&lt;/p&gt;

&lt;h2 id=&quot;2-handling-diverse-modalities-with-an-adapter-based-design&quot;&gt;2. Handling diverse modalities with an adapter-based design&lt;/h2&gt;

&lt;p&gt;The second major challenge is the sheer variety of data modalities: user IDs, text, numerical values, locations, and more. To manage this diversity, our model uses a flexible &lt;strong&gt;adapter-based design&lt;/strong&gt;. Each adapter acts as a specialised “expert” encoder for a specific modality, transforming its unique data format into a unified, high-dimensional vector space.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For modalities like &lt;strong&gt;text&lt;/strong&gt;, adapters can be initialised with powerful pre-trained language models to leverage their existing knowledge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For ID data like &lt;strong&gt;user/merchant/driver IDs&lt;/strong&gt;, we initialise dedicated embedding layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For complex and specialised data like &lt;strong&gt;location coordinates&lt;/strong&gt; or not-so-well-modeled modalities like numbers in existing LLMs, we design custom adapters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After each token passes through its corresponding modality adapter, an additional &lt;strong&gt;alignment layer&lt;/strong&gt; ensures that all the resulting vectors are projected into the same representation space. This step is critical for allowing the model to compare and combine insights from different data types, for example, to understand the relationship between a text search query (“chicken rice”) and a location pin (a specific hawker center). Finally, we feed the aligned vectors into the main transformer model.&lt;/p&gt;

&lt;p&gt;This modular adapter approach is highly scalable and future-proof, enabling us to easily incorporate new modalities like images or audio and upgrade individual components as more advanced architectures become available.&lt;/p&gt;

&lt;h2 id=&quot;3-unsupervised-pre-training-for-a-complex-ecosystem&quot;&gt;3. Unsupervised pre-training for a complex ecosystem&lt;/h2&gt;

&lt;p&gt;A powerful model architecture is only half the story; the learning strategy determines the quality and generality of the knowledge captured in the final embeddings.&lt;/p&gt;

&lt;p&gt;In the industry, recommender models are often trained using a semi-supervised approach. A model is trained on a specific, supervised objective, such as predicting the next movie a user will watch or whether they will click on an ad. After this training, the internal embeddings, which now carry information fine-tuned for that one task, can be extracted and used for related applications. This method is highly effective for platforms with a relatively homogeneous primary task, like video recommendation or social media platforms.&lt;/p&gt;

&lt;p&gt;However, this single-task approach is fundamentally misaligned with the needs of a superapp. At Grab, we need to power a vast and diverse set of downstream use cases, including food recommendations, ad targeting, transport optimisation, fraud detection, and churn prediction. Training a model solely on one of these objectives would create biased embeddings, limiting their utility for all other tasks. Furthermore, focusing on a single vertical like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Food&lt;/code&gt; would mean ignoring the rich signals from a user’s activity in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Transport&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GrabMart&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Financial Services&lt;/code&gt;, preventing the model from forming a truly holistic understanding.&lt;/p&gt;

&lt;p&gt;Our goal is to capture the complex and diverse interactions between our users, merchants, and drivers across all verticals. To achieve this, we concluded that &lt;strong&gt;unsupervised pre-training&lt;/strong&gt; is the most effective path forward. This approach allows us to leverage the full breadth of data available, learning a universal representation of the entire Grab ecosystem without being constrained to a single predictive task.&lt;/p&gt;

&lt;p&gt;To pre-train our model on tabular and time-series data, we combine masked language modeling (reconstructing randomly masked tokens) with next action prediction. On a superapp like Grab, a user’s journey is inherently unpredictable. A user might finish a ride and immediately search for a place to eat, or transition from browsing groceries on GrabMart to sending a package with GrabExpress. The next action could belong to any of our diverse services like mobility, deliveries, or financial services.&lt;/p&gt;

&lt;p&gt;This ambiguity means the model faces a complex challenge: it’s not enough to predict &lt;em&gt;which&lt;/em&gt; item a user might choose; it must first predict the &lt;em&gt;type&lt;/em&gt; of interaction they will even initiate. Therefore, to capture the full complexity of user intent, our model performs a dual prediction that directly mirrors our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; token structure:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It predicts the &lt;strong&gt;type of the next action&lt;/strong&gt;, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;click_restaurant&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;book_ride&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search_mart&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It predicts the &lt;strong&gt;value associated with that action&lt;/strong&gt;, like the specific restaurant ID, the destination coordinates, or the text of the search query.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This dual-prediction task forces the model to learn the intricate patterns of user behavior, creating a powerful foundation that can be extended across our entire platform. To handle these predictions, where the output could be of any modality (an ID, a location, text, etc.), we employ modality-specific reconstruction heads. Each head is designed for a particular data type and uses a tailored loss function (e.g. cross-entropy for categorical IDs, mean squared error for numerical values) to accurately evaluate the model’s predictions.&lt;/p&gt;

&lt;h2 id=&quot;4-the-id-reconstruction-challenge&quot;&gt;4. The ID reconstruction challenge&lt;/h2&gt;

&lt;p&gt;A significant challenge is the sheer scale of our categorical ID vocabularies. The total number of unique merchants, users, and drivers on the Grab platform runs into the hundreds of millions. A standard cross-entropy loss function would require a final prediction layer with a massive output dimension. For instance, a vocabulary of 100 million IDs with a 768-dimension embedding would result in a prediction head of nearly 80 billion parameters, blowing up model parameter count.&lt;/p&gt;

&lt;p&gt;To overcome this, we employ &lt;strong&gt;hierarchical classification&lt;/strong&gt;. Instead of predicting from a single flat list of millions of IDs, we first classify IDs into smaller, meaningful groups based on their attributes (e.g. by city, cuisine type, etc). This is followed by a second-stage prediction within that much smaller subgroup. This technique dramatically reduces the computational complexity, making it feasible to learn meaningful representations for an enormous vocabulary of entities.&lt;/p&gt;

&lt;h2 id=&quot;extracting-value-from-our-foundation-model&quot;&gt;Extracting value from our foundation model&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Our foundation model is pre-trained with tabular and time-series data.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Once our foundation model is pre-trained on the vast and diverse data within the Grab ecosystem, it becomes a powerful engine for driving business value. There are two primary pathways to harness its capabilities: fine-tuning and embedding extraction.&lt;/p&gt;

&lt;p&gt;The first pathway involves fine-tuning the entire model on a labeled dataset for a specific downstream task, such as churn probability or fraud detection, to create a highly specialised and performant predictor.&lt;/p&gt;

&lt;p&gt;The second, more flexible pathway is to use the model to generate powerful pre-trained embeddings. These embeddings serve as rich, general-purpose features that can support a wide range of separate downstream models. The remainder of this section will focus on this second pathway, exploring the types of embeddings we extract and how they empower our applications.&lt;/p&gt;

&lt;h2 id=&quot;the-dual-embedding-strategy-long-term-and-short-term-memory&quot;&gt;The dual-embedding strategy: Long-term and short-term memory&lt;/h2&gt;

&lt;p&gt;Our architecture is deliberately designed to produce two distinct but complementary types of user embeddings, providing a holistic view by capturing both the user’s stable, long-term identity and their dynamic, short-term intent.&lt;/p&gt;

&lt;h4 id=&quot;the-long-term-representation-a-stable-identity-profile&quot;&gt;The long-term representation: A stable identity profile&lt;/h4&gt;

&lt;p&gt;The long-term embedding captures a user’s persistent habits, established preferences, and overall persona. This representation is the learned vector for a given &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id,&lt;/code&gt; which is stored within the specialised User ID adapter. As the model trains on countless sequences from a user’s history, the adapter learns to distill their consistent behaviors into this single, stable vector. After training, we can directly extract this embedding, which effectively serves as the user’s “long-term memory” on the platform.&lt;/p&gt;

&lt;h4 id=&quot;the-short-term-representation-a-snapshot-of-recent-intent&quot;&gt;The short-term representation: A snapshot of recent intent&lt;/h4&gt;

&lt;p&gt;The short-term embedding is designed to capture a user’s immediate context and current mission. To generate this, a sequence of the user’s most recent interactions is processed through the model’s adapters and main transformer block. A &lt;strong&gt;Sequence Aggregation Module&lt;/strong&gt; then condenses the transformer’s output into a single vector. This creates a snapshot of recent user intent, reflecting their most up-to-date activities and providing a fresh understanding of what they are trying to accomplish.&lt;/p&gt;

&lt;h2 id=&quot;scaling-the-foundation-from-terabytes-of-data-to-millions-of-daily-embeddings&quot;&gt;Scaling the foundation: From terabytes of data to millions of daily embeddings&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/user-found-model-img/img-6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Ray framework&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Building a foundation model of this magnitude introduces monumental engineering challenges that extend beyond the model architecture itself. The practical success of our system hinges on our ability to solve two distinct scalability problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Massive-scale training:&lt;/strong&gt; Pre-training our model involves processing terabytes of diverse, multimodal data. This requires a distributed computing framework that is not only powerful but also flexible enough to handle our unique data processing needs efficiently.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High-throughput inference:&lt;/strong&gt; To keep our user understanding current, we must regenerate embeddings for millions of active users daily. This demands a highly efficient, scalable, and reliable batch processing system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To meet these challenges, we built upon the &lt;strong&gt;Ray framework&lt;/strong&gt;, an open-source standard for scalable computing. This choice allows us to manage both training and inference within a unified ecosystem, tailored to our specific needs.&lt;/p&gt;

&lt;h2 id=&quot;core-principle-a-unified-architecture-for-heterogeneous-workloads&quot;&gt;Core principle: A unified architecture for heterogeneous workloads&lt;/h2&gt;

&lt;p&gt;As illustrated by the Ray framework, both our training and inference pipelines share a fundamental workflow: they begin with a complex Central Processing Unit (CPU) intensive data preprocessing stage (tokenisation), which is followed by a Graphics Processing Unit (GPU) intensive neural network computation.&lt;/p&gt;

&lt;p&gt;A naive approach would bundle these tasks together, forcing expensive GPU resources to sit idle while the CPU handles data preparation. Our core architectural principle is to decouple these workloads. Using Ray’s native ability to manage heterogeneous hardware, we create distinct, independently scalable pools of CPU and GPU workers.&lt;/p&gt;

&lt;p&gt;This allows for a highly efficient, assembly-line-style process. Data is first ingested by the CPU workers for parallelised tokenisation. The resulting tensors are then streamed directly to the GPU workers for model computation. This separation is the key to achieving near-optimal GPU utilisation, which dramatically reduces costs and accelerates processing times for both training and inference.&lt;/p&gt;

&lt;h4 id=&quot;distributed-training&quot;&gt;Distributed training&lt;/h4&gt;

&lt;p&gt;Applying this core principle, our training pipeline efficiently processes terabytes of raw data. The CPU workers handle the complex &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key:value&lt;/code&gt; tokenisation at scale, ensuring the GPU workers are consistently fed with training batches. This robust setup significantly reduces the end-to-end training time, enabling faster experimentation and iteration. We will go into more detail on our training framework in a future blog post.&lt;/p&gt;

&lt;h4 id=&quot;efficient-and-scalable-daily-inference&quot;&gt;Efficient and scalable daily inference&lt;/h4&gt;

&lt;p&gt;This same efficient architecture is mirrored for our daily inference task. To generate fresh embeddings for millions of users, we leverage Ray Data—an open-source library used for data processing in AI and Machine Learning (ML) workload, to execute a distributed batch inference pipeline. The process seamlessly orchestrates our CPU workers for tokenisation and our GPU workers for model application.&lt;/p&gt;

&lt;p&gt;This batch-oriented approach is the key to our efficiency, allowing us to process thousands of users’ data simultaneously and maximise throughput. This robust and scalable inference setup ensures that our dozens of downstream systems are always equipped with fresh, high-quality embeddings, enabling the timely and personalised experiences our users expect.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-a-general-foundation-for-intelligence-across-grab&quot;&gt;Conclusion: A general foundation for intelligence across Grab&lt;/h2&gt;

&lt;p&gt;The development of our user foundation model marks a pivotal shift in how Grab leverages AI. It moves us beyond incremental improvements on task-specific models toward a general, unified intelligence layer designed to understand our entire ecosystem. While previous efforts at Grab have combined different data modalities, this model is the first to do so at a foundational level, creating a truly holistic and reusable understanding of our users, merchants, and drivers.&lt;/p&gt;

&lt;p&gt;The generality of this model is its core strength. By pre-training on diverse and distinct data sources from across our platform—ranging from deep, vertical-specific interactions to broader behavioral signals—it is designed to capture rich, interconnected signals that task-specific models invariably miss. The potential of this approach is immense: a user’s choice of transport can become a powerful signal to inform food recommendations, and a merchant’s location can help predict ride demand.&lt;/p&gt;

&lt;p&gt;This foundational approach fundamentally accelerates AI development across the organisation. Instead of starting from scratch, teams can now build new models on top of our high-quality, pre-trained embeddings, significantly reducing development time and improving performance. Existing models can be enhanced by incorporating these rich features, leading to better predictions and more personalised user experiences. Key areas such as ad optimisation, dual app prediction, fraud detection, and churn probability already heavily benefit from our foundation model, but this is just the beginning.&lt;/p&gt;

&lt;h2 id=&quot;our-vision-for-the-future&quot;&gt;Our vision for the future&lt;/h2&gt;

&lt;p&gt;Our work on this foundation model is just the beginning. The ultimate goal is to deliver “embeddings as a product”. A stable, reliable, and powerful basis for any AI-driven application at Grab. While our initial embeddings for users, driver-partners, and merchant-partners have already proven their value, our vision extends to becoming the central provider for all fundamental entities within our ecosystem, including Locations, Bookings, Marketplace items, and more.&lt;/p&gt;

&lt;p&gt;To realise this vision, we are focused on a path of continuous improvement across several key areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unifying and enriching our datasets:&lt;/strong&gt; Our current success comes from leveraging distinct, powerful data sources that capture different facets of the user journey. The next frontier is to unify these streams into a single, cohesive training corpus that holistically represents user activity across all of Grab’s services. This effort will create a comprehensive, low-noise view of user behavior, unlocking an even deeper level of insight.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Evolving the model architecture:&lt;/strong&gt; We will continue to evolve the model itself, focusing on research to enhance its learning capabilities and predictive power to make the most of our increasingly rich data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Improving scale and efficiency:&lt;/strong&gt; As Grab grows, so must our systems. We are dedicated to further scaling our training and inference infrastructure to handle more data and complexity at an even greater efficiency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By providing a continuously improving, general-purpose understanding of these core components, we are not just building a better model; we are building a more intelligent future for Grab. This enables us to innovate faster and deliver exceptional value to the millions who rely on our platform every day.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebfoundation&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Sep 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/user-foundation-models-for-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/user-foundation-models-for-grab</guid>
        
        <category>ai</category>
        
        <category>artificial-intelligence</category>
        
        <category>machine-learning</category>
        
        <category>llm</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Powering Partner Gateway metrics with Apache Pinot</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Grab operates as a dynamic ecosystem involving partners and various service providers, necessitating real-time intelligence and decision-making for seamless integration and service delivery. To facilitate this, &lt;a href=&quot;http://developer.grab.com/&quot;&gt;&lt;strong&gt;GrabDeveloper&lt;/strong&gt;&lt;/a&gt; serves as Grab’s centralized platform for developers and partners. It supports API integration, partner onboarding, and product management. It also provides tech support through staging and production portals with detailed documentation.&lt;/p&gt;

&lt;p&gt;Working alongside Developer Home, &lt;strong&gt;Partner Gateway&lt;/strong&gt; acts as Grab’s secure interface for exposing APIs to third-party entities. It enables seamless interactions between Grab’s hosted services and external consumers, such as mobile apps, web browsers, and partners. Partner Gateway enhances the experience by offering advanced metrics tracking through time-series charts and dashboards. Partner Gateway delivers actionable insights that ensure high performance, reliability, and user satisfaction in application integrations with Grab services.&lt;/p&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use cases&lt;/h2&gt;

&lt;p&gt;Let’s explore GrabDeveloper integration use cases with one of our partners, whom we’ll refer to as “Alpha.” Alpha is a company that specializes in producing and distributing a diverse range of perishable goods. To optimize their operations, time-series charts tracking API traffic request status codes and average API response times play a crucial role.&lt;/p&gt;

&lt;h3 id=&quot;api-traffic-request-service-status-codes-chart&quot;&gt;API traffic request service status codes chart&lt;/h3&gt;

&lt;p&gt;Time-series charts tracking API traffic request status codes offer valuable insights into the performance and reliability of APIs used for managing supply chain logistics, customer orders, and distribution networks. By monitoring these status codes, Alpha can promptly detect and resolve disruptions or failures in their digital systems, ensuring seamless operations and minimizing downtime.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1: API traffic chart from 5th Jan 2025 to 4th Mar 2025.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;api-average-response-times-chart&quot;&gt;API average response times chart&lt;/h3&gt;

&lt;p&gt;Analyzing average response times helps the company maintain efficient communication between various systems, enhancing the speed and reliability of transactions and data exchanges. This proactive monitoring supports Alpha in delivering consistent, high-quality service to customers and partners, ultimately contributing to improved operational efficiency and customer satisfaction.&lt;/p&gt;

&lt;p&gt;Analyzing average response times enables a company to ensure efficient communication across various systems, enhancing transaction speed and data exchange reliability. Proactive monitoring helps Alpha deliver consistent, high-quality service to customers and partners, boosting operational efficiency and customer satisfaction.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2: Average response time chart from 12 Mar 2025 3am to 12 Mar 2025 3pm (Endpoints are mocked for security purposes).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;endpoint-status-dashboard&quot;&gt;Endpoint status dashboard&lt;/h3&gt;

&lt;p&gt;For Alpha, the endpoint status dashboard delivers real-time insights into API performance, enabling swift issue resolution and seamless integration with the company’s systems. The dashboard enhances service reliability, supports business operations, and ensures uninterrupted data exchange, all of which are critical for Alpha’s business processes and customer satisfaction. Furthermore, the transparency and reliability provided by the dashboard strengthens trust in the partnership, ensuring Alpha to confidently rely on the integration to drive their digital initiatives and operational goals.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3: Endpoint status dashboard of express API for company Alpha. *Endpoints are mocked for security purposes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;why-choose-apache-pinot-and-what-is-it&quot;&gt;Why choose Apache Pinot and what is it?&lt;/h2&gt;

&lt;p&gt;To accommodate these use cases, we need a backend storage system engineered for low-latency queries across a wide range of temporal intervals, spanning from one-hour snapshots to 30-day retrospective analyses, whereby it could contain up to ~6.8 billion rows of data in a 30 day period for a particular dataset. This led us to choose Apache Pinot for these use cases, a distributed Online Analytical Processing (OLAP) system designed for low-latency analytical queries on large-scale data with millisecond query latencies.&lt;/p&gt;

&lt;p&gt;Apache Pinot is a real-time distributed OLAP datastore designed to deliver low-latency analytics on large-scale data. It is optimized for high-throughput ingestion and real-time query processing making it ideal for scenarios such as user-facing analytics, dashboards, and anomaly detection. Apache Pinot supports complex queries, including aggregations and filtering. It delivers sub-second response times by leveraging techniques like columnar storage, indexing, and data partitioning to achieve efficient query execution.&lt;/p&gt;

&lt;h3 id=&quot;data-ingestion-process&quot;&gt;Data ingestion process&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4: Data ingestion process.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;API call initiation&lt;/strong&gt;: An API call is made on the partner application and routed through the Partner Gateway.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metric tracking&lt;/strong&gt;: Dimensions such as client ID, partner ID, status code, endpoint, metric name, timestamp, and value (which is the metric) are tracked and uploaded to Datadog, a cloud-based monitoring platform.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka message transformation&lt;/strong&gt;: Within the partner gateway code, an Apache Kafka Producer converts these metrics into Kafka messages and stores them in a Kafka Topic. Grab utilizes Protobuf for serialization and deserialization of Kafka messages. Since Grab’s Golang Kafka ecosystem does not use the Confluent Schema Registry, Kafka messages must be serialized with a magic byte which indicates that they are using Confluent’s Schema Registry, followed by the Schema ID.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Serialization via Apache Flink&lt;/strong&gt;: Serialization is managed using Apache Flink, an open-source stream processing framework. This ensures compatibility with the Confluent Schema Registry Protobuf Decoder plugin on Apache Pinot. The messages are then written to a separate Kafka Topic.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ingestion to Apache Pinot&lt;/strong&gt;: Messages from the Kafka Topic containing the magic byte are ingested directly into Pinot, which references the Confluent Schema Registry to accurately deserialize the messages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Query execution&lt;/strong&gt;: Queries on the Pinot table can be executed via the Pinot Rest Proxy API.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data visualization&lt;/strong&gt;: Users can view their project charts and dashboards on the GrabDeveloper Home UI, where data points are retrieved from queries executed in step 6.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;challenges-faced&quot;&gt;Challenges faced&lt;/h2&gt;

&lt;p&gt;During the initial setup, we encountered significant performance challenges when executing aggregation queries on large datasets exceeding 150GB. Specifically, attempts to retrieve and process data for periods ranging from 20 to 30 days resulted in frequent timeout issues as the queries took longer than 10 seconds. This was particularly concerning as it compromised our ability to meet our Service Level Agreement (SLA) of delivering query results within 300 milliseconds. The existing query infrastructure struggled to efficiently manage the volume and complexity of data within the required timeframe, necessitating optimization efforts to improve performance and reliability.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Drawing from the insights gained on the limitations of our initial solutions, we implemented these strategic optimizations to significantly enhance our table’s performance.&lt;/p&gt;

&lt;h3 id=&quot;partitioning-by-metric-name&quot;&gt;Partitioning by metric name&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Improved data locality&lt;/strong&gt;: Partitioning the Kafka Topic by metric name ensures that related data is grouped together. When a query filters on a specific metric, Pinot can directly access the relevant partitions, minimizing the need to scan unrelated data. This significantly reduces I/O overhead and processing time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficient query pruning&lt;/strong&gt;: By physically partitioning data, only the servers holding the relevant partitions are queried. This leads to more efficient query pruning, as irrelevant data is excluded early in the process, further optimizing performance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enhanced parallel processing&lt;/strong&gt;: Partitioning enables Pinot to distribute queries across multiple nodes, allowing different metrics to be processed in parallel. This leverages distributed computing resources, accelerating query execution and improving scalability for large datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;column-based-on-aggregation-intervals&quot;&gt;Column based on aggregation intervals&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/table-1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 1&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Facilitates time-based aggregations&lt;/strong&gt;: Rounded time columns (e.g., Timestamp_1h for hourly intervals) group data into coarser time buckets, enabling efficient aggregations such as hourly or daily metrics. This simplifies indexing and optimizes storage by precomputing aggregates for specific time intervals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficient data filtering&lt;/strong&gt;: Rounded time columns allow for precise filtering of data within specific aggregation intervals. For example, the query &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT SUM(Value) FROM Table WHERE Timestamp_1h = &apos;2025-01-20 01:00:00&apos;&lt;/code&gt; can exclude irrelevant columns (e.g., column 2) and focus only on rows within the specified time interval, further enhancing query efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;utilizing-the-star-tree-index-in-apache-pinot&quot;&gt;Utilizing the Star-tree index in Apache Pinot&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://docs.pinot.apache.org/basics/indexing/star-tree-index&quot;&gt;Star-tree Index&lt;/a&gt; in Apache Pinot is an advanced indexing structure that enhances query performance by pre-aggregating data across multiple dimensions (e.g., D1, D2). It features a hierarchical tree with a root node, leaf nodes (holding up to T records), and non-leaf nodes that split into child nodes when exceeding T records. Special star nodes store pre-aggregated records by omitting the splitting dimension. The tree is constructed based on a dimensionSplitOrder, dictating node splitting at each level.&lt;/p&gt;

&lt;h4 id=&quot;sample-table-configuration-for-star-tree-index&quot;&gt;Sample table configuration for Star-tree index:&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;tableIndexConfig&quot;: {
  &quot;starTreeIndexConfigs&quot;: [{
    &quot;dimensionsSplitOrder&quot;: [
      &quot;Metric&quot;,
      &quot;Endpoint&quot;,
      &quot;Timestamp_1h&quot;
    ],
    &quot;skipStarNodeCreationForDimensions&quot;: [
    ],
    &quot;functionColumnPairs&quot;: [
      &quot;AVG__Value&quot;
    ],
    &quot;maxLeafRecords&quot;: 1
  }],
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;configuration-explanation&quot;&gt;Configuration explanation:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;dimensionsSplitOrder&lt;/strong&gt;: This specifies the order in which dimensions are split at each level of the tree. The order is “Metric”, “Endpoint”, “Timestamp_1h”. This means the tree will first split by Metric, then by Endpoint, and finally by Timestamp_1h.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;skipStarNodeCreationForDimensions&lt;/strong&gt;: This array is empty, indicating that star nodes will be created for all dimensions specified in the split order. No dimensions are omitted from star node creation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;functionColumnPairs&lt;/strong&gt;: This specifies the aggregation functions to be applied to columns when creating star nodes. The configuration includes “AVG__Value”, meaning the average of the “Value” column will be calculated and stored in star nodes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;maxLeafRecords&lt;/strong&gt;: This is set to 1, indicating that each leaf node will contain only one record. If a node exceeds this number, it will split into child nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;star-tree-diagram&quot;&gt;Star-tree diagram&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5: Star-tree Index Structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Components:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Root node (orange)&lt;/strong&gt;: This is the starting point for traversing the tree structure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Leaf node (blue)&lt;/strong&gt;: These nodes contain up to a configurable number of records, denoted by T. In this configuration, maxLeafRecords is set to 1, meaning each leaf node will contain a maximum of one record.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-leaf node (green)&lt;/strong&gt;: These nodes will split into child nodes if they exceed the maxLeafRecords threshold. Since maxLeafRecords is set to 1, any node with more than one record will split.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Star-node (yellow)&lt;/strong&gt;: These nodes store pre-aggregated records by omitting the dimension used for splitting at that level. This helps in reducing the data size and improving query performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;A practical explanation of the start-tree diagram would be to display the star-tree documents in a table format along with the sample queries used to retrieve the data.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/table-2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Table 2: Star-tree documents table&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Sample queries&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;With&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;no&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimensions&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quickly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obtain&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregated&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;processing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;just&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;XYZ_Req_Count&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XYZ_Req_Count&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;This&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduces&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;processing&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;returning&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregated&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;130&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;instead&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filtering&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregating&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;three&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2025-01-20 00:00:00&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2025-01-20 00:00:00&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;This&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allows&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggregation&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yielding&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Table&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;With&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Metric&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timestamp_1h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Endpoint&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Process&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obtain&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;efficiently&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;comparing-performance-after-the-optimization&quot;&gt;Comparing performance after the optimization&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/partner-gateway/figure-6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6: Chart of query latency with and without optimization.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The graph above in Figure 6, provides a comparison analysis of query performance, showcasing the significant improvements achieved through the implemented optimization solutions. The query execution times are significantly reduced, as evidenced by the logarithmic scale values.&lt;/p&gt;

&lt;p&gt;For the first query which calculates the latency for a particular aggregation interval, the log scale indicates a reduction from 4.64 to 2.32, translating to a decrease in query latency from 43,713 to 209 milliseconds.&lt;/p&gt;

&lt;p&gt;Similarly, the second query, which aggregates the sum of the latency based on the tags for a particular metric, shows a log scale reduction from 3.71 to 1.54, with query latency improving from 5,072 to 35 milliseconds. These results underscore the efficacy of optimization in enhancing query performance, enabling faster data retrieval and processing&lt;/p&gt;

&lt;h2 id=&quot;tradeoffs&quot;&gt;Tradeoffs&lt;/h2&gt;

&lt;p&gt;Star-tree indexes in Apache Pinot are designed to significantly enhance query performance by pre-computing aggregations. This approach allows for rapid query execution by utilizing pre-calculated results, rather than computing aggregations on-the-fly. However, this performance boost comes with a tradeoff in terms of storage space.&lt;/p&gt;

&lt;p&gt;Before implementing the Star-tree index, the total storage size for 30 days of data was approximately 192GB. With the Star-tree index, this increased to 373GB, nearly doubling the storage requirements. Despite the increase in storage, the performance benefits substantially outweigh the costs associated with additional storage.&lt;/p&gt;

&lt;p&gt;The cost impact is relatively minor. We utilize &lt;a href=&quot;https://aws.amazon.com/ebs/pricing/&quot;&gt;AWS gp3 EBS&lt;/a&gt; volumes, which roughly cost $14.48 USD monthly for the extra table (calculated as 0.08 USD x 181 GB). This cost is considered insignificant when compared to the substantial gains in query performance. Alternatively, precomputing the metrics via an ETL job is also feasible; however, it is less cost-effective due to the additional expenses required to maintain the pipeline.&lt;/p&gt;

&lt;p&gt;The decision to use Star-tree indexes is justified by the dramatic improvement in query speed, which enhances user experience and efficiency. The modest increase in storage costs is a worthwhile investment for achieving optimal performance.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In conclusion, Grab’s integration of Apache Pinot as a backend solution within the Partner Gateway represents a forward-thinking strategy to meet the evolving demands of real-time analytics. Apache Pinot’s ability to deliver low-latency queries empowers our partners with immediate, actionable insights into API performance that enhances their integration experience and operational efficiency. This is crucial for partners who require rapid data access to make informed decisions and optimize their services.&lt;/p&gt;

&lt;p&gt;The adoption of Star-tree indexing within Pinot further refines our analytics infrastructure by strategically balancing the trade-offs between query latency and storage costs. This optimization ensures Partner Gateway can support a diverse range of use cases with subsecond query latencies while maintaining high performance and reliability in service delivery reinforcing Grab’s commitment to delivering superior performance across its ecosystem.&lt;/p&gt;

&lt;p&gt;Ultimately, the integration of Apache Pinot enhances Grab’s real-time analytics capabilities while empowering the company to drive innovation and consistently deliver exceptional service to both partners and users.&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Credits to Manh Nguyen from the Coban Infrastructure Team, Michael Wengle from the Midas Team and Yuqi Wang from the DevHome team.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grb.to/gebpinot&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/pinot-partnergateway-tech-blog</link>
        <guid isPermaLink="true">https://engineering.grab.com/pinot-partnergateway-tech-blog</guid>
        
        <category>Database</category>
        
        <category>Data</category>
        
        <category>Apache</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
