<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&apos;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 23 Feb 2024 10:33:18 +0000</pubDate>
    <lastBuildDate>Fri, 23 Feb 2024 10:33:18 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Enabling near real-time data analytics on the data lake</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the domain of data processing, data analysts run their ad hoc queries on the data lake. The lake serves as an interface between our analytics and production environment, preventing downstream queries from impacting upstream data ingestion pipelines. To ensure efficient data processing in the data lake, choosing appropriate storage formats is crucial.&lt;/p&gt;

&lt;p&gt;The vanilla data lake solution is built on top of cloud object storage with Hive metastore, where data files are written in Parquet format. Although this setup is optimised for scalable analytics query patterns, it struggles to handle frequent updates to the data due to two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The Hive table format requires us to rewrite the Parquet files with the latest data. For instance, to update one record in a Hive unpartitioned table, we would need to read all the data, update the record, and write back the entire data set.&lt;/li&gt;
  &lt;li&gt;Writing Parquet files is expensive due to the overhead of organising the data to a compressed columnar format, which is more complex than a row format.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The issue is further exacerbated by the scheduled downstream transformations. These necessary steps, which clean and process the data for use, increase the latency because the total delay now includes the combined scheduled intervals of these processing jobs.&lt;/p&gt;

&lt;p&gt;Fortunately, the introduction of the Hudi format, which supports fast writes by allowing Avro and Parquet files to co-exist on a Merge On Read (MOR) table, opens up the possibility of having a data lake with minimal data latency. The concept of a commit timeline further allows data to be served with Atomicity, Consistency, Isolation, and Durability (ACID) guarantees.&lt;/p&gt;

&lt;p&gt;We employ different sets of configurations for the different characteristics of our input sources:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;High or low throughput.&lt;/strong&gt; A high-throughput source refers to one that has a high level of activity. One example of this can be our stream of booking events generated from each customer transaction. On the other hand, a low-throughput source would be one that has a relative low level of activity. An example of this can be transaction events generated from reconciliation happening on a nightly basis.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kafka (unbounded) or Relational Database Sources (bounded).&lt;/strong&gt; Our sinks have sources that can be broadly categorised into unbounded and bounded sources. Unbounded sources are usually related to transaction events materialised as Kafka topics, representing user-generated events as they interact with the Grab superapp. Bounded sources usually refer to Relational Database (RDS) sources, whose size is bound to storage provisioned.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The following sections will delve into the differences between each source and our corresponding configurations optimised for them.&lt;/p&gt;

&lt;h2 id=&quot;high-throughput-source&quot;&gt;High throughput source&lt;/h2&gt;

&lt;p&gt;For our data sources with high throughput, we have chosen to write the files in MOR format since the writing of files in Avro format allows for fast writes to meet our latency requirements.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/near-realtime-data-analytics/image2.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 Architecture for MOR tables&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As seen in Figure 1, we use Flink to perform the stream processing and write out log files in Avro format in our setup. We then set up a separate Spark writer which periodically converts the Avro files into Parquet format in the Hudi &lt;a href=&quot;https://hudi.apache.org/docs/next/compaction&quot;&gt;compaction process&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have further simplified the coordination between the Flink and Spark writers by enabling asynchronous services on the Flink writer so it can generate the compaction plans for Spark writers to act on. During the Spark job runs, it checks for available compaction plans and acts on them, placing the burden of orchestrating the writes solely on the Flink writer. This approach could help minimise potential concurrency problems that might otherwise arise, as there would be a single actor
orchestrating the associated Hudi table services.&lt;/p&gt;

&lt;h2 id=&quot;low-throughput-source&quot;&gt;Low throughput source&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/near-realtime-data-analytics/image1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 Architecture for COW tables&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For low throughput sources, we gravitate towards the choice of Copy On Write (COW) tables given the simplicity of its design, since it only involves one component, which is the Flink writer. The downside is that it has higher data latency because this setup only generates Parquet format data snapshots at each checkpoint interval, which is typically about 10-15 minutes.&lt;/p&gt;

&lt;h2 id=&quot;connecting-to-our-kafka-unbounded-data-source&quot;&gt;Connecting to our Kafka (unbounded) data source&lt;/h2&gt;

&lt;p&gt;Grab uses Protobuf as our central data format in Kafka, ensuring schema evolution compatibility. However, the derivation of the schema of these topics still requires some transformation to make it compatible with Hudi’s accepted schema. Some of these transformations include ensuring that Avro record fields do not contain just a single array field, and handling logical decimal schemas to transform them to fixed byte schema for Spark compatibility.&lt;/p&gt;

&lt;p&gt;Given the unbounded nature of the source, we decided to partition it by Kafka event time up to the hour level. This ensured that our Hudi operations would be faster. Parquet file writes would be faster since they would only affect files within the same partition, and each Parquet file within the same event time partition would have a bounded size given the monotonically increasing nature of Kafka event time.&lt;/p&gt;

&lt;p&gt;By partitioning tables by Kafka event time, we can further optimise compaction planning operations, since the amount of file lookups required is now reduced with the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BoundedPartitionAwareCompactionStrategy&lt;/code&gt;. Only log files in recent partitions would be selected for compaction and the job manager need not list every partition to figure out which log files to select for compaction during the planning phase anymore.&lt;/p&gt;

&lt;h2 id=&quot;connecting-to-our-rds-bounded-data-source&quot;&gt;Connecting to our RDS (bounded) data source&lt;/h2&gt;

&lt;p&gt;For our RDS, we decided to use the Flink Change Data Capture (CDC) connectors by Veverica to obtain the binlog streams. The RDS would then treat the Flink writer as a replication server and start streaming its binlog data to it for each MySQL change. The Flink CDC connector presents the data as a Kafka Connect (KC) Source record, since it uses the Debezium connector under the hood. It is then a straightforward task to deserialise these records and transform them into Hudi records, since
the Avro schema and associated data changes are already captured within the KC source record.&lt;/p&gt;

&lt;p&gt;The obtained binlog timestamp is also emitted as a metric during consumption for us to monitor the observed data latency at the point of ingestion.&lt;/p&gt;

&lt;p&gt;Optimising for these sources involves two phases:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, assigning more resources for the cold start incremental snapshot process where Flink takes a snapshot of the current data state in the RDS and loads the Hudi table with that snapshot. This phase is usually resource-heavy as there are a lot of file writes and data ingested during this process.&lt;/li&gt;
  &lt;li&gt;Once the snapshotting is completed, Flink would then start to process the binlog stream and the observed throughput would drop to a level similar to the DB write throughput. The resources required by the Flink writer at this stage would be much lower than in the snapshot phase.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;indexing-for-hudi-tables&quot;&gt;Indexing for Hudi tables&lt;/h2&gt;

&lt;p&gt;Indexing is important for upserting Hudi tables when the writing engine performs updates, allowing it to efficiently locate the file groups of the data to be updated.&lt;/p&gt;

&lt;p&gt;As of version 0.14, the Flink engine only supports Bucket Index or Flink State Index. Bucket Index performs indexing of the file record by hashing the record key and matching it to a specific bucket of files indicated by the naming convention of the written data files. Flink State Index on the other hand stores the index map of record keys to files in memory.&lt;/p&gt;

&lt;p&gt;Given that our tables include unbounded Kafka sources, there is a possibility for our state indexes to grow indefinitely. Furthermore, the requirement of state preservation for Flink State Index across version deployments and configuration updates adds complexity to the overall solution.&lt;/p&gt;

&lt;p&gt;Thus, we opted for the simple Bucket Index for its simplicity and the fact that our Hudi table size per partition does not change drastically across the week. However, this comes with a limitation whereby the number of buckets cannot be updated easily and imposes a parallelism limit at which our Flink pipelines can scale. Thus, as traffic grows organically, we would find ourselves in a situation whereby our configuration grows obsolete and cannot handle the increased load.&lt;/p&gt;

&lt;p&gt;To resolve this going forward, using consistent hashing for the Bucket Index would be something to explore to optimise our Parquet file sizes and allow the number of buckets to grow seamlessly as traffic grows.&lt;/p&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;h3 id=&quot;fresh-business-metrics&quot;&gt;Fresh business metrics&lt;/h3&gt;

&lt;p&gt;Post creation of our Hudi Data Ingestion solution, we have enabled various users such as our data analysts to perform ad hoc queries much more easily on data that has lower latency. Furthermore, Hudi tables can be seamlessly joined with Hive tables in Trino for additional context. This enabled the construction of operational dashboards reflecting fresh business metrics to our various operators, empowering them with the necessary information to quickly respond to any abnormalities (such as high-demand events like F1 or seasonal holidays).&lt;/p&gt;

&lt;h3 id=&quot;quicker-fraud-detection&quot;&gt;Quicker fraud detection&lt;/h3&gt;

&lt;p&gt;Another significant user of our solution is our fraud detection analysts. This enabled them to rapidly access fresh transaction events and analyse them for fraudulent patterns, particularly during the emergence of a new attack pattern that hadn’t been detected by their rules engine. Our solution also allowed them to perform multiple ad hoc queries that involve lookbacks of various days’ worth of data without impacting our production RDS and Kafka clusters by using the data lake as the data interface, reducing the data latency to the minute level and, in turn, empowering them to respond more quickly to attacks.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;As the landscape of data storage solutions evolves rapidly, we are eager to test and integrate new features like Record Level Indexing and the creation of Pre Join tables. This evolution extends beyond the Hudi community to other table formats such as IceBerg and DeltaLake. We remain ready to adapt ourselves to these changes and incorporate the advantages of each format into our data lake within Grab.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Hudi: &lt;a href=&quot;https://hudi.apache.org/docs/next/overview/&quot;&gt;https://hudi.apache.org/docs/next/overview/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ververica Flink CDC: &lt;a href=&quot;https://github.com/ververica/flink-cdc-connectors&quot;&gt;https://github.com/ververica/flink-cdc-connectors&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Debezium: &lt;a href=&quot;https://debezium.io/documentation/reference/stable/connectors/mysql.html&quot;&gt;https://debezium.io/documentation/reference/stable/connectors/mysql.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Feb 2024 00:22:10 +0000</pubDate>
        <link>https://engineering.grab.com/enabling-near-realtime-data-analytics</link>
        <guid isPermaLink="true">https://engineering.grab.com/enabling-near-realtime-data-analytics</guid>
        
        <category>Data Analytics</category>
        
        <category>Stream Processing</category>
        
        <category>Kafka</category>
        
        <category>Real-Time</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>The journey of building a comprehensive attribution platform</title>
        <description>&lt;p&gt;The Grab superapp offers a comprehensive array of services from ride-hailing and food delivery to financial services. This creates multifaceted user journeys, traversing homepages, product pages, checkouts, and interactions with diverse content, including advertisements and promo codes.&lt;/p&gt;

&lt;h2 id=&quot;background-why-ads-and-attribution-matter-in-our-superapp&quot;&gt;Background: Why ads and attribution matter in our superapp&lt;/h2&gt;

&lt;p&gt;Ads are crucial for Grab in driving user engagement and supporting our ecosystem by seamlessly connecting users with our services. In the ever-evolving world of advertising, the ability to gauge the impact of marketing investments takes on pivotal significance. Advertisers dedicate substantial resources to promote their businesses, necessitating a clear understanding of the return on AdSpend (ROAS) for each campaign. In this context, attribution plays a central role, serving as the guiding compass for advertisers and marketers, elucidating the effectiveness of touchpoints within campaigns.&lt;/p&gt;

&lt;p&gt;For instance, a merchant-partner seeks to enhance its reach by advertising on the Grab food delivery homepage. With the assistance of our attribution system, the merchant-partner can now precisely gauge the impact of their homepage ads on Grab. This involves tracking user engagement and monitoring the resulting orders that stem from these interactions. This level of granularity not only highlights the value of attribution but also demonstrates its capability in providing detailed insights into the effectiveness of advertising campaigns and enabling merchant-partners to optimise their campaigns with more precision.&lt;/p&gt;

&lt;p&gt;In this blog, we delve into the technical intricacies, software architecture, challenges, and solutions involved in crafting a state-of-the-art engineering solution for the attribution platform.&lt;/p&gt;

&lt;h2 id=&quot;genesis-pre-project-landscape&quot;&gt;Genesis: Pre-project landscape&lt;/h2&gt;

&lt;p&gt;When our journey began in 2020, Grab’s marketing efforts had limited attribution capabilities and data analytics was predominantly reliant on ad hoc queries conducted by business and data analysts. Before the introduction of a standardised approach, we had to manage discrepant results and a time-consuming manual process of data preparation, cleansing, and storage across teams. When issues arose in the analytical pipeline, resolution efforts took relatively longer and were reoccurring. We needed a comprehensive engineering solution that would address the identified gaps, and significantly enhance metrics related to ROI, attribution accuracy, and data-handling efficiency.&lt;/p&gt;

&lt;h2 id=&quot;inception-the-pure-ads-attribution-engine-kappa-architecture&quot;&gt;Inception: The pure ads attribution engine (Kappa architecture)&lt;/h2&gt;

&lt;p&gt;We chose Kappa architecture due to its imperative role in achieving near real-time attribution, especially in support of our new pricing model, cost per order (CPO). With this solution, we aimed to drastically reduce data latency from 2-3 days to just a few minutes. Traditional ETL (Extract, Transform, and Load) based batch processing methods were evaluated but quickly found to be inadequate for our purposes, mainly due to their speed.&lt;/p&gt;

&lt;p&gt;In the advertising industry, rapid decision-making is critical. Traditional batch processing solutions would introduce significant latency, hampering our ability to make real-time, data-driven decisions. With its architecture’s inherent capability for real-time stream processing, Kappa emerged as the logical choice. Additionally, Kappa offers the agility required to empower our ad-serving team for real-time decision support, and better ad ranking and selection, enabling dynamic and effective targeting decisions without delay.&lt;/p&gt;

&lt;p&gt;The first step on this journey was to create a pure and near real-time stream processing Ads Attribution Engine. This engine was based on the Kappa architecture to provide advertisers with quick insights into their ROAS offering real-time attribution, enabling advertisers to optimise their campaigns efficiently.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/attribution-platform/image6.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;High-level workflow of the Ads Attribution Engine&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In this solution, we used the following tools in our tech stack:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka for event streams&lt;/li&gt;
  &lt;li&gt;DDB for events storage&lt;/li&gt;
  &lt;li&gt;Amazon S3 as the data lake&lt;/li&gt;
  &lt;li&gt;An in-house stream processing framework similar to Keystone&lt;/li&gt;
  &lt;li&gt;Redis for caching events&lt;/li&gt;
  &lt;li&gt;ScyllaDB for storing ad metadata&lt;/li&gt;
  &lt;li&gt;Amazon relational database service (RDS) for analytics&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/attribution-platform/image3.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Architecture of the near real-time stream processing Ads Attribution Engine&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;evolution-merging-marketing-levers---ads-and-promos&quot;&gt;Evolution: Merging marketing levers - Ads and promos&lt;/h2&gt;

&lt;p&gt;We began to envision a world where we could merge various marketing levers into a unified Attribution Engine, starting with ads and promos. This evolved vision also aimed to prevent order double counting (when a user interacts with both ads and promos in the same checkout), which would provide a more holistic attribution solution.&lt;/p&gt;

&lt;p&gt;With the unified Attribution Engine, we would also enable more sophisticated personalisation through machine learning models and drive higher conversions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/attribution-platform/image1.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;The unified Attribution Engine workflow, which included Promo touch points&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The unified attribution engine used mostly the same tech stack, except for analytics where Druid was used instead of RDS.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/attribution-platform/image5.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Architecture of the unified Attribution Engine&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;introspection-identifying-shortcomings-and-the-path-to-improvement&quot;&gt;Introspection: Identifying shortcomings and the path to improvement&lt;/h2&gt;

&lt;p&gt;While the unified attribution engine was a step in the right direction, it wasn’t without its challenges. There were challenges related to real-time data processing costs, scalability for longer attribution windows, latency and lag issues, out-of-order events leading to misattribution, and the complexity of implementing multi-touch attribution models. To truly empower advertisers and enhance the attribution process, we knew we needed to evolve further.&lt;/p&gt;

&lt;h2 id=&quot;rebirth-the-birth-of-a-full-fledged-attribution-platform-lambda-architecture&quot;&gt;Rebirth: The birth of a full-fledged attribution platform (&lt;a href=&quot;https://www.databricks.com/glossary/lambda-architecture&quot;&gt;Lambda architecture&lt;/a&gt;)&lt;/h2&gt;

&lt;p&gt;This journey eventually led us to build a full-fledged attribution platform using Lambda architecture, which blended both batch and real-time stream processing methods. With this change, our platform could rapidly and accurately process data and attribute the impact of ads and promos on user behaviour.&lt;/p&gt;

&lt;h3 id=&quot;why-lambdaarchitecture&quot;&gt;Why Lambda architecture?&lt;/h3&gt;

&lt;p&gt;This choice was a strategic one – real-time processing is vital for tracking events as they occur, but it offers only a current snapshot of user behaviour. This means we would not be able to analyse historical data, which is a crucial aspect of accurate attribution and exploring multiple attribution models. Historical data allows us to identify trends, patterns, and correlations not evident in real-time data alone.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/attribution-platform/image2.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;High level workflow for the full-fledged attribution platform with Lambda architecture&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In this system’s tech stack, the key components are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Coban, an in-house stream processing framework used for real-time data processing&lt;/li&gt;
  &lt;li&gt;Spark-based ETL jobs for batch processing&lt;/li&gt;
  &lt;li&gt;Amazon S3 as the data warehouse&lt;/li&gt;
  &lt;li&gt;An offline layer that is capable of providing historical context, handling large data volumes, performing complex analytics, and so on.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;key-benefits-of-the-offline-layer&quot;&gt;Key benefits of the offline layer&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Provides historical context: The offline layer enriches the attribution process by providing a historical perspective on user interactions, essential for precise attribution analysis spanning extended time periods.&lt;/li&gt;
  &lt;li&gt;Handles enormous data volumes: This layer efficiently manages and processes extensive data generated by advertising campaigns, ensuring that attribution seamlessly accommodates large-scale data sets.&lt;/li&gt;
  &lt;li&gt;Performs complex analytics: Enables more intricate computations and data analysis than real-time processing alone, the offline layer is instrumental in fine-tuning attribution models and enhancing their accuracy.&lt;/li&gt;
  &lt;li&gt;Ensures reliability in the face of challenges: By providing fault tolerance and resilience against system failures, the offline layer ensures the continuous and dependable operation of the attribution system, even during unexpected events.&lt;/li&gt;
  &lt;li&gt;Optimises data storage and serving: Relying on Amazon S3, the storage layer for raw data optimises storage by building interactive reporting APIs.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/attribution-platform/image4.png&quot; alt=&quot;&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Architecture of our comprehensive offline attribution platform&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;challenges-with-lambda-and-mitigation&quot;&gt;Challenges with Lambda and mitigation&lt;/h3&gt;

&lt;p&gt;Lambda architecture allows us to have the accuracy and robustness of batch processing along with real-time stream processing. However, we noticed some drawbacks that may lead to complexity due to maintaining both batch and stream processing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Operating two parallel systems for batch and stream processing can lead to increased complexity in production environments.&lt;/li&gt;
  &lt;li&gt;Lambda architecture requires two sets of business logic - one for the batch layer and another for the stream layer.&lt;/li&gt;
  &lt;li&gt;Synchronisation across both layers can make system alterations more challenging.&lt;/li&gt;
  &lt;li&gt;This dual implementation could also allude to inconsistencies and introduce potential bugs into the system.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To mitigate these complications, we’re establishing an optimisation strategy for our current system. By distinctly separating the responsibilities of our real-time pipelines from those of our offline jobs, we intend to harness the full potential of each approach, while simultaneously curbing the added complexity.&lt;/p&gt;

&lt;p&gt;Hence, redefining the way we utilise Lambda architecture, striking an efficient balance between real-time responsiveness and sturdy accuracy with the below proposal.&lt;/p&gt;

&lt;h2 id=&quot;vanguard-enhancements-in-the-future&quot;&gt;Vanguard: Enhancements in the future&lt;/h2&gt;

&lt;p&gt;In the coming months, we will be implementing the optimisation strategy and improving our attribution platform solution. This strategy can be broken down into the following sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-time pipeline handling time-sensitive data&lt;/strong&gt;: Real-time pipelines can process and deliver time-sensitive metrics like CPO-related data in near real-time, allowing for budget capping and immediate adjustments to marketing spend. This can provide us with actionable insights that can help with areas like real-time bidding, real-time marketing, or dynamic pricing. By limiting the volume of data through the real-time path, we can ensure it’s more manageable and focused on immediate actionable data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Batch jobs handling all other reporting data&lt;/strong&gt;: Batch processing is best suited for computations that are not time-bound and where completeness is more important. By dedicating more time to the processing phase, batch processing can handle larger volumes and more complex computations, providing more comprehensive and accurate reporting.&lt;/p&gt;

&lt;p&gt;This approach will simplify our Lambda architecture, as the batch and real-time pipelines will have clear separation of duties. It may also reduce the chance of discrepancies between the real-time and batch-processing datasets and lower the operational load of our real-time system.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-a-holistic-attribution-picture&quot;&gt;Conclusion: A holistic attribution picture&lt;/h2&gt;

&lt;p&gt;Through our journey of building a comprehensive attribution platform, we can now deliver a holistic and dependable view of user behaviour and empower merchant-partners to use insights from advertisements and promotions. This journey has been a long one, but we were able to improve our attribution solution in several ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Attribution latency: Successfully reduced attribution latency from 2-3 days to just a few minutes, ensuring that advertisers can access real-time insights and feedback.&lt;/li&gt;
  &lt;li&gt;Data accuracy: Through improved data collection and processing, we achieved data discrepancies of less than 1%, enhancing the accuracy and reliability of attribution data.&lt;/li&gt;
  &lt;li&gt;Conversion rate: Advertisers witnessed a significant increase in conversion rates, a direct result of our real-time attribution capabilities.&lt;/li&gt;
  &lt;li&gt;Cost efficiency: Embracing the Lambda architecture led to a ~25% reduction in real-time data processing costs, allowing for more efficient campaign optimisations.&lt;/li&gt;
  &lt;li&gt;Operational resilience: Building an offline layer provided fault tolerance and resilience against system failures, ensuring that our attribution system continued to operate seamlessly, even during unexpected events.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Feb 2024 00:22:10 +0000</pubDate>
        <link>https://engineering.grab.com/attribution-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/attribution-platform</guid>
        
        <category>Attribution platform</category>
        
        <category>User journeys</category>
        
        <category>Advertising</category>
        
        
        <category>Engineering</category>
        
        <category>Product</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Managing dynamic marketplace content at scale: Grab&apos;s approach to content moderation</title>
        <description>&lt;p&gt;In the fast-paced world of on-demand delivery, maintaining safe marketplaces is a complex undertaking. Grab, a leading superapp in Southeast Asia, operates GrabFood and GrabMart, two popular marketplaces that connect consumers with a wide range of food and daily necessities. With more than 100k listings for different items updated daily by our merchants across eight different countries, Grab is rising to the challenge of ensuring that its marketplaces remain compliant with its own policies, government regulations as well as platform policies.&lt;/p&gt;

&lt;p&gt;This article explores how Grab employs a combination of automated and manual content moderation to manage its dynamic marketplace content efficiently, while also collaborating with Google to ensure marketplace safety.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-marketplace-landscape&quot;&gt;Dynamic Marketplace Landscape&lt;/h2&gt;

&lt;p&gt;Marketplaces like GrabFood and GrabMart are at the forefront of connecting merchants and consumers. These marketplaces provide an avenue for merchants to showcase their offerings, enabling consumers to conveniently access a plethora of on-demand options. However, in an environment characterized by rapid changes as well as evolving regulatory frameworks, maintaining the integrity of these marketplaces becomes a formidable task.&lt;/p&gt;

&lt;h2 id=&quot;scale-and-flexibility-a-dual-challenge&quot;&gt;Scale and Flexibility: A Dual Challenge&lt;/h2&gt;

&lt;p&gt;The cornerstone of Grab’s success lies in its ability to adapt to the unique regulations and requirements of each country it operates in. This necessitates a nuanced and multifaceted approach to content moderation. To achieve both scale and flexibility, Grab employs a proactive strategy that combines and leverages automated and manual moderation processes.&lt;/p&gt;

&lt;h2 id=&quot;automated-moderation&quot;&gt;Automated Moderation&lt;/h2&gt;

&lt;p&gt;Automated moderation plays a pivotal role in efficiently managing the high volume of listings that undergo daily updates. Grab utilises advanced algorithms and machine learning technologies, built in-house, to scan listings everyday for potential violations of its own policies, government regulations and platform policies. This automation not only speeds up the process to put eligible listings on the Grab platform, but also ensures consistent adherence to predefined guidelines. However, automated moderation is not without its limitations, as contextual understanding and subjective judgment often require human intervention.&lt;/p&gt;

&lt;h2 id=&quot;manual-moderation&quot;&gt;Manual Moderation&lt;/h2&gt;

&lt;p&gt;Recognising the nuanced nature of content moderation, Grab employs a team of human moderators who possess the cultural awareness and contextual understanding necessary to assess complex cases. These moderators review listings flagged by algorithms and machine learning technologies that require human judgment, ensuring that content aligns with Grab’s policies, local regulations as well as platform policies. Manual moderation adds a layer of human insight that automated systems may lack, contributing to a more accurate and contextually sensitive approach.&lt;/p&gt;

&lt;p&gt;In its commitment to ensuring marketplace safety, Grab has also established a strong collaboration with Google. Grab works hand in hand with Google to collectively ensure adherence to Play Store policies and guidelines.&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;grab&quot;&gt;Grab&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Programme Management: Poonam Gambhire, Shuyang Sun&lt;/li&gt;
  &lt;li&gt;Product: Chris Collard&lt;/li&gt;
  &lt;li&gt;Engineering: Shuya Ding, Kirubakaran Duraisamy, Xu Chen&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;google&quot;&gt;Google&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Play Policy: Siddhartha Paul Tiwari&lt;/li&gt;
  &lt;li&gt;Business Development: Mika Igarashi&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Feb 2024 00:22:10 +0000</pubDate>
        <link>https://engineering.grab.com/dynamic-marketplace</link>
        <guid isPermaLink="true">https://engineering.grab.com/dynamic-marketplace</guid>
        
        <category>Dynamic marketplace</category>
        
        <category>Content moderation</category>
        
        <category>Scaling</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Rethinking Stream Processing: Data Exploration</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this digital age, companies collect multitudes of data that enable the tracking of business metrics and performance. Over the years, data analytics tools for data storage and processing have evolved from the days of Excel sheets and macros to more advanced Map Reduce model tools like Spark, Hadoop, and Hive. This evolution has allowed companies, including Grab, to perform modern analytics on the data ingested into the Data Lake, empowering them to make better &lt;strong&gt;data-driven business decisions&lt;/strong&gt;. This form of data will be referenced within this document as &lt;strong&gt;“Offline Data”&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;With innovations in stream processing technology like Spark and Flink, there is now more interest in unlocking value from streaming data. This form of continuously-generated data in high volume will be referenced within this document as &lt;strong&gt;“Online Data”&lt;/strong&gt;. In the context of Grab, the streaming data is usually materialised as Kafka topics (“Kafka Stream”) as the result of stream processing in its framework. This data is largely unexplored until they are eventually sunk into the Data Lake as Offline Data, part of the data journey (see Figure 1 below). This induces some data latency before the data can be used by data analysts to inform decisions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rethinking-streaming-processing-data-exploration/figure-1-data-journey-online-offline-data.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1. Simplified data journey for Offline Data vs. Online Data, from data generation to data analysis.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As seen in Figure 1 above, the &lt;strong&gt;Time to Value (“TTV”)&lt;/strong&gt; of Online Data is shorter as compared to that of Offline Data in a simplified data journey from data generation to data analysis where complexities of data cleaning and transformation have been removed. This is because the role of the data analyst or data scientist (“Data End User”) has been enabled forward to the Kafka stage for Online Data instead of the Data Lake stage for Offline Data. We recognise that allowing earlier data exploration on Online Data allows Data End Users to build context around the data inputs they are using in an earlier stage. This can help them process Offline Data more meaningfully in subsequent stages. We are interested in opening up the possibility for Data End Users to at least explore the Online Data before they architect a full solution to clean and/or process the data directly or more efficiently post-ingestion into the Data Lake. After their data exploration, the users would have more information to decide whether to spin up a stream processing pipeline for Online Data, or to continue processing Offline Data with their current solution, but with a more refined understanding and logic strategy against their source data inputs. However, of course, in this blog, we acknowledge that not all analysis on Online Data could be done in this manner.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;Online Data is underutilised within Grab mainly because of, among other reasons, difficulty in performing data exploration on data that is not yet properly stored in the Data Lake.&lt;/p&gt;

&lt;p&gt;For the purpose of this blog post, we will focus only on the problem of exploration of Online Data because this problem is the precursor to allowing us to fully democratise such data.&lt;/p&gt;

&lt;p&gt;The problem of data exploration manifests itself when Data End Users need to find the proper data inputs to base and develop their data models. These users would then often need to parse through a multitude of documentation and connect with multiple upstream data producers, to know the range of data signals that are currently available and understand what each data signal is trying to measure.&lt;/p&gt;

&lt;p&gt;Given the ephemeral nature of Online Data, this implies that the &lt;strong&gt;lack of correct tool adoption&lt;/strong&gt; to seamlessly perform quick tests with application logic on Online Data disincentivises the Data End Users to work on these Online Data. Testing such logic on Offline Data is generally much easier since iteration testing on the exact same dataset is possible.&lt;/p&gt;

&lt;p&gt;This difficulty in &lt;strong&gt;performing data exploration including ad hoc queries on Online Data&lt;/strong&gt; has therefore made development of stream processing applications hard for Data End Users, creating headwinds in Grab’s aim to evolve from making &lt;strong&gt;data-driven business decisions&lt;/strong&gt; to also making &lt;strong&gt;data-driven operation decisions&lt;/strong&gt;. Doing both would allow Grab to react much quicker to abrupt changes in its business landscape.&lt;/p&gt;

&lt;h2 id=&quot;adoption-of-zeppelin-notebook-environment&quot;&gt;Adoption of Zeppelin notebook environment&lt;/h2&gt;

&lt;p&gt;To address the difficulty in performing data exploration on Online Data, we have adopted &lt;a href=&quot;https://zeppelin.apache.org/&quot; target=&quot;_blank&quot;&gt;Apache Zeppelin&lt;/a&gt;, a web-based notebook that enables data-drive, interactive data analytics with the support of multiple interpreters to work with various data processing backends e.g. Spark, Flink. The full solution of the adopted Zeppelin notebook environment is enabled seamlessly within our internal data-streaming platform, through its control plane. If you are interested, you may check out our previous blog post titled &lt;a href=&quot;/an-elegant-platform&quot; target=&quot;_blank&quot;&gt;An elegant platform&lt;/a&gt; for more details on the abovementioned streaming platform and its control plane.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rethinking-streaming-processing-data-exploration/figure-2-zeppelin-enablement.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2. Zeppelin login page via web-based notebook environment.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As seen from Figure 2 above, after successful creation of the Zeppelin cluster, users can log in with their generated credentials delivered to them via the integrated instant messenger, and start using the notebook environment.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rethinking-streaming-processing-data-exploration/figure-3-process-data-exploration-online-data.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3. Zeppelin programme flow in the notebook environment.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 above explains the Zeppelin notebook programme flow as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The users enter their queries into the notebook session and run querying statements interactively with the established web-based notebook session.&lt;/li&gt;
  &lt;li&gt;The queries are passed to the Flink interpreter within the cluster to generate the Flink job as a Jar file, to be then submitted to a Flink session cluster.&lt;/li&gt;
  &lt;li&gt;When the Flink session cluster job manager receives the job, it would spin up the corresponding Flink task managers (workers) to run the application and retrieve the results.&lt;/li&gt;
  &lt;li&gt;The query results would then be piped back to the notebook session, to be displayed back to the user on the notebook session.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-query-and-visualisation&quot;&gt;Data query and visualisation&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rethinking-streaming-processing-data-exploration/figure-4-example-query-on-kafka-data.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4. Example of simple select query of data on Kafka. &lt;br /&gt;Note: All variable names, schema, values, and other details used in this article are only created for use as examples.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Flink has a planned roadmap to create a unified streaming language for both stream processing and data analytics. In line with the roadmap, we have based our Zeppelin solution on supporting Structured Query Language (“SQL”) as the query language of choice as seen in Figure 4 above. Data End Users can now write queries in SQL, which is a language that they are comfortable with, and perform adequate data exploration.&lt;/p&gt;

&lt;p&gt;As discussed in this section, data exploration on streaming data at the Kafka stage by adopting the right tool enables Data End Users to seamlessly have visibility to quickly understand the current schema of a Kafka topic (explained more in the next &lt;a href=&quot;#need-for-dynamic-table-schema&quot;&gt;section&lt;/a&gt;. This kind of data exploration also enables Data End Users to understand the type of data the Kafka topic represents, such as the ability to determine if a country code data field is in alpha-2 or alpha-3 &lt;a href=&quot;https://www.iso.org/iso-3166-country-codes.html#:~:text=The%20country%20codes%20can%20be,to%20avoid%20using%20Latin%20script&quot; target=&quot;_blank&quot;&gt;format&lt;/a&gt; while the data is still part of streaming data. This might seem inconsequential and immediately identifiable even in Offline Data, but by enabling data exploration at an earlier stage in the data journey for Online Data, Data End Users have the opportunity to react much more quickly. For example, a change of expected country code format from the data producer would usually lead to errors in the downstream joins or other stream processing pipelines due to incompatible parsing or filtering of the modified country codes. Instead of waiting for the data to be ingested to Offline Data, users can investigate the issue with Online Data retrieved from Kafka.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rethinking-streaming-processing-data-exploration/figure-5-data-visualisation-on-kafka-data.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5. Simple visualisation of queried data on Zeppelin’s notebook environment. &lt;br /&gt;Note: All variable names, schema, values, and other details used in this article are only created for use as examples.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Besides query features, Zeppelin notebook provides simple visualisation and analytics of the off-the-shelf data as presented above in Figure 5. Furthermore, users are now able to perform interactive ad hoc queries on Online Data. These queries will eventually become much more advanced and/or effective SQL queries to be deployed as a streaming pipeline later on in the data journey. This reduces the inertia in setting up a separate development environment or learning other programming languages like Java or Scala during the development of streaming pipelines. With Zeppelin’s notebook environment, our Data End Users are more empowered to quickly derive value from Online Data.&lt;/p&gt;

&lt;h2 id=&quot;need-for-a-more-dynamic-table-schema-derivation-process-&quot;&gt;Need for a more dynamic table schema derivation process &lt;a id=&quot;need-for-dynamic-table-schema&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;For the Data End Users performing data exploration on Online Data, we see a need for these users to derive the Data Definition Language (“DDL”) associated with a Kafka stream at an earlier stage of the data journey. Within Grab, even though Kafka streams are transmitted in &lt;a href=&quot;https://protobuf.dev/&quot; target=&quot;_blank&quot;&gt;Protobuf&lt;/a&gt; format and are thus structured, both the schema and the corresponding DDL changes are added over time as new fields. Typically, the data producer (service owners) and the data engineers responsible for the data ingestion pipeline coordinate to perform such updates. Since the  Data End Users are not involved in such schema update processes nor do they directly interact with the data producers, many of them find the discovery of changes in the current Kafka stream schema an issue. Granted that this is an issue our metadata platform is actively solving using &lt;a href=&quot;https://datahubproject.io/&quot; target=&quot;_blank&quot;&gt;Datahub&lt;/a&gt;, we hope to also solve the challenge by being able to derive the DDL more dynamically within the tooling, for data exploration on Online Data to reduce friction.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/rethinking-streaming-processing-data-exploration/figure-6-derive-schema-kafka-data.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6. Common functions to derive DDL of a Kafka Stream in SQL. &lt;br /&gt;Note: All variable names, schema, values, and other details used in this article are only created for use as examples.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;As seen from Figure 6 above, we have an integrated tooling for Data End Users to derive the DDL associated with a Kafka stream using SQL language. A Kafka stream in Grab’s context is a logical concept describing a Kafka topic, associating it with its metadata like Kafka bootstrap servers and associated Java class created by &lt;a href=&quot;https://grpc.io/docs/protoc-installation/&quot; target=&quot;_blank&quot;&gt;Protoc&lt;/a&gt;. This tool maps the Protobuf schema definition of a Kafka stream to a DDL, allowing it to be expressed and used in SQL language. This reduces the manual effort involved in creating these table definitions from scratch based on the associated Protobuf schema. Users can now derive the DDL associated with a Kafka stream more easily.&lt;/p&gt;

&lt;h2 id=&quot;mitigating-risks-arising-from-data-exploration-on-online-data---data-access-authorisationaudit&quot;&gt;Mitigating risks arising from data exploration on Online Data - data access authorisation/audit&lt;/h2&gt;
&lt;p&gt;While we rethink stream processing and are open to options that enable data exploration on Online Data as mentioned above, we realised that new security requirements related to data access authorisation and maintaining proper audit trail have emerged. Even with Personally Identifiable Information (PII) obfuscation enforcement by our streaming pipeline, it means we need to implement stricter guardrails in place along with audit trails to ensure users only have access to what they are allowed to, and this access can be removed in a break-glass scenario. If you are interested, you may check out our previous blog post titled &lt;a href=&quot;/pii-masking&quot; target=&quot;_blank&quot;&gt;PII masking for privacy-grade machine learning&lt;/a&gt; for more details about how we enforce PII masking on machine learning data streaming pipelines.&lt;/p&gt;

&lt;p&gt;To enable data access authorisation, we utilised Strimzi, the operator of running Kafka on Kubernetes. We integrated Strimzi’s Open Policy Agent (&lt;a href=&quot;https://www.openpolicyagent.org/&quot; target=&quot;_blank&quot;&gt;OPA&lt;/a&gt;) with Kafka to define policies that authorise specific read-only user access to specific Kafka Topics. The identification of users is done via mutualTLS (&lt;a href=&quot;https://docs.confluent.io/platform/current/kafka/configure-mds/mutual-tls-auth-rbac.html#principal-mapping-rules-for-ssl-listeners-extract-a-principal-from-a-certificate&quot; target=&quot;_blank&quot;&gt;mTLS&lt;/a&gt;) connection with our Kafka clusters, where their user details are part of the SSL certificate details used for authentication.&lt;/p&gt;

&lt;p&gt;With these tools in place, each user’s request to explore Online Data would be properly logged, and each data access can be controlled by an OPA policy managed by a central team.&lt;/p&gt;

&lt;p&gt;If you are interested, you may check out our previous post &lt;a href=&quot;/zero-trust-with-kafka&quot; target=&quot;_blank&quot;&gt;Zero trust with Kafka&lt;/a&gt; where we discussed our efforts to continue strengthening the security of our data-streaming platform.&lt;/p&gt;

&lt;h2 id=&quot;impact&quot;&gt;Impact&lt;/h2&gt;

&lt;p&gt;With the proliferation of our data-streaming platform, we expect to see improvements in the way our data becomes gradually democratised. We have already been receiving use cases from the Data End Users who are interested in validating a chain of events on Online Data, i.e. retrieving information of all events associated with a particular booking, which is not currently something that can be done easily.&lt;/p&gt;

&lt;p&gt;More importantly, the tools in place for data exploration on Online Data form the foundation required for us to embark on our next step of the stream processing journey. This foundation makes the development and validation of the stream processing logic much quicker. This occurs when ad hoc queries in a notebook environment are possible, removing the need for local developer environment setups and the need to go through the whole pipeline deployment process for eventual validation of the developed logic. We believe that this would prove to reduce our lead time in creating stream processing pipelines significantly.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Our next step is to rethink further how our stream processing pipelines are defined and start to provision SQL as the unified streaming language of our pipelines. This helps facilitate better discussion between upstream data producers, data engineers, and Data End Users, since SQL is the common language among these stakeholders.&lt;/p&gt;

&lt;p&gt;We will also explore handling schema discovery in a more controlled manner by utilising a Hive catalogue to store our Kafka table definitions. This removes the need for users to retrieve and run the table DDL statement for every session, making the data exploration experience even more seamless.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;[1] Apache Zeppelin | Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala, Python, R and more.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/an-elegant-platform&quot;&gt;[2] An elegant platform | Grab engineering blog.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://flink.apache.org/what-is-flink/roadmap/#unified-sql-platform&quot;&gt;[3] Apache Flink | Roadmap on Unified SQL Platform.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.iso.org/iso-3166-country-codes.html#:~:text=The%20country%20codes%20can%20be,to%20avoid%20using%20Latin%20script&quot;&gt;[4] ISO | ISO 3166 Country Codes.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://protobuf.dev/&quot;&gt;[5] Protobuf (Protocol Buffers)| Language-neutral, platform-neutral extensible mechanisms for serializing structured data.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://datahubproject.io/&quot;&gt;[6] Datahub | Extensible metadata platform that enables data discovery, data observability and federated governance to help tame the complexity of your data ecosystem.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://grpc.io/docs/protoc-installation/&quot;&gt;[7] Protoc | Protocol buffer compiler installation.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/pii-masking&quot;&gt;[8] PII masking for privacy-grade machine learning | Grab engineering blog.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/zero-trust-with-kafka&quot;&gt;[9] Zero trust with Kafka | Grab engineering blog.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openpolicyagent.org/&quot;&gt;[10] Open Policy Agent (OPA) | Policy-based control for cloud native environments.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://strimzi.io/blog/2020/08/05/using-open-policy-agent-with-strimzi-and-apache-kafka/&quot;&gt;[11] Strimzi | Using Open Policy Agent with Strimzi and Apache Kafka.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.confluent.io/platform/current/kafka/configure-mds/mutual-tls-auth-rbac.html#principal-mapping-rules-for-ssl-listeners-extract-a-principal-from-a-certificate&quot;&gt;[12] Confluent Documentation | Configure mTLS authentication and RBAC for kafka brokers.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Jan 2024 00:10:10 +0000</pubDate>
        <link>https://engineering.grab.com/rethinking-streaming-processing-data-exploration</link>
        <guid isPermaLink="true">https://engineering.grab.com/rethinking-streaming-processing-data-exploration</guid>
        
        <category>Kafka</category>
        
        <category>Kubernetes</category>
        
        <category>Data Streaming</category>
        
        <category>Deployments</category>
        
        <category>Streaming applications</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Kafka on Kubernetes: Reloaded for fault tolerance</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Coban - Grab’s real-time data streaming platform - has been operating &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; on &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; with &lt;a href=&quot;https://strimzi.io/&quot;&gt;Strimzi&lt;/a&gt; in 
production for about two years. In a previous article (&lt;a href=&quot;/zero-trust-with-kafka&quot;&gt;Zero trust with Kafka&lt;/a&gt;), we explained how we leveraged Strimzi to enhance the security of our data streaming offering.&lt;/p&gt;

&lt;p&gt;In this article, we are going to describe how we improved the fault tolerance of our initial design, to the point where we no longer need to intervene if a Kafka broker is unexpectedly terminated.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;We operate Kafka in the AWS Cloud. For the Kafka on Kubernetes design described in this article, we rely on &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (EKS), the managed Kubernetes offering by AWS, with the worker nodes deployed as &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/worker.html&quot;&gt;self-managed nodes&lt;/a&gt; on &lt;a href=&quot;https://aws.amazon.com/ec2/&quot;&gt;Amazon Elastic Compute Cloud&lt;/a&gt; (EC2).&lt;/p&gt;

&lt;p&gt;To make our operations easier and limit the blast radius of any incidents, we deploy exactly one Kafka cluster for each EKS cluster. We also give a full worker node to each Kafka broker. In terms of storage, we initially relied on EC2 instances with &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ssd-instance-store.html&quot;&gt;non-volatile memory express (NVMe) instance store volumes&lt;/a&gt; for 
maximal I/O performance. Also, each Kafka cluster is accessible beyond its own &lt;a href=&quot;https://aws.amazon.com/vpc/&quot;&gt;Virtual Private Cloud&lt;/a&gt; (VPC) via a &lt;a href=&quot;https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-share-your-services.html&quot;&gt;VPC Endpoint Service&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-on-kubernetes/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 1 Initial design of a 3-node Kafka cluster running on Kubernetes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 1 shows a logical view of our initial design of a 3-node Kafka on Kubernetes cluster, as typically run by Coban. The Zookeeper and Cruise-Control components are not shown for clarity.&lt;/p&gt;

&lt;p&gt;There are four Kubernetes services (1): one for the initial connection - referred to as “bootstrap” - that redirects incoming traffic to any Kafka pods, plus one for each Kafka pod, for the clients to target each Kafka broker individually (a requirement to produce or consume from/to a partition that resides on any particular Kafka broker). Four different listeners on the Network Load Balancer (NLB) listening on four different TCP ports, enable the Kafka clients to target either the bootstrap 
service or any particular Kafka broker they need to reach. This is very similar to what we previously described in &lt;a href=&quot;/exposing-kafka-cluster&quot;&gt;Exposing a Kafka Cluster via a VPC Endpoint Service&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Each worker node hosts a single Kafka pod (2). The NVMe instance store volume is used to create a Kubernetes Persistent Volume (PV), attached to a pod via a Kubernetes Persistent Volume Claim (PVC).&lt;/p&gt;

&lt;p&gt;Lastly, the worker nodes belong to &lt;a href=&quot;https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html&quot;&gt;Auto-Scaling Groups&lt;/a&gt; (ASG) (3), one by &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-availability-zones&quot;&gt;Availability Zone&lt;/a&gt; (AZ). Strimzi adds in node affinity to make sure that the brokers are evenly distributed across AZs. In this initial design, ASGs are not for auto-scaling though, because we want to keep the size of the cluster under control. We only use ASGs - with a fixed size - to facilitate manual scaling operation and to automatically replace the terminated worker nodes.&lt;/p&gt;

&lt;p&gt;With this initial design, let us see what happens in case of such a worker node termination.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-on-kubernetes/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 2 Representation of a worker node termination. Node C is terminated and replaced by node D. However the Kafka broker 3 pod is unable to restart on node D.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 2 shows the worker node C being terminated along with its NVMe instance store volume C, and replaced (by the ASG) by a new worker node D and its new, empty NVMe instance store volume D. On start-up, the worker node D automatically joins the Kubernetes cluster. The Kafka broker 3 pod that was running on the faulty worker node C is scheduled to restart on the new worker node D.&lt;/p&gt;

&lt;p&gt;Although the NVMe instance store volume C is terminated along with the worker node C, there is no data loss because all of our Kafka topics are configured with a minimum of three replicas. The data is poised to be copied over from the surviving Kafka brokers 1 and 2 back to Kafka broker 3, as soon as Kafka broker 3 is effectively restarted on the worker node D.&lt;/p&gt;

&lt;p&gt;However, there are three fundamental issues with this initial design:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The Kafka clients that were in the middle of producing or consuming to/from the partition leaders of Kafka broker 3 are suddenly facing connection errors, because the broker was not gracefully demoted beforehand.&lt;/li&gt;
  &lt;li&gt;The target groups of the NLB for both the bootstrap connection and Kafka broker 3 still point to the worker node C. Therefore, the network communication from the NLB to Kafka broker 3 is broken. A manual reconfiguration of the target groups is required.&lt;/li&gt;
  &lt;li&gt;The PVC associating the Kafka broker 3 pod with its instance store PV is unable to automatically switch to the new NVMe instance store volume of the worker node D. Indeed, static provisioning is an intrinsic characteristic of Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/volumes/#local&quot;&gt;local volumes&lt;/a&gt;. The PVC is still in &lt;em&gt;Bound&lt;/em&gt; state, so Kubernetes does not take any action. However, the actual storage beneath the PV does not exist anymore. Without any storage, the Kafka broker 3 pod is unable to start.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this stage, the Kafka cluster is running in a degraded state with only two out of three brokers, until a Coban engineer intervenes to reconfigure the target groups of the NLB and delete the zombie PVC (this, in turn, triggers its re-creation by Strimzi, this time using the new instance store PV).&lt;/p&gt;

&lt;p&gt;In the next section, we will see how we have managed to address the three issues mentioned above to make this design fault-tolerant.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;h3 id=&quot;graceful-kafka-shutdown&quot;&gt;Graceful Kafka shutdown&lt;/h3&gt;

&lt;p&gt;To minimise the disruption for the Kafka clients, we leveraged the &lt;a href=&quot;https://aws-quickstart.github.io/cdk-eks-blueprints/addons/aws-node-termination-handler/&quot;&gt;AWS Node Termination Handler&lt;/a&gt; (NTH). This component provided by AWS for Kubernetes environments is able to cordon and drain a worker node that is going to be terminated. This draining, in turn, triggers a graceful shutdown of the Kafka 
process by sending a polite &lt;a href=&quot;https://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html&quot;&gt;SIGTERM&lt;/a&gt; signal to all pods running on the worker node that is being drained (instead of the brutal &lt;a href=&quot;https://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html&quot;&gt;SIGKILL&lt;/a&gt; of a normal termination).&lt;/p&gt;

&lt;p&gt;The termination events of interest that are captured by the NTH are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scale-in operations by an ASG.&lt;/li&gt;
  &lt;li&gt;Manual termination of an instance.&lt;/li&gt;
  &lt;li&gt;AWS maintenance events, typically EC2 instances scheduled for upcoming retirement.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This suffices for most of the disruptions our clusters can face in normal times and our common maintenance operations, such as terminating a worker node to refresh it. Only sudden hardware failures (AWS issue events) would fall through the cracks and still trigger errors on the Kafka client side.&lt;/p&gt;

&lt;p&gt;The NTH comes in two modes: &lt;a href=&quot;https://github.com/aws/aws-node-termination-handler#major-features&quot;&gt;Instance Metadata Service (IMDS) and Queue Processor&lt;/a&gt;. We chose to go with the latter as it is able to capture a broader range of events, widening the fault tolerance capability.&lt;/p&gt;

&lt;h4 id=&quot;scale-in-operations-by-an-asg&quot;&gt;Scale-in operations by an ASG&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-on-kubernetes/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 3 Architecture of the NTH with the Queue Processor.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 3 shows the NTH with the Queue Processor in action, and how it reacts to a scale-in operation (typically triggered manually, during a maintenance operation):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As soon as the scale-in operation is triggered, an &lt;a href=&quot;https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html&quot;&gt;Auto Scaling lifecycle hook&lt;/a&gt; is invoked to pause the termination of the instance.&lt;/li&gt;
  &lt;li&gt;Simultaneously, an Auto Scaling lifecycle hook event is issued to an &lt;a href=&quot;https://aws.amazon.com/sqs/&quot;&gt;Amazon Simple Queue Service&lt;/a&gt; (SQS) queue. In Fig. 3, we have also materialised EC2 events (e.g. manual termination of an instance, AWS maintenance events, etc.) that transit via &lt;a href=&quot;https://aws.amazon.com/eventbridge/&quot;&gt;Amazon EventBridge&lt;/a&gt; to eventually end up in the same SQS queue. We will discuss EC2 events in the next two sections.&lt;/li&gt;
  &lt;li&gt;The NTH, a pod running in the Kubernetes cluster itself, constantly polls that SQS queue.&lt;/li&gt;
  &lt;li&gt;When a scale-in event pertaining to a worker node of the Kubernetes cluster is read from the SQS queue, the NTH sends to the Kubernetes API the instruction to &lt;a href=&quot;https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration&quot;&gt;cordon&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/&quot;&gt;drain&lt;/a&gt; the impacted worker node.&lt;/li&gt;
  &lt;li&gt;On draining, Kubernetes sends a SIGTERM signal to the Kafka pod residing on the worker node.&lt;/li&gt;
  &lt;li&gt;Upon receiving the SIGTERM signal, the Kafka pod gracefully migrates the leadership of its leader partitions to other brokers of the cluster before shutting down, in a transparent manner for the clients. This behaviour is ensured by the &lt;a href=&quot;https://kafka.apache.org/documentation/#basic_ops_restarting&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;controlled.shutdown.enable&lt;/code&gt;&lt;/a&gt; parameter of Kafka, which is enabled by default.&lt;/li&gt;
  &lt;li&gt;Once the impacted worker node has been drained, the NTH eventually resumes the termination of the instance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Strimzi also comes with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terminationGracePeriodSeconds&lt;/code&gt; parameter, which we have set to 180 seconds to give the Kafka pods enough time to migrate all of their partition leaders gracefully on termination. We have verified that this is enough to migrate all partition leaders on our Kafka clusters (about 60 seconds for 600 partition leaders).&lt;/p&gt;

&lt;h4 id=&quot;manual-termination-of-an-instance&quot;&gt;Manual termination of an instance&lt;/h4&gt;

&lt;p&gt;The Auto Scaling lifecycle hook that pauses the termination of an instance (Fig. 3, step 1) as well as the corresponding resuming by the NTH (Fig. 3, step 7) are invoked only for ASG scaling events.&lt;/p&gt;

&lt;p&gt;In case of a manual termination of an EC2 instance, the termination is captured as an EC2 event that also reaches the NTH. Upon receiving that event, the NTH cordons and drains the impacted worker node. However, the instance is immediately terminated, most likely before the leadership of all of its Kafka partition leaders has had the time to get migrated to other brokers.&lt;/p&gt;

&lt;p&gt;To work around this and let a manual termination of an EC2 instance also benefit from the ASG lifecycle hook, the instance must be terminated using the &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/reference/autoscaling/terminate-instance-in-auto-scaling-group.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terminate-instance-in-auto-scaling-group&lt;/code&gt;&lt;/a&gt; AWS CLI command.&lt;/p&gt;

&lt;h4 id=&quot;aws-maintenance-events&quot;&gt;AWS maintenance events&lt;/h4&gt;

&lt;p&gt;For AWS maintenance events such as instances scheduled for upcoming retirement, the NTH acts immediately when the event is first received (typically adequately in advance). It cordons and drains the soon-to-be-retired worker node, which in turn triggers the SIGTERM signal and the graceful termination of Kafka as described above. At this stage, the impacted instance is not terminated, so the Kafka partition leaders have plenty of time to complete their migration to other brokers.&lt;/p&gt;

&lt;p&gt;However, the evicted Kafka pod has nowhere to go. There is a need for spinning up a new worker node for it to be able to eventually restart somewhere.&lt;/p&gt;

&lt;p&gt;To make this happen seamlessly, we doubled the maximum size of each of our ASGs and installed the &lt;a href=&quot;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md&quot;&gt;Kubernetes Cluster Autoscaler&lt;/a&gt;. With that, when such a maintenance event is received:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The worker node scheduled for retirement is cordoned and drained by the NTH. The state of the impacted Kafka pod becomes &lt;em&gt;Pending&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;The Kubernetes Cluster Autoscaler comes into play and triggers the corresponding ASG to spin up a new EC2 instance that joins the Kubernetes cluster as a new worker node.&lt;/li&gt;
  &lt;li&gt;The impacted Kafka pod restarts on the new worker node.&lt;/li&gt;
  &lt;li&gt;The Kubernetes Cluster Autoscaler detects that the previous worker node is now under-utilised and terminates it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this scenario, the impacted Kafka pod only remains in &lt;em&gt;Pending&lt;/em&gt; state for about four minutes in total.&lt;/p&gt;

&lt;p&gt;In case of multiple simultaneous AWS maintenance events, the Kubernetes scheduler would honour our &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&quot;&gt;PodDisruptionBudget&lt;/a&gt; and not evict more than one Kafka pod at a time.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-nlb-configuration&quot;&gt;Dynamic NLB configuration&lt;/h3&gt;

&lt;p&gt;To automatically map the NLB’s target groups with a newly spun up EC2 instance, we leveraged the &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html&quot;&gt;AWS Load Balancer Controller&lt;/a&gt; (LBC).&lt;/p&gt;

&lt;p&gt;Let us see how it works.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-on-kubernetes/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 4 Architecture of the LBC managing the NLB&apos;s target groups via TargetGroupBinding custom resources.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 4 shows how the LBC automates the reconfiguration of the NLB’s target groups:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It first retrieves the desired state described in Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&quot;&gt;custom resources&lt;/a&gt; (CR) of type &lt;a href=&quot;https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/guide/targetgroupbinding/targetgroupbinding&quot;&gt;TargetGroupBinding&lt;/a&gt;. There is one such resource per target group to maintain. Each TargetGroupBinding CR associates its respective target group with a Kubernetes service.&lt;/li&gt;
  &lt;li&gt;The LBC then watches over the changes of the Kubernetes services that are referenced in the TargetGroupBinding CRs’ definition, specifically the private IP addresses exposed by their respective &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/#endpoints&quot;&gt;Endpoints resources&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;When a change is detected, it dynamically updates the corresponding NLB’s target groups with those IP addresses as well as the TCP port of the target containers (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containerPort&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This automated design sets up the NLB’s target groups with IP addresses (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;targetType: ip&lt;/code&gt;) instead of EC2 instance IDs (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;targetType: instance&lt;/code&gt;). Although the LBC can handle both target types, the IP address approach is actually more straightforward in our case, since each pod has a routable private IP address in the AWS subnet, thanks to the &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html&quot;&gt;AWS Container Networking Interface&lt;/a&gt; (CNI) plug-in.&lt;/p&gt;

&lt;p&gt;This dynamic NLB configuration design comes with a challenge. Whenever we need to update the Strimzi CR, the rollout of the change to each Kafka pod in a rolling update fashion is happening too fast for the NLB. This is because the NLB inherently takes some time to mark each target as healthy before enabling it. The Kafka brokers that have just been rolled out start advertising their broker-specific endpoints to the Kafka clients via the bootstrap service, but those 
endpoints are actually not immediately available because the NLB is still checking their health. To mitigate this, we have reduced the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HealthCheckIntervalSeconds&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HealthyThresholdCount&lt;/code&gt; parameters of each target group to their minimum values of 5 and 2 respectively. This reduces the maximum delay for the NLB to detect that a target has become healthy to 10 seconds. In addition, we have configured the LBC with a &lt;a href=&quot;https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/deploy/pod_readiness_gate/&quot;&gt;Pod Readiness Gate&lt;/a&gt;. This feature makes the Strimzi rolling deployment wait for the health check of the NLB to pass, before marking the current pod as &lt;em&gt;Ready&lt;/em&gt; and proceeding with the next pod.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-on-kubernetes/image7.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 5 Steps for a Strimzi rolling deployment with a Pod Readiness Gate. Only one Kafka broker and one NLB listener and target group are shown for simplicity.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 5 shows how the Pod Readiness Gate works during a Strimzi rolling deployment:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The old Kafka pod is terminated.&lt;/li&gt;
  &lt;li&gt;The new Kafka pod starts up and joins the Kafka cluster. Its individual endpoint for direct access via the NLB is immediately advertised by the Kafka cluster. However, at this stage, it is not reachable, as the target group of the NLB still points to the IP address of the old Kafka pod.&lt;/li&gt;
  &lt;li&gt;The LBC updates the target group of the NLB with the IP address of the new Kafka pod, but the NLB health check has not yet passed, so the traffic is not forwarded to the new Kafka pod just yet.&lt;/li&gt;
  &lt;li&gt;The LBC then waits for the NLB health check to pass, which takes 10 seconds. Once the NLB health check has passed, the NLB resumes forwarding the traffic to the Kafka pod.&lt;/li&gt;
  &lt;li&gt;Finally, the LBC updates the pod readiness gate of the new Kafka pod. This informs Strimzi that it can proceed with the next pod of the rolling deployment.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;data-persistence-with-ebs&quot;&gt;Data persistence with EBS&lt;/h3&gt;

&lt;p&gt;To address the challenge of the residual PV and PVC of the old worker node preventing Kubernetes from mounting the local storage of the new worker node after a node rotation, we adopted &lt;a href=&quot;https://aws.amazon.com/ebs/&quot;&gt;Elastic Block Store&lt;/a&gt; (EBS) volumes instead of NVMe instance store volumes. Contrary to the latter, EBS volumes can conveniently be attached and detached. The trade-off is that their performance is significantly lower.&lt;/p&gt;

&lt;p&gt;However, relying on EBS comes with additional benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The cost per GB is lower, compared to NVMe instance store volumes.&lt;/li&gt;
  &lt;li&gt;Using EBS decouples the size of an instance in terms of CPU and memory from its storage capacity, leading to further cost savings by independently right-sizing the instance type and its storage. Such a separation of concerns also opens the door to new use cases requiring disproportionate amounts of storage.&lt;/li&gt;
  &lt;li&gt;After a worker node rotation, the time needed for the new node to get back in sync is faster, as it only needs to catch up the data that was produced during the downtime. This leads to shorter maintenance operations and higher iteration speed. Incidentally, the associated inter-AZ traffic cost is also lower, since there is less data to transfer among brokers during this time.&lt;/li&gt;
  &lt;li&gt;Increasing the storage capacity is an online operation.&lt;/li&gt;
  &lt;li&gt;Data backup is supported by taking snapshots of EBS volumes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have verified with our historical monitoring data that the performance of &lt;a href=&quot;https://aws.amazon.com/ebs/general-purpose/&quot;&gt;EBS General Purpose 3&lt;/a&gt; (gp3) volumes is significantly above our maximum historical values for both throughput and I/O per second (IOPS), and we have successfully benchmarked a test EBS-based Kafka cluster. We have also set up new monitors to be alerted in case we need to 
provision either additional throughput or IOPS, beyond the baseline of EBS gp3 volumes.&lt;/p&gt;

&lt;p&gt;With that, we updated our instance types from storage optimised instances to either general purpose or memory optimised instances. We added the &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html&quot;&gt;Amazon EBS Container Storage Interface (CSI) driver&lt;/a&gt; to the Kubernetes cluster and created a new Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/storage-classes/&quot;&gt;storage class&lt;/a&gt; to let the cluster dynamically provision EBS gp3 volumes.&lt;/p&gt;

&lt;p&gt;We configured Strimzi to use that storage class to create any new PVCs. This makes Strimzi able to automatically create the EBS volumes it needs, typically when the cluster is first set up, but also to attach/detach the volumes to/from the EC2 instances whenever a Kafka pod is relocated to a different worker node.&lt;/p&gt;

&lt;p&gt;Note that the EBS volumes are not part of any ASG &lt;a href=&quot;https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html&quot;&gt;Launch Template&lt;/a&gt;, nor do they scale automatically with the ASGs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-on-kubernetes/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 6 Steps for the Strimzi Operator to create an EBS volume and attach it to a new Kafka pod.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 6 illustrates how this works when Strimzi sets up a new Kafka broker, for example the first broker of the cluster in the initial setup:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;a href=&quot;https://strimzi.io/docs/operators/latest/overview#overview-components-cluster-operator-str&quot;&gt;Strimzi Cluster Operator&lt;/a&gt; first creates a new PVC, specifying a volume size and EBS gp3 as its storage class. The storage class is configured with the EBS CSI Driver as the volume provisioner, so that volumes are dynamically provisioned &lt;a href=&quot;#1&quot;&gt;[1]&lt;/a&gt;. However, because it is also set up with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volumeBindingMode: WaitForFirstConsumer&lt;/code&gt;, the volume is not yet provisioned until a pod actually claims the PVC.&lt;/li&gt;
  &lt;li&gt;The Strimzi Cluster Operator then creates the Kafka pod, with a reference to the newly created PVC. The pod is scheduled to start, which in turn claims the PVC.&lt;/li&gt;
  &lt;li&gt;This triggers the EBS CSI Controller. As the volume provisioner, it dynamically creates a new EBS volume in the AWS VPC, in the AZ of the worker node where the pod has been scheduled to start.&lt;/li&gt;
  &lt;li&gt;It then attaches the newly created EBS volume to the corresponding EC2 instance.&lt;/li&gt;
  &lt;li&gt;After that, it creates a Kubernetes PV with &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#node-affinity&quot;&gt;nodeAffinity&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reserving-a-persistentvolume&quot;&gt;claimRef&lt;/a&gt; specifications, making sure that the PV is reserved for the Kafka broker 1 pod.&lt;/li&gt;
  &lt;li&gt;Lastly, it updates the PVC with the reference of the newly created PV. The PVC is now in &lt;em&gt;Bound&lt;/em&gt; state and the Kafka pod can start.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One important point to take note of is that EBS volumes can only be attached to EC2 instances residing in their own AZ. Therefore, when rotating a worker node, the EBS volume can only be re-attached to the new instance if both old and new instances reside in the same AZ. A simple way to guarantee this is to set up one ASG per AZ, instead of a single ASG spanning across 3 AZs.&lt;/p&gt;

&lt;p&gt;Also, when such a rotation occurs, the new broker only needs to synchronise the recent data produced during the brief downtime, which is typically an order of magnitude faster than replicating the entire volume (depending on the overall retention period of the hosted Kafka topics).&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;caption style=&quot;text-align:center&quot;&gt;Table 1 Comparison of the resynchronization of the Kafka data after a broker rotation between the initial design and the new design with EBS volumes.&lt;/caption&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;Initial design (NVMe instance store volumes)&lt;/th&gt;
    &lt;th&gt;New design (EBS volumes)&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td&gt;Data to synchronise&lt;/td&gt;
    &lt;td&gt;All of the data&lt;/td&gt;
    &lt;td&gt;Recent data produced during the brief downtime&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Function of (primarily)&lt;/td&gt;
    &lt;td&gt;Retention period&lt;/td&gt;
    &lt;td&gt;Downtime&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Typical duration&lt;/td&gt;
    &lt;td&gt;Hours&lt;/td&gt;
    &lt;td&gt;Minutes&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;outcome&quot;&gt;Outcome&lt;/h2&gt;

&lt;p&gt;With all that, let us revisit the initial scenario, where a malfunctioning worker node is being replaced by a fresh new node.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/kafka-on-kubernetes/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 7 Representation of a worker node termination after implementing the solution. Node C is terminated and replaced by node D. This time, the Kafka broker 3 pod is able to start and serve traffic.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 7 shows the worker node C being terminated and replaced (by the ASG) by a new worker node D, similar to what we have described in the initial problem statement. The worker node D automatically joins the Kubernetes cluster on start-up.&lt;/p&gt;

&lt;p&gt;However, this time, a seamless failover takes place:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The Kafka clients that were in the middle of producing or consuming to/from the partition leaders of Kafka broker 3 are gracefully redirected to Kafka brokers 1 and 2, where Kafka has migrated the leadership of its leader partitions.&lt;/li&gt;
  &lt;li&gt;The target groups of the NLB for both the bootstrap connection and Kafka broker 3 are automatically updated by the LBC. The connectivity between the NLB and Kafka broker 3 is immediately restored.&lt;/li&gt;
  &lt;li&gt;Triggered by the creation of the Kafka broker 3 pod, the Amazon EBS CSI driver running on the worker node D re-attaches the EBS volume 3 that was previously attached to the worker node C, to the worker node D instead. This enables Kubernetes to automatically re-bind the corresponding PV and PVC to Kafka broker 3 pod. With its storage dependency resolved, Kafka broker 3 is able to start successfully and re-join the Kafka cluster. From there, it only needs to catch up with the new data that was produced 
during its short downtime, by replicating it from Kafka brokers 1 and 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this fault-tolerant design, when an EC2 instance is being retired by AWS, no particular action is required from our end.&lt;/p&gt;

&lt;p&gt;Similarly, our EKS version upgrades, as well as any operations that require rotating all worker nodes of the cluster in general, are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Simpler and less error-prone&lt;/strong&gt;: We only need to rotate each instance in sequence, with no need for manually reconfiguring the target groups of the NLB and deleting the zombie PVCs anymore.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Faster&lt;/strong&gt;: The time between each instance rotation is limited to the short amount of time it takes for the restarted Kafka broker to catch up with the new data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;More cost-efficient&lt;/strong&gt;: There is less data to transfer across AZs (which is charged by AWS).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is worth noting that we have chosen to omit Zookeeper and Cruise Control in this article, for the sake of clarity and simplicity. In reality, all pods in the Kubernetes cluster - including Zookeeper and Cruise Control - now benefit from the same graceful stop, triggered by the AWS termination events and the NTH. Similarly, the EBS CSI driver improves the fault tolerance of any pods that use EBS volumes for persistent storage, which includes the Zookeeper pods.&lt;/p&gt;

&lt;h2 id=&quot;challenges-faced&quot;&gt;Challenges faced&lt;/h2&gt;

&lt;p&gt;One challenge that we are facing with this design lies in the EBS volumes’ management.&lt;/p&gt;

&lt;p&gt;On the one hand, the size of EBS volumes cannot be increased consecutively before the end of a cooldown period (minimum of 6 hours and can exceed 24 hours in some cases &lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt;). Therefore, when we need to urgently extend some EBS volumes because the size of a Kafka topic is suddenly growing, we need to be relatively generous when sizing the new required capacity and add a comfortable security margin, to make sure that we are not running out of storage in the short run.&lt;/p&gt;

&lt;p&gt;On the other hand, shrinking a Kubernetes PV is not a supported operation. This can affect the cost efficiency of our design if we overprovision the storage capacity by too much, or in case the workload of a particular cluster organically diminishes.&lt;/p&gt;

&lt;p&gt;One way to mitigate this challenge is to tactically scale the cluster horizontally (ie. adding new brokers) when there is a need for more storage and the existing EBS volumes are stuck in a cooldown period, or when the new storage need is only temporary.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;In the future, we can improve the NTH’s capability by utilising webhooks. Upon receiving events from SQS, the NTH can also forward the events to the specified webhook URLs.&lt;/p&gt;

&lt;p&gt;This can potentially benefit us in a few ways, e.g.:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Proactively spinning up a new instance without waiting for the old one to be terminated, whenever a termination event is received. This would shorten the rotation time even further.&lt;/li&gt;
  &lt;li&gt;Sending Slack notifications to Coban engineers to keep them informed of any actions taken by the NTH.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We would need to develop and maintain an application that receives webhook events from the NTH and performs the necessary actions.&lt;/p&gt;

&lt;p&gt;In addition, we are also rolling out &lt;a href=&quot;https://karpenter.sh/&quot;&gt;Karpenter&lt;/a&gt; to replace the Kubernetes Cluster Autoscaler, as it is able to spin up new instances slightly faster, helping reduce the four minutes delay a Kafka pod remains in &lt;em&gt;Pending&lt;/em&gt; state during a node rotation. Incidentally, Karpenter also removes the need for setting up one ASG by AZ, as it is able to deterministically provision instances in a specific AZ, for example where a particular EBS volume resides.&lt;/p&gt;

&lt;p&gt;Lastly, to ensure that the performance of our EBS gp3 volumes is both sufficient and cost-efficient, we want to explore autoscaling their throughput and IOPS beyond the baseline, based on the usage metrics collected by our monitoring stack.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;1&quot; href=&quot;#1&quot;&gt;[1]&lt;/a&gt; &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/&quot;&gt;Dynamic Volume Provisioning | Kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;2&quot; href=&quot;#2&quot;&gt;[2]&lt;/a&gt; &lt;a href=&quot;https://repost.aws/knowledge-center/ebs-volume-stuck-optimizing-on-modification&quot;&gt;Troubleshoot EBS volume stuck in Optimizing state during modification | AWS re:Post&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;We would like to thank our team members and Grab Kubernetes gurus that helped review and improve this blog before publication: Will Ho, Gable Heng, Dewin Goh, Vinnson Lee, Siddharth Pandey, Shi Kai Ng, Quang Minh Tran, Yong Liang Oh, Leon Tay, Tuan Anh Vu. &lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Dec 2023 00:10:10 +0000</pubDate>
        <link>https://engineering.grab.com/kafka-on-kubernetes</link>
        <guid isPermaLink="true">https://engineering.grab.com/kafka-on-kubernetes</guid>
        
        <category>Kafka</category>
        
        <category>Kubernetes</category>
        
        <category>AWS</category>
        
        <category>Data Streaming</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Championing CyberSecurity: Grab&apos;s bug bounty programme in 2023</title>
        <description>&lt;p&gt;Launched in 2015, &lt;a href=&quot;https://hackerone.com/grab?type%3Dteam&quot;&gt;Grab’s Security bug bounty programme&lt;/a&gt; has achieved remarkable success and forged strong partnerships within a thriving bounty community. By holding quarterly campaigns with HackerOne, Grab has been dedicated to security and giving back to the global security community to research further. Over the years, Grab has paid over $700,000 in cumulative payments to committed security researchers, aiding their research.&lt;/p&gt;

&lt;p&gt;Our journey doesn’t stop there – we’ve also expanded our internal bug bounty team, ensuring that we have the necessary resources to stay at the forefront of security challenges. As we continue to innovate and evolve, it’s critical that our team remains at the cutting edge of security developments.&lt;/p&gt;

&lt;p&gt;Marking its eighth year in 2023, this initiative has achieved new milestones and continues to set the stage for an even more successful ninth year. In 2023, this included a special campaign in Threatcon Nepal, aimed at increasing our bounty engagements. A key development was the enrichment of monetary incentives to honour our hacker community’s remarkable contributions to our programme’s success.&lt;/p&gt;

&lt;p&gt;Let’s look at the key takeaways we gained from the bug bounty programme in 2023.&lt;/p&gt;

&lt;h2 id=&quot;highlights-from-2023&quot;&gt;Highlights from 2023&lt;/h2&gt;

&lt;p&gt;This year, we had some of the highest participation and engagement rates we’ve seen since the programme launched.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We’ve processed ~1000 submissions through our HackerOne bug bounty programme.&lt;/li&gt;
  &lt;li&gt;Impressive record of 400 submissions in the Q1 2023 campaign.&lt;/li&gt;
  &lt;li&gt;We’ve maintained a consistent schedule of campaigns and innovative efforts to enhance hacker engagement.&lt;/li&gt;
  &lt;li&gt;Released a comprehensive report of our seven-year bug bounty journey – check out some key highlights in the image below.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/cybersec-bug/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;As Grab expands and transforms its product and service portfolio, we are dedicated to ensuring that our bug bounty programme reflects this growth. In our rigorous pursuit of boosting security, we regularly introduce new areas of focus to our scope. In 2024, expect the inclusion of new scopes, enhanced response times, heightened engagement from the hacker community, and more competitive rewards.&lt;/p&gt;

&lt;p&gt;In the past year, we have incorporated Joint Ventures and Acquisitions into the scope of our bug bounty programme. By doing so, we proactively address emerging security challenges, while fortifying the safety and integrity of our expanding ecosystem. We remain fully dedicated to embracing change and growth as integral parts of our journey to provide a secure and seamless experience for our users.&lt;/p&gt;

&lt;p&gt;On top of that, we continue to improve our methods of motivating researchers through the bug bounty programme. One recent change is to diversify our reward methods by incorporating both financial rewards and recognition. This allows us to cater to different researcher motivations, cultivate stronger relationships, and acknowledge researchers’ contributions.&lt;/p&gt;

&lt;p&gt;That said, we recognise that there’s always room for improvement and the bug bounty programme is uniquely poised for substantial expansion. In the near future, we will be:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Introducing more elements to the scope of our bug bounty programme&lt;/li&gt;
  &lt;li&gt;Enhancing feedback loops on the HackerOne platform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these improvements, we can drive continuous improvement efforts to provide a secure experience for our users while strengthening our connection with the security research community.&lt;/p&gt;

&lt;h2 id=&quot;a-word-of-thanks&quot;&gt;A word of thanks&lt;/h2&gt;

&lt;p&gt;2023 has been an exhilarating year for our team. We’re grateful for the continued support from all the security researchers who’ve actively participated in our programme.&lt;/p&gt;

&lt;p&gt;Here are the top three researchers in 2023:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/damian89&quot;&gt;Damian89&lt;/a&gt; &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/happy_csr&quot;&gt;Happy_csr&lt;/a&gt; &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/mclaren650sspider&quot;&gt;mclaren650sspider&lt;/a&gt; &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As we head into our ninth year, we know there are new opportunities and challenges that await us. We strive to remain dedicated to the values of collaboration and continuous improvement, working hand in hand with the security community to enhance our superapp’s security and deliver an even safer experience for our users.&lt;/p&gt;

&lt;p&gt;We’re gearing up for another exciting year ahead in our programme, and looking forward to interesting submissions from our participants. We extend an open invitation to all researchers to submit reports to our bug bounty programme. Your contributions hold immense value and have a significant impact on the safety and security of our products, our users, and the broader security community. For comprehensive information about the programme scope, rules, and rewards, &lt;a href=&quot;https://hackerone.com/grab?type%3Dteam&quot;&gt;visit our website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Until next year, keep up the great work, and happy hacking!&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Dec 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/cybersec-bug</link>
        <guid isPermaLink="true">https://engineering.grab.com/cybersec-bug</guid>
        
        <category>Security</category>
        
        <category>Bug bounty</category>
        
        <category>HackerOne</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Sliding window rate limits in distributed systems</title>
        <description>&lt;p&gt;Like many other companies, Grab uses marketing communications to notify users of promotions or other news. If a user receives these notifications from multiple companies, it would be a form of information overload and they might even start considering these communications as spam. Over time, this could lead to some users revoking their consent to receive marketing communications altogether. Hence, it is important to find a rate-limited solution that sends the right amount of communications to our users.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;In Grab, marketing emails and push notifications are part of carefully designed campaigns to ensure that users get the right notifications (i.e. based on past orders or usage patterns). Trident is Grab’s in-house tool to compose these campaigns so that they run efficiently at scale. An example of a campaign is scheduling a marketing email blast to 10 million users at 4 pm. Read more about Trident’s architecture &lt;a href=&quot;/supporting-large-campaigns-at-scale&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trident relies on Hedwig, another in-house service, to deliver the messages to users. Hedwig does the heavy lifting of delivering large amounts of emails and push notifications to users while maintaining a high query per second (QPS) rate and minimal delay. The following high-level architectural illustration demonstrates the interaction between Trident and Hedwig.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Diagram of data interaction between Trident and Hedwig&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The aim is to regulate the number of marketing comms sent to users daily and weekly, tailored based on their interaction patterns with the Grab superapp.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Based on their interaction patterns with our superapp, we have clustered users into a few segments.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;New: Users recently signed up to the Grab app but haven’t taken any rides yet.
Active: Users who took rides in the past month.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With these metrics, we came up with optimal daily and weekly frequency limit values for each clustered user segment. The solution discussed in this article ensures that the comms sent to a user do not exceed the daily and weekly thresholds for the segment. This is also called frequency capping.&lt;/p&gt;

&lt;p&gt;However, frequency capping can be split into two sub-problems:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Efficient storage of clustered user data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With a huge customer base of over 270 million users, storing the user segment membership information has to be cost-efficient and memory-sleek. Querying the segment to which a user belongs should also have minimal latency.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Persistent tracking of comms sent per user&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To stay within the daily and weekly thresholds, we need to actively track the number of comms sent to each user, which can be referred to make rate limiting decisions. The rate limiting logic should also have minimal latency, be cost efficient, and not take up too much memory storage.&lt;/p&gt;

&lt;h4 id=&quot;optimising-storage-of-user-segment-data&quot;&gt;Optimising storage of user segment data&lt;/h4&gt;

&lt;p&gt;The problem here is figuring out which segment a particular user belongs to and ensuring that the user doesn’t appear in more than one segment. There are two options that suit our needs and we’ll explain more about each option, as well as what was the best option for us.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bloom filter&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/&quot;&gt;Bloom filter&lt;/a&gt; is a space-efficient probabilistic data structure that addresses this problem well. Simply put, Bloom filters internally use arrays to track memberships of the elements.&lt;/p&gt;

&lt;p&gt;For our scenario, each user segment would need its own bloom filter. We used &lt;a href=&quot;https://hur.st/bloomfilter/?n%3D270000000%26p%3D1.0E-7%26m%3D%26k%3D&quot;&gt;this bloom filter&lt;/a&gt; calculator to estimate the memory required for each bloom filter. We found that we needed approximately 1 GB of memory and 23 hash functions to accurately represent the membership information of 270 million users in an array. Additionally, this method guarantees a false positive rate of  1.0E-7, which means 1 in 1 million elements may get wrong membership results because of hash collision.&lt;/p&gt;

&lt;p&gt;With Grab’s existing segments, this approach needs 4GB of memory, which may increase as we increase the number of segments in the future. Moreover, the potential hash collision needs to be handled by increasing the memory size with even more hash functions. Another thing to note is that Bloom filters do not support deletion so every time a change needs to be done, you need to create a new version of the Bloom filter. Although Bloom filters have many advantages, these shortcomings led us to explore another approach.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Roaring bitmaps&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Roaring bitmaps are sets of unsigned integers consisting of containers of disjoint subsets, which can store large amounts of data in a compressed form. Essentially, roaring bitmaps could reduce memory storage significantly and overcome the hash collision problem. To understand the intuition behind this, first, we need to know how bitmaps work and the possible drawbacks behind it.&lt;/p&gt;

&lt;p&gt;To represent a list of numbers as a bitmap, we first need to create an array with a size equivalent to the largest element in the list. For every element in the list, we then mark the bit value as 1 in the corresponding index in the array. While bitmaps work very well for storing integers in closer intervals, they occupy more space and become sparse when storing integer ranges with uneven distribution, as shown in the image below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Diagram of bitmaps with uneven distribution&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To reduce memory footprint and improve the performance of bitmaps, there are compression techniques such as Run-Length Encoding (RLE), and Word Aligned Hybrid (WAH). However, this would require additional effort to implement, whereas using roaring bitmaps would solve these issues.&lt;/p&gt;

&lt;p&gt;Roaring bitmaps’ hybrid data storage approach offers the following advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster set operations (union, intersection, differencing).&lt;/li&gt;
  &lt;li&gt;Better compression ratio when handling mixed datasets (both dense and sparse data distribution).&lt;/li&gt;
  &lt;li&gt;Ability to scale to large datasets without significant performance loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To summarise, roaring bitmaps can store positive integers from 0 to (2^32)-1. Each positive integer value is converted to a 32-bit binary, where the 16 Most Significant Bits (MSB) are used as the key and the remaining 16 Least Significant Bits (LSB) are represented as the value. The values are then stored in an array, a bitmap, or used to run containers with RLE encoding data structures.&lt;/p&gt;

&lt;p&gt;If the number of integers mapped to the key is less than 4096, then all the integers are stored in an array in sorted order and converted into a bitmap container in the runtime as the size exceeds. Roaring bitmap analyses the distribution of set bits in the bitmap container i.e. if the continuous interval of set bits is more than a given threshold, the bitmap container can be more efficiently represented using the RLE container. Internally, the RLE container uses an array where the even indices store the beginning of the runs and the odd indices represent the length of the runs. This enables the roaring bitmap to dynamically switch between the containers to optimise storage and performance.&lt;/p&gt;

&lt;p&gt;The following diagram shows how a set of elements with different distributions are stored in roaring bitmaps.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Diagram of how roaring bitmaps store elements with different distributions &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Grab, we developed a microservice that abstracts roaring bitmaps implementations and provides an API to check set membership and enumeration of elements in the sets. &lt;a href=&quot;/streamlining-grabs-segmentation-platform&quot;&gt;Check out this blog to learn more about it.&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;distributed-rate-limiting&quot;&gt;Distributed rate limiting&lt;/h4&gt;

&lt;p&gt;The second part of the problem involves rate limiting the number of communication messages sent to users on a daily or weekly basis and each segment has specific daily and weekly limits. By utilising roaring bitmaps, we can determine the segment to which a user belongs. After identifying the appropriate segment, we will apply the personalised limits to the user using a distributed rate limiter, which will be discussed in further detail in the following sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Choosing the right datastore&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Based on our use case, Amazon ElasticCache for Redis and DynamoDB were two viable options for storing the sent communication messages count per user. However, we decided to choose Redis due to a number of factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Higher throughput at lower latency – Redis shards data across nodes in the cluster.&lt;/li&gt;
  &lt;li&gt;Cost-effective – Usage of Lua script reduces unnecessary data transfer overheads.&lt;/li&gt;
  &lt;li&gt;Better at handling spiky rate limiting workloads at scale.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Distributed rate limiter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To appropriately limit the comms our users receive, we needed a rate limiting algorithm, which could execute directly in the datastore cluster, then return the results in the application logic for further processing. The two rate limiting algorithms we considered were the sliding window rate limiter and sliding log rate limiter.&lt;/p&gt;

&lt;p&gt;The sliding window rate limiter algorithm divides time into a fixed-size window (we defined this as 1 minute) and counts the number of requests within each window. On the other hand, the sliding log maintains a log of each request timestamp and counts the number of requests between two timestamp ranges, providing a more fine-grained method of rate limiting. Although sliding log consumes more memory to store the log of request timestamp, we opted for the sliding log approach as the accuracy of the rate limiting was more important than memory consumption.&lt;/p&gt;

&lt;p&gt;The sliding log rate limiter utilises a Redis sorted set data structure to efficiently track and organise request logs. Each timestamp in milliseconds is stored as a unique member in the set. The score assigned to each member represents the corresponding timestamp, allowing for easy sorting in ascending order. This design choice optimises the speed of search operations when querying for the total request count within specific time ranges.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Sliding Log Rate limiter Algorithm:

Input:
  # user specific redis key where the request timestamp logs are stored as sorted set
  keys =&amp;gt; user_redis_key

  # limit_value is the limit that needs to be applied for the user
  # start_time_in_millis is the starting point of the time window
  # end_time_in_millis is the ending point of the time window
  # current_time_in_millis is the current time the request is sent
  # eviction_time_in_millis, members in the set whose value is less than this will be evicted from the set

  args =&amp;gt; limit_value, start_time_in_millis, end_time_in_millis, current_time_in_millis, eviction_time_in_millis

Output:
  # 0 means not_allowed and 1 means allowed
  response =&amp;gt; 0 / 1

Logic:
  # zcount fetches the count of the request timestamp logs falling between the start and the end timestamp
  request_count = zcount user_redis_key start_time_in_millis end_time_in_millis

  response = 0
  # if the count of request logs is less than allowed limits then record the usage by adding current timestamp in sorted set

  if request_count &amp;lt; limit_value then
    zadd user_redis_key current_time_in_millis current_time_in_millis
    response = 1

  # zremrangebyscore removes the members in the sorted set whose score is less than eviction_time_in_millis

  zremrangebyscore user_redis_key -inf eviction_time_in_millis
  return response
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This algorithm takes O(log n) time complexity, where n is the number of request logs stored in the sorted set. It is not possible to evict entries in the sorted set like how we have time-to-live (TTL) for Redis keys. To prevent the size of the sorted set from increasing over time, we have a fixed variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eviction_time_in_millis&lt;/code&gt; that is passed to the script. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zremrangebyscore&lt;/code&gt; command then deletes members from the sorted set whose score is less than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eviction_time_in_millis&lt;/code&gt; in O(log n) time complexity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lua script optimisations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Redis Cluster mode, all Redis keys accessed by a Lua script must be present on the same node, and they should be passed as part of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KEYS&lt;/code&gt; input array of the script. If the script attempts to access keys located on different nodes within the cluster, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CROSSSLOT&lt;/code&gt; error will be thrown. Redis keys, or userIDs, are distributed across multiple nodes in the cluster so it is not feasible to send a batch of userIDs within the same Lua script for rate limiting, as this might result in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CROSSSLOT&lt;/code&gt; error.&lt;/p&gt;

&lt;p&gt;Invoking a separate Lua script call for each user is a possible approach, but it incurs a significant number of network calls, which can be optimised further with the following approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Upload the Lua script into the Redis server during the server startup with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SCRIPT LOAD&lt;/code&gt; command and we get the SHA1 hash of the script if the upload is successful.&lt;/li&gt;
  &lt;li&gt;The SHA1 hash can then be used to invoke the Lua script with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVALSHA&lt;/code&gt; command passing the keys and arguments as script input.&lt;/li&gt;
  &lt;li&gt;Redis pipelining takes in multiple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVALSHA&lt;/code&gt; commands that call the Lua script and each invocation corresponds to a userID for getting the rate limiting result.&lt;/li&gt;
  &lt;li&gt;Redis pipelining groups the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVALSHA&lt;/code&gt; Redis commands with Redis keys located on the same nodes internally. It then sends the grouped commands in a single network call to the relevant nodes within the Redis cluster and provides the rate limiting outcome to the client.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since Redis operates on a single thread, any long-running Lua script can cause other Redis commands to be blocked until the script completes execution. Thus, it’s optimal for the Lua script to execute in under 5 milliseconds. Additionally, the current time is passed as an argument to the script to account for potential variations in time when the script is executed on a node’s replica, which could be caused by &lt;a href=&quot;https://medium.com/geekculture/all-things-clock-time-and-order-in-distributed-systems-physical-time-in-depth-3c0a4389a838%23:~:text%3DClock%2520Drift%253A%2520As%2520mentioned%252C%2520no,rate%2520is%2520called%2520clock%2520drift.&quot;&gt;clock drift&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By bringing together roaring bitmaps and the distributed rate limiter, this is what our final solution looks like:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Our final solution using roaring bitmaps and distributed rate limiter&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The roaring bitmaps structure is serialised and stored in an AWS S3 bucket, which is then downloaded in the instance during server startup. After which, triggering a user segment membership check can simply be done with a local method call. The configuration service manages the mapping information between the segment and allowed rate limiting values.&lt;/p&gt;

&lt;p&gt;Whenever a marketing message needs to be sent to a user, we first find the segment to which the user belongs, retrieve the defined rate limiting values from the configuration service, then execute the Lua script to get the rate limiting decision. If there is enough quota available for the user, we send the comms.&lt;/p&gt;

&lt;p&gt;The architecture of the messaging service looks something like this:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/frequency-capping/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Architecture of the messaging service&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;In addition to decreasing the unsubscription rate, there was a significant enhancement in the latency of sending communications. Eliminating redundant communications also alleviated the system load, resulting in a reduction of the delay between the scheduled time and the actual send time of comms.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Applying rate limiters to safeguard our services is not only a standard practice but also a necessary process. Many times, this can be achieved by configuring the rate limiters at the instance level. The need for rate limiters for business logic may not be as common, but when you need it, the solution must be lightning-fast, and capable of seamlessly operating within a distributed environment.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Dec 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/frequency-capping</link>
        <guid isPermaLink="true">https://engineering.grab.com/frequency-capping</guid>
        
        <category>Data</category>
        
        <category>Big data</category>
        
        <category>Rate limiting</category>
        
        <category>Frequency capping</category>
        
        <category>Distributed systems</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>An elegant platform</title>
        <description>&lt;p&gt;Coban is Grab’s real-time data streaming platform team. As a platform team, we thrive on providing our internal users from all verticals with self-served data-streaming resources, such as &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; topics, &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; and &lt;a href=&quot;https://www.confluent.io/learn/change-data-capture/&quot;&gt;Change Data Capture&lt;/a&gt; (CDC) pipelines, various kinds of &lt;a href=&quot;https://docs.confluent.io/platform/current/connect/&quot;&gt;Kafka-Connect&lt;/a&gt; connectors, as well as &lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;Apache Zeppelin&lt;/a&gt; notebooks, so that they can effortlessly leverage real-time data to build intelligent applications and services.&lt;/p&gt;

&lt;p&gt;In this article, we present our journey from pure Infrastructure-as-Code (IaC) towards a more sophisticated control plane that has revolutionised the way data streaming resources are self-served at Grab. This change also leads to improved scalability, stability, security, and user adoption of our data streaming platform.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;In the early ages of public cloud, it was a common practice to create virtual resources by clicking through the web console of a cloud provider, which is sometimes referred to as &lt;em&gt;ClickOps&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;ClickOps&lt;/em&gt; has many downsides, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inability to review, track, and audit changes to the infrastructure.&lt;/li&gt;
  &lt;li&gt;Inability to massively scale the infrastructure operations.&lt;/li&gt;
  &lt;li&gt;Inconsistencies between environments, e.g. staging and production.&lt;/li&gt;
  &lt;li&gt;Inability to quickly recover from a disaster by re-creating the infrastructure at a different location.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That said, &lt;em&gt;ClickOps&lt;/em&gt; has one tremendous advantage; it makes creating resources using a graphical User Interface (UI) fairly easy for anyone like Infrastructure Engineers, Software Engineers, Data Engineers etc. This also leads to a high iteration speed towards innovation in general.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
IaC resolved many of the limitations of &lt;em&gt;ClickOps&lt;/em&gt;, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Changes are committed to a Version Control System (VCS) like Git: They can be reviewed by peers before being merged. The full history of all changes is available for investigating issues and for audit.&lt;/li&gt;
  &lt;li&gt;The infrastructure operations scale better: Code for similar pieces of infrastructure can be modularised. Changes can be rolled out automatically by Continuous Integration (CI) pipelines in the VCS system, when a change is merged to the main branch.&lt;/li&gt;
  &lt;li&gt;The same code can be used to deploy the staging and production environments consistently.&lt;/li&gt;
  &lt;li&gt;The infrastructure can be re-created anytime from its source code, in case of a disaster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, IaC unwittingly posed a new entry barrier too, requiring the learning of new tools like Ansible, Puppet, Chef, Terraform, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Some organisations set up dedicated Site Reliability Engineer (SRE) teams to centrally manage, operate, and support those tools and the infrastructure as a whole, but that soon created the potential of new bottlenecks in the path to innovation.&lt;/p&gt;

&lt;p&gt;On the other hand, others let engineering teams manage their own infrastructure, and Grab adopted that same approach. We use &lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt; to manage infrastructure, and all teams are expected to have select engineers who have received Terraform training and have a clear understanding of it.&lt;/p&gt;

&lt;p&gt;In this context, Coban’s platform initially started as a handful of Git repositories where users had to submit their Merge Requests (MR) of Terraform code to create their data streaming resources. Once reviewed by a Coban engineer, those Terraform changes would be applied by a CI pipeline running &lt;a href=&quot;https://www.runatlantis.io/&quot;&gt;Atlantis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
While this was a meaningful first step towards self-service and platformisation of Coban’s offering within Grab, it had several significant downsides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Due to the lack of control on the Terraform changes, the CI pipeline was prone to human errors and frequent failures. For example, users would initiate a new Terraform project by duplicating an existing one, but then would forget to change the location of the remote Terraform state, leading to the in-place replacement of an existing resource.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The Coban team needed to review all MRs and provide ad hoc support whenever the pipeline failed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: In the absence of Identity and Access Management (IAM), MRs could potentially contain changes pertaining to other teams’ resources, or even changes to Coban’s core infrastructure, with code review as the only guardrail.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited user growth&lt;/strong&gt;: We could only acquire users who were well-versed in Terraform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It soon became clear that we needed to build a layer of abstraction between our users and the Terraform code, to increase the level of control and lower the entry barrier to our platform, while still retaining all of the benefits of IaC under the hood.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;We designed and built an in-house three-tier control plane made of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Coban UI&lt;/strong&gt;, a front-end web interface, providing our users with a seamless ClickOps experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Heimdall&lt;/strong&gt;, the Go back-end of the web interface, transforming ClickOps into IaC.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Khone&lt;/strong&gt;, the storage and provisioner layer, a Git repository storing Terraform code and metadata of all resources as well as the CI pipelines to plan and apply the changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the next sections, we will deep dive in those three components.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image6.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 1 Simplified architecture of a request flowing from the user to the Coban infrastructure, via the three components of the control plane: the Coban UI, Heimdall, and Khone.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Although we designed the user journey to start from the Coban UI, our users can still opt to communicate with Heimdall and with Khone directly, e.g. for batch changes, or just because many engineers love Git and we want to encourage broad adoption. To make sure that data is eventually consistent across the three systems, we made Khone the only persistent storage layer. Heimdall regularly fetches data from Khone, caches it, and presents it to the Coban UI upon each query.&lt;/p&gt;

&lt;p&gt;We also continued using Terraform for all resources, instead of mixing various declarative infrastructure approaches (e.g. Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&quot;&gt;Custom Resource Definition&lt;/a&gt;, &lt;a href=&quot;https://helm.sh/docs/topics/charts/&quot;&gt;Helm charts&lt;/a&gt;), for the sake of consistency of the logic in Khone’s CI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;coban-ui&quot;&gt;Coban UI&lt;/h3&gt;

&lt;p&gt;The Coban UI is a &lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt; &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Glossary/SPA&quot;&gt;Single Page Application&lt;/a&gt; (React SPA) designed by our partner team Chroma, a dedicated team of front-end engineers who thrive on building legendary UIs and reusable components for platform teams at Grab.&lt;/p&gt;

&lt;p&gt;It serves as a comprehensive self-service portal, enabling users to effortlessly create data streaming resources by filling out web forms with just a few clicks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image7.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 2 Screen capture of a new Kafka topic creation in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition to facilitating resource creation and configuration, the Coban UI is seamlessly integrated with multiple monitoring systems. This integration allows for real-time monitoring of critical metrics and health status for Coban infrastructure components, including Kafka clusters, Kafka topic bytes in/out rates, and more. Under the hood, all this information is exposed by Heimdall APIs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image4.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 3 Screen capture of the metrics of a Kafka cluster in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In terms of infrastructure, the Coban UI is hosted in &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html&quot;&gt;AWS S3 website hosting&lt;/a&gt;. All dynamic content is generated by querying the APIs of the back-end: Heimdall.&lt;/p&gt;

&lt;h3 id=&quot;heimdall&quot;&gt;Heimdall&lt;/h3&gt;

&lt;p&gt;Heimdall is the Go back-end of the Coban UI. It serves a collection of APIs for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Managing the data streaming resources of the Coban platform with Create, Read, Update and Delete (CRUD) operations, treating the Coban UI as a first-class citizen.&lt;/li&gt;
  &lt;li&gt;Exposing the metadata of all Coban resources, so that they can be used by other platforms or searched in the Coban UI.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All operations are authenticated and authorised. Read more about Heimdall’s access control in &lt;a href=&quot;/migrating-to-abac&quot;&gt;Migrating from Role to Attribute-based Access Control&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the next sections, we are going to dive deeper into these two features.&lt;/p&gt;

&lt;h4 id=&quot;managing-the-data-streaming-resources&quot;&gt;Managing the data streaming resources&lt;/h4&gt;

&lt;p&gt;First and foremost, Heimdall enables our users to self-manage their data streaming resources. It primarily relies on Khone as its storage and provisioner layer for actual resource management via Git CI pipelines. Therefore, we designed Heimdall’s resource management workflow to leverage the underlying Git flow.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image2.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 4 Diagram flow of a request in Heimdall.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Fig. 4 shows the diagram flow of a typical request in Heimdall to create, update, or delete a resource.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An authenticated user initiates a request, either by navigating in the Coban UI or by calling the Heimdall API directly. At this stage, the request state is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Initiated&lt;/code&gt; on Heimdall.&lt;/li&gt;
  &lt;li&gt;Heimdall validates the request against multiple validation rules. For example, if an ongoing change request exists for the same resource, the request fails. If all tests succeed, the request state moves to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ongoing&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Heimdall then creates an MR in Khone, which contains the Terraform files describing the desired state of the resource, as well as an in-house metadata file describing the key attributes of both resource and requester.&lt;/li&gt;
  &lt;li&gt;After the MR has been created successfully, Heimdall notifies the requester via Slack and shares the MR URL.&lt;/li&gt;
  &lt;li&gt;After that, Heimdall starts polling the status of the MR in a loop.&lt;/li&gt;
  &lt;li&gt;For changes pertaining to production resources, an approver who is code owner in the repository of the resource has to approve the MR. Typically, the approver is an immediate teammate of the requester. Indeed, as a platform team, we empower our users to manage their own resources in a self-service fashion. Ultimately, the requester would merge the MR to trigger the CI pipeline applying the actual Terraform changes. Note that for staging resources, this entire step 6 is automatically performed by Heimdall.&lt;/li&gt;
  &lt;li&gt;Depending on the MR status and the status of its CI pipeline in Khone, the final state of the request can be:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Failed&lt;/code&gt; if the CI pipeline has failed in Khone.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Completed&lt;/code&gt; if the CI pipeline has succeeded in Khone.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cancelled&lt;/code&gt; if the MR was closed in Khone.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Heimdall exposes APIs to let users track the status of their requests. In the Coban UI, a page queries those APIs to elegantly display the requests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image5.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 5 Screen capture of the Coban UI showing all requests.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;exposing-the-metadata&quot;&gt;Exposing the metadata&lt;/h4&gt;

&lt;p&gt;Apart from managing the data streaming resources, Heimdall also centralises and exposes the metadata pertaining to those resources so other Grab systems can fetch and use it. They can make various queries, for example, listing the producers and consumers of a given Kafka topic, or determining if a database (DB) is the data source for any CDC pipeline.&lt;/p&gt;

&lt;p&gt;To make this happen, Heimdall not only retains the metadata of all of the resources that it creates, but also regularly ingests additional information from a variety of upstream systems and platforms, to enrich and make this metadata comprehensive.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image1.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 6 Diagram showing some of Heimdall&apos;s upstreams (on the left) and downstreams (on the right) for metadata collection, enrichment, and serving. The arrows show the data flow. The network connection (client -&amp;gt; server) is actually the other way around.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
On the left side of Fig. 6, we illustrate Heimdall’s ingestion mechanism with several examples (step 1):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The metadata of all Coban resources is ingested from Khone. This means the metadata of the resources that were created directly in Khone is also available in Heimdall.&lt;/li&gt;
  &lt;li&gt;The list of Kafka producers is retrieved from our monitoring platform, where most of them emit metrics.&lt;/li&gt;
  &lt;li&gt;The list of Kafka consumers is retrieved directly from the respective Kafka clusters, by listing the &lt;a href=&quot;https://docs.confluent.io/platform/current/clients/consumer.html#consumer-groups&quot;&gt;consumer groups&lt;/a&gt; and respective &lt;a href=&quot;https://developer.confluent.io/faq/apache-kafka/kafka-clients/#kafka-clients-what-is-clientid-in-kafka&quot;&gt;Client IDs&lt;/a&gt; of each partition.&lt;/li&gt;
  &lt;li&gt;The metadata of all DBs, that are used as a data source for CDC pipelines, is fetched from Grab’s internal DB management platform.&lt;/li&gt;
  &lt;li&gt;The Kafka stream schemas are retrieved from the Coban schema repository.&lt;/li&gt;
  &lt;li&gt;The Kafka stream configuration of each stream is retrieved from Grab Universal Configuration Management platform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all of this ingested data, Heimdall can provide comprehensive and accurate information about all data streaming resources to any other Grab platforms via a set of dedicated APIs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The right side of Fig. 6 shows some examples (step 2) of Heimdall’s serving mechanism:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As a downstream of Heimdall, the Coban UI enables our direct users to conveniently browse their data streaming resources and access their attributes.&lt;/li&gt;
  &lt;li&gt;The entire resource inventory is ingested into the broader Grab inventory platform, based on &lt;a href=&quot;https://backstage.io/&quot;&gt;backstage.io&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The Kafka streams are ingested into Grab’s internal data discovery platform, based on &lt;a href=&quot;https://datahubproject.io/&quot;&gt;DataHub&lt;/a&gt;, where users can discover and trace the lineage of any piece of data.&lt;/li&gt;
  &lt;li&gt;The CDC connectors pertaining to DBs are ingested by Grab internal DB management platform, so that they are made visible in that platform when users are browsing their DBs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the downstream platforms that ingest data from Heimdall each expose a particular view of the Coban inventory that serves their purpose, but the Coban platform remains the only source of truth for any data streaming resource at Grab.&lt;/p&gt;

&lt;p&gt;Lastly, Heimdall leverages an internal MySQL DB to support quick data query and exploration. The corresponding API is called by the Coban UI to let our users conveniently search globally among all resources’ attributes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image8.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 7 Screen capture of the global search feature in the Coban UI.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;khone&quot;&gt;Khone&lt;/h3&gt;

&lt;p&gt;Khone is the persistent storage layer of our platform, as well as the executor for actual resource creation, changes, and deletion. Under the hood, it is actually a GitLab repository of Terraform code in typical &lt;a href=&quot;https://about.gitlab.com/topics/gitops/&quot;&gt;GitOps&lt;/a&gt; fashion, with CI pipelines to plan and apply the Terraform changes automatically. In addition, it also stores a metadata file for each resource.&lt;/p&gt;

&lt;p&gt;Compared to letting the platform create the infrastructure directly and keep track of the desired state in its own way, relying on a standard IaC tool like Terraform for the actual changes to the infrastructure presents two major advantages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The Terraform code can directly be used for disaster recovery. In case of a disaster, any entitled Cobaner with a local copy of the main branch of the Khone repository is able to recreate all our platform resources directly from their machine. There is no need to rebuild the entire platform’s control plane, thus reducing our Recovery Time Objective (RTO).&lt;/li&gt;
  &lt;li&gt;Minimal effort required to follow the API changes of our infrastructure ecosystem (AWS, Kubernetes, Kafka, etc.). When such a change happens, all we need to do is to update the corresponding Terraform provider.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’d like to read more about Khone, check out &lt;a href=&quot;/securing-gitops-pipeline&quot;&gt;Securing GitOps pipelines&lt;/a&gt;. In this section, we will only focus on Khone’s features that are relevant from the platform perspective.&lt;/p&gt;

&lt;h4 id=&quot;lightweightterraform&quot;&gt;Lightweight Terraform&lt;/h4&gt;

&lt;p&gt;In Khone, each resource is stored as a Terraform definition. There are two major differences from a normal Terraform project:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No Terraform environment, such as the required Terraform providers and the location of the remote Terraform state file. They are automatically generated by the CI pipeline via a simple wrapper.&lt;/li&gt;
  &lt;li&gt;Only vetted Khone Terraform modules can be used. This is controlled and enforced by the CI pipeline via code inspection. There is one such Terraform module for each kind of supported resource of our platform (e.g. Kafka topic, Flink pipeline, Kafka Connect mirror source connector etc.). Furthermore, those in-house Terraform modules are designed to automatically derive their key variables (e.g. resource name, cluster name, environment) from the relative path of the parent Terraform project in the Khone repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those characteristics are designed to limit the risk and blast radius of human errors. They also make sure that all resources created in Khone are supported by our platform, so that they can also be discovered and managed in Heimdall and the Coban UI. Lastly, by generating the Terraform environment on the fly, we can destroy resources simply by deleting the directory of the project in the code base – this would not be possible otherwise.&lt;/p&gt;

&lt;h4 id=&quot;resource-metadata&quot;&gt;Resource metadata&lt;/h4&gt;

&lt;p&gt;All resource metadata is stored in a YAML file that is present in the Terraform directory of each resource in the Khone repository. This is mainly used for ownership and cost attribution.&lt;/p&gt;

&lt;p&gt;With this metadata, we can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Better communicate with our users whenever their resources are impacted by an incident or an upcoming maintenance operation.&lt;/li&gt;
  &lt;li&gt;Help teams understand the costs of their usage of our platform, a significant step towards cost efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two different ways resource metadata can be created:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Automatically through Heimdall: The YAML metadata file is automatically generated by Heimdall.&lt;/li&gt;
  &lt;li&gt;Through Khone by a human user: The user needs to prepare the YAML metadata file and include it in the MR. This file is then verified by the CI pipeline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outcome&quot;&gt;Outcome&lt;/h2&gt;

&lt;p&gt;The initial version of the three-tier Coban platform, as described in this article, was internally released in March 2022, supporting only Kafka topic management at the time. Since then, we have added support for Flink pipelines, four kinds of Kafka Connect connectors, CDC pipelines, and more recently, Apache Zeppelin notebooks. At the time of writing, the Coban platform manages about 5000 data streaming resources, all described as IaC under the hood.&lt;/p&gt;

&lt;p&gt;Our platform also exposes enriched metadata that includes the full data lineage from Kafka producers to Kafka consumers, as well as ownership information, and cost attribution.&lt;/p&gt;

&lt;p&gt;With that, our monthly active users have almost quadrupled, truly moving the needle towards democratising the usage of real-time data within all Grab verticals.&lt;/p&gt;

&lt;p&gt;In spite of that user growth, the end-to-end workflow success rate for self-served resource creation, change or deletion, remained well above 90% in the first half of 2023, while the Heimdall API uptime was above 99.95%.&lt;/p&gt;

&lt;h2 id=&quot;challenges-faced&quot;&gt;Challenges faced&lt;/h2&gt;

&lt;p&gt;A common challenge for platform teams resides in the misalignment between the Service Level Objective (SLO) of the platform, and the various environments (e.g. staging, production) of the managed resources and upstream/downstream systems and platforms.&lt;/p&gt;

&lt;p&gt;Indeed, the platform aims to guarantee the same level of service, regardless of whether it is used to create resources in the staging or the production environment. From the platform team’s perspective, the platform as a whole is considered production-grade, as soon as it serves actual users.&lt;/p&gt;

&lt;p&gt;A naive approach to address this challenge is to let the production version of the platform manage all resources regardless of their respective environments. However, doing so does not permit a hermetic segregation of the staging and production environments across the organisation, which is a good security practice, and often a requirement for compliance. For example, the production version of the platform would have to connect to upstream systems in the staging environment, e.g. staging Kafka clusters to collect their consumer groups, in the case of Heimdall. Conversely, the staging version of certain downstreams would have to connect to the production version of Heimdall, to fetch the metadata of relevant staging resources.&lt;/p&gt;

&lt;p&gt;The alternative approach, generally adopted across Grab, is to instantiate all platforms in each environment (staging and production), while still considering both instances as production-grade and guaranteeing tight SLOs in both environments.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/elegant-platform/image3.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Fig. 8 Architecture of the Coban platform, broken down by environment.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Fig. 8, both instances of Heimdall have equivalent SLOs. The caveat is that all upstream systems and platforms must also guarantee a strict SLO in both environments. This obviously comes with a cost, for example, tighter maintenance windows for the operations pertaining to the Kafka clusters in the staging environment.&lt;/p&gt;

&lt;p&gt;A strong “platform” culture is required for platform teams to fully understand that their instance residing in the staging environment is not their own staging environment and should not be used for testing new features.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Currently, users creating, updating, or deleting production resources in the Coban UI (or directly by calling Heimdall API) receive the URL of the generated GitLab MR in a Slack message. From there, they must get the MR approved by a code owner, typically another team member, and finally merge the MR, for the requested change to be actually implemented by the CI pipeline.&lt;/p&gt;

&lt;p&gt;Although this was a fairly easy way to implement a maker/checker process that was immediately compliant with our regulatory requirements for any changes in production, the user experience is not optimal. In the near future, we plan to bring the approval mechanism into Heimdall and the Coban UI, while still providing our more advanced users with the option to directly create, approve, and merge MRs in GitLab. In the longer run, we would also like to enhance the Coban UI with the output of the Khone CI jobs that include the Terraform plan and apply results.&lt;/p&gt;

&lt;p&gt;There is another aspect of the platform that we want to improve. As Heimdall regularly polls the upstream platforms to collect their metadata, this introduces a latency between a change in one of those platforms and its reflection in the Coban platform, which can hinder the user experience. To refresh resource metadata in Heimdall in near real time, we plan to leverage an existing Grab-wide event stream, where most of the configuration and code changes at Grab are produced as events. Heimdall will soon be able to consume those events and update the metadata of the affected resources immediately, without waiting for the next periodic refresh.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/an-elegant-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/an-elegant-platform</guid>
        
        <category>Data</category>
        
        <category>Data streaming</category>
        
        <category>Real-time streaming</category>
        
        <category>Platformisation</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Road localisation in GrabMaps</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2022, Grab achieved self-sufficiency in its Geo services. As part of this transition, one crucial step was moving towards using an internally-developed map tailored specifically to the market in which Grab operates. Now that we have full control over the map layer, we can add more data to it or improve it according to the needs of the services running on top. One key aspect that this transition unlocked for us was the possibility of creating hyperlocal data at map level.&lt;/p&gt;

&lt;p&gt;For instance, by determining the country to which a road belongs, we can now automatically infer the official language of that country and display the street name in that language. In another example, knowing the country for a specific road, we can automatically infer the driving side (left-handed or right-handed) leading to an improved navigation experience. Furthermore, this capability also enables us to efficiently handle various scenarios. For example, if we know that a road is part of a gated community, an area where our driver partners face restricted access, we can prevent the transit through that area.&lt;/p&gt;

&lt;p&gt;These are just some examples of the possibilities from having full control over the map layer. By having an internal map, we can align our maps with specific markets and provide better experiences for our driver-partners and customers.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;For all these to be possible, we first needed to localise the roads inside the map. Our goal was to include hyperlocal data into the map, which refers to data that is specific to a certain area, such as a country, city, or even a smaller part of the city like a gated community. At the same time, we aimed to deliver our map with a high cadence, thus, we needed to find the right way to process this large amount of data while continuing to create maps in a cost-effective manner.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;In the following sections of this article, we will use an extract from the Southeast Asia map to provide visual representations of the concepts discussed.&lt;/p&gt;

&lt;p&gt;In Figure 1, Image 1 shows a visualisation of the road network, the roads belonging to this area. The coloured lines in Image 2 represent the borders identifying the countries in the same area. Overlapping the information from Image 1 and Image 2, we can extrapolate and say that the entire surface included in a certain border could have the same set of common properties as shown in Image 3. In Image 4, we then proceed with adding localised roads for each area.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/localisation.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Map of Southeast Asia&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For this to be possible, we have to find a way to localise each road and identify its associated country. Once this localisation process is complete, we can replicate all this information specific to a given border onto each individual road. This information includes details such as the country name, driving side, and official language. We can go even further and infer more information, and add hyperlocal data. For example, in Vietnam, we can automatically prevent motorcycle access on the motorways.&lt;/p&gt;

&lt;p&gt;Assigning each road on the map to a specific area, such as a country, service area, or subdivision, presents a complex task. So, how can we efficiently accomplish this?&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;The most straightforward approach would be to test the inclusion of each road into each area boundary, but that is easier said than done. With close to 30 million road segments in the Southeast Asia map and over 10 thousand areas, the computational cost of determining inclusion or intersection between a polyline and a polygon is expensive.&lt;/p&gt;

&lt;p&gt;Our solution to this challenge involves replacing the expensive yet precise operation with a decent approximation. We introduce a proxy entity, the geohash, and we use it to approximate the areas and also to localise the roads.&lt;/p&gt;

&lt;p&gt;We replace the geometrical inclusion with a series of simpler and less expensive operations. First, we conduct an inexpensive precomputation where we identify all the geohases that belong to a certain area or within a defined border. We then identify the geohashes to which the roads  belong to. Finally, we use these precomputed values to assign roads to their respective areas. This process is also computationally inexpensive.&lt;/p&gt;

&lt;p&gt;Given the large area we process, we leverage big data techniques to distribute the execution across multiple nodes and thus speed up the operation. We want to deliver the map daily and this is one of the many operations that are part of the map-making process.&lt;/p&gt;

&lt;h3 id=&quot;what-is-a-geohash&quot;&gt;What is a geohash?&lt;/h3&gt;

&lt;p&gt;To further understand our implementation we will first explain the &lt;a href=&quot;https://en.wikipedia.org/wiki/Geohash&quot;&gt;geohash concept&lt;/a&gt;. A geohash is a unique identifier of a specific region on the Earth. The basic idea is that the Earth is divided into regions of user-defined size and each region is assigned a unique id, which is known as its geohash. For a given location on earth, the geohash algorithm converts its latitude and longitude into a string.&lt;/p&gt;

&lt;p&gt;Geohashes uses a Base-32 alphabet encoding system comprising characters ranging from  0 to 9 and A to Z, excluding “A”, “I”, “L” and “O”. Imagine dividing the world into a grid with 32 cells. The first character in a geohash identifies the initial location of one of these 32 cells. Each of these cells are then further subdivided into 32 smaller cells.This subdivision process continues and refines to specific areas in the world. Adding characters to the geohash sub-divides a cell, effectively zooming in to a more detailed area.&lt;/p&gt;

&lt;p&gt;The precision factor of the geohash determines the size of the cell. For instance, a precision factor of one creates a cell 5,000 km high and 5,000 km wide. A precision factor of six creates a cell 0.61km high and 1.22 km wide. Furthermore, a precision factor of nine creates a cell 4.77 m high and 4.77 m wide. It is important to note that cells are not always square and can have varying dimensions.&lt;/p&gt;

&lt;p&gt;In Figure 2,  we have exemplified a geohash 6 grid and its code is &lt;strong&gt;wsdt33&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-code-wsdt33.jpg&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 2 - An example of geohash code wsdt33&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;using-less-expensive-operations&quot;&gt;Using less expensive operations&lt;/h3&gt;

&lt;p&gt;Calculating the inclusion of the roads inside a certain border is an expensive operation. However, quantifying the exact expense is challenging as it depends on several factors. One factor is the complexity of the border. Borders are usually irregular and very detailed, as they need to correctly reflect the actual border. The complexity of the road geometry is another factor that plays an important role as roads are not always straight lines.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/roads-to-localise.png&quot; alt=&quot;&quot; style=&quot;width:30%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 3 - Roads to localise&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Since this operation is expensive both in terms of cloud cost and time to run, we need to identify a cheaper and faster way that would yield similar results. Knowing that the complexity of the border lines is the cause of the problem, we tried using a different alternative, a rectangle. Calculating the inclusion of a polyline inside a rectangle is a cheaper operation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/roads-inside-rectangle.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 4 - Roads inside a rectangle&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;So we transformed this large, one step operation, where we test each road segment for inclusion in a border, into a series of smaller operations where we perform the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify all the geohashes that are part of a certain area or belong to a certain border. In this process we include additional areas to make sure that we cover the entire surface inside the border.&lt;/li&gt;
  &lt;li&gt;For each road segment, we identify the list of geohashes that it belongs to. A road, depending on its length or depending on its shape, might belong to multiple geohashes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In Figure 5, we identify that the road belongs to two geohashes and that the two geohashes are part of the border we use.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-proxy.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 5 - Geohashes as proxy&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Now, all we need to do is join the two data sets together. This kind of operation is a great candidate for a big data approach, as it allows us to run it in parallel and speed up the processing time.&lt;/p&gt;

&lt;h2 id=&quot;precision-tradeoff&quot;&gt;Precision tradeoff&lt;/h2&gt;

&lt;p&gt;We mentioned earlier that, for the sake of argument, we replace precision with a decent approximation. Let’s now delve into the real tradeoff by adopting this approach.&lt;/p&gt;

&lt;p&gt;The first thing that stands out with this approach is that we traded precision for cost. We are able to reduce the cost as this approach uses less hardware resources and computation time. However, this reduction in precision suffers, particularly for roads located near the borders as they might be wrongly classified.&lt;/p&gt;

&lt;p&gt;Going back to the initial example, let’s take the case of the external road, on the left side of the area. As you can see in Figure 6, it is clear that the road does not belong to our border. But when we apply the geohash approach it gets included into the middle geohash.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/wrong-road-localisation.png&quot; alt=&quot;&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 6 - Wrong road localisation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Given that just a small part of the geohash falls inside the border, the entire geohash will be classified as belonging to that area, and, as a consequence, the road that belongs to that geohash will be wrongly localised and we’ll end up adding the wrong localisation information to that road. This is clearly a consequence of the precision tradeoff. So, how can we solve this?&lt;/p&gt;

&lt;h3 id=&quot;geohash-precision&quot;&gt;Geohash precision&lt;/h3&gt;

&lt;p&gt;One option is to increase the geohash precision. By using smaller and smaller geohashes, we can better reflect the actual area. As we go deeper and we further split the geohash, we can accurately follow the border. However, a high geohash precision also equates to a computationally intensive operation bringing us back to our initial situation. Therefore, it is crucial to find the right balance between the geohash size and the complexity of operations.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-precision.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 7 - Geohash precision&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;geohash-coverage-percentage&quot;&gt;Geohash coverage percentage&lt;/h3&gt;

&lt;p&gt;To find a balance between precision and data loss, we looked into calculating the geohash coverage percentage. For example, in Figure 8, the blue geohash is entirely within the border.  Here we can say that it has a 100% geohash coverage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-inside-border.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 8 - Geohash inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However, take for example the geohash in Figure 9. It touches the border and has only around 80% of its surface inside the area. Given that most of its surface is within the border, we still can say that it belongs to the area.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-partial-border.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 9 - Geohash partially inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Let’s look at another example. In Figure 10, only a small part of the geohash is within the border. We can say that the geohash coverage percentage here is around 5%. For these cases, it becomes difficult for us to determine whether the geohash does belong to the area. What would be a good tradeoff in this case?&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-barely-border.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 10 - Geohash barely inside the border&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;border-shape&quot;&gt;Border shape&lt;/h3&gt;

&lt;p&gt;To go one step further, we can consider a mixed solution, where we use the border shape but only for the geohashes touching the border. This would still be an intensive computational operation but the number of roads located in these geohashes will be much smaller, so it is still a gain.&lt;/p&gt;

&lt;p&gt;For the geohashes with full coverage inside the area, we’ll use the geohash for the localisation, the simpler operation. For the geohashes that are near the border, we’ll use a different approach. To increase the precision around the borders, we can cut the geohash following the  border’s shape. Instead of having a rectangle, we’ll use a more complex shape which is still simpler than the initial border shape.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/geohash-border-shape.png&quot; alt=&quot;&quot; style=&quot;width:50%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 11 - Geohash following a border’s shape&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;We began with a simple approach and we enhanced it to improve precision. This also increased the complexity of the operation. We then asked, what are the actual gains? Was it worthwhile to go through all this process? In this section, we put this to the test.&lt;/p&gt;

&lt;p&gt;We first created a benchmark by taking a small sample of the data and ran the localisation process on a laptop. The sample comprised approximately 2% of the borders and 0.0014% of the roads. We ran the localisation process using two approaches.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;With the first approach, we calculated the intersection between all the roads and borders. The entire operation took around 38 minutes.&lt;/li&gt;
  &lt;li&gt;For the second approach, we optimised the operation using geohashes. In this approach, the runtime was only 78 seconds (1.3 minutes).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, it is important to note that this is not an apples-to-apples comparison. The operation that we measured was the localisation of the roads but we did not include the border filling operation where we fill the borders with geohashes. This is because this operation does not need to be run every time. It can be run once and reused multiple times.&lt;/p&gt;

&lt;p&gt;Though not often required, it is still crucial to understand and consider the operation of precomputing areas and filling borders with geohashes. The precomputation process depends on several factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number and shape of the borders - The more borders and the more complex the borders are, the longer the operation will take.&lt;/li&gt;
  &lt;li&gt;Geohash precision - How accurate do we need our localisation to be? The more accurate it needs to be, the longer it will take.&lt;/li&gt;
  &lt;li&gt;Hardware availability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Going back to our hypothesis, although this precomputation might be expensive, it is rarely run as the borders don’t change often and can be triggered only when needed. However, regular computation, where we find the area to which each road belongs to, is often run as the roads change constantly. In our system, we run this localisation for each map processing.&lt;/p&gt;

&lt;p&gt;We can also further optimise this process by applying the opposite approach. Geohashes that have full coverage inside a border can be merged together into larger geohashes thus simplifying the computation inside the border. In the end, we can have a solution that is fully optimised for our needs with the best cost-to-performance ratio.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/optimised-geohash.png&quot; alt=&quot;&quot; style=&quot;width:20%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 12 - Optimised geohashes&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Although geohashes seem to be the right solution for this kind of problem, we also need to monitor their content. One consideration is the road density inside a geohash. For example, a geohash inside a city centre usually has a lot of roads while one in the countryside may have much less. We need to consider this aspect to have a balanced computation operation and take full advantage of the big data approach. In our case, we achieve this balance by considering the number of road kilometres within a geohash.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/unbalanced-data.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 13 - Unbalanced data&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, the resources that we choose also matter. To optimise time and cost, we need to find the right balance between the running time and resource cost. As shown in Figure 14, based on a sample data we ran, sometimes, we get the best result when using smaller machines.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/road-localisation-grabmaps/cost-vs-runtime.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 14 - Cost vs runtime&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;The achievements and insights showcased in this article are indebted to the contributions made by Mihai Chintoanu. His expertise and collaborative efforts have profoundly enriched the content and findings presented herein.&lt;/small&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

</description>
        <pubDate>Fri, 17 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/road-localisation-grabmaps</link>
        <guid isPermaLink="true">https://engineering.grab.com/road-localisation-grabmaps</guid>
        
        <category>Maps</category>
        
        <category>Data</category>
        
        <category>Big Data</category>
        
        <category>Data processing</category>
        
        <category>Hyperlocalisation</category>
        
        <category>GrabMaps</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Graph modelling guidelines</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Graph modelling is a highly effective technique for representing and analysing complex and interconnected data across various domains. By deciphering relationships between entities, graph modelling can reveal insights that might be otherwise difficult to identify using traditional data modelling approaches. In this article, we will explore what graph modelling is and guide you through a step-by-step process of implementing graph modelling to create a social network graph.&lt;/p&gt;

&lt;h2 id=&quot;what-is-graph-modelling&quot;&gt;What is graph modelling?&lt;/h2&gt;

&lt;p&gt;Graph modelling is a method for representing real-world entities and their relationships using nodes, edges, and properties. It employs graph theory, a branch of mathematics that studies graphs, to visualise and analyse the structure and patterns within complex datasets. Common applications of graph modelling include social network analysis, recommendation systems, and biological networks.&lt;/p&gt;

&lt;h2 id=&quot;graph-modelling-process&quot;&gt;Graph modelling process&lt;/h2&gt;

&lt;h3 id=&quot;step-1-define-your-domain&quot;&gt;Step 1: Define your domain&lt;/h3&gt;
&lt;p&gt;Before diving into graph modelling, it’s crucial to have a clear understanding of the domain you’re working with. This involves getting acquainted with the relevant terms, concepts, and relationships that exist in your specific field. To create a social network graph, familiarise yourself with terms like users, friendships, posts, likes, and comments.&lt;/p&gt;

&lt;h3 id=&quot;step-2-identify-entities-and-relationships&quot;&gt;Step 2: Identify entities and relationships&lt;/h3&gt;
&lt;p&gt;After defining your domain, you need to determine the entities (nodes) and relationships (edges) that exist within it. Entities are the primary objects in your domain, while relationships represent how these entities interact with each other. In a social network graph, users are entities, and friendships are relationships.&lt;/p&gt;

&lt;h3 id=&quot;step-3-establish-properties&quot;&gt;Step 3: Establish properties&lt;/h3&gt;
&lt;p&gt;Each entity and relationship may have a set of properties that provide additional information. In this step, identify relevant properties based on their significance to the domain. A user entity might have properties like name, age, and location. A friendship relationship could have a ‘since’ property to denote the establishment of the friendship.&lt;/p&gt;

&lt;h3 id=&quot;step-4-choose-a-graph-model&quot;&gt;Step 4: Choose a graph model&lt;/h3&gt;
&lt;p&gt;Once you’ve identified the entities, relationships, and properties, it’s time to choose a suitable graph model. Two common models are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Property graph&lt;/strong&gt;: A versatile model that easily accommodates properties on both nodes and edges. It’s well-suited for most applications.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Description Framework (RDF)&lt;/strong&gt;: A World Wide Web Consortium (W3C) standard model, using triples of subject-predicate-object to represent data. It is commonly used in semantic web applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a social network graph, a property graph model is typically suitable. This is because user entities have many attributes and features. Property graphs provide a clear representation of the relationships between people and their attribute profiles.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/graph-modelling-guidelines/graph.png&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;Figure 1 - Social network graph&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;step-5-develop-a-schema&quot;&gt;Step 5: Develop a schema&lt;/h3&gt;
&lt;p&gt;Although not required, developing a schema can be helpful for large-scale projects and team collaborations. A schema defines the structure of your graph, including entity types, relationships, and properties. In a social network graph, you might have a schema that specifies the types of nodes (users, posts) and the relationships between them (friendships, likes, comments).&lt;/p&gt;

&lt;h3 id=&quot;step-6-import-or-generate-data&quot;&gt;Step 6: Import or generate data&lt;/h3&gt;
&lt;p&gt;Next, acquire the data needed to populate your graph. This can come in the form of existing datasets or generated data from your application. For a social network graph, you can import user information from a CSV file and generate simulated friendships, posts, likes, and comments.&lt;/p&gt;

&lt;h3 id=&quot;step-7-implement-the-graph-using-a-graph-database-or-other-storage-options&quot;&gt;Step 7: Implement the graph using a graph database or other storage options&lt;/h3&gt;
&lt;p&gt;Finally, you need to store your graph data using a suitable graph database. Neo4j, Amazon Neptune, or Microsoft Azure Cosmos DB are examples of graph databases. Alternatively, depending on your specific requirements, you can use a non-graph database or an in-memory data structure to store the graph.&lt;/p&gt;

&lt;h3 id=&quot;step-8-analyse-and-visualise-the-graph&quot;&gt;Step 8: Analyse and visualise the graph&lt;/h3&gt;
&lt;p&gt;After implementing the graph, you can perform various analyses using graph algorithms, such as shortest path, centrality, or community detection. In addition, visualising your graph can help you gain insights and facilitate communication with others.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By following these steps, you can effectively create and analyse graph models for your specific domain. Remember to adjust the steps according to your unique domain and requirements, and always ensure that confidential and sensitive data is properly protected.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://neo4j.com/developer/graph-database/&quot;&gt;What is a Graph Database?&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 08 Nov 2023 00:00:10 +0000</pubDate>
        <link>https://engineering.grab.com/graph-modelling-guidelines</link>
        <guid isPermaLink="true">https://engineering.grab.com/graph-modelling-guidelines</guid>
        
        <category>Graph technology</category>
        
        <category>Graphs</category>
        
        <category>Graph networks</category>
        
        <category>Security</category>
        
        <category>Data</category>
        
        
        <category>Engineering</category>
        
        <category>Security</category>
        
      </item>
    
  </channel>
</rss>
