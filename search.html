<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Grab Tech</title>
    <meta name="description" content="Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Open Graph -->
    <meta property="og:url" content="https://engineering.grab.com/search">
    <meta property="og:title" content="Grab Tech">
    <meta property="og:description" content="Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
">
    <meta property="og:site_name" content="Grab Tech">
    <meta property="og:type" content="article">
    <meta property="og:image" content="https://engineering.grab.com/img/banner.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">

    <!-- Favicons -->
    <link rel="icon" href="/favicon.ico">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">

    <!-- CSS -->
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,400i,700,700i" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap-theme.min.css">
    <script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="https://engineering.grab.com/search">

    <!-- RSS -->
    <link rel="alternate" type="application/rss+xml" title="RSS for Official Grab Tech Blog" href="/feed.xml">
    <!-- OneTrust Cookies Consent Notice start for grab.com -->
    <script type="text/javascript" src="https://cdn-apac.onetrust.com/consent/a3be3527-7455-48e0-ace6-557ddbd506d5/OtAutoBlock.js" ></script>
    <script src="https://cdn-apac.onetrust.com/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="a3be3527-7455-48e0-ace6-557ddbd506d5" ></script>
    <script type="text/javascript">
    function OptanonWrapper() { }
    </script>
    <!-- OneTrust Cookies Consent Notice end for grab.com -->
  </head>

  <body>
    <header class="site-header">
  <div class="wrapper-navbar">
    <div class="site-title-wrapper">
      <div class="row site-title-wrapper-inner">
        <div class="col-xs-1 visible-xs hamburger-nav" id="mobile-menu-btn">
          <div class="menu-btn"></div>
        </div>
        <div class="col-sm-3 col-xs-3">
          <div class="site-title-container">
            <a class="site-title" href="/"></a>
            <span class="site-subtitle">&nbsp;Tech Blog</span>
          </div>
        </div>
        <div class="col-sm-9 col-xs-8 text-right">
          <ul class="nav-category hidden-xs">
            
              
              <li>
                <a href="/categories/engineering/">Engineering</a>
              </li>
            
              
              <li>
                <a href="/categories/data-science/">Data Science</a>
              </li>
            
              
              <li>
                <a href="/categories/design/">Design</a>
              </li>
            
              
              <li>
                <a href="/categories/product/">Product</a>
              </li>
            
              
              <li>
                <a href="/categories/security/">Security</a>
              </li>
            
          </ul>
          <div class="site-search text-right">
            <form action="/search.html" role="search" class="search-form">
  <div class="search-icon"> </div>
  <input type="search" name="q" class="search-text" placeholder="Search...">
  <button class="search-submit"><i class="fa fa-chevron-right"></i></button>
</form>
    

          </div>
        </div>
      </div>
      <!--  Only visible on mobile view -->
      <div class="mobile-menu-container">
        <ul class="mobile-menu">
          
            
            <li>
              <a href="/categories/engineering/">Engineering</a>
            </li>
          
            
            <li>
              <a href="/categories/data-science/">Data Science</a>
            </li>
          
            
            <li>
              <a href="/categories/design/">Design</a>
            </li>
          
            
            <li>
              <a href="/categories/product/">Product</a>
            </li>
          
            
            <li>
              <a href="/categories/security/">Security</a>
            </li>
          
        </ul>
      </div>
    </div>
  </div>
</header>
<script src="/js/main.js"></script>

    <div class="page-content">
      <div class="wrapper">
  <h1 class="page-heading">Search Results</h1>
  <ul class="posts-summary-posts-list posts-search-results" id="search-results"></ul>
</div>
<script>
  window.store = {
    
      "frequency-capping": {
        "title": "Sliding window rate limits in distributed systems",
        "author": "naveen-kumarabdullah-mamun",
        "tags": "[&quot;Data&quot;, &quot;Big data&quot;, &quot;Rate limiting&quot;, &quot;Frequency capping&quot;, &quot;Distributed systems&quot;]",
        "category": "",
        "content": "Like many other companies, Grab uses marketing communications to notify users of promotions or other news. If a user receives these notifications from multiple companies, it would be a form of information overload and they might even start considering these communications as spam. Over time, this could lead to some users revoking their consent to receive marketing communications altogether. Hence, it is important to find a rate-limited solution that sends the right amount of communications to our users.BackgroundIn Grab, marketing emails and push notifications are part of carefully designed campaigns to ensure that users get the right notifications (i.e. based on past orders or usage patterns). Trident is Grab’s in-house tool to compose these campaigns so that they run efficiently at scale. An example of a campaign is scheduling a marketing email blast to 10 million users at 4 pm. Read more about Trident’s architecture here.Trident relies on Hedwig, another in-house service, to deliver the messages to users. Hedwig does the heavy lifting of delivering large amounts of emails and push notifications to users while maintaining a high query per second (QPS) rate and minimal delay. The following high-level architectural illustration demonstrates the interaction between Trident and Hedwig.  Diagram of data interaction between Trident and Hedwig  The aim is to regulate the number of marketing comms sent to users daily and weekly, tailored based on their interaction patterns with the Grab superapp.SolutionBased on their interaction patterns with our superapp, we have clustered users into a few segments.For example:New: Users recently signed up to the Grab app but haven’t taken any rides yet.Active: Users who took rides in the past month.With these metrics, we came up with optimal daily and weekly frequency limit values for each clustered user segment. The solution discussed in this article ensures that the comms sent to a user do not exceed the daily and weekly thresholds for the segment. This is also called frequency capping.However, frequency capping can be split into two sub-problems:Efficient storage of clustered user dataWith a huge customer base of over 270 million users, storing the user segment membership information has to be cost-efficient and memory-sleek. Querying the segment to which a user belongs should also have minimal latency.Persistent tracking of comms sent per userTo stay within the daily and weekly thresholds, we need to actively track the number of comms sent to each user, which can be referred to make rate limiting decisions. The rate limiting logic should also have minimal latency, be cost efficient, and not take up too much memory storage.Optimising storage of user segment dataThe problem here is figuring out which segment a particular user belongs to and ensuring that the user doesn’t appear in more than one segment. There are two options that suit our needs and we’ll explain more about each option, as well as what was the best option for us.Bloom filter A Bloom filter is a space-efficient probabilistic data structure that addresses this problem well. Simply put, Bloom filters internally use arrays to track memberships of the elements.For our scenario, each user segment would need its own bloom filter. We used this bloom filter calculator to estimate the memory required for each bloom filter. We found that we needed approximately 1 GB of memory and 23 hash functions to accurately represent the membership information of 270 million users in an array. Additionally, this method guarantees a false positive rate of  1.0E-7, which means 1 in 1 million elements may get wrong membership results because of hash collision.With Grab’s existing segments, this approach needs 4GB of memory, which may increase as we increase the number of segments in the future. Moreover, the potential hash collision needs to be handled by increasing the memory size with even more hash functions. Another thing to note is that Bloom filters do not support deletion so every time a change needs to be done, you need to create a new version of the Bloom filter. Although Bloom filters have many advantages, these shortcomings led us to explore another approach.Roaring bitmaps Roaring bitmaps are sets of unsigned integers consisting of containers of disjoint subsets, which can store large amounts of data in a compressed form. Essentially, roaring bitmaps could reduce memory storage significantly and overcome the hash collision problem. To understand the intuition behind this, first, we need to know how bitmaps work and the possible drawbacks behind it.To represent a list of numbers as a bitmap, we first need to create an array with a size equivalent to the largest element in the list. For every element in the list, we then mark the bit value as 1 in the corresponding index in the array. While bitmaps work very well for storing integers in closer intervals, they occupy more space and become sparse when storing integer ranges with uneven distribution, as shown in the image below.  Diagram of bitmaps with uneven distribution  To reduce memory footprint and improve the performance of bitmaps, there are compression techniques such as Run-Length Encoding (RLE), and Word Aligned Hybrid (WAH). However, this would require additional effort to implement, whereas using roaring bitmaps would solve these issues.Roaring bitmaps’ hybrid data storage approach offers the following advantages:  Faster set operations (union, intersection, differencing).  Better compression ratio when handling mixed datasets (both dense and sparse data distribution).  Ability to scale to large datasets without significant performance loss.To summarise, roaring bitmaps can store positive integers from 0 to (2^32)-1. Each positive integer value is converted to a 32-bit binary, where the 16 Most Significant Bits (MSB) are used as the key and the remaining 16 Least Significant Bits (LSB) are represented as the value. The values are then stored in an array, a bitmap, or used to run containers with RLE encoding data structures.If the number of integers mapped to the key is less than 4096, then all the integers are stored in an array in sorted order and converted into a bitmap container in the runtime as the size exceeds. Roaring bitmap analyses the distribution of set bits in the bitmap container i.e. if the continuous interval of set bits is more than a given threshold, the bitmap container can be more efficiently represented using the RLE container. Internally, the RLE container uses an array where the even indices store the beginning of the runs and the odd indices represent the length of the runs. This enables the roaring bitmap to dynamically switch between the containers to optimise storage and performance.The following diagram shows how a set of elements with different distributions are stored in roaring bitmaps.  Diagram of how roaring bitmaps store elements with different distributions   In Grab, we developed a microservice that abstracts roaring bitmaps implementations and provides an API to check set membership and enumeration of elements in the sets. Check out this blog to learn more about it.Distributed rate limitingThe second part of the problem involves rate limiting the number of communication messages sent to users on a daily or weekly basis and each segment has specific daily and weekly limits. By utilising roaring bitmaps, we can determine the segment to which a user belongs. After identifying the appropriate segment, we will apply the personalised limits to the user using a distributed rate limiter, which will be discussed in further detail in the following sections.Choosing the right datastoreBased on our use case, Amazon ElasticCache for Redis and DynamoDB were two viable options for storing the sent communication messages count per user. However, we decided to choose Redis due to a number of factors:  Higher throughput at lower latency – Redis shards data across nodes in the cluster.  Cost-effective – Usage of Lua script reduces unnecessary data transfer overheads.  Better at handling spiky rate limiting workloads at scale.Distributed rate limiterTo appropriately limit the comms our users receive, we needed a rate limiting algorithm, which could execute directly in the datastore cluster, then return the results in the application logic for further processing. The two rate limiting algorithms we considered were the sliding window rate limiter and sliding log rate limiter.The sliding window rate limiter algorithm divides time into a fixed-size window (we defined this as 1 minute) and counts the number of requests within each window. On the other hand, the sliding log maintains a log of each request timestamp and counts the number of requests between two timestamp ranges, providing a more fine-grained method of rate limiting. Although sliding log consumes more memory to store the log of request timestamp, we opted for the sliding log approach as the accuracy of the rate limiting was more important than memory consumption.The sliding log rate limiter utilises a Redis sorted set data structure to efficiently track and organise request logs. Each timestamp in milliseconds is stored as a unique member in the set. The score assigned to each member represents the corresponding timestamp, allowing for easy sorting in ascending order. This design choice optimises the speed of search operations when querying for the total request count within specific time ranges.Sliding Log Rate limiter Algorithm:Input:  # user specific redis key where the request timestamp logs are stored as sorted set  keys =&gt; user_redis_key  # limit_value is the limit that needs to be applied for the user  # start_time_in_millis is the starting point of the time window  # end_time_in_millis is the ending point of the time window  # current_time_in_millis is the current time the request is sent  # eviction_time_in_millis, members in the set whose value is less than this will be evicted from the set  args =&gt; limit_value, start_time_in_millis, end_time_in_millis, current_time_in_millis, eviction_time_in_millisOutput:  # 0 means not_allowed and 1 means allowed  response =&gt; 0 / 1Logic:  # zcount fetches the count of the request timestamp logs falling between the start and the end timestamp  request_count = zcount user_redis_key start_time_in_millis end_time_in_millis  response = 0  # if the count of request logs is less than allowed limits then record the usage by adding current timestamp in sorted set  if request_count &lt; limit_value then    zadd user_redis_key current_time_in_millis current_time_in_millis    response = 1  # zremrangebyscore removes the members in the sorted set whose score is less than eviction_time_in_millis  zremrangebyscore user_redis_key -inf eviction_time_in_millis  return responseThis algorithm takes O(log n) time complexity, where n is the number of request logs stored in the sorted set. It is not possible to evict entries in the sorted set like how we have time-to-live (TTL) for Redis keys. To prevent the size of the sorted set from increasing over time, we have a fixed variable eviction_time_in_millis that is passed to the script. The zremrangebyscore command then deletes members from the sorted set whose score is less than eviction_time_in_millis in O(log n) time complexity.Lua script optimisationsIn Redis Cluster mode, all Redis keys accessed by a Lua script must be present on the same node, and they should be passed as part of the KEYS input array of the script. If the script attempts to access keys located on different nodes within the cluster, a CROSSSLOT error will be thrown. Redis keys, or userIDs, are distributed across multiple nodes in the cluster so it is not feasible to send a batch of userIDs within the same Lua script for rate limiting, as this might result in a CROSSSLOT error.Invoking a separate Lua script call for each user is a possible approach, but it incurs a significant number of network calls, which can be optimised further with the following approach:  Upload the Lua script into the Redis server during the server startup with the SCRIPT LOAD command and we get the SHA1 hash of the script if the upload is successful.  The SHA1 hash can then be used to invoke the Lua script with the EVALSHA command passing the keys and arguments as script input.  Redis pipelining takes in multiple EVALSHA commands that call the Lua script and each invocation corresponds to a userID for getting the rate limiting result.  Redis pipelining groups the EVALSHA Redis commands with Redis keys located on the same nodes internally. It then sends the grouped commands in a single network call to the relevant nodes within the Redis cluster and provides the rate limiting outcome to the client.Since Redis operates on a single thread, any long-running Lua script can cause other Redis commands to be blocked until the script completes execution. Thus, it’s optimal for the Lua script to execute in under 5 milliseconds. Additionally, the current time is passed as an argument to the script to account for potential variations in time when the script is executed on a node’s replica, which could be caused by clock drift.By bringing together roaring bitmaps and the distributed rate limiter, this is what our final solution looks like:  Our final solution using roaring bitmaps and distributed rate limiter  The roaring bitmaps structure is serialised and stored in an AWS S3 bucket, which is then downloaded in the instance during server startup. After which, triggering a user segment membership check can simply be done with a local method call. The configuration service manages the mapping information between the segment and allowed rate limiting values.Whenever a marketing message needs to be sent to a user, we first find the segment to which the user belongs, retrieve the defined rate limiting values from the configuration service, then execute the Lua script to get the rate limiting decision. If there is enough quota available for the user, we send the comms.The architecture of the messaging service looks something like this:  Architecture of the messaging service  ImpactIn addition to decreasing the unsubscription rate, there was a significant enhancement in the latency of sending communications. Eliminating redundant communications also alleviated the system load, resulting in a reduction of the delay between the scheduled time and the actual send time of comms.ConclusionApplying rate limiters to safeguard our services is not only a standard practice but also a necessary process. Many times, this can be achieved by configuring the rate limiters at the instance level. The need for rate limiters for business logic may not be as common, but when you need it, the solution must be lightning-fast, and capable of seamlessly operating within a distributed environment.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/frequency-capping"
      }
      ,
    
      "an-elegant-platform": {
        "title": "An elegant platform",
        "author": "fabrice-harbulotminhkhoi-nguyen",
        "tags": "[&quot;Data&quot;, &quot;Data streaming&quot;, &quot;Real-time streaming&quot;, &quot;Platformisation&quot;]",
        "category": "",
        "content": "Coban is Grab’s real-time data streaming platform team. As a platform team, we thrive on providing our internal users from all verticals with self-served data-streaming resources, such as Kafka topics, Flink and Change Data Capture (CDC) pipelines, various kinds of Kafka-Connect connectors, as well as Apache Zeppelin notebooks, so that they can effortlessly leverage real-time data to build intelligent applications and services.In this article, we present our journey from pure Infrastructure-as-Code (IaC) towards a more sophisticated control plane that has revolutionised the way data streaming resources are self-served at Grab. This change also leads to improved scalability, stability, security, and user adoption of our data streaming platform.Problem statementIn the early ages of public cloud, it was a common practice to create virtual resources by clicking through the web console of a cloud provider, which is sometimes referred to as ClickOps.ClickOps has many downsides, such as:  Inability to review, track, and audit changes to the infrastructure.  Inability to massively scale the infrastructure operations.  Inconsistencies between environments, e.g. staging and production.  Inability to quickly recover from a disaster by re-creating the infrastructure at a different location.That said, ClickOps has one tremendous advantage; it makes creating resources using a graphical User Interface (UI) fairly easy for anyone like Infrastructure Engineers, Software Engineers, Data Engineers etc. This also leads to a high iteration speed towards innovation in general.IaC resolved many of the limitations of ClickOps, such as:  Changes are committed to a Version Control System (VCS) like Git: They can be reviewed by peers before being merged. The full history of all changes is available for investigating issues and for audit.  The infrastructure operations scale better: Code for similar pieces of infrastructure can be modularised. Changes can be rolled out automatically by Continuous Integration (CI) pipelines in the VCS system, when a change is merged to the main branch.  The same code can be used to deploy the staging and production environments consistently.  The infrastructure can be re-created anytime from its source code, in case of a disaster.However, IaC unwittingly posed a new entry barrier too, requiring the learning of new tools like Ansible, Puppet, Chef, Terraform, etc.Some organisations set up dedicated Site Reliability Engineer (SRE) teams to centrally manage, operate, and support those tools and the infrastructure as a whole, but that soon created the potential of new bottlenecks in the path to innovation.On the other hand, others let engineering teams manage their own infrastructure, and Grab adopted that same approach. We use Terraform to manage infrastructure, and all teams are expected to have select engineers who have received Terraform training and have a clear understanding of it.In this context, Coban’s platform initially started as a handful of Git repositories where users had to submit their Merge Requests (MR) of Terraform code to create their data streaming resources. Once reviewed by a Coban engineer, those Terraform changes would be applied by a CI pipeline running Atlantis.While this was a meaningful first step towards self-service and platformisation of Coban’s offering within Grab, it had several significant downsides:  Stability: Due to the lack of control on the Terraform changes, the CI pipeline was prone to human errors and frequent failures. For example, users would initiate a new Terraform project by duplicating an existing one, but then would forget to change the location of the remote Terraform state, leading to the in-place replacement of an existing resource.  Scalability: The Coban team needed to review all MRs and provide ad hoc support whenever the pipeline failed.  Security: In the absence of Identity and Access Management (IAM), MRs could potentially contain changes pertaining to other teams’ resources, or even changes to Coban’s core infrastructure, with code review as the only guardrail.  Limited user growth: We could only acquire users who were well-versed in Terraform.It soon became clear that we needed to build a layer of abstraction between our users and the Terraform code, to increase the level of control and lower the entry barrier to our platform, while still retaining all of the benefits of IaC under the hood.SolutionWe designed and built an in-house three-tier control plane made of:  Coban UI, a front-end web interface, providing our users with a seamless ClickOps experience.  Heimdall, the Go back-end of the web interface, transforming ClickOps into IaC.  Khone, the storage and provisioner layer, a Git repository storing Terraform code and metadata of all resources as well as the CI pipelines to plan and apply the changes.In the next sections, we will deep dive in those three components.  Fig. 1 Simplified architecture of a request flowing from the user to the Coban infrastructure, via the three components of the control plane: the Coban UI, Heimdall, and Khone.  Although we designed the user journey to start from the Coban UI, our users can still opt to communicate with Heimdall and with Khone directly, e.g. for batch changes, or just because many engineers love Git and we want to encourage broad adoption. To make sure that data is eventually consistent across the three systems, we made Khone the only persistent storage layer. Heimdall regularly fetches data from Khone, caches it, and presents it to the Coban UI upon each query.We also continued using Terraform for all resources, instead of mixing various declarative infrastructure approaches (e.g. Kubernetes Custom Resource Definition, Helm charts), for the sake of consistency of the logic in Khone’s CI pipelines.Coban UIThe Coban UI is a React Single Page Application (React SPA) designed by our partner team Chroma, a dedicated team of front-end engineers who thrive on building legendary UIs and reusable components for platform teams at Grab.It serves as a comprehensive self-service portal, enabling users to effortlessly create data streaming resources by filling out web forms with just a few clicks.  Fig. 2 Screen capture of a new Kafka topic creation in the Coban UI.  In addition to facilitating resource creation and configuration, the Coban UI is seamlessly integrated with multiple monitoring systems. This integration allows for real-time monitoring of critical metrics and health status for Coban infrastructure components, including Kafka clusters, Kafka topic bytes in/out rates, and more. Under the hood, all this information is exposed by Heimdall APIs.  Fig. 3 Screen capture of the metrics of a Kafka cluster in the Coban UI.  In terms of infrastructure, the Coban UI is hosted in AWS S3 website hosting. All dynamic content is generated by querying the APIs of the back-end: Heimdall.HeimdallHeimdall is the Go back-end of the Coban UI. It serves a collection of APIs for:  Managing the data streaming resources of the Coban platform with Create, Read, Update and Delete (CRUD) operations, treating the Coban UI as a first-class citizen.  Exposing the metadata of all Coban resources, so that they can be used by other platforms or searched in the Coban UI.All operations are authenticated and authorised. Read more about Heimdall’s access control in Migrating from Role to Attribute-based Access Control.In the next sections, we are going to dive deeper into these two features.Managing the data streaming resourcesFirst and foremost, Heimdall enables our users to self-manage their data streaming resources. It primarily relies on Khone as its storage and provisioner layer for actual resource management via Git CI pipelines. Therefore, we designed Heimdall’s resource management workflow to leverage the underlying Git flow.  Fig. 4 Diagram flow of a request in Heimdall.  Fig. 4 shows the diagram flow of a typical request in Heimdall to create, update, or delete a resource.  An authenticated user initiates a request, either by navigating in the Coban UI or by calling the Heimdall API directly. At this stage, the request state is Initiated on Heimdall.  Heimdall validates the request against multiple validation rules. For example, if an ongoing change request exists for the same resource, the request fails. If all tests succeed, the request state moves to Ongoing.  Heimdall then creates an MR in Khone, which contains the Terraform files describing the desired state of the resource, as well as an in-house metadata file describing the key attributes of both resource and requester.  After the MR has been created successfully, Heimdall notifies the requester via Slack and shares the MR URL.  After that, Heimdall starts polling the status of the MR in a loop.  For changes pertaining to production resources, an approver who is code owner in the repository of the resource has to approve the MR. Typically, the approver is an immediate teammate of the requester. Indeed, as a platform team, we empower our users to manage their own resources in a self-service fashion. Ultimately, the requester would merge the MR to trigger the CI pipeline applying the actual Terraform changes. Note that for staging resources, this entire step 6 is automatically performed by Heimdall.  Depending on the MR status and the status of its CI pipeline in Khone, the final state of the request can be:          Failed if the CI pipeline has failed in Khone.      Completed if the CI pipeline has succeeded in Khone.      Cancelled if the MR was closed in Khone.      Heimdall exposes APIs to let users track the status of their requests. In the Coban UI, a page queries those APIs to elegantly display the requests.  Fig. 5 Screen capture of the Coban UI showing all requests.  Exposing the metadataApart from managing the data streaming resources, Heimdall also centralises and exposes the metadata pertaining to those resources so other Grab systems can fetch and use it. They can make various queries, for example, listing the producers and consumers of a given Kafka topic, or determining if a database (DB) is the data source for any CDC pipeline.To make this happen, Heimdall not only retains the metadata of all of the resources that it creates, but also regularly ingests additional information from a variety of upstream systems and platforms, to enrich and make this metadata comprehensive.  Fig. 6 Diagram showing some of Heimdall's upstreams (on the left) and downstreams (on the right) for metadata collection, enrichment, and serving. The arrows show the data flow. The network connection (client -&gt; server) is actually the other way around.  On the left side of Fig. 6, we illustrate Heimdall’s ingestion mechanism with several examples (step 1):  The metadata of all Coban resources is ingested from Khone. This means the metadata of the resources that were created directly in Khone is also available in Heimdall.  The list of Kafka producers is retrieved from our monitoring platform, where most of them emit metrics.  The list of Kafka consumers is retrieved directly from the respective Kafka clusters, by listing the consumer groups and respective Client IDs of each partition.  The metadata of all DBs, that are used as a data source for CDC pipelines, is fetched from Grab’s internal DB management platform.  The Kafka stream schemas are retrieved from the Coban schema repository.  The Kafka stream configuration of each stream is retrieved from Grab Universal Configuration Management platform.With all of this ingested data, Heimdall can provide comprehensive and accurate information about all data streaming resources to any other Grab platforms via a set of dedicated APIs.The right side of Fig. 6 shows some examples (step 2) of Heimdall’s serving mechanism:  As a downstream of Heimdall, the Coban UI enables our direct users to conveniently browse their data streaming resources and access their attributes.  The entire resource inventory is ingested into the broader Grab inventory platform, based on backstage.io.  The Kafka streams are ingested into Grab’s internal data discovery platform, based on DataHub, where users can discover and trace the lineage of any piece of data.  The CDC connectors pertaining to DBs are ingested by Grab internal DB management platform, so that they are made visible in that platform when users are browsing their DBs.Note that the downstream platforms that ingest data from Heimdall each expose a particular view of the Coban inventory that serves their purpose, but the Coban platform remains the only source of truth for any data streaming resource at Grab.Lastly, Heimdall leverages an internal MySQL DB to support quick data query and exploration. The corresponding API is called by the Coban UI to let our users conveniently search globally among all resources’ attributes.  Fig. 7 Screen capture of the global search feature in the Coban UI.  KhoneKhone is the persistent storage layer of our platform, as well as the executor for actual resource creation, changes, and deletion. Under the hood, it is actually a GitLab repository of Terraform code in typical GitOps fashion, with CI pipelines to plan and apply the Terraform changes automatically. In addition, it also stores a metadata file for each resource.Compared to letting the platform create the infrastructure directly and keep track of the desired state in its own way, relying on a standard IaC tool like Terraform for the actual changes to the infrastructure presents two major advantages:  The Terraform code can directly be used for disaster recovery. In case of a disaster, any entitled Cobaner with a local copy of the main branch of the Khone repository is able to recreate all our platform resources directly from their machine. There is no need to rebuild the entire platform’s control plane, thus reducing our Recovery Time Objective (RTO).  Minimal effort required to follow the API changes of our infrastructure ecosystem (AWS, Kubernetes, Kafka, etc.). When such a change happens, all we need to do is to update the corresponding Terraform provider.If you’d like to read more about Khone, check out Securing GitOps pipelines. In this section, we will only focus on Khone’s features that are relevant from the platform perspective.Lightweight TerraformIn Khone, each resource is stored as a Terraform definition. There are two major differences from a normal Terraform project:  No Terraform environment, such as the required Terraform providers and the location of the remote Terraform state file. They are automatically generated by the CI pipeline via a simple wrapper.  Only vetted Khone Terraform modules can be used. This is controlled and enforced by the CI pipeline via code inspection. There is one such Terraform module for each kind of supported resource of our platform (e.g. Kafka topic, Flink pipeline, Kafka Connect mirror source connector etc.). Furthermore, those in-house Terraform modules are designed to automatically derive their key variables (e.g. resource name, cluster name, environment) from the relative path of the parent Terraform project in the Khone repository.Those characteristics are designed to limit the risk and blast radius of human errors. They also make sure that all resources created in Khone are supported by our platform, so that they can also be discovered and managed in Heimdall and the Coban UI. Lastly, by generating the Terraform environment on the fly, we can destroy resources simply by deleting the directory of the project in the code base – this would not be possible otherwise.Resource metadataAll resource metadata is stored in a YAML file that is present in the Terraform directory of each resource in the Khone repository. This is mainly used for ownership and cost attribution.With this metadata, we can:  Better communicate with our users whenever their resources are impacted by an incident or an upcoming maintenance operation.  Help teams understand the costs of their usage of our platform, a significant step towards cost efficiency.There are two different ways resource metadata can be created:  Automatically through Heimdall: The YAML metadata file is automatically generated by Heimdall.  Through Khone by a human user: The user needs to prepare the YAML metadata file and include it in the MR. This file is then verified by the CI pipeline.OutcomeThe initial version of the three-tier Coban platform, as described in this article, was internally released in March 2022, supporting only Kafka topic management at the time. Since then, we have added support for Flink pipelines, four kinds of Kafka Connect connectors, CDC pipelines, and more recently, Apache Zeppelin notebooks. At the time of writing, the Coban platform manages about 5000 data streaming resources, all described as IaC under the hood.Our platform also exposes enriched metadata that includes the full data lineage from Kafka producers to Kafka consumers, as well as ownership information, and cost attribution.With that, our monthly active users have almost quadrupled, truly moving the needle towards democratising the usage of real-time data within all Grab verticals.In spite of that user growth, the end-to-end workflow success rate for self-served resource creation, change or deletion, remained well above 90% in the first half of 2023, while the Heimdall API uptime was above 99.95%.Challenges facedA common challenge for platform teams resides in the misalignment between the Service Level Objective (SLO) of the platform, and the various environments (e.g. staging, production) of the managed resources and upstream/downstream systems and platforms.Indeed, the platform aims to guarantee the same level of service, regardless of whether it is used to create resources in the staging or the production environment. From the platform team’s perspective, the platform as a whole is considered production-grade, as soon as it serves actual users.A naive approach to address this challenge is to let the production version of the platform manage all resources regardless of their respective environments. However, doing so does not permit a hermetic segregation of the staging and production environments across the organisation, which is a good security practice, and often a requirement for compliance. For example, the production version of the platform would have to connect to upstream systems in the staging environment, e.g. staging Kafka clusters to collect their consumer groups, in the case of Heimdall. Conversely, the staging version of certain downstreams would have to connect to the production version of Heimdall, to fetch the metadata of relevant staging resources.The alternative approach, generally adopted across Grab, is to instantiate all platforms in each environment (staging and production), while still considering both instances as production-grade and guaranteeing tight SLOs in both environments.  Fig. 8 Architecture of the Coban platform, broken down by environment.  In Fig. 8, both instances of Heimdall have equivalent SLOs. The caveat is that all upstream systems and platforms must also guarantee a strict SLO in both environments. This obviously comes with a cost, for example, tighter maintenance windows for the operations pertaining to the Kafka clusters in the staging environment.A strong “platform” culture is required for platform teams to fully understand that their instance residing in the staging environment is not their own staging environment and should not be used for testing new features.What’s next?Currently, users creating, updating, or deleting production resources in the Coban UI (or directly by calling Heimdall API) receive the URL of the generated GitLab MR in a Slack message. From there, they must get the MR approved by a code owner, typically another team member, and finally merge the MR, for the requested change to be actually implemented by the CI pipeline.Although this was a fairly easy way to implement a maker/checker process that was immediately compliant with our regulatory requirements for any changes in production, the user experience is not optimal. In the near future, we plan to bring the approval mechanism into Heimdall and the Coban UI, while still providing our more advanced users with the option to directly create, approve, and merge MRs in GitLab. In the longer run, we would also like to enhance the Coban UI with the output of the Khone CI jobs that include the Terraform plan and apply results.There is another aspect of the platform that we want to improve. As Heimdall regularly polls the upstream platforms to collect their metadata, this introduces a latency between a change in one of those platforms and its reflection in the Coban platform, which can hinder the user experience. To refresh resource metadata in Heimdall in near real time, we plan to leverage an existing Grab-wide event stream, where most of the configuration and code changes at Grab are produced as events. Heimdall will soon be able to consume those events and update the metadata of the affected resources immediately, without waiting for the next periodic refresh.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/an-elegant-platform"
      }
      ,
    
      "road-localisation-grabmaps": {
        "title": "Road localisation in GrabMaps",
        "author": "roxana-crisan",
        "tags": "[&quot;Maps&quot;, &quot;Data&quot;, &quot;Big Data&quot;, &quot;Data processing&quot;, &quot;Hyperlocalisation&quot;, &quot;GrabMaps&quot;]",
        "category": "",
        "content": "IntroductionIn 2022, Grab achieved self-sufficiency in its Geo services. As part of this transition, one crucial step was moving towards using an internally-developed map tailored specifically to the market in which Grab operates. Now that we have full control over the map layer, we can add more data to it or improve it according to the needs of the services running on top. One key aspect that this transition unlocked for us was the possibility of creating hyperlocal data at map level.For instance, by determining the country to which a road belongs, we can now automatically infer the official language of that country and display the street name in that language. In another example, knowing the country for a specific road, we can automatically infer the driving side (left-handed or right-handed) leading to an improved navigation experience. Furthermore, this capability also enables us to efficiently handle various scenarios. For example, if we know that a road is part of a gated community, an area where our driver partners face restricted access, we can prevent the transit through that area.These are just some examples of the possibilities from having full control over the map layer. By having an internal map, we can align our maps with specific markets and provide better experiences for our driver-partners and customers.BackgroundFor all these to be possible, we first needed to localise the roads inside the map. Our goal was to include hyperlocal data into the map, which refers to data that is specific to a certain area, such as a country, city, or even a smaller part of the city like a gated community. At the same time, we aimed to deliver our map with a high cadence, thus, we needed to find the right way to process this large amount of data while continuing to create maps in a cost-effective manner.SolutionIn the following sections of this article, we will use an extract from the Southeast Asia map to provide visual representations of the concepts discussed.In Figure 1, Image 1 shows a visualisation of the road network, the roads belonging to this area. The coloured lines in Image 2 represent the borders identifying the countries in the same area. Overlapping the information from Image 1 and Image 2, we can extrapolate and say that the entire surface included in a certain border could have the same set of common properties as shown in Image 3. In Image 4, we then proceed with adding localised roads for each area.  Figure 1 - Map of Southeast Asia  For this to be possible, we have to find a way to localise each road and identify its associated country. Once this localisation process is complete, we can replicate all this information specific to a given border onto each individual road. This information includes details such as the country name, driving side, and official language. We can go even further and infer more information, and add hyperlocal data. For example, in Vietnam, we can automatically prevent motorcycle access on the motorways.Assigning each road on the map to a specific area, such as a country, service area, or subdivision, presents a complex task. So, how can we efficiently accomplish this?ImplementationThe most straightforward approach would be to test the inclusion of each road into each area boundary, but that is easier said than done. With close to 30 million road segments in the Southeast Asia map and over 10 thousand areas, the computational cost of determining inclusion or intersection between a polyline and a polygon is expensive.Our solution to this challenge involves replacing the expensive yet precise operation with a decent approximation. We introduce a proxy entity, the geohash, and we use it to approximate the areas and also to localise the roads.We replace the geometrical inclusion with a series of simpler and less expensive operations. First, we conduct an inexpensive precomputation where we identify all the geohases that belong to a certain area or within a defined border. We then identify the geohashes to which the roads  belong to. Finally, we use these precomputed values to assign roads to their respective areas. This process is also computationally inexpensive.Given the large area we process, we leverage big data techniques to distribute the execution across multiple nodes and thus speed up the operation. We want to deliver the map daily and this is one of the many operations that are part of the map-making process.What is a geohash?To further understand our implementation we will first explain the geohash concept. A geohash is a unique identifier of a specific region on the Earth. The basic idea is that the Earth is divided into regions of user-defined size and each region is assigned a unique id, which is known as its geohash. For a given location on earth, the geohash algorithm converts its latitude and longitude into a string.Geohashes uses a Base-32 alphabet encoding system comprising characters ranging from  0 to 9 and A to Z, excluding “A”, “I”, “L” and “O”. Imagine dividing the world into a grid with 32 cells. The first character in a geohash identifies the initial location of one of these 32 cells. Each of these cells are then further subdivided into 32 smaller cells.This subdivision process continues and refines to specific areas in the world. Adding characters to the geohash sub-divides a cell, effectively zooming in to a more detailed area.The precision factor of the geohash determines the size of the cell. For instance, a precision factor of one creates a cell 5,000 km high and 5,000 km wide. A precision factor of six creates a cell 0.61km high and 1.22 km wide. Furthermore, a precision factor of nine creates a cell 4.77 m high and 4.77 m wide. It is important to note that cells are not always square and can have varying dimensions.In Figure 2,  we have exemplified a geohash 6 grid and its code is wsdt33.  Figure 2 - An example of geohash code wsdt33  Using less expensive operationsCalculating the inclusion of the roads inside a certain border is an expensive operation. However, quantifying the exact expense is challenging as it depends on several factors. One factor is the complexity of the border. Borders are usually irregular and very detailed, as they need to correctly reflect the actual border. The complexity of the road geometry is another factor that plays an important role as roads are not always straight lines.  Figure 3 - Roads to localise  Since this operation is expensive both in terms of cloud cost and time to run, we need to identify a cheaper and faster way that would yield similar results. Knowing that the complexity of the border lines is the cause of the problem, we tried using a different alternative, a rectangle. Calculating the inclusion of a polyline inside a rectangle is a cheaper operation.  Figure 4 - Roads inside a rectangle  So we transformed this large, one step operation, where we test each road segment for inclusion in a border, into a series of smaller operations where we perform the following steps:  Identify all the geohashes that are part of a certain area or belong to a certain border. In this process we include additional areas to make sure that we cover the entire surface inside the border.  For each road segment, we identify the list of geohashes that it belongs to. A road, depending on its length or depending on its shape, might belong to multiple geohashes.In Figure 5, we identify that the road belongs to two geohashes and that the two geohashes are part of the border we use.  Figure 5 - Geohashes as proxy  Now, all we need to do is join the two data sets together. This kind of operation is a great candidate for a big data approach, as it allows us to run it in parallel and speed up the processing time.Precision tradeoffWe mentioned earlier that, for the sake of argument, we replace precision with a decent approximation. Let’s now delve into the real tradeoff by adopting this approach.The first thing that stands out with this approach is that we traded precision for cost. We are able to reduce the cost as this approach uses less hardware resources and computation time. However, this reduction in precision suffers, particularly for roads located near the borders as they might be wrongly classified.Going back to the initial example, let’s take the case of the external road, on the left side of the area. As you can see in Figure 6, it is clear that the road does not belong to our border. But when we apply the geohash approach it gets included into the middle geohash.  Figure 6 - Wrong road localisation  Given that just a small part of the geohash falls inside the border, the entire geohash will be classified as belonging to that area, and, as a consequence, the road that belongs to that geohash will be wrongly localised and we’ll end up adding the wrong localisation information to that road. This is clearly a consequence of the precision tradeoff. So, how can we solve this?Geohash precisionOne option is to increase the geohash precision. By using smaller and smaller geohashes, we can better reflect the actual area. As we go deeper and we further split the geohash, we can accurately follow the border. However, a high geohash precision also equates to a computationally intensive operation bringing us back to our initial situation. Therefore, it is crucial to find the right balance between the geohash size and the complexity of operations.  Figure 7 - Geohash precision  Geohash coverage percentageTo find a balance between precision and data loss, we looked into calculating the geohash coverage percentage. For example, in Figure 8, the blue geohash is entirely within the border.  Here we can say that it has a 100% geohash coverage.  Figure 8 - Geohash inside the border  However, take for example the geohash in Figure 9. It touches the border and has only around 80% of its surface inside the area. Given that most of its surface is within the border, we still can say that it belongs to the area.  Figure 9 - Geohash partially inside the border  Let’s look at another example. In Figure 10, only a small part of the geohash is within the border. We can say that the geohash coverage percentage here is around 5%. For these cases, it becomes difficult for us to determine whether the geohash does belong to the area. What would be a good tradeoff in this case?  Figure 10 - Geohash barely inside the border  Border shapeTo go one step further, we can consider a mixed solution, where we use the border shape but only for the geohashes touching the border. This would still be an intensive computational operation but the number of roads located in these geohashes will be much smaller, so it is still a gain.For the geohashes with full coverage inside the area, we’ll use the geohash for the localisation, the simpler operation. For the geohashes that are near the border, we’ll use a different approach. To increase the precision around the borders, we can cut the geohash following the  border’s shape. Instead of having a rectangle, we’ll use a more complex shape which is still simpler than the initial border shape.  Figure 11 - Geohash following a border’s shape  ResultWe began with a simple approach and we enhanced it to improve precision. This also increased the complexity of the operation. We then asked, what are the actual gains? Was it worthwhile to go through all this process? In this section, we put this to the test.We first created a benchmark by taking a small sample of the data and ran the localisation process on a laptop. The sample comprised approximately 2% of the borders and 0.0014% of the roads. We ran the localisation process using two approaches.  With the first approach, we calculated the intersection between all the roads and borders. The entire operation took around 38 minutes.  For the second approach, we optimised the operation using geohashes. In this approach, the runtime was only 78 seconds (1.3 minutes).However, it is important to note that this is not an apples-to-apples comparison. The operation that we measured was the localisation of the roads but we did not include the border filling operation where we fill the borders with geohashes. This is because this operation does not need to be run every time. It can be run once and reused multiple times.Though not often required, it is still crucial to understand and consider the operation of precomputing areas and filling borders with geohashes. The precomputation process depends on several factors:  Number and shape of the borders - The more borders and the more complex the borders are, the longer the operation will take.  Geohash precision - How accurate do we need our localisation to be? The more accurate it needs to be, the longer it will take.  Hardware availabilityGoing back to our hypothesis, although this precomputation might be expensive, it is rarely run as the borders don’t change often and can be triggered only when needed. However, regular computation, where we find the area to which each road belongs to, is often run as the roads change constantly. In our system, we run this localisation for each map processing.We can also further optimise this process by applying the opposite approach. Geohashes that have full coverage inside a border can be merged together into larger geohashes thus simplifying the computation inside the border. In the end, we can have a solution that is fully optimised for our needs with the best cost-to-performance ratio.  Figure 12 - Optimised geohashes  ConclusionAlthough geohashes seem to be the right solution for this kind of problem, we also need to monitor their content. One consideration is the road density inside a geohash. For example, a geohash inside a city centre usually has a lot of roads while one in the countryside may have much less. We need to consider this aspect to have a balanced computation operation and take full advantage of the big data approach. In our case, we achieve this balance by considering the number of road kilometres within a geohash.  Figure 13 - Unbalanced data  Additionally, the resources that we choose also matter. To optimise time and cost, we need to find the right balance between the running time and resource cost. As shown in Figure 14, based on a sample data we ran, sometimes, we get the best result when using smaller machines.  Figure 14 - Cost vs runtime  The achievements and insights showcased in this article are indebted to the contributions made by Mihai Chintoanu. His expertise and collaborative efforts have profoundly enriched the content and findings presented herein.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/road-localisation-grabmaps"
      }
      ,
    
      "graph-modelling-guidelines": {
        "title": "Graph modelling guidelines",
        "author": "wenxiang-lumuqi-liwei-yang-wangwenhui-wu",
        "tags": "[&quot;Graph technology&quot;, &quot;Graphs&quot;, &quot;Graph networks&quot;, &quot;Security&quot;, &quot;Data&quot;]",
        "category": "",
        "content": "IntroductionGraph modelling is a highly effective technique for representing and analysing complex and interconnected data across various domains. By deciphering relationships between entities, graph modelling can reveal insights that might be otherwise difficult to identify using traditional data modelling approaches. In this article, we will explore what graph modelling is and guide you through a step-by-step process of implementing graph modelling to create a social network graph.What is graph modelling?Graph modelling is a method for representing real-world entities and their relationships using nodes, edges, and properties. It employs graph theory, a branch of mathematics that studies graphs, to visualise and analyse the structure and patterns within complex datasets. Common applications of graph modelling include social network analysis, recommendation systems, and biological networks.Graph modelling processStep 1: Define your domainBefore diving into graph modelling, it’s crucial to have a clear understanding of the domain you’re working with. This involves getting acquainted with the relevant terms, concepts, and relationships that exist in your specific field. To create a social network graph, familiarise yourself with terms like users, friendships, posts, likes, and comments.Step 2: Identify entities and relationshipsAfter defining your domain, you need to determine the entities (nodes) and relationships (edges) that exist within it. Entities are the primary objects in your domain, while relationships represent how these entities interact with each other. In a social network graph, users are entities, and friendships are relationships.Step 3: Establish propertiesEach entity and relationship may have a set of properties that provide additional information. In this step, identify relevant properties based on their significance to the domain. A user entity might have properties like name, age, and location. A friendship relationship could have a ‘since’ property to denote the establishment of the friendship.Step 4: Choose a graph modelOnce you’ve identified the entities, relationships, and properties, it’s time to choose a suitable graph model. Two common models are:  Property graph: A versatile model that easily accommodates properties on both nodes and edges. It’s well-suited for most applications.  Resource Description Framework (RDF): A World Wide Web Consortium (W3C) standard model, using triples of subject-predicate-object to represent data. It is commonly used in semantic web applications.For a social network graph, a property graph model is typically suitable. This is because user entities have many attributes and features. Property graphs provide a clear representation of the relationships between people and their attribute profiles.  Figure 1 - Social network graph  Step 5: Develop a schemaAlthough not required, developing a schema can be helpful for large-scale projects and team collaborations. A schema defines the structure of your graph, including entity types, relationships, and properties. In a social network graph, you might have a schema that specifies the types of nodes (users, posts) and the relationships between them (friendships, likes, comments).Step 6: Import or generate dataNext, acquire the data needed to populate your graph. This can come in the form of existing datasets or generated data from your application. For a social network graph, you can import user information from a CSV file and generate simulated friendships, posts, likes, and comments.Step 7: Implement the graph using a graph database or other storage optionsFinally, you need to store your graph data using a suitable graph database. Neo4j, Amazon Neptune, or Microsoft Azure Cosmos DB are examples of graph databases. Alternatively, depending on your specific requirements, you can use a non-graph database or an in-memory data structure to store the graph.Step 8: Analyse and visualise the graphAfter implementing the graph, you can perform various analyses using graph algorithms, such as shortest path, centrality, or community detection. In addition, visualising your graph can help you gain insights and facilitate communication with others.ConclusionBy following these steps, you can effectively create and analyse graph models for your specific domain. Remember to adjust the steps according to your unique domain and requirements, and always ensure that confidential and sensitive data is properly protected.References[1] What is a Graph Database?Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/graph-modelling-guidelines"
      }
      ,
    
      "llm-powered-data-classification": {
        "title": "LLM-powered data classification for data entities at scale",
        "author": "hualin-liustefan-jaroharvey-lijerome-tongandrew-lamchamal-sapumohottifeng-chengaaqib-kufran",
        "tags": "[&quot;Data&quot;, &quot;Machine Learning&quot;, &quot;Generative AI&quot;]",
        "category": "",
        "content": "IntroductionAt Grab, we deal with PetaByte-level data and manage countless data entities ranging from database tables to Kafka message schemas. Understanding the data inside is crucial for us, as it not only streamlines the data access management to safeguard the data of our users, drivers and merchant-partners, but also improves the data discovery process for data analysts and scientists to easily find what they need.The Caspian team (Data Engineering team) collaborated closely with the Data Governance team on automating governance-related metadata generation. We started with Personal Identifiable Information (PII) detection and built an orchestration service using a third-party classification service. With the advent of the Large Language Model (LLM), new possibilities dawned for metadata generation and sensitive data identification at Grab. This prompted the inception of the project, which aimed to integrate LLM classification into our existing service. In this blog, we share insights into the transformation from what used to be a tedious and painstaking process to a highly efficient system, and how it has empowered the teams across the organisation.For ease of reference, here’s a list of terms we’ve used and their definitions:  Data Entity: An entity representing a schema that contains rows/streams of data, for example, database tables, stream messages, data lake tables.  Prediction: Refers to the model’s output given a data entity, unverified manually.  Data Classification: The process of classifying a given data entity, which in the context of this blog, involves generating tags that represent sensitive data or Grab-specific types of data.  Metadata Generation: The process of generating the metadata for a given data entity. In this blog, since we limit the metadata to the form of tags, we often use this term and data classification interchangeably.  Sensitivity: Refers to the level of confidentiality of data. High sensitivity means that the data is highly confidential. The lowest level of sensitivity often refers to public-facing or publicly-available data.BackgroundWhen we first approached the data classification problem, we aimed to solve something more specific - Personal Identifiable Information (PII) detection. Initially, to protect sensitive data from accidental leaks or misuse, Grab implemented manual processes and campaigns targeting data producers to tag schemas with sensitivity tiers. These tiers ranged from Tier 1, representing schemas with highly sensitive information, to Tier 4, indicating no sensitive information at all. As a result, half of all schemas were marked as Tier 1, enforcing the strictest access control measures.The presence of a single Tier 1 table in a schema with hundreds of tables justifies classifying the entire schema as Tier 1. However, since Tier 1 data is rare, this implies that a large volume of non-Tier 1 tables, which ideally should be more accessible, have strict access controls.Shifting access controls from the schema-level to the table-level could not be done safely due to the lack of table classification in the data lake. We could have conducted more manual classification campaigns for tables, however this was not feasible for two reasons:  The volume, velocity, and variety of data had skyrocketed within the organisation, so it took significantly more time to classify at table level compared to schema level. Hence, a programmatic solution was needed to streamline the classification process, reducing the need for manual effort.  App developers, despite being familiar with the business scope of their data, interpreted internal data classification policies and external data regulations differently, leading to inconsistencies in understanding.A service called Gemini (named before Google announced the Gemini model!) was built internally to automate the tag generation process using a third party data classification service. Its purpose was to scan the data entities in batches and generate column/field level tags. These tags would then go through a review process by the data producers. The data governance team provided classification rules and used regex classifiers, alongside the third-party tool’s own machine learning classifiers, to discover sensitive information.After the implementation of the initial version of Gemini, a few challenges remained.  The third-party tool did not allow customisations of its machine learning classifiers, and the regex patterns produced too many false positives during our evaluation.  Building in-house classifiers would require a dedicated data science team to train a customised model. They would need to invest a significant amount of time to understand data governance rules thoroughly and prepare datasets with manually labelled training data.LLM came up on our radar following its recent “iPhone moment” with ChatGPT’s explosion onto the scene. It is trained using an extremely large corpus of text and contains trillions of parameters. It is capable of conducting natural language understanding tasks, writing code, and even analysing data based on requirements. The LLM naturally solves the mentioned pain points as it provides a natural language interface for data governance personnel. They can express governance requirements through text prompts, and the LLM can be customised effortlessly without code or model training.MethodologyIn this section, we dive into the implementation details of the data classification workflow. Please refer to the diagram below for a high-level overview:  Figure 1 - Overview of data classification workflow  This diagram illustrates how data platforms, the metadata generation service (Gemini), and data owners work together to classify and verify metadata. Data platforms trigger scan requests to the Gemini service to initiate the tag classification process. After the tags are predicted, data platforms consume the predictions, and the data owners are notified to verify these tags.Orchestration  Figure 2 - Architecture diagram of the orchestration service Gemini  Our orchestration service, Gemini, manages the data classification requests from data platforms. From the diagram, the architecture contains the following components:  Data platforms: These platforms are responsible for managing data entities and initiating data classification requests.  Gemini: This orchestration service communicates with data platforms, schedules and groups data classification requests.  Classification engines: There are two available engines (a third-party classification service and GPT3.5) for executing the classification jobs and return results. Since we are still in the process of evaluating two engines, both of the engines are working concurrently.When the orchestration service receives requests, it helps aggregate the requests into reasonable mini-batches. Aggregation is achievable through the message queue at fixed intervals. In addition, a rate limiter is attached at the workflow level. It allows the service to call the Cloud Provider APIs with respective rates to prevent the potential throttling from the service providers.Specific to LLM orchestration, there are two limits to be mindful of. The first one is the context length. The input length cannot surpass the context length, which was 4000 tokens for GPT3.5 at the time of development (or around 3000 words). The second one is the overall token limit (since both the input and output share the same token limit for a single request). Currently, all Azure OpenAI model deployments share the same quota under one account, which is set at 240K tokens per minute.ClassificationIn this section, we focus on LLM-powered column-level tag classification. The tag classification process is defined as follows:Given a data entity with a defined schema, we want to tag each field of the schema with metadata classifications that follow an internal classification scheme from the data governance team. For example, the field can be tagged as a &lt;particular kind of business metric&gt; or a &lt;particular type of personally identifiable information (PII). These tags indicate that the field contains a business metric or PII.We ask the language model to be a column tag generator and to assign the most appropriate tag to each column. Here we showcase an excerpt of the prompt we use:You are a database column tag classifier, your job is to assign the most appropriate tag based on table name and column name. The database columns are from a company that provides ride-hailing, delivery, and financial services. Assign one tag per column. However not all columns can be tagged and these columns should be assigned &lt;None&gt;. You are precise, careful and do your best to make sure the tag assigned is the most appropriate.The following is the list of tags to be assigned to a column. For each line, left hand side of the : is the tag and right hand side is the tag definition…&lt;Personal.ID&gt; : refers to government-provided identification numbers that can be used to uniquely identify a person and should be assigned to columns containing \"NRIC\", \"Passport\", \"FIN\", \"License Plate\", \"Social Security\" or similar. This tag should absolutely not be assigned to columns named \"id\", \"merchant id\", \"passenger id\", “driver id\" or similar since these are not government-provided identification numbers. This tag should be very rarely assigned.&lt;None&gt; : should be used when none of the above can be assigned to a column.…Output Format is a valid json string, for example:[{        \"column_name\": \"\",        \"assigned_tag\": \"\"}]Example question`These columns belong to the \"deliveries\" table        1. merchant_id        2. status        3. delivery_time`Example response[{        \"column_name\": \"merchant_id\",        \"assigned_tag\": \"&lt;Personal.ID&gt;\"},{        \"column_name\": \"status\",        \"assigned_tag\": \"&lt;None&gt;\"},{        \"column_name\": \"delivery_time\",        \"assigned_tag\": \"&lt;None&gt;\"}]We also curated a tag library for LLM to classify. Here is an example:            Column-level Tag      Definition                  Personal.ID      Refers to external identification numbers that can be used to uniquely identify a person and should be assigned to columns containing \"NRIC\", \"Passport\", \"FIN\", \"License Plate\", \"Social Security\" or similar.              Personal.Name       Refers to the name or username of a person and should be assigned to columns containing \"name\", \"username\" or similar.              Personal.Contact_Info      Refers to the contact information of a person and should be assigned to columns containing \"email\", \"phone\", \"address\", \"social media\" or similar.              Geo.Geohash      Refers to a geohash and should be assigned to columns containing \"geohash\" or similar.              None      Should be used when none of the above can be assigned to a column.      The output of the language model is typically in free text format, however, we want the output in a fixed format for downstream processing. Due to this nature, prompt engineering is a crucial component to make sure downstream workflows can process the LLM’s output.Here are some of the techniques we found useful during our development:  Articulate the requirements: The requirement of the task should be as clear as possible, LLM is only instructed to do what you ask it to do.  Few-shot learning: By showing the example of interaction, models understand how they should respond.  Schema Enforcement: Leveraging its ability of understanding code, we explicitly provide the DTO (Data Transfer Object) schema to the model so that it understands that its output must conform to it.  Allow for confusion: In our prompt we specifically added a default tag – the LLM is instructed to output the default &lt;None&gt; tag when it cannot make a decision or is confused.Regarding classification accuracy, we found that it is surprisingly accurate with its great semantic understanding. For acknowledged tables, users on average change less than one tag. Also, during an internal survey done among data owners at Grab in September 2023, 80% reported that this new tagging process helped them in tagging their data entities.Publish and verificationThe predictions are published to the Kafka queue to downstream data platforms. The platforms inform respective users weekly to verify the classified tags to improve the model’s correctness and to enable iterative prompt improvement. Meanwhile, we plan to remove the verification mandate for users once the accuracy reaches a certain level.  Figure 3 - Verification message shown in the data platform for user to verify the tags  ImpactSince the new system was rolled out, we have successfully integrated this with Grab’s metadata management platform and production database management platform. Within a month since its rollout, we have scanned more than 20,000 data entities, averaging around 300-400 entities per day.Using a quick back-of-the-envelope calculation, we can see the significant time savings achieved through automated tagging. Assuming it takes a data owner approximately 2 minutes to classify each entity, we are saving approximately 360 man-days per year for the company. This allows our engineers and analysts to focus more on their core tasks of engineering and analysis rather than spending excessive time on data governance.The classified tags pave the way for more use cases downstream. These tags, in combination with rules provided by data privacy office in Grab, enable us to determine the sensitivity tier of data entities, which in turn will be leveraged for enforcing the Attribute-based Access Control (ABAC) policies and enforcing Dynamic Data Masking for downstream queries. To learn more about the benefits of ABAC, readers can refer to another engineering blog posted earlier.Cost wise, for the current load, it is extremely affordable contrary to common intuition. This affordability enables us to scale the solution to cover more data entities in the company.What’s next?Prompt improvementWe are currently exploring feeding sample data and user feedback to greatly increase accuracy. Meanwhile, we are experimenting on outputting the confidence level from LLM for its own classification. With confidence level output, we would only trouble users when the LLM is uncertain of its answers. Hopefully this can remove even more manual processes in the current workflow.Prompt evaluationTo track the performance of the prompt given, we are building analytical pipelines to calculate the metrics of each version of the prompt. This will help the team better quantify the effectiveness of prompts and iterate better and faster.Scaling outWe are also planning to scale out this solution to more data platforms to streamline governance-related metadata generation to more teams. The development of downstream applications using our metadata is also on the way. These exciting applications are from various domains such as security, data discovery, etc.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/llm-powered-data-classification"
      }
      ,
    
      "scaling-marketing-for-merchants": {
        "title": "Scaling marketing for merchants with targeted and intelligent promos",
        "author": "sharon-teng",
        "tags": "[&quot;Data&quot;, &quot;Advertising&quot;, &quot;Scalability&quot;, &quot;Data science&quot;, &quot;Marketing&quot;]",
        "category": "",
        "content": "IntroductionA promotional campaign is a marketing effort that aims to increase sales, customer engagement, or brand awareness for a product, service, or company. The target is to have more orders and sales by assigning promos to consumers within a given budget during the campaign period.  Figure 1 - Merchant feedback on marketing  From our research, we found that merchants have specific goals for the promos they are willing to offer. They want a simple and cost-effective way to achieve their specific business goals by providing well-designed offers to target the correct customers. From Grab’s perspective, we want to help merchants set up and run campaigns efficiently, and help them achieve their specific business goals.Problem statementOne of Grab’s platform offerings for merchants is the ability to create promotional campaigns. With the emergence of AI technologies, we found that there are opportunities for us to further optimise the platform. The following are the gaps and opportunities we identified:  Globally assigned promos without smart targeting: The earlier method targeted every customer, so everyone could redeem until the promo reached the redemption limits. However, this method did not accurately meet business goals or optimise promo spending. The promotional campaign should intelligently target the best promo for each customer to increase sales and better utilise promo spending.  No customised promos for every merchant: To better optimise sales for each merchant, merchants should offer customised promos based on their historical consumer trends, not just a general offer set. For example, for a specific merchant, a 27% discount may be the appropriate offer to uplift revenue and sales based on user bookings. However, merchants do not always have the expertise to decide which offer to select to increase profit.  No AI-driven optimisation: Without AI models, it was harder for merchants to assign the right promos at scale to each consumer and optimise their business goals.As shown in the following figure, AI-driven promotional campaigns are expected to bring higher sales with more promo spend than heuristic ones. Hence, at Grab we looked to introduce an automated, AI-driven tool that helps merchants intelligently target consumers with appropriate promos, while optimising sales and promo spending. That’s where Bullseye comes in.  Figure 2 - Graph showing the sales expectations for AI-driven pomotional campaigns  SolutionBullseye is an automated, AI-driven promo assignment system that leverages the following capabilities:  Automated user segmentation: Enables merchants to target new, churned, and active users or all users.  Automatic promo design: Enables a merchant-level promo design framework to customise promos for each merchant or merchant group according to their business goals.  Assign each user the optimal promo: Users will receive promos selected from an array of available promos based on the merchant’s business objective.  Achieve different Grab and merchant objectives: Examples of objectives are to increase merchant sales and decrease Grab promo spend.  Flexibility to optimise for an individual merchant brand or group of merchant brands: For promotional campaigns, targeting and optimisation can be performed for a single or group of merchants (e.g. enabling GrabFood to run cuisine-oriented promo campaigns).Architecture  Figure 3 - Bullseye architecture  The Bullseye architecture consists of a user interface (UI) and a backend service to handle requests. To use Bullseye, our operations team inputs merchant information into the Bullseye UI. The backend service will then interact with APIs to process the information using the AI model. As we work with a large customer population, data is stored in S3 and the API service triggering Chimera Spark job is used to run the prediction model and generate promo assignments. During the assignment, the Spark job parses the input parameters, pre-validates the input, makes some predictions, and then returns the promo assignment results to the backend service.ImplementationThe key components in Bullseye are shown in the following figure:  Figure 4 - Key components of Bullseye    Eater Segments Identifier: Identifies each user as active, churned, or new based on their historical orders from target merchants.  Promo Designer: We constructed a promo variation design framework to adaptively design promo variations for each campaign request as shown in the diagram below.          Offer Content Candidate Generation: Generates variant settings of promos based on the promo usage history.      Campaign Impact Simulator: Predicts business metrics such as revenue, sales, and cost based on the user and merchant profiles and offer features.              Optimal Promo Selection: Selects the optimal offer based on the predicted impact and the given campaign objective. The optimal would be based on how you define optimal. For example, if the goal is to maximise merchant sales, the model selects the top candidate which can bring the highest revenue. Finally, with the promo selection, the service returns the promo set to be used in the target campaign.          Figure 5 - Optimal Promo Selection                    Customer Response Model: Predicts customer responses such as order value, redemption, and take-up rate if assigning a specific promo. Bullseye captures various user attributes and compares it with an offer’s attributes. Examples of attributes are cuisine type, food spiciness, and discount amount. When there is a high similarity in the attributes, there is a higher probability that the user will take up the offer.        Figure 6 - Customer Response Model              Hyper-parameter Selection: Optimises toward multiple business goals. Tuning of hyper-parameters allows the AI assignment model to learn how to meet success criteria such as cost per merchant sales (cpSales) uplift and sales uplift. The success criteria is the achieving of business goals. For example, the merchant wants the sales uplift after assigning promo, but cpSales uplift cannot be higher than 10%. With tuning, the optimiser can find optimal points to meet business goals and use AI models to search for better settings with high efficiency compared to manual specification. We need to constantly tune and iterate models and hyper-parameters to adapt to ever-evolving business goals and the local landscape.    As shown in the image below, AI assignments without hyper-parameter tuning (HPT) leads to a high cpSales uplift but low sales uplift (red dot). So the  hyper-parameters would help to fine-tune the assignment result to be in the optimal space such as the blue dot, which may have lower sales than the red dot but meet the success criteria.        Figure 7 - Graph showing the impact of using AI assignments with HPT        ImpactWe started using Bullseye in 2021. From its use we found that:  Hyper-parameters tuning and auto promo design can increase sales and reduce promo spend for food campaigns.  Promo Designer optimises budget utilisation and increases the number of promo redemptions for food campaigns.  The Customer Response Model reduced promo spending for Mart promotional campaigns.ConclusionWe have seen positive results with the implementation of Bullseye such as reduced promo spending and maximised budget spending returns. In our efforts to serve our merchants better and help them achieve their business goals, we will continue to improve Bullseye. In the next phase, we plan to implement a more intelligent service, enabling reinforcement learning, and online assignment. We also aim to scale AI adoption by onboarding regional promotional campaigns as much as possible.Special thanks to William Wu, Rui Tan, Rahadyan Pramudita, Krishna Murthy, and Jiesin Chia for making this project a success.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/scaling-marketing-for-merchants"
      }
      ,
    
      "scalable-lookalike-audiences": {
        "title": "Stepping up marketing for advertisers: Scalable lookalike audience",
        "author": "william-wu",
        "tags": "[&quot;Data&quot;, &quot;Advertising&quot;, &quot;Scalability&quot;, &quot;Data science&quot;, &quot;Marketing&quot;, &quot;Lookalike audience&quot;]",
        "category": "",
        "content": "The advertising industry is constantly evolving, driven by advancements in technology and changes in consumer behaviour. One of the key challenges in this industry is reaching the right audience, reaching people who are most likely to be interested in your product or service. This is where the concept of a lookalike audience comes into play. By identifying and targeting individuals who share similar characteristics with an existing customer base, businesses can significantly improve the effectiveness of their advertising campaigns.    However, as the scale of Grab advertisements grows, there are several optimisations needed to maintain the efficacy of creating lookalike audiences such as high service level agreement (SLA), high cost of audience creation, and unstable data ingestion.The need for an even more efficient and scalable solution for creating lookalike audiences was the motivation behind the development of the scalable lookalike audience platform. By developing a high-performance in-memory lookalike audience retrieval service and embedding-based lookalike audience creation and updating pipelines, t​his improved platform builds on the existing system and provides an even more effective tool for advertisers to reach their target audience.Constant optimisation for greater precisionIn the dynamic world of digital advertising, the ability to quickly and efficiently reach the right audience is paramount and a key strategy is targeted advertising. As such, we have to constantly find ways to improve our current approach to creating lookalike audiences that impacts both advertisers and users. Some of the gaps we identified included:      Long SLA for audience creation. Earlier, the platform stored results on Segmentation Platform (SegP) and it took two working days to generate a lookalike audience list. This is because inserting a single audience into SegP took three times longer than generating the audience. Extended creation times impacted the effectiveness of advertising campaigns, as it limited the ability of advertisers to respond quickly to changing market dynamics.        Low scalability. As the number of onboarded merchant-partners increased, the time and cost of generating lookalike audiences also increased proportionally. This limited the availability of lookalike audience generation for all advertisers, particularly those with large customer bases or rapidly changing audience profiles.        Low updating frequency of lookalike audiences. With automated updates only occurring on a weekly basis, this increased the likelihood that audiences may become outdated and ineffective. This meant there was scope to further improve to help advertisers more effectively reach their campaign goals, by targeting individuals who fit the desired audience profile.        High cost of creation. The cost of producing one segment can add up quickly for advertisers who need to generate multiple audiences. This could impact scalability for advertisers as they could hesitate to effectively use multiple lookalike audiences in their campaigns.  Solution    To efficiently identify the top N lookalike audiences for each Grab user from our pool of millions of users, we developed a solution that leverages user and audience representations in the form of embeddings. Embeddings are vector representations of data that utilise linear distances to capture structure from the original datasets. With embeddings, large sets of data are compressed and easily processed without affecting data integrity. This approach ensures high accuracy, low latency, and low cost in retrieving the most relevant audiences.Our solution takes into account the fact that representation drift varies among entities as data is added. For instance, merchant-partner embeddings are more stable than passenger embeddings. By acknowledging this reality, we optimised our process to minimise cost while maintaining a desirable level of accuracy. Furthermore, we believe that having a strong representation learning strategy in the early stages reduced the need for complex models in the following stages.Our solution comprises two main components:      Real-time lookalike audience retrieving: We developed an in-memory high-performance retrieving service that stores passenger embeddings, audience embeddings, and audience score thresholds. To further reduce cost, we designed a passenger embedding compression algorithm that reduces the memory needs of passenger embeddings by around 90%.        Embedding-based audience creation and updating: The output of this part of the project is an online retrieving model that includes passenger embeddings, audience embeddings, and thresholds. To minimise costs, we leverage the passenger embeddings that are also utilised by other projects within Grab, beyond advertising, thus sharing the cost. The audience embeddings and thresholds are produced with a low-cost small neural network.  In summary, our approach to creating scalable lookalike audiences is designed to be cost-effective, accurate, and efficient, leveraging the power of embeddings and smart computational strategies to deliver the best possible audiences for our advertisers.Solution architecture      The advertiser creates a campaign with a custom audience, which triggers the audience creation process. During this process, the audience service stores the audience metadata provided by advertisers in a message queue.  A scheduled Data Science (DS) job then retrieves the pending audience metadata, creates the audience, and updates the TensorFlow Serving (TFS) model.  During the serving period, the Backend (BE) service calls the DS service to retrieve all audiences that include the target user. Ads that are targeting these audiences are then selected by the Click-Through Rate (CTR) model to be displayed to the user.ImplementationTo ensure the efficiency of the lookalike audience retrieval model and minimise the costs associated with audience creation and serving, we’ve trained the user embedding model using billions of user actions. This extensive training allows us to employ straightforward methods for audience creation and serving, while still maintaining high levels of accuracy.Creating lookalike audiencesThe Audience Creation Job retrieves the audience metadata from the online audience service, pulls the passenger embeddings, and then averages these embeddings to generate the audience embedding.We use the cosine score of a user and the audience embedding to identify the audiences the user belongs to. Hence, it’s sufficient to store only the audience embedding and score threshold. Additionally, a global target-all-pax Audience list is stored to return these audiences for each online request.    Serving lookalike audiencesThe online audience service is also tasked with returning all the audiences to which the current user belongs. This is achieved by utilising the cosine score of the user embedding and audience embeddings, and filtering out all audiences that surpass the audience thresholds.To adhere to latency requirements, we avoid querying any external feature stores like Redis and instead, store all the embeddings in memory. However, the embeddings of all users are approximately 20 GB, which could affect model loading. Therefore, we devised an embedding compression method based on hash tricks inspired by Bloom Filter.      We utilise hash functions to obtain the hash64 value of the paxID, which is then segmented into four 16-bit values. Each 16-bit value corresponds to a 16-dimensional embedding block, and the compressed embedding is the concatenation of these four 16-dimensional embeddings.  For each paxID, we have both the original user embedding and the compressed user embedding. The compressed user embeddings are learned by minimising the Mean Square Error loss.  We can balance the storage cost and the accuracy by altering the number of hash functions used.Impact  Users can see advertisements targeting a new audience within 15 mins after the advertiser creates a campaign.  This new system doubled the impressions and clicks, while also improving the CTR, conversion rate, and return on investment.  Costs for generating lookalike audiences decreased by 98%.Learnings/ConclusionTo evaluate the effectiveness of our new scalable system besides addressing these issues, we conducted an A/B test to compare it with the earlier system. The results revealed that this new system effectively doubled the number of impressions and clicks while also enhancing the CTR, conversion rate, and return on investment.Over the years, we have amassed over billions of user actions, which have been instrumental in training the model and creating a comprehensive representation of user interests in the form of embeddings.What’s next?While this scalable system has proved its effectiveness and demonstrated impressive results in CTR, conversion rate, and return on investment, there is always room for improvement.  In the next phase, we plan to explore more advanced algorithms, refine our feature engineering process, and conduct more extensive hyperparameter tuning. Additionally, we will continue to monitor the system’s performance and make necessary adjustments to ensure it remains robust and effective in serving our advertisers’ needs.References  Real-time Attention Based Look-alike Model for Recommender System  Bloom Filter  Smart Targeting: A Relevance-driven and Configurable Targeting Framework for Advertising System  GUIM - General User and Item Embedding with Mixture of Representation in E-commerceJoin usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/scalable-lookalike-audiences"
      }
      ,
    
      "building-hyperlocal-grabmaps": {
        "title": "Building hyperlocal GrabMaps",
        "author": "adriana-lazar",
        "tags": "[&quot;Maps&quot;, &quot;Data&quot;, &quot;Big Data&quot;, &quot;Data processing&quot;, &quot;hyperlocalisation&quot;, &quot;GrabMaps&quot;, &quot;navigation&quot;]",
        "category": "",
        "content": "IntroductionSoutheast Asia (SEA) is a dynamic market, very different from other parts of the world. When travelling on the road, you may experience fast-changing road restrictions, new roads appearing overnight, and high traffic congestion. To address these challenges, GrabMaps has adapted to the SEA market by leveraging big data solutions. One of the solutions is the integration of hyperlocal data in GrabMaps.Hyperlocal information is oriented around very small geographical communities and obtained from the local knowledge that our map team gathers. The map team is spread across SEA, enabling us to define clear specifications (e.g. legal speed limits), and validate that our solutions are viable.  Figure 1 - Map showing detections from images and probe data, and hyperlocal data.  Hyperlocal inputs make our mapping data even more robust, adding to the details collected from our image and probe detection pipelines. Figure 1 shows how data from our detection pipeline is overlaid with hyperlocal data, and then mapped across the SEA region. If you are curious and would like to check out the data yourself, you can download it here.Processing hyperlocal dataNow let’s go through the process of detecting hyperlocal data.Download dataGrabMaps is based on OpenStreetMap (OSM). The first step in the process is to download the .pbf file for Asia from geofabrick.de. This .pbf file contains all the data that is available on OSM, such as details of places, trees, and roads. Take for example a park, the .pbf file would contain data on the park name, wheelchair accessibility, and many more.For this article, we will focus on hyperlocal data related to the road network. For each road, you can obtain data such as the type of road (residential or motorway), direction of traffic (one-way or more), and road name.Convert dataTo take advantage of big data computing, the next step in the process is to convert the .pbf file into Parquet format using a Parquetizer. This will convert the binary data in the .pbf file into a table format. Each road in SEA is now displayed as a row in a table as shown in Figure 2.  Figure 2 - Road data in Parquet format.  Identify hyperlocal dataAfter the data is prepared, GrabMaps then identifies and inputs all of our hyperlocal data, and delivers a consolidated view to our downstream services. Our hyperlocal data is obtained from various sources, either by looking at geometry, or other attributes in OSM such as the direction of travel and speed limit. We also apply customised rules defined by our local map team, all in a fully automated manner. This enhances the map together with data obtained from our rides and deliveries GPS pings and from KartaView, Grab’s product for imagery collection.  Figure 3 - Architecture diagram showing how hyperlocal data is integrated into GrabMaps.  Benefit of our hyperlocal GrabMapsGrabNav, a turn-by-turn navigation tool available on the Grab driver app, is one of our products that benefits from having hyperlocal data. Here are some hyperlocal data that are made available through our approach:  Localisation of roads: The country, state/county, or city the road is in  Language spoken, driving side, and speed limit  Region-specific default speed regulations  Consistent name usage using language inference  Complex attributes like intersection linksTo further explain the benefits of this hyperlocal feature, we will use intersection links as an example. In the next section, we will explain how intersection links data is used and how it impacts our driver-partners and passengers.Identifying hyperlocal data - intersection linksAn intersection link is when two or more roads meet. Figure 4 and 5 illustrates what an intersection link looks like in a GrabMaps mock and in OSM.  Figure 4 - Mock of an intersection link.     Figure 5 - Intersection link illustration from a real road network in OSM.    To locate intersection links in a road network, there are computations involved. We would first combine big data processing (which we do using Spark) with graphs. We use geohash as the unit of processing, and for each geohash, a bi-directional graph is created.From such resulting graphs, we can determine intersection links if:  Road segments are parallel  The roads have the same name  The roads are one way roads  Angles and the shape of the road are in the intervals or requirements we seekEach intersection link we identify is tagged in the map as intersection_links. Our downstream service teams can then identify them by searching for the tag.ImpactThe impact we create with our intersection link can be explained through the following example.  Figure 6 - Longer route, without GrabMaps intersection link feature. The arrow indicates where the route should have suggested a U-turn.    Figure 7 - Shorter route using GrabMaps by taking a closer link between two main roads.  Figure 6 and Figure 7 show two different routes for the same origin and destination. However, you can see that Figure 7 has a shorter route and this is made available by taking an intersection link early on in the route. The highlighted road segment in Figure 7 is an intersection link, tagged by the process we described earlier. The route is now much shorter making GrabNav more efficient in its route suggestion.There are numerous factors that can impact a driver-partner’s trip, and intersection links are just one example. There are many more features that GrabMaps offers across Grab’s services that allow us to “outserve” our partners.ConclusionGrabMaps and GrabNav deliver enriched experiences to our driver-partners. By integrating certain hyperlocal data features, we are also able to provide more accurate pricing for both our driver-partners and passengers. In our mission towards sustainable growth, this is an area that we will keep on improving by leveraging scalable tech solutions.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/building-hyperlocal-grabmaps"
      }
      ,
    
      "streamlining-grabs-segmentation-platform": {
        "title": "Streamlining Grab's Segmentation Platform with faster creation and lower latency",
        "author": "jake-ng",
        "tags": "[&quot;Back End&quot;, &quot;Performance&quot;]",
        "category": "",
        "content": "Launched in 2019, Segmentation Platform has been Grab’s one-stop platform for user segmentation and audience creation across all business verticals. User segmentation is the process of dividing passengers, driver-partners, or merchant-partners (users) into sub-groups (segments) based on certain attributes. Segmentation Platform empowers Grab’s teams to create segments using attributes available within our data ecosystem and provides APIs for downstream teams to retrieve them.Checking whether a user belongs to a segment (Membership Check) influences many critical flows on the Grab app:  When a passenger launches the Grab app, our in-house experimentation platform will tailor the app experience based on the segments the passenger belongs to.  When a driver-partner goes online on the Grab app, the Drivers service calls Segmentation Platform to ensure that the driver-partner is not blacklisted.  When launching marketing campaigns, Grab’s communications platform relies on Segmentation Platform to determine which passengers, driver-partners, or merchant-partners to send communication to.This article peeks into the current design of Segmentation Platform and how the team optimised the way segments are stored to reduce read latency thus unlocking new segmentation use cases.ArchitectureSegmentation Platform comprises two major subsystems:  Segment creation  Segment serving  Fig 1. Segmentation Platform architecture  Segment creationSegment creation is powered by Spark jobs. When a Grab team creates a segment, a Spark job is started to retrieve data from our data lake. After the data is retrieved, cleaned, and validated, the Spark job calls the serving sub-system to populate the segment with users.Segment servingSegment serving is powered by a set of Go services. For persistence and serving, we use ScyllaDB as our primary storage layer. We chose to use ScyllaDB as our NoSQL store due to its ability to scale horizontally and meet our &lt;80ms p99 SLA. Users in a segment are stored as rows indexed by the user ID. The table is partitioned by the user ID ensuring that segment data is evenly distributed across the ScyllaDB clusters.      User ID    Segment Name    Other metadata columns        1221    Segment_A    …        3421    Segment_A    …        5632    Segment_B    …        7889    Segment_B    …  With this design, Segmentation Platform handles up to 12K read and 36K write QPS, with a p99 latency of 40ms.ProblemsThe existing system has supported Grab, empowering internal teams to create rich and personalised experiences. However, with the increased adoption and use, certain challenges began to emerge:  As more and larger segments are being created, the write QPS became a bottleneck leading to longer wait times for segment creation.  Grab services requested even lower latency for membership checks.Long segment creation timesAs more segments were created by different teams within Grab, the write QPS was no longer able to keep up with the teams’ demands. Teams would have to wait for hours for their segments to be created, reducing their operational velocity.Read latencyFurther, while the platform already offers sub-40ms p99 latency for reads, this was still too slow for certain services and their use cases. For example, Grab’s communications platform needed to check whether a user belongs to a set of segments before sending out communication and incurring increased latency for every communication request was not acceptable. Another use case was for Experimentation Platform, where checks must have low latency to not impact the user experience. Thus, the team explored alternative ways of storing the segment data with the goals of:  Reducing segment creation time  Reducing segment read latency  Maintaining or reducing costSolutionSegments as bitmapsOne of the main engineering challenges was scaling the write throughput of the system to keep pace with the number of segments being created. As a segment is stored across multiple rows in ScyllaDB, creating a large segment incurs a huge number of writes to the database. What we needed was a better way to store a large set of user IDs. Since user IDs are represented as integers in our system, a natural solution to storing a set of integers was a bitmap.For example, a segment containing the following user IDs: 1, 6, 25, 26, 89 could be represented with a bitmap as follows:  Fig 2. Bitmap representation of a segment  To perform a membership check, a bitwise operation can be used to check if the bit at the user ID’s index is 0 or 1. As a bitmap, the segment can also be stored as a single Blob in object storage instead of inside ScyllaDB.However, as the number of user IDs in the system is large, a small and sparse segment would lead to prohibitively large bitmaps. For example, if a segment contains 2 user IDs 100 and 200,000,000, it will require a bitmap containing 200 million bits (25MB) where all but 2 of the bits are just 0. Thus, the team needed an encoding to handle sparse segments more efficiently.Roaring BitmapsAfter some research, we landed on Roaring Bitmaps, which are compressed uint32 bitmaps. With roaring bitmaps, we are able to store a segment with 1 million members in a Blob smaller than 1 megabyte, compared to 4 megabytes required by a naive encoding.Roaring Bitmaps achieve good compression ratios by splitting the set into fixed-size (216) integer chunks and using three different data structures (containers) based on the data distribution within the chunk. The most significant 16 bits of the integer are used as the index of the chunk, and the least significant 16 bits are stored in the containers.Array containersArray containers are used when data is sparse (&lt;= 4096 values). An array container is a sorted array of 16-bit integers. It is memory-efficient for sparse data and provides logarithmic-time access.Bitmap containersBitmap containers are used when data is dense. A bitmap container is a 216 bit container where each bit represents the presence or absence of a value. It is memory-efficient for dense data and provides constant-time access.Run containersFinally, run containers are used when a chunk has long consecutive values. Run containers use run-length encoding (RLE) to reduce the storage required for dense bitmaps. Run containers store a pair of values representing the start and the length of the run. It provides good memory efficiency and fast lookups.The diagram below shows how a dense bitmap container that would have required 91 bits can be compressed into a run container by storing only the start (0) and the length (90). It should be noted that run containers are used only if it reduces the number of bytes required compared to a bitmap.  Fig 3. A dense bitmap container compressed into a run container  By using different containers, Roaring Bitmaps are able to achieve good compression across various data distributions, while maintaining excellent lookup performance. Additionally, as segments are represented as Roaring Bitmaps, service teams are able to perform set operations (union, interaction, and difference, etc) on the segments on the fly, which previously required re-materialising the combined segment into the database.Caching with an SDKEven though the segments are now compressed, retrieving a segment from the Blob store for each membership check would incur an unacceptable latency penalty. To mitigate the overhead of retrieving a segment, we developed an SDK that handles the retrieval and caching of segments.  Fig 4. How the SDK caches segments  The SDK takes care of the retrieval, decoding, caching, and watching of segments. Users of the  SDK are only required to specify the maximum size of the cache to prevent exhausting the service’s memory. The SDK provides a cache with a least-recently-used eviction policy to ensure that hot segments are kept in the cache. They are also able to watch for updates on a segment and the SDK will automatically refresh the cached segment when it is updated.Hero teamsCommunications PlatformCommunications Platform has adopted the SDK to implement a new feature to control the communication frequency based on which segments a user belongs to. Using the SDK, the team is able to perform membership checks on multiple multi-million member segments, achieving peak QPS 15K/s with a p99 latency of &lt;1ms. With the new feature, they have been able to increase communication engagement and reduce the communication unsubscribe rate.Experimentation PlatformExperimentation Platform powers experimentation across all Grab services. Segments are used heavily in experiments to determine a user’s experience. Prior to using the SDK, Experimentation Platform limited the maximum size of the segments that could be used to prevent exhausting a service’s memory.After migrating to the new SDK, they were able to lift this restriction due to the compression efficiency of Roaring Bitmaps. Users are now able to use any segments as part of their experiment without worrying that it would require too much memory.ClosingThis blog post discussed the challenges that Segmentation Platform faced when scaling and how the team explored alternative storage and encoding techniques to improve segment creation time, while also achieving low latency reads. The SDK allows our teams to easily make use of segments without having to handle the details of caching, eviction, and updating of segments.Moving forward, there are still existing use cases that are not able to use the Roaring Bitmap segments and thus continue to rely on segments from ScyllaDB. Therefore, the team is also taking steps to optimise and improve the scalability of our service and database.Special thanks to Axel, the wider Segmentation Platform team, and Data Technology team for reviewing the post. Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/streamlining-grabs-segmentation-platform"
      }
      ,
    
      "graph-anomaly-model": {
        "title": "Unsupervised graph anomaly detection - Catching new fraudulent behaviours",
        "author": "rizal-fathonyjenn-ngjia-chen",
        "tags": "[&quot;Data science&quot;, &quot;Graph networks&quot;, &quot;Graphs&quot;, &quot;Graph visualisation&quot;, &quot;Security&quot;, &quot;Fraud detection&quot;, &quot;Anomaly detection&quot;, &quot;Machine learning&quot;]",
        "category": "",
        "content": "Earlier in this series, we covered the importance of graph networks, graph concepts, graph visualisation, and graph-based fraud detection methods. In this article, we will discuss how to automatically detect new types of fraudulent behaviour and swiftly take action on them.One of the challenges in fraud detection is that fraudsters are incentivised to always adversarially innovate their way of conducting frauds, i.e., their modus operandi (MO in short). Machine learning models trained using historical data may not be able to pick up new MOs, as they are new patterns that are not available in existing training data. To enhance Grab’s existing security defences and protect our users from these new MOs, we needed a machine learning model that is able to detect them quickly without the need for any label supervision, i.e., an unsupervised learning model rather than the regular supervised learning model.To address this, we developed an in-house machine learning model for detecting anomalous patterns in graphs, which has led to the discovery of new fraud MOs. Our focus was initially on GrabFood and GrabMart verticals, where we monitored the interactions between consumers and merchants. We modelled these interactions as a bipartite graph (a type of graph for modelling interactions between two groups) and then performed anomaly detection on the graph. Our in-house anomaly detection model was also presented at the International Joint Conference on Neural Networks (IJCNN) 2023, a premier academic conference in the area of neural networks, machine learning, and artificial intelligence.In this blog, we discuss the model and its application within Grab. For avid audiences that want to read the details of our model, you can access our paper below. Note that even though we implemented our model for anomaly detection in GrabFood and GrabMart, the model is designed for general purposes and is applicable to interaction graphs between any two groups.Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed GraphsBy Rizal Fathony, Jenn Ng, Jia ChenPresented at the International Joint Conference on Neural Networks (IJCNN) 2023DOI: 10.1109/IJCNN54540.2023.10191331 (citation)Before we dive into how our model works, it is important to understand the process of graph construction in our application as the model assumes the availability of the graphs in a standardised format.Graph construction We modelled the interactions between consumers and merchants in GrabFood and GrabMart platforms as bipartite graphs (G), where the first group of nodes (U) represents the consumers, the second group of nodes (V) represents the merchants, and the edges (E) connecting them means that the consumers have placed some food/mart orders to the merchants. The graph is also supplied with rich transactional information about the consumers and the merchants in the form of node features (Xu and Xv), as well as order information in the form of edge features (Xe).  Fig 1. Graph construction process  The goal of our anomaly model is to detect anomalous and suspicious behaviours from the consumers or merchants (node-level anomaly detection), as well as anomalous order interactions (edge-level anomaly detection). As mentioned, this detection needs to be done without any label supervision.Model architectureWe designed our graph anomaly model as a type of autoencoder, with an encoder and two decoders – a feature decoder and a structure decoder. The key feature of our model is that it accepts a bipartite graph with both node and edge attributes as the input. This is important as both node and edge attributes encode essential information for determining if certain behaviours are suspicious. Many previous works on graph anomaly detection only support node attributes. In addition, our model can produce both node and edge level anomaly scores, unlike most of the previous works that produce node-level scores only. We named our model GraphBEAN, which is short for Bipartite Node-and-Edge-Attributed Networks.From the input, the encoder then processes the attributed bipartite graph into a series of graph convolution layers to produce latent representations for both node groups. Our graph convolution layers produce new representations for each node in both node groups (U and V), as well as for each edge in the graph. Note that the last convolution layer in the encoder only produces the latent representations for nodes, without producing edge representations. The reason for this design is that we only put the latent representations for the active actors, the nodes representing consumers and merchants, but not their interactions.  Fig 2. GraphBEAN architecture  From the nodes’ latent representations, the feature decoder is tasked to reconstruct the original graph with both node and edge attributes via a series of graph convolution layers. As the graph structure is provided by the feature decoder, we task the structure decoder to learn the graph structure by predicting if there exists an edge connecting two nodes. This edge prediction, as well as the graph reconstructed by the feature decoder, are then compared to the original input graph via a reconstruction loss function.The model is then trained using the bipartite graph constructed from GrabFood and GrabMart transactions. We use a reconstruction-based loss function as the training objective of the model. After the training is completed, we compute the anomaly score of each node and edge in the graph using the trained model.Anomaly score computationOur anomaly scores are reconstruction-based. The score design assumes that normal behaviours are common in the dataset and thus, can be easily reconstructed by the model. On the other hand, anomalous behaviours are rare. Therefore the model will have a hard time reconstructing them, hence producing high errors.  Fig 3. Edge-level and node-level anomaly scores computation  The model produces two types of anomaly scores. First, the edge-level anomaly scores, which are calculated from the edge reconstruction error. Second, the node-level anomaly scores, which are calculated from node reconstruction error plus an aggregate over the edge scores from the edges connected to the node. This aggregate could be a mean or max aggregate.Actioning systemIn our implementation of GraphBEAN within Grab, we designed a full pipeline of anomaly detection and actioning systems. It is a fully-automated system for constructing a bipartite graph from GrabFood and GrabMart transactions, training a GraphBEAN model using the graph, and computing anomaly scores. After computing anomaly scores for all consumers and merchants (node-level), as well as all of their interactions (edge-level), it automatically passes the scores to our actioning system. But before that, it also passes them through a system we call fraud type tagger. This is also a fully-automated heuristic-based system that tags some of the detected anomalies with some fraud tags. The purpose of this tagging is to provide some context in general, like the types of detected anomalies. Some examples of these tags are promo abuse or possible collusion.  Fig 4. Pipeline in our actioning system  Both the anomaly scores and the fraud type tags are then forwarded to our actioning system. The system consists of two subsystems:  Human expert actioning system: Our fraud experts analyse the detected anomalies and perform certain actioning on them, like suspending certain transaction features from suspicious merchants.  Automatic actioning system: Combines the anomaly scores and fraud type tags with other external signals to automatically do actioning on the detected anomalies, like preventing promos from being used by fraudsters or preventing fraudulent transactions from occurring. These actions vary depending on the type of fraud and the scores.What’s next?The GraphBEAN model enables the detection of suspicious behaviour on graph data without the need for label supervision. By implementing the model on GrabFood and GrabMart platforms, we learnt that having such a system enables us to quickly identify new types of fraudulent behaviours and then swiftly perform action on them. This also allows us to enhance Grab’s defence against fraudulent activity and actively protect our users.We are currently working on extending the model into more generic heterogeneous (multi-entity) graphs. In addition, we are also working on implementing it to more use cases within Grab.Citation information(*) If you use the paper for academic purposes, please cite the following publication:R. Fathony, J. Ng and J. Chen, “Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs,” 2023 International Joint Conference on Neural Networks (IJCNN), Gold Coast, Australia, 2023, pp. 1-10, doi: 10.1109/IJCNN54540.2023.10191331.IEEE copyright notice:© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/graph-anomaly-model"
      }
      ,
    
      "zero-traffic-cost": {
        "title": "Zero traffic cost for Kafka consumers",
        "author": "fabrice-harbulotquangminh-tran",
        "tags": "[&quot;Engineering&quot;, &quot;Kafka&quot;, &quot;Performance&quot;, &quot;Access control&quot;]",
        "category": "",
        "content": "IntroductionCoban, Grab’s real-time data streaming platform team, has been building an ecosystem around Kafka, serving all Grab verticals. Along with stability and performance, one of our priorities is also cost efficiency.In this article, we explain how the Coban team has substantially reduced Grab’s annual cost for data streaming by enabling Kafka consumers to fetch from the closest replica.Problem statementThe Grab platform is primarily hosted on AWS cloud, located in one region, spanning over three Availability Zones (AZs). When it comes to data streaming, both the Kafka brokers and Kafka clients run across these three AZs.  Figure 1 - Initial design, consumers fetching from the partition leader  Figure 1 shows the initial design of our data streaming platform. To ensure high availability and resilience, we configured each Kafka partition to have three replicas. We have also set up our Kafka clusters to be rack-aware (i.e. 1 “rack” = 1 AZ) so that all three replicas reside in three different AZs.The problem with this design is that it generates staggering cross-AZ network traffic. This is because, by default, Kafka clients communicate only with the partition leader, which has a 67% probability of residing in a different AZ.This is a concern as we are charged for cross-AZ traffic as per AWS’s network traffic pricing model. With this design, our cross-AZ traffic amounted to half of the total cost of our Kafka platform.The Kafka cross-AZ traffic for this design can be broken down into three components as shown in Figure 1:  Producing (step 1): Typically, a single service produces data to a given Kafka topic. Cross-AZ traffic occurs when the producer does not reside in the same AZ as the partition leader it is producing data to. This cross-AZ traffic cost is minimal, because the data is transferred to a different AZ at most once (excluding retries).  Replicating (step 2): The ingested data is replicated from the partition leader to the two partition followers, which reside in two other AZs. The cost of this is also relatively small, because the data is only transferred to a different AZ twice.  Consuming (step 3): Most of the cross-AZ traffic occurs here because there are many consumers for a single Kafka topic. Similar to the producers, the consumers incur cross-AZ traffic when they do not reside in the same AZ as the partition leader. However, on the consuming side, cross-AZ traffic can occur as many times as there are consumers (on average, two-thirds of the number of consumers). The solution described in this article addresses this particular component of the cross-AZ traffic in the initial design.SolutionKafka 2.3 introduced the ability for consumers to fetch from partition replicas. This opens the door to a more cost-efficient design.  Figure 2 - Target design, consumers fetching from the closest replica  Step 3 of Figure 2 shows how consumers can now consume data from the replica that resides in their own AZ. Implementing this feature requires rack-awareness and extra configurations for both the Kafka brokers and consumers. We will describe this in the following sections.The Coban journeyKafka upgradeOur journey started with the upgrade of our legacy Kafka clusters. We decided to upgrade them directly to version 3.1, in favour of capturing bug fixes and optimisations over version 2.3. This was a safe move as version 3.1 was deemed stable for almost a year and we projected no additional operational cost for this upgrade.To perform an online upgrade with no disruptions for our users, we broke down the process into three stages.  Stage 1: Upgrading Zookeeper. All versions of Kafka are tested by the community with a specific version of Zookeeper. To ensure stability, we followed this same process. The upgraded Zookeeper would be backward compatible with the pre-upgrade version of Kafka which was still in use at this early stage of the operation.  Stage 2: Rolling out the upgrade of Kafka to version 3.1 with an explicit backward-compatible inter-broker protocol version (inter.broker.protocol.version). During this progressive rollout, the Kafka cluster is temporarily composed of brokers with heterogeneous Kafka versions, but they can communicate with one another because they are explicitly set up to use the same inter-broker protocol version. At this stage, we also upgraded Cruise Control to a compatible version, and we configured Kafka to import the updated cruise-control-metrics-reporter JAR file on startup.  Stage 3: Upgrading the inter-broker protocol version. This last stage makes all brokers use the most recent version of the inter-broker protocol. During the progressive rollout of this change, brokers with the new protocol version can still communicate with brokers on the old protocol version.ConfigurationEnabling Kafka consumers to fetch from the closest replica requires a configuration change on both Kafka brokers and Kafka consumers. They also need to be aware of their AZ, which is done by leveraging Kafka rack-awareness (1 “rack” = 1 AZ).BrokersIn our Kafka brokers’ configuration, we already had broker.rack set up to distribute the replicas across different AZs for resiliency. Our Ansible role for Kafka automatically sets it with the AZ ID that is dynamically retrieved from the EC2 instance’s metadata at deployment time.- name: Get availability zone ID  uri:    url: http://169.254.169.254/latest/meta-data/placement/availability-zone-id    method: GET    return_content: yes  register: ec2_instance_az_idNote that we use AWS AZ IDs (suffixed az1, az2, az3) instead of the typical AWS AZ names (suffixed 1a, 1b, 1c) because the latter’s mapping is not consistent across AWS accounts.Also, we added the new replica.selector.class parameter, set with value org.apache.kafka.common.replica.RackAwareReplicaSelector, to enable the new feature on the server side.ConsumersOn the Kafka consumer side, we mostly rely on Coban’s internal Kafka SDK in Golang, which streamlines how service teams across all Grab verticals utilise Coban Kafka clusters. We have updated the SDK to support fetching from the closest replica.Our users only have to export an environment variable to enable this new feature. The SDK then dynamically retrieves the underlying host’s AZ ID from the host’s metadata on startup, and sets a new client.rack parameter with that information. This is similar to what the Kafka brokers do at deployment time.We have also implemented the same logic for our non-SDK consumers, namely Flink pipelines and Kafka Connect connectors.ImpactWe rolled out fetching from the closest replica at the turn of the year and the feature has been progressively rolled out on more and more Kafka consumers since then.  Figure 3 - Variation of our cross-AZ traffic before and after enabling fetching from the closest replica  Figure 3 shows the relative impact of this change on our cross-AZ traffic, as reported by AWS Cost Explorer. AWS charges cross-AZ traffic on both ends of the data transfer, thus the two data series. On the Kafka brokers’ side, less cross-AZ traffic is sent out, thereby causing the steep drop in the dark green line. On the Kafka consumers’ side, less cross-AZ traffic is received, causing the steep drop in the light green line. Hence, both ends benefit by fetching from the closest replica.Throughout the observeration period, we maintained a relatively stable volume of data consumption. However, after three months, we observed a substantial 25% drop in our cross-AZ traffic compared to December’s average. This reduction had a direct impact on our cross-AZ costs as it directly correlates with the cross-AZ traffic volume in a linear manner.CaveatsIncreased end-to-end latencyAfter enabling fetching from the closest replica, we have observed an increase of up to 500ms in end-to-end latency, that comes from the producer to the consumers. Though this is expected by design, it makes this new feature unsuitable for Grab’s most latency-sensitive use cases. For these use cases, we retained the traditional design whereby consumers fetch directly from the partition leaders, even when they reside in different AZs.  Figure 4 - End-to-end latency (99th percentile) of one of our streams, before and after enabling fetching from the closest replica  Inability to gracefully isolate a brokerWe have also verified the behaviour of Kafka clients during a broker rotation; a common maintenance operation for Kafka. One of the early steps of our corresponding runbook is to demote the broker that is to be rotated, so that all of its partition leaders are drained and moved to other brokers.In the traditional architecture design, Kafka clients only communicate with the partition leaders, so demoting a broker gracefully isolates it from all of the Kafka clients. This ensures that the maintenance is seamless for them. However, by fetching from the closest replica, Kafka consumers still consume from the demoted broker, as it keeps serving partition followers. When the broker effectively goes down for maintenance, those consumers are suddenly disconnected. To work around this, they must handle connection errors properly and implement a retry mechanism.Potentially skewed loadAnother caveat we have observed is that the load on the brokers is directly determined by the location of the consumers. If they are not well balanced across all of the three AZs, then the load on the brokers is similarly skewed. At times, new brokers can be added to support an increasing load on an AZ. However, it is undesirable to remove any brokers from the less loaded AZs as more consumers can suddenly relocate there at any time. Having these additional brokers and underutilisation of existing brokers on other AZs can also impact cost efficiency.  Figure 5 - Average CPU utilisation by AZ of one of our critical Kafka clusters  Figure 5 shows the CPU utilisation by AZ for one of our critical Kafka clusters. The skewage is visible after 01/03/2023. To better manage this skewage in load across AZs, we have updated our SDK to expose the AZ as a new metric. This allows us to monitor the skewness of the consumers and take measures proactively, for example, moving some of them to different AZs.What’s next?We have implemented the feature to fetch from the closest replica on all our Kafka clusters and all Kafka consumers that we control. This includes internal Coban pipelines as well as the managed pipelines that our users can self-serve as part of our data streaming offering.We are now evangelising and advocating for more of our users to adopt this feature.Beyond Coban, other teams at Grab are also working to reduce their cross-AZ traffic, notably, Sentry, the team that is in charge of Grab’s service mesh.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/zero-traffic-cost"
      }
      ,
    
      "go-module-proxy": {
        "title": "Go module proxy at Grab",
        "author": "jerry-ng",
        "tags": "[&quot;Engineering&quot;]",
        "category": "",
        "content": "At Grab, we rely heavily on a large Go monorepo for backend development, which offers benefits like code reusability and discoverability. However, as we continue to grow, managing a large monorepo brings about its own set of unique challenges.As an example, using Go commands such as go get and go list can be incredibly slow when fetching Go modules residing in a large multi-module repository. This sluggishness takes a toll on developer productivity, burdens our Continuous Integration (CI) systems, and strains our Version Control System host (VCS), GitLab.In this blog post, we look at how Athens, a Go module proxy, helps to improve the overall developer experience of engineers working with a large Go monorepo at Grab.Key highlights  We reduced the time of executing the go get command from ~18 minutes to ~12 seconds when fetching monorepo Go modules.  We scaled in and scaled down our entire Athens cluster by 70% by utilising the fallback network mode in Athens along with Golang’s GOVCS mode, resulting in cost savings and enhanced efficiency.Problem statements and solutions1. Painfully slow performance of Go commandsProblem summary: Running the go get command in our monorepo takes a considerable amount of time and can lead to performance degradation in our VCS.When working with the Go programming language, go get is one of the most common commands that you’ll use every day. Besides developers, this command is also used by CI systems.What does go get do?The go get command is used to download and install packages and their dependencies in Go. Note that it operates differently depending on whether it is run in legacy GOPATH mode or module-aware mode. In Grab, we’re using the module-aware mode in a multi-module repository setup.    Every time go get is run, it uses Git commands, like git ls-remote, git tag, git fetch, etc, to search and download the entire worktree. The excessive use of these Git commands on our monorepo contributes to the long processing time and can be strenuous to our VCS.How big is our monorepo?To fully grasp the challenges faced by our engineering teams, it’s crucial to understand the vast scale of the monorepo that we work with daily. For this, we use git-sizer to analyse our monorepo.Here’s what we found:  Overall repository size: The monorepo has a total uncompressed size of 69.3 GiB, a fairly substantial figure. To put things into perspective, the Linux kernel repository, known for its vastness, currently stands at 55.8 GiB.  Trees: The total number of trees is 3.21M and tree entries are 99.8M, which consume 3.65 GiB. This may cause performance issues during some Git operations.  References: Totalling 10.7k references.  Biggest checkouts: There are 64.7k directories in our monorepo. This affects operations like git status and git checkout. Moreover, our monorepo has a maximum path depth of 20. This contributes to a slow processing time on Git and negatively impacts developer experience. The number of files (354k) and the total size of files (5.08 GiB) are also concerns due to their potential impact on the repository’s performance.To draw a comparison, refer to the git-sizer output of the Linux repository.How slow is “slow”?To illustrate the issue further, we will compare the time taken for various Go commands to fetch a single module in our monorepo at a 10 MBps download speed.This is an example of how a module is structured in our monorepo:gitlab.company.com/monorepo/go  |-- go.mod  |-- commons/util/gk        |-- go.mod            Go commands      GOPROXY      Previously cached?      Description      Result (time taken)                  go get -x gitlab.company.com/monorepo/go/commons/util/gk      proxy.golang.org,direct      Yes      Download and install the latest version of the module. This is a common scenario that developers often encounter.      18:50.71 minutes              go get -x gitlab.company.com/monorepo/go/commons/util/gk      proxy.golang.org,direct      No      Download and install the latest version of the module without any module cache      1:11:54.56 hour              go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk      proxy.golang.org,direct      Yes      List information about the module      3.873 seconds              go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk      proxy.golang.org,direct      No      List information about the module without any module cache      3:18.58 minutes      In this example, using go get to fetch a module took over 18 minutes to complete. If we needed to retrieve more than one module in our monorepo, it can be incredibly time-consuming.Why is it slow in a monorepo?In a large Go monorepo, go get commands can be slow due to several factors:  Large number of files and directories: When running go get, the command needs to search and download the entire worktree. In a large multi-module monorepo, the vast number of files and directories make this search process very expensive and time-consuming.  Number of refs: A large number of refs (branches or tags) in our monorepo can affect performance. Ref advertisements (git ls-remote), which contain every ref in our monorepo, are the first phase in any remote Git operation, such as git clone or git fetch. With a large number of refs, performance takes a hit when performing these operations.  Commit history traversal: Operations that need to traverse a repository’s commit history and consider each ref will be slow in a monorepo. The larger the monorepo, the more time-consuming these operations become.The consequences: Stifled productivity and strained systemsDevelopers and CIWhen Go command operations like go get are slow, they contribute to significant delays and inefficiencies in software development workflows. This leads to reduced productivity and demotivated developers.Optimising Go command operations’ speed is crucial to ensure efficient software development workflows and high-quality software products.Version Control SystemIt’s also worth noting that overusing go get commands can also lead to performance issues for VCS. When Go packages are frequently downloaded using go get, we saw that it caused a bottleneck in our VCS cluster, which can lead to performance degradation or even cause rate-limiting queue issues.This negatively impacts the performance of our VCS infrastructure, causing delays or sometimes unavailability for some users and CI.Solution: Athens + fallback Network Mode + GOVCS + Custom Cache Refresh SolutionProblem summary: Speed up go get command by not fetching from our VCSWe addressed the speed issue by using Athens, a proxy server for Go modules (read more about the GOPROXY protocol).How does Athens work?The following sequence diagram describes the default flow of go get command with Athens.    Athens uses a storage system for Go module packages, which can also be configured to use various storage systems such as Amazon S3, and Google Cloud Storage, among others.By caching these module packages in storage, Athens can serve the packages directly from storage rather than requesting them from an upstream VCS while serving Go commands such as go mod download and certain go build modes. However, just using a Go module proxy didn’t fully resolve our issue since the go get and go list commands still hit our VCS through the proxy.With this in mind, we thought “what if we could just serve the Go modules directly from Athens’ storage for go get?” This question led us to discover Athens network mode.What is Athens network mode?Athens NetworkMode configures how Athens will return the results of the Go commands. It can be assembled from both its own storage and the upstream VCS. As of Athens v0.12.1, it currently supports these 3 modes:  strict: merge VCS versions with storage versions, but fail if either of them fails.  offline: only get storage versions, never reach out to VCS.  fallback: only return storage versions, if VCS fails. Fallback mode does the best effort of giving you what’s available at the time of requesting versions.Our Athens clusters were initially set to use strict network mode, but this was not ideal for us. So we explored the other network modes.Exploring offline modeWe initially sought to explore the idea of putting Athens in offline network mode, which would allow Athens to serve Go requests only from its storage. This concept aligned with our aim of reducing VCS hits while also leading to significant performance improvement in Go workflows.    However in practice, it’s not an ideal approach. The default Athens setup (strict mode) automatically updates the module version when a user requests a new module version. Nevertheless, switching Athens to offline mode would disable the automatic updates as it wouldn’t connect to the VCS.Custom cache refresh solutionTo solve this, we implemented a CI pipeline that refreshes Athens’ module cache whenever a new module is released in our monorepo. Employing this with offline mode made Athens effective for the monorepo but it resulted in the loss of automatic updates for other repositoriesRestoring this feature requires applying our custom cache refresh solution to all other Go repositories. However, implementing this workaround can be quite cumbersome and significant additional time and effort. We decided to look for another solution that would be easier to maintain in the long run.A balanced approach: fallback Mode and GOVCSThis approach builds upon our aforementioned custom cache refresh which is specifically designed for the monorepo.We came across the GOVCS environment variable, which we use in combination with the fallback network mode to effectively put only the monorepo in “offline” mode.When GOVCS is set to gitlab.company.com/monorepo/go:off, Athens encounters an error whenever it tries to fetch modules from VCS:gitlab.company.com/monorepo/go/commons/util/gk@v1.1.44: unrecognized import path \"gitlab.company.com/monorepo/go/commons/util/gk\": GOVCS disallows using git for private gitlab.company.com/monorepo/go; see 'go help vcs'If Athens network mode is set to strict, Athens returns 404 errors to the user. By switching to fallback mode, Athens tries to retrieve the module from its storage if a GOVCS failure occurs.Here’s the updated Athens configuration (example default config):GoBinaryEnvVars = [\"GOPROXY=direct\", \"GOPRIVATE=gitlab.company.com\", \"GOVCS=gitlab.company.com/monorepo/go:off\"]NetworkMode = \"fallback\"With the custom cache refresh solution coupled with this approach, we not only accelerate the retrieval of Go modules within the monorepo but also allow for automatic updates for non-monorepo Go modules.Final resultsThis solution resulted in a significant improvement in the performance of Go commands for our developers. With Athens, the same command is completed in just ~12 seconds (down from ~18 minutes), which is impressively fast.            Go commands      GOPROXY      Previously cached?      Description      Result (time taken)                  go get -x gitlab.company.com/monorepo/go/commons/util/gk      goproxy.company.com      Yes      Download and install the latest version of the module. This is a common scenario that developers often encounter.      11.556 seconds              go get -x gitlab.company.com/monorepo/go/commons/util/gk      goproxy.company.com      No      Download and install the latest version of the module without any module cache      1:05.60 minutes              go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk      goproxy.company.com      Yes      List information about the monorepo module      0.592 seconds              go list -x -m -json -versions gitlab.company.com/monorepo/go/util/gk      goproxy.company.com      No      List information about the monorepo module without any module cache      1.023 seconds        Average cluster CPU utlisation    Average cluster memory utlisation  In addition, this change to our Athens cluster also leads to substantial reduction in average cluster CPU and memory utilisation. This also enabled us to scale in and scale down our entire Athens cluster by 70%, resulting in cost savings and enhanced efficiency. On top of that, we were also able to effectively eliminate VCS’s rate-limiting issues while making the monorepo’s command operation considerably faster.2. Go modules in GitLab subgroupsProblem summary: Go modules are unable to work natively with private or internal repositories under GitLab subgroups.When it comes to managing code repositories and packages, GitLab subgroups and Go modules have become an integral part of the development process at Grab. Go modules help to organise and manage dependencies, and GitLab subgroups provide an additional layer of structure to group related repositories together.However, a common issue when using Go modules is that they do not work natively with private or internal repositories under a GitLab subgroup (see this GitHub issue).For example, using go get to retrieve a module from gitlab.company.com/gitlab-org/subgroup/repo will result in a failure. This problem is not specific to Go modules, all repositories under the subgroup will face the same issue.A cumbersome workaroundTo overcome this issue, we had to use workarounds. One workaround is to authenticate the HTTPS calls to GitLab by adding authentication details to the .netrc file on your machine.The following lines can be added to the .netrc file:machine gitlab.company.com    login user@company.com    password &lt;personal-access-token&gt;In our case, we are using a Personal Access Token (PAT) since we have 2FA enabled. If 2FA is not enabled, the GitLab password can be used instead. However, this approach would mean configuring the .netrc file in the CI environments as well as on the machine of every Go developer.Solution: Athens + .netrcA feasible solution is to set up the .netrc file in the Go proxy server. This method eliminates the need for N number of developers to configure their own .netrc files. Instead, the responsibility for this task is delegated to the Go proxy server.3. Sharing common librariesProblem summary: Distributing internal common libraries within a monorepo without granting direct repository access can be challenging.At Grab, we work with various cross-functional teams, and some could have distinct network access like different VPNs. This adds complexity to sharing our monorepo’s internal common libraries with them. To maintain the security and integrity of our monorepo, we use a Go proxy for controlled access to necessary libraries.The key difference between granting direct access to the monorepo via VCS and using a Go proxy is that the former allows users to read everything in the repository, while the latter enables us to grant access only to the specific libraries users need within the monorepo. This approach ensures secure and efficient collaboration across diverse network configurations.Without Go module proxyWithout Athens, we would need to create a separate repository to store the code we want to share and then use a build system to automatically mirror the code from the monorepo to the public repository.This process can be cumbersome and lead to inconsistencies in code versions between the two repositories, ultimately making it challenging to maintain the shared libraries.Furthermore, copying code can lead to errors and increase the risk of security breaches by exposing confidential or sensitive information.Solution: Athens + Download Mode FileTo tackle this problem statement, we utilise Athens’ download mode file feature using an allowlist approach to specify which repositories can be downloaded by users.Here’s an example of the Athens download mode config file:downloadURL = \"https://proxy.golang.org\"mode = \"sync\"download \"gitlab.company.com/repo/a\" {    mode = \"sync\"}download \"gitlab.company.com/repo/b\" {    mode = \"sync\"}download \"gitlab.company.com/*\" {    mode = \"none\"}In the configuration file, we specify allowlist entries for each desired repo, including their respective download modes. For example, in the snippet above, repo/a and repo/b are allowed (mode = “sync”), while everything else is blocked using mode = “none”.Final resultsBy using Athens’ download mode feature in this case, the benefits are clear. Athens provides a secure, centralised place to store Go modules. This approach not only provides consistency but also improves maintainability, as all code versions are managed in one single location.Additional benefits of Go proxyAs we’ve touched upon the impressive results achieved by implementing Athens Go proxy at Grab, it’s crucial to explore the supplementary advantages that accompany this powerful solution.These unsung benefits, though possibly overlooked, play a vital role in enriching the overall developer experience at Grab and promoting more robust software development practices:  Module immutability: ​​As the software world continues to face issues around changing or disappearing libraries, Athens serves as a useful tool in mitigating build disruptions by providing immutable storage for copied VCS code. The use of a Go proxy also ensures that builds remain deterministic, improving consistency across our software.  Uninterrupted development: Athens allows users to fetch dependencies even when VCS is down, ensuring continuous and seamless development workflows.  Enhanced security: Athens offers access control by enabling the blocking of specific packages within Grab. This added layer of security protects our work against potential risks from malicious third-party packages.  Vendor directory removal: Athens prepares us for the eventual removal of the vendor directory, fostering faster workflows in the future.What’s next?Since adopting Athens as a Go module proxy, we have observed considerable benefits, such as:  Accelerated Go command operations  Reduced infrastructure costs  Mitigated VCS load issuesMoreover, its lesser-known advantages like module immutability, uninterrupted development, enhanced security, and vendor directory transition have also contributed to improved development practices and an enriched developer experience for Grab engineers.Today, the straightforward process of exporting three environment variables has greatly influenced our developers’ experience at Grab.export GOPROXY=\"goproxy.company.com|proxy.golang.org,direct\"export GONOSUMDB=\"gitlab.company.com\"export GONOPROXY=\"none\"At Grab, we are always looking for ways to improve and optimise the way we work, so we contribute to open-sourced projects like Athens, where we help with bug fixes. If you are interested in setting up a Go module proxy, do give Athens (github.com/gomods/athens) a try!Special thanks to Swaminathan Venkatraman, En Wei Soh, Anuj More, Darius Tan, and Fernando Christyanto for contributing to this project and this article.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/go-module-proxy"
      }
      ,
    
      "pii-masking": {
        "title": "PII masking for privacy-grade machine learning",
        "author": "fabrice-harbulot",
        "tags": "[&quot;Engineering&quot;, &quot;Privacy&quot;, &quot;Data masking&quot;, &quot;Machine learning&quot;]",
        "category": "",
        "content": "At Grab, data engineers work with large sets of data on a daily basis. They design and build advanced machine learning models that provide strategic insights using all of the data that flow through the Grab Platform. This enables us to provide a better experience to our users, for example by increasing the supply of drivers in areas where our predictive models indicate a surge in demand in a timely fashion.Grab has a mature privacy programme that complies with applicable privacy laws and regulations and we use tools to help identify, assess, and appropriately manage our privacy risks. To ensure that our users’ data are well-protected and avoid any human-related errors, we always take extra measures to secure this data.However, data engineers will still require access to actual production data in order to tune effective machine learning models and ensure the models work as intended in production.In this article, we will describe how the Grab’s data streaming team (Coban), along with the data platform and user teams, have enforced Personally Identifiable Information (PII) masking on machine learning data streaming pipelines. This ensures that we uphold a high standard and embody a privacy by design culture, while enabling data engineers to refine their models with sanitised production data.PII taggingData streaming at Grab leverages the Protocol Buffers (protobuf) data format to structure in-transit data. When creating a new stream, developers must describe its fields in a protobuf schema that is then used for serialising the data wherever it is sent over the wire, and deserialising it wherever it is consumed.A fictional example schema looks like this (the indexes are arbitrary, but commonly created in sequence):message Booking {  string bookingID = 1;  int64 creationTime = 2;  int64 passengerID = 3;  string passengerName = 4;  ... truncated output ...}Over here, the fourth field passengerName involves a PII and the data pertaining to that field should never be accessible by any data engineer. Therefore, developers owning the stream must tag that field with a PII label like this:import \"streams/coban/options/v1/pii.proto\";message Booking {  string bookingID = 1;  int64 creationTime = 2;  int64 passengerID = 3;  string passengerName = 4 [(streams.coban.options.v1.pii_type) = PII_TYPE_NAME];  ... truncated output ...}The imported pii.proto library defines the tags for all possible types of PII. In the example above, the passengerName field has not only been flagged as PII, but is also marked as PII_TYPE_NAME – a specific type of PII that conveys the names of individuals. This high-level typing enables more flexible PII masking methods, which we will explain later.Once the PII fields have been properly identified and tagged, developers need to publish the schema of their new stream into Coban’s Git repository. A Continuous Integration (CI) pipeline described below ensures that all fields describing PII are correctly tagged.The following diagram shows this CI pipeline in action.  Fig. 1 CI pipeline failure due to untagged PII fields  When a developer creates a Merge Request (MR) or pushes a new commit to create or update a schema (step 1), the CI pipeline is triggered. It runs an in-house Python script that scans each variable name of the committed schema and tests it against an extensive list of PII keywords that is regularly updated, such as name, address, email, phone, etc (step 2). If there is a match and the variable is not tagged with the expected PII label, the pipeline fails (step 3) with an explicit error message in the CI pipeline’s output, similar to this:Field name [Booking.passengerName] should have been marked with type streams.coban.options.v1.pii_type = PII_TYPE_NAMEThere are cases where a variable name in the schema is a partial match against a PII keyword but is legitimately not a PII – for example, carModelName is a partial match against name but does not contain PII data. In this case, the developer can choose to add it to a whitelist to pass the CI.However, modifying the whitelist requires approval from the Coban team for verification purposes. Apart from this particular case, the requesting team can autonomously approve their MR in a self-service fashion.Now let us look at an example of a successful CI pipeline execution.  Fig. 2 CI pipeline success and schema publishing  In Fig. 2, the committed schema (step 1) is properly tagged so our in-house Python script is unable to find any untagged PII fields (step 2). The MR is approved by a code owner (step 3), then merged to the master branch of the repository (step 4).Upon merging, another CI pipeline is triggered to package the protobuf schema in a Java Archive (JAR) of Scala classes (step 5), which in turn is stored into a package registry (step 6). We will explain the reason for this in a later section.Production environmentWith the schemas published and all of their PII fields properly tagged, we can now take a look at the data streaming pipelines.  Fig. 3 PII flow in the production environment  In this example, the user generates data by interacting with the Grab superapp and making a booking (step 1). The booking service, compiled with the stream’s schema definition, generates and produces Kafka records for other services to consume (step 2). Among those consuming services are the production machine learning pipelines that are of interest to this article (step 3).PII is not masked in this process because it is actually required by the consuming services. For example, the driver app needs to display the passenger’s actual name, so the driver can confirm their identity easily.At this part of the process, this is not much of a concern because access to the sacrosanct production environment is highly restricted and monitored by Grab.PII maskingTo ensure the security, stability, and privacy of our users, data engineers who need to tune their new machine learning models based on production data are not granted access to the production environment. Instead, they have access to the staging environment, where production data is mirrored and PII is masked.  Fig. 4 PII masking pipeline from the production environment to the staging environment  The actual PII masking is performed by an in-house Flink application that resides in the production environment. Flink is a reference framework for data streaming that we use extensively. It is also fault tolerant, with the ability to restart from a checkpoint.The Flink application is compiled along with the JAR containing the schema as Scala classes previously mentioned. Therefore, it is able to consume the original data as a regular Kafka consumer (step 1). It then dynamically masks the PII of the consumed data stream, based on the PII tags of the schema (step 2). Ultimately, it produces the sanitised data to the Kafka cluster in the staging environment as a normal Kafka producer (step 3).Depending on the kind of PII, there are several methods of masking such as:  Names and strings of characters: They are replaced by consistent HMAC (Hash-based message authentication code). A HMAC is a digest produced by a one-way cryptographic hash function that takes a secret key as a parameter. Leveraging a secret key here is a defence against chosen plaintext attacks, i.e. computing the digest of a particular plaintext, like a targeted individual’s name.  Numbers and dates: Similarly, they are transformed in a consistent manner, by leveraging a random generator that takes the unmasked value as a seed, so that the same PII input consistently produces the same masked output.Note that consistency is a recurring pattern. This is because it is a key requirement for certain machine learning models.This sanitised data produced to the Kafka cluster in the staging environment is then consumed by the staging machine learning pipelines (step 4). There, it is used by data engineers to tune their models effectively with near real-time production data (step 5).The Kafka cluster in the staging environment is secured with authorisation and authentication (see Zero Trust with Kafka). This is an extra layer of security in case some PII data inadvertently fall through the cracks of PII tagging, following the defence in depth principle.Finally, whenever a new PII-tagged field is added to a schema, the PII masking Flink application needs to be compiled and deployed again. If the schema is not updated, the Flink pipeline is unable to decode this new field when deserialising the stream. Thus, the added field is just dropped and the new PII data does not make it to the staging environment.What’s next?For the immediate next steps, we are going to enhance this design with an in-house product based on AWS Macie to automatically detect the PII that would have fallen through the cracks. Caspian, Grab’s data lake team and one of Coban’s sister teams, has built a service that is already able to detect PII data in relational databases and data lake tables. It is currently being adapted for data streaming.In the longer run, we are committed to taking our privacy by design posture to the next level. Indeed, the PII masking described in this article does not prevent a bad actor from retrieving the consistent hash of a particular individual based on their non-PII data. For example, the target might be identifiable by a signature in the masked data set, such as unique food or transportation habits.A possible counter-measure could be one or a combination of the following techniques, ordered by difficulty of implementation:  Data minimisation: Non-essential fields in the data stream should not be mirrored at all. E.g. fields of the data stream that are not required by the data engineers to tune their models. We can introduce a dedicated tag in the schema to flag those fields and instruct the mirroring pipeline to drop them. This is the most straightforward approach.  Differential privacy: The mirroring pipeline could introduce some noise in the mirrored data, in a way that would obfuscate the signatures of particular individuals while still preserving the essential statistical properties of the dataset required for machine learning. It happens that Flink is a suitable framework to do so, as it can split a stream into multiple windows and apply computation over those windows. Designing and generalising a logic that meets the objective is challenging though.  PII encryption at source: PII could be encrypted by the producing services (like the booking service), and dynamically decrypted where plaintext values are required. However, key management and performance are two tremendous challenges of this approach.We will explore these techniques further to find the solution that works best for Grab and ensures the highest level of privacy for our users.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/pii-masking"
      }
      ,
    
      "performance-bottlenecks-go-apps": {
        "title": "Performance bottlenecks of Go application on Kubernetes with non-integer (floating) CPU allocation",
        "author": "shubham-badkuralvis-chew",
        "tags": "[&quot;Engineering&quot;]",
        "category": "",
        "content": "Grab’s real-time data platform team, Coban, has been running its stream processing framework on Kubernetes, as detailed in Plumbing at scale. We’ve also written another article (Scaling Kafka consumers) about vertical pod autoscaling (VPA) and the benefits of using it.    In this article, we cover the performance bottlenecks and other issues we came across for Go applications on Kubernetes.BackgroundWe noticed CPU throttling issues on some pipelines leading to consumption lag, which meant there was a delay between data production and consumption. This was an issue because the data might no longer be relevant or accurate when it gets consumed. This led to incorrect data-driven conclusions, costly mistakes, and more.While debugging this issue, we focused primarily on the SinktoS3 pipeline. It is essentially used for sinking data from Kafka topics to AWS S3. Depending on your requirements, data sinking is primarily for archival purposes and can be used for analytical purposes.InvestigationAfter conducting a thorough investigation, we found two main issues:  Resource throttling  Issue with VPAResource throttlingWe redesigned our SinktoS3 pipeline architecture to concurrently perform the most CPU intensive operations using parallel goroutines (workers). This improved performance and considerably reduced consumer lag.But the high-performance architecture needed more intensive resource configuration. As mentioned in Scaling kafka consumers, VPA helps remove manual resource configuration. So, we decided to let the SinktoS3 pipeline run on VPA, but this exposed a new set of problems.We tested our hypothesis on one of the highest traffic pipelines with parallel goroutines (workers). When the pipeline was left running on VPA, it tried optimising the resources by slowly reducing from 2.5 cores to 2.05 cores, and then to 1.94 cores.  CPU requests dropped from 2.05 cores to 1.94 cores, since the maximum performance can be seen at ~1.7 cores.  As you can see from the image above, CPU usage and performance reduced significantly after VPA changed the CPU cores to less than 2. The pipeline ended up with a huge backlog to clear and although it had resources on pod (around 1.94 cores), it did not process any faster and instead, slowed down significantly, resulting in throttling.    From the image above, we can see that after VPA scaled the limits of CPU down to 1.94 cores per pod, there was a sudden drop in CPU usage in each of the pods.  Stream production rate  You can see that at 21:00, CPU usage reached a maximum of 80%. This value dropped to around 50% between 10:00 to 12:00, which is our consecutive peak production rate.  Significant drop in consumption rate from Day_Before    Consumer lag in terms of records pending to be consumed and in terms of minutes  In the image above, we compared this data with trends from previous data, where the purple line indicates the day before. We noticed a significant drop in consumption rate compared to the day before, which resulted in consumer lag. This drop was surprising since we didn’t tweak the application configuration. The only change was done by VPA, which brought the CPU request and limit down to less than 2 cores.To revert this change, we redeployed the pipeline by retaining the same application setting but adjusting the minimum VPA limit to 2 cores. This helps to prevent VPA from bringing down the CPU cores below 2. With this simple change, performance and CPU utilisation improved almost instantly.  CPU usage percentage jumped back up to ~95%    Pipeline consumption rate compared to Day_Before  In the image above, we compared the data with trends from the day before (indicated in purple), where the pipeline was lagging and had a large backlog. You can see that the improved consumption rate was even better than the day before and the application consumed even more records. This is because it was catching up on the backlog from the previous consumer lag.Deep dive into the root causeThis significant improvement just from increasing CPU allocation from 1.94 to 2 cores was unexpected as we had AUTO-GOMAXPROCS enabled in our SPF pipelines and this only uses integer values for CPU.Upon further investigation, we found that the GOMAXPROCS is useful to control the CPU that golang uses on a kubernetes node when kubernetes Cgroup masks the actual CPU cores of the nodes. GOMAXPROCS only allocates the requested resources of the pod, hence configuring this value correctly helps the runtime to preallocate the correct CPU resources.Without configuring GOMAXPROCS, go runtime assumes the node’s entire CPU capacity is available for its execution, which is sub-optimal when we run the Golang application on Kubernetes. Thus, it is important to configure GOMAXPROCS correctly so your application pre-allocates the right number of threads based on CPU resources. More details can be found in this article.Let’s look at how Kubernetes resources relate to GOMAXPROCS value in the following table:            Kubernetes resources      GOMAXPROCS value      Remarks                  2.5 core      2      Go runtime will just take and utilise 2 cores efficiently.              2 core      2      Go runtime will take and utilise the maximum CPU of the pod efficiently if the workload requires it.              1.5 core      1      AUTO-GOMAXPROCS will set the value as 1 since it rounds down the non-integer CPU value to an integer number. Hence the performance will be the same as if you had 1 core CPU.              0.5 core      1      AUTO-GOMAXPROCS will set the value as 1 CPU as the minimum allowed value for GOMAXPROCS is 1. Here we will see some throttling as Kubernetes will only give 0.5 core but runtime configures itself as it would have 1  hence it will starve for a few CPU cycles.      Issue with VPAThe vertical pod autoscaler enables you to easily scale pods vertically so you don’t have to make manual adjustments. It automatically allocates resources based on usage and allows proper scheduling so that there will be appropriate resources available for each pod. However, in our case, the throttling and CPU starvation issue was because VPA brought resources down to less than 2 cores.To better visualise the issue, let’s use an example. Assume that this application needs roughly 1.7 cores to perform all its operations without any resource throttling. Let’s see how the VPA journey in this scenario looks like and where it will fail to correctly scale.            Timeline      VPA recommendation      CPU Utilisation      AUTO-GOMAXPROCS      Remarks                  T0      0.5 core      &gt;90%      1      Throttled by Kubernetes Cgroup as it does give only 0.5 core.              T1      1 core      &gt;90%      1      CPU utilisation will still be &gt;90% as GOMAXPROCS setting for the application remains the same. In reality, it will need even more.              T2      1.2 core      &lt;85%      1      Since the application actually needs more resources, VPA sets a non-integer value but GOMAXPROCS never utilised that extra resource and continued to throttle. Now, VPA computes that the CPU is underutilised and it won't scale further.              T3      2 core (manual override)      80-90%      2      Since the application has enough resources, it will perform most optimally without throttling and will have maximum throughput.      SolutionDuring our investigation, we saw that AUTO-GOMAXPROCS sets an integer value (minimum 1). To avoid CPU throttling, we need VPA to propose integer values while scaling.In v0.13 of VPA, this feature is available but only for Kubernetes versions ≥1.25 – see #5313 in the image below.    We acknowledge that if we define a default minimum integer CPU value of 1 core for Coban’s stream processing pipelines, it might be excessive for those that only require less than 1 core. So we propose to only enable this default setting for pipelines with heavy resource requirements and require more than 1 core.That said, you should make this decision by evaluating your application’s needs. For example, some Coban pipelines still run on VPA with less than one core but they do not experience any lag. As we mentioned earlier  AUTO-GOMAXPROCS would be configured to 1 in this case, still they can catch up with message production rates. However, technically these pipelines are actually throttled and do not perform optimally but these pipelines don’t have consumer lag.As we move from single to concurrent goroutine processing, we need more intensive CPU allocation. In the following table, we consider some scenarios where we have a few pipelines with heavy workloads that are not able to catch up with the production rate.            Actual CPU requirement      VPA recommendation (after upgrade to v0.13)      GOMAXPROCS value      Remarks                  0.8      1 core      1      Optimal setting for this pipeline. It should not lag and should utilise the CPU resources optimally via concurrent goroutines.              1.2      2      2      No CPU throttling and no lag. But not very cost efficient.              1.8      2      2      Optimal performance with no lag and cost efficiency.      Learnings/ConclusionFrom this experience, we learnt several things:  Incorrect GOMAXPROCS configuration can lead to significant throttling and CPU starvation issues.  Autoscaling solutions are important, but can only take you so far. Depending on your application needs, manual intervention might still be needed to ensure optimal performance.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/performance-bottlenecks-go-apps"
      }
      ,
    
      "ios-ci-infrastructure-with-observability-tools": {
        "title": "How we improved our iOS CI infrastructure with observability tools",
        "author": "bunty-madankrist-foodenis-sakhapov",
        "tags": "[&quot;iOS&quot;, &quot;Mobile&quot;, &quot;Engineering&quot;, &quot;UITesting&quot;]",
        "category": "",
        "content": "Note: Timestamps used in this article are in UTC+8 Singapore time, unless stated otherwise.BackgroundWhen we upgraded to Xcode 13.1 in April 2022, we noticed a few issues such as instability of the CI tests and other problems related to the switch to Xcode 13.1. After taking a step back, we investigated this issue by integrating some observability tools into our iOS CI development process. This gave us a comprehensive perspective of the entire process, from the beginning to the end of the UITest job. In this article, we share the improvements we made, the insights we gathered, and the impact of these improvements on the overall process and resource utilisation.SolutionIn the following sections, we elaborate the various steps we took to investigate the issues, like unstable CI tests and high CPU utilisation, and the improvements we made to make our iOS CI infrastructure more reliable.Analyse Xcode 13.1 CPU utilisationAs an iOS developer, we are certain that you have also experienced Spotlight process-related CPU usage problems with Xcode 13.1, which have since been resolved in Xcode 13.2. After investigating, we found that the CPU usage issues were one of the root causes of UITest’s instability and it was something we needed to fix urgently. We decided not to wait for Apple’s update as it would cost us more time to perform another round of migration.Before we started UITest, we moved the spotlight.app into a new folder. When the test was complete, we restored the application to its original location. This significantly decreased CPU utilisation by more than 50%.This section helps you better visualise how the different versions of Xcode affected CPU utilisation.  Xcode 12.1    Xcode 13.1 before fix    Xcode 13.1 after fix  Remove iOS Safari’s dependency during deep link testingAs a superapp, there are countless scenarios that need to be thoroughly tested at Grab before the feature is released in production. One of these tests is deep link testing.More than 10% of the total number of tests are deep link tests. Typically, it is advised to mock the dependencies throughout the test to ensure that it runs quickly and reliably. However, this creates another reliance on iOS Safari.As a result, we created a mock browser in UITest. We used the URL to the mock browser as the launch argument, and the same URL is then called back. This method results in a 20% reduction in CI time and more stable tests.    Boot the iOS simulator with permissionIt is always a good idea to reset the simulator before running UITest so that there are no residual presets or simulated data from a different test. Additionally, using any of the simulator’s services (location, ATT, contacts, etc.) will prompt the simulator to request permission, which slows down execution. We used UIInterruptionHandler (a handler block for managing alerts and other dialogues) to manage asynchronous UI interruptions during the test.We wanted to reduce the time taken for test execution, which we knew includes many permissions. Therefore, in order to speed up execution, we boot the simulator with permissions. This removes the need for permissions during UITest, which speeds up performance by 5%.    Monitor HTTP traffic during the UITestWhen writing tests, it is important to mock all resources as this enables us to focus on the code that’s being tested and not how external dependencies interact or respond. However, with a large team working concurrently, it can be challenging to ensure that nothing is actually downloaded from the internet.Developers often make changes to code, and UITests are essential for ensuring that these modifications do not adversely affect existing functionality. It is advised to mock all dependencies while writing tests to simulate all possible behavior. We discovered that a significant number of resources were being downloaded each time we ran the tests, which was highly inefficient.In large teams working simultaneously, preventing downloads from the internet can be quite challenging. To tackle this issue, we devised a custom tool that tracks all URLs accessed throughout the UITest. This enabled us to identify resources being downloaded from the internet during the testing process.By using our custom tool to analyse network traffic, we were able to ensure that no resources were being downloaded during testing. Instead, we relied on mocked dependencies, resulting in reduced testing times and improved stability.GitLab load runner analysisAt Grab, we have many teams of developers who maintain the app, make code changes, and raise merge requests (MRs) on a daily basis. To make sure that new changes don’t conflict with existing code, these MRs are integrated with CI.Additionally, to manage the number of MRs, we maintain a list of clusters that run test runners concurrently for better resource utilisation and performance. We frequently run these tests to determine how many parallel processors are required for stable results.####Return HTTP responses to the local mock serverWe have a tool that we use to mock API requests, which we improved to also support HTML responses. This increases the scope of testing and ensures the HTML response sequences work properly.Use explicit waiting commandsWhen running multiple tests, timing issues are inevitable and they cause tests to occasionally pass and fail. To mitigate this, most of the developers prefer to add a sleep command so there is time for the element to render properly before we verify it – but this slows down execution. In order to improve CI execution, we introduced a link that allows us to track sleep function usage and suggest developers use waitForExistence wrappers in UI tests.Track each failure stateWith large codebases, it is quite common to see flakiness in UITests, where tests occasionally succeed and fail without any code changes. This means that test results can be inconsistent and in some cases, faulty. Faulty testing can be frustrating, and quite expensive. This is because engineers need to re-trigger entire builds, which ends up consuming more time.Initially, we used an internal tool that required all tests to pass on the first run, before merging was allowed. However, we realised that this significantly increased engineers’ manual retry time, hence, we modified the rules to allow merging as long as a subsequent retry passes the tests. This minor change improved our engineers’ CI overall experience and did not result in more flaky tests.    Learnings/ConclusionOur journey to improve iOS CI infrastructure is still ongoing, but from this experience, we learnt several things:  Focus on the feature being tested by ensuring all external responses are mocked.  A certain degree of test flakiness is expected, but you should monitor past trends. If flakiness increases, there’s probably a deeper lying issue within your code.  Regularly monitor resource utilisation and performance – detecting a sudden spike early could save you a lot of time and money.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/iOS-CI-infrastructure-with-observability-tools"
      }
      ,
    
      "faster-using-the-go-plugin-to-replace-lua-vm": {
        "title": "2.3x faster using the Go plugin to replace Lua virtual machine",
        "author": "yonghao-hufabianto-wangsamulya",
        "tags": "[&quot;Engineering&quot;, &quot;Virtual machines&quot;, &quot;Faster&quot;, &quot;Go plugin&quot;, &quot;Lua VM&quot;]",
        "category": "",
        "content": "AbstractWe’re excited to share with you the latest update on our open-source project Talaria. In our efforts to improve performance and overcome infrastructure limitations, we’ve made significant strides by implementing the Go plugin to replace Lua VM.Our team has found that the Go plugin is roughly 2.3x faster and uses 2.3x less memory than the Lua VM. This significant performance boost has helped us improve overall functionality, scalability, and speed.For those who aren’t familiar, Talaria is a distributed, highly available, and low-latency time-series database that’s designed for Big Data systems. Originally developed and implemented at Grab, Talaria is a critical component in processing millions and millions of transactions and connections every day, which demands scalable, data-driven decision-making.BackgroundOne of the methods we previously used for processing ingested data was Lua script. This method allowed users to customise the ingestion process, providing a high degree of flexibility.The config below is an example of using Lua script to JSON encode the row as a data column:computed:  - name: data    type: json    func: |      local json = require(\"json\")      function main(row)        return json.encode(row)      end     ProblemWe found that loading a Lua script required launching a Lua virtual machine (VM) to execute the script, which had a significant impact on performance, especially when ingesting large amounts of events.This performance issue led us to reevaluate our approach to processing ingested data and make changes to improve Talaria’s performance.As a result, this is the code we used on Lua VM to run the trim, remove keys “key1”, “key2”, “key3”, “key4”, “key5”, in the ingested data:import \"github.com/kelindar/lua\"func luaTrim() string {    s, err := lua.FromString(\"test.lua\", `    local json = require(\"json\")    local keys = {    \"key1\", \"key2\", \"key3\", \"key4\", \"key5\",    }    function main(input)        local data = json.decode(input)        for i, key in ipairs(keys) do            data[key] = nil        end        return json.encode(data)    end`)    if err != nil {        panic(err)    }    result, err := s.Run(context.Background(), jsonstr)    if err != nil {        panic(err)    }    return result.String()}Here is the benchmark, using Lua VM is 1000 times slower and uses 1000 times more memory than Golang’s native function on a Trim function:            BenchmarkTrim-12        543541       2258 ns/op      848 B/op      12 allocs/op                  BenchmarkLuaTrim-12       553      2105506 ns/op      5006319 B/op      10335 allocs/op      But, anything can be improved by adding a cache, what if we cache the Lua VM and reuse them? Here is the new improved benchmark:            BenchmarkTrim-8      232105       4995 ns/op       2192 B/op       53 allocs/op                  BenchmarkLuaTrim-8      97536      12108 ns/op       4573 B/op       121 allocs/op      So we can conclude that Lua VMs are roughly 2.3x faster and use 2.3x less memory than Golang’s native function.Use the Go plugin as Lua VM to execute custom codeWe came up with the idea of using a Linux shared library to execute the custom function instead of using Lua VM to run the custom script. Maybe you will be more familiar with the files with suffixes .so; they are shared libraries designed to package similar functionality in a single unit and shared with other developers so that they can call the function without writing it again.In Golang, a similar idea is called Go plugin, which allows you to build Golang code as a shared library (Golang names it a plugin). Open this file and call the Go function inside this plugin.How to use the Go pluginLet’s say you have a function F that wants to be called via the plugin.package mainimport \"fmt\"func F() { fmt.Printf(\"Hello, world\") }After writing the function F, you can compile it as a Go plugin file f_plugin.so via Go build -buildmode=plugin -o f_plugin.so. And you can open the file and use the function F like this:p, err := plugin.Open(\"f_plugin.so\")if err != nil {    panic(err)}f, err := p.Lookup(\"F\")if err != nil {    panic(err)}f.(func())() // prints \"Hello, world\"Go plugin benchmarkHere is the result that compares Golang native function, Golang plugin call.Golang native function: 2.3x faster and 2.3x lesser memory than using the Lua VM.Golang plugin call has almost the same performance as Golang native function.            BenchmarkNativeFunctionCall-12      2917465       401.7 ns/op       200 B/op      6 allocs/op                  BenchmarkPluginCall-12      2778988       447.1 ns/op        200 B/op       6 allocs/op      Integrated into TalariaThis is the MR we integrated the Go plugin into Talaria: https://github.com/talariadb/talaria/pull/87, adding it as a loader like LuaLoader.They both implemented the Handler interfaces.type Handler interface {    Load(uriOrCode string) (Handler, error)    String() string    Value(map[string]interface{}) (interface{}, error)}The implementation of this interface is listed here:For Lua loaderLoad: Load the Lua code or Lua script file path (local file path or s3 path) as the loader.String: Return “lua” so that we can call it to get what the loader is.Value: Run the Lua script, and take the arg as input.For Go plugin loaderLoad: Read the plugin file path (local file path or s3 path) as the plugin, lookup the function name defined by the user, save the function for later use.String: Return “plugin” so that we can call it to get what the loader is.Value: Run the saved function, take the arg as input.Things you need to noticeThe Go version you used to build the  Golang plugin must be the same as the service used in that plugin. We use Docker to build the service, so that we can ensure the Go version is the same.Reference (Benchmark plugin and LUA)https://github.com/atlas-comstock/talaria_benchmark/tree/master/benchmark_plugin_and_luaJoin usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/faster-using-the-go-plugin-to-replace-Lua-VM"
      }
      ,
    
      "safer-flink-deployments": {
        "title": "Safer deployment of streaming applications",
        "author": "shikai-ng",
        "tags": "[&quot;Engineering&quot;, &quot;Deployments&quot;, &quot;Streaming applications&quot;]",
        "category": "",
        "content": "The Flink framework has gained popularity as a real-time stateful stream processing solution for distributed stream and batch data processing. Flink also provides data distribution, communication, and fault tolerance for distributed computations over data streams. To fully leverage Flink’s features, Coban, Grab’s real-time data platform team, has adopted Flink as part of our service offerings.In this article, we explore how we ensure that deploying Flink applications remain safe as we incorporate the lessons learned through our journey to continuous delivery.Background  Figure 1. Flink platform architecture within Coban  Users interact with our systems to develop and deploy Flink applications in three different ways.Firstly, users create a Merge Request (MR) to develop their Flink applications on our Flink Scala repository, according to business requirements. After the MR is merged, GitOps Continuous Integration/Continuous Deployment (CI/CD) automatically runs and dockerises the application, allowing the containerised applications to be deployed easily.Secondly, users create another MR to our infrastructure as a code repository. The GitOps CI/CD that is integrated with Terraform runs and configures the created Spinnaker application. This process configures the Flink application that will be deployed.Finally, users trigger the actual deployment of the Flink applications on Spinnaker, which orchestrates the deployment of the Docker image onto our Kubernetes cluster. Flink applications are deployed as standalone clusters in Grab to ensure resource isolation.ProblemThe main issue we noticed with streaming pipelines like these, is that they are often interconnected, where application A depends on application B’s output. This makes it hard to find a solution that perfectly includes integration tests and ensures that propagated changes do not affect downstream applications.However, this problem statement is too large to solve with a single solution. As such, we are narrowing the problem statement to focus on ensuring safety of our applications, where engineers can deploy Flink applications that will be rolled back if they fail health checks. In our case, the definition of a Flink application’s health is limited to the uptime of the Flink application itself.It is worth noting that Flink applications are designed to be stateful streaming applications, meaning a “state” is shared between events (stream entities) and thus, past events can influence the way current events are processed. This also implies that traditional deployment strategies do not apply to the deployment of Flink applications.Current strategy  Figure 2. Current deployment stages  In Figure 2, our current deployment stages are split into three parts:  Delete current deployment: Remove current configurations (if applicable) to allow applications to pick up the new configurations.  Bake (Manifest): Bake the Helm charts with the provided configurations.  Deploy (Manifest): Deploy the charts onto Kubernetes.Over time, we learnt that this strategy can be risky. Part 2 can result in a loss of Flink application states due to how internal CI/CD processes are set up. There is also no easy way to rollback if an issue arises. Engineers will need to revert all config changes and rollback the deployment manually by re-deploying the older Docker image – which results in slower operation recovery.Lastly, there are no in-built monitoring mechanisms that perform regular health probes. Engineers need to manually monitor their applications to see if their deployment was successful or if they need to perform a rollback.With all these issues, deploying Flink applications for engineers are often stressful and fraught with uncertainty. Common mitigation strategies are canary and blue-green deployments, which we cover in the next section.Canary deployments  Figure 3. Canary deployment  In canary deployments, you gradually roll out new versions of the application in parallel with the production version, while serving a percentage of total traffic before promoting it gradually.This does not work for Flink deployments due to the nature of stream processing. Applications are frequently required to do streaming operations like stream joining, which involves matching related events in different Kafka topics. So, if a Flink application is only receiving a portion of the total traffic, the data generated will be considered inaccurate due to incomplete data inputs.Blue-green deployments  Figure 4. Blue-green deployment  Blue-green deployments work by running two versions of the application with a Load Balancer that acts as a traffic switch, which determines which version traffic is directed to.This method might work for Flink applications if we only allow one version of the application to consume Kafka messages at any point in time. However, we noticed some issues when switching traffic to another version. For example, the state of both versions will be inconsistent because of the different data traffic each version receives, which complicates the process of switching Kafka consumption traffic.So if there’s a failure and we need to rollback from Green to Blue deployment, or vice versa, we will need to take an extra step and ensure that before the failure, the data traffic received is exactly the same for both deployments.SolutionAs previously mentioned, it is crucial for streaming applications to ensure that at any point in time, only one application is receiving data traffic to ensure data completeness and accuracy. Although employing blue-green deployments can technically fulfil this requirement, the process must be modified to handle state consistency such that both versions have the same starting internal state and receive the same data traffic as each other, if a rollback is needed.  Figure 5. Visualised deployment flow  This deployment flow will operate in the following way:  Collect metadata regarding current application  Take savepoint and stop the current application  Clear up high availability configurations  Bake and deploy the new application  Monitor application and rollback if the health check failsLet’s elaborate on the key changes implemented in this new process.SavepointingFlink’s savepointing feature helps address the issue of state consistency and ensures safer deployments.A savepoint in Flink is a snapshot of a Flink application’s state at the point in time. This savepoint allows us to pause the Flink application and restore the application to this snapshot state, if there’s an issue.Before deploying a Flink application, we perform a savepoint via the Flink API before killing the current application. This would enable us to save the current state of the Flink application and rollback if our deployment fails – just like how you would do a quick save before attempting a difficult level when playing games. This mechanism ensures that both deployment versions have the same internal state during deployment as they both start from the same savepoint.Additionally, this feature allows us to easily handle Kafka offsets since these consumed offsets are stored as part of the savepoint. As Flink manages their own state, they don’t need to rely on Kafka’s consumer offset management. With this savepoint feature, we can ensure that the application receives the same data traffic post rollback and that no messages are lost due to processing on the failed version.MonitoringTo consistently monitor Flink applications, we can conduct health probes to the respective API endpoints to check if the application is stuck in a restart state or if it is running healthily.We also configured our monitoring jobs to wait for a few minutes for the deployment to stabilise before probing it over a defined duration, to ensure that the application is in a stable running state.RollbackIf the health checks fail, we then perform an automatic rollback. Typically, Flink applications are deployed as a standalone cluster and a rollback involves changes in one of the following:  Application and Flink configurations  Taskmanager or Jobmanager resource provisionApplication and Flink configuration changesFor configuration changes, we leverage the fact that Spinnaker performs versioned deployment of configmap resources. In this case, a rollback simply involves mounting the old configmap back onto the Kubernetes deployment.To retrieve the old version of the configmap mount, we can simply utilise Kubernetes’ rollback mechanisms – Kubernetes updates a deployment by creating a new replicaset with an incremental version before attaching it to the current deployment and scaling the previous replicaset to 0. To retrieve previous deployment specs, we just need to list all replicasets related to the deployment and find the previous deployed version, before updating the current deployment to mimic the previous template specifications.However, this deployment does not contain the number of replicas of previously configured task managers. Kubernetes does not register the number of replicas as part of deployment configuration as this is a dynamic configuration and might be changed during processing due to auto scaling operations.Our Flink applications are deployed as standalone clusters and do not use native or yarn resource providers. Coupled with the fact that Flink has strict resource provision, we realised that we do not have enough information to perform rollbacks, without the exact number of replicas created.Taskmanager or Jobmanager resource provision changesTo gather information about resource provision changes, we can simply include the previously configured number of replicas as part of our metadata annotation. This allows us to retrieve it in future during rollback.Making this change involves creating an additional step of metadata retrieval to retrieve and store previous deployment states as annotations of the new deployment.ImpactWith this solution, the deployment flow on Spinnaker looks like this:  Figure 6. New deployment flow on Spinnaker  Engineers no longer need to monitor the deployment pipeline as closely as they get notified of their application’s deployment status via Slack. They only need to interact or take action when they get notified that the different stages of the deployment pipeline are completed.  Figure 7. Slack notifications on deployment status  It is also easier to deploy Flink applications since failures and rollbacks are handled automatically. Furthermore, application state management is also automated, which reduces the amount of uncertainties.What’s next?As we work to further improve our deployment pipeline, we will look into extending the capabilities at our monitoring stage to allow engineers to define and configure their own health probes, allowing our deployment configurations to be more extendable.Another interesting improvement will be to make this deployment flow seamlessly, ensuring as little downtime as possible by minimising cold start duration.Coban also looks forward to pushing more features on our Flink platform to enable our engineers to explore more use cases that utilises real-time data to allow our operations to become auto adaptive and make data-driven decisions.References  Flink Savepointing  Flink API  Kubernetes rollbackJoin usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/safer-flink-deployments"
      }
      ,
    
      "message-center": {
        "title": "Message Center - Redesigning the messaging experience on the Grab superapp",
        "author": "jonathan-leejie-zhangvasu-krishnamoorthy",
        "tags": "[&quot;Engineering&quot;, &quot;GrabChat&quot;, &quot;Redesign&quot;, &quot;Messaging&quot;, &quot;Chat support&quot;]",
        "category": "",
        "content": "Since 2016, Grab has been using GrabChat, a built-in messaging feature to connect our users with delivery-partners or driver-partners. However, as the Grab superapp grew to include more features, the limitations of the old system became apparent. GrabChat could only handle two-party chats because that’s what it was designed to do. To make our messaging feature more extensible for future features, we decided to redesign the messaging experience, which is now called Message Center.  Migrating from the old GrabChat to the new Message Center  To some, building our own chat function might not be the ideal approach, especially with open source alternatives like Signal. However, Grab’s business requirements introduce some level of complexity, which required us to develop our own solution.Some of these requirements include, but are not limited to:  Handle multiple user types (passengers, driver-partners, consumers, delivery-partners, customer support agents, merchant-partners, etc.) with custom user interface (UI) rendering logic and behaviour.  Enable other Grab backend services to send system generated messages (e.g. your driver is reaching) and customise push notifications.  Persist message state even if users uninstall and reinstall their apps. Users should be able to receive undelivered messages even if they were offline for hours.  Provide translation options for non-native speakers.  Filter profanities in the chat.  Allow users to handle group chats. This feature might come in handy in future if there needs to be communication between passengers, driver-partners, and delivery-partners.Solution architecture  Message Center architecture  The new Message Center was designed to have two components:  Message-center backend: Message processor service that handles logical and database operations.  Message-center postman: Message delivery service that can scale independently from the backend service.This architecture allows the services to be sufficiently decoupled and scale independently. For example, if you have a group chat with N participants and each message sent results in N messages being delivered, this architecture would enable message-center postman to scale accordingly to handle the higher load.As Grab delivers millions of events a day via the Message Center service, we need to ensure that our system can handle high throughput. As such, we are using Apache Kafka as the low-latency high-throughput event stream connecting both services and Amazon SQS as a redundant delay queue that attempts a retry 10 seconds later.Another important aspect for this service is the ability to support low-latency and bi-directional communications from the client to the server. That’s why we chose Transmission Control Protocol (TCP) as the main protocol for client-server communication. Mobile and web clients connect to Hermes, Grab’s TCP gateway service, which then digests the TCP packets and proxies the payloads to Message Center via gRPC. If both recipients and senders are online, the message is successfully delivered in a matter of milliseconds.Unlike HTTP, individual TCP packets do not require a response so there is an inherent uncertainty in whether the messages were successfully delivered. Message delivery can fail due to several reasons, such as the client terminating the connection but the server’s connection remaining established. This is why we built a system of acknowledgements (ACKs) between the client and server, which ensures that every event is received by the receiving party.The following diagram shows the high-level sequence of events when sending a message.  Events involved in sending a message on Message Center  Following the sequence of events involved in sending a message and updating its status for the sender from sending to sent to delivered to read, the process can get very complicated quickly. For example, the sender will retry the 1302 TCP new message until it receives a server ACK. Similarly, the server will also keep attempting to send the 1402 TCP message receipt or 1303 TCP message unless it receives a client ACK. With this in mind, we knew we had to give special attention to the ACK implementation, to prevent infinite retries on the client and server, which can quickly cascade to a system-wide failure.Lastly, we also had to consider dropped TCP connections on mobile devices, which happens quite frequently. What happens then? Message Center relies on Hedwig, another in-house notification service, to send push notifications to the mobile device when it receives a failed response from Hermes. Message Center also maintains a user-events DynamoDB database, which updates the state of every pending event of the client to delivered whenever a client ACK is received.Every time the mobile client reconnects to Hermes, it also sends a special TCP message to notify Message Center that the client is back online, and then the server retries sending all the pending events to the client.Learnings/ConclusionWith large-scale features like Message Center, it’s important to:  Decouple services so that each microservice can function and scale as needed.  Understand our feature requirements well so that we can make the best choices and design for extensibility.  Implement safeguards to prevent system timeouts, infinite loops, or other failures from cascading to the entire system, i.e. rate limiting, message batching, and idempotent eventIDs.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/message-center"
      }
      ,
    
      "evolution-of-quality": {
        "title": "Evolution of quality at Grab",
        "author": "abby-alcantaraxuanthu-doanrenu-yadav",
        "tags": "[&quot;Engineering&quot;, &quot;Technology stack&quot;, &quot;Exploration&quot;]",
        "category": "",
        "content": "To achieve our vision of becoming the leading superapp in Southeast Asia, we constantly need to balance development velocity with maintaining the high quality of the Grab app. Like most tech companies, we started out with the traditional software development lifecycle (SDLC) but as our app evolved, we soon noticed several challenges like high feature bugs and production issues.  In this article, we dive deeper into our quality improvement journey that officially began in 2019, the challenges we faced along the way, and where we stand as of 2022.Background  Figure 1 - Software development life cycle (SDLC) sample  When Grab first started in 2012, we were using the Agile SDLC (Figure 1) across all teams and features. This meant that every new feature went through the entire process and was only released to app distribution platforms (PlayStore or AppStore) after the quality assurance (QA) team manually tested and signed off on it.Over time, we discovered that feature testing took longer, with more bugs being reported and impact areas that needed to be tested. This was the same for regression testing as QA engineers had to manually test each feature in the app before a release. Despite the best efforts of our QA teams, there were still many major and critical production issues reported on our app – the highest numbers were in 2019 (Figure 2).  Figure 2 - Critical open production issue (OPI) trend  This surge in production issues and feature bugs was directly impacting our users’ experience on our app. To directly address the high production issues and slow testing process, we changed our testing strategy and adopted shift-left testing.SolutionShift-left testing is an approach that brings testing forward to the early phases of software development. This means testing can start as early as the planning and design phases.  Figure 3 - Shift-left testing  By adopting shift-left testing, engineering teams at Grab are able to proactively prevent possible defect leakage in the early stages of testing, directly addressing our users’ concerns without delaying delivery times.With shift-left testing, we made three significant changes to our SDLC:  Software engineers conduct acceptance testing  Incorporate Definition of Ready (DoR) and Definition of Done (DoD)  Balanced testing strategyLet’s dive deeper into how we implemented each change, the challenges, and learnings we gained along the way.Software engineers conduct acceptance testingAcceptance testing determines whether a feature satisfies the defined acceptance criteria, which helps the team evaluate if the feature fulfills our consumers’ needs. Typically, acceptance testing is done after development. But our QA engineers still discovered many bugs and the cost of fixing bugs at this stage is more expensive and time-consuming. We also realised that the most common root causes of bugs were associated with insufficient requirements, vague details, or missing test cases.With shift-left testing, QA engineers start writing test cases before development starts and these acceptance tests will be executed by the software engineers during development. Writing acceptance tests early helps identify potential gaps in the requirements before development begins. It also prevents possible bugs and streamlines the testing process as engineers can find and fix bugs even before the testing phase. This is because they can execute the test cases directly during the development stage.On top of that, QA and Product managers also made Given/When/Then (GWT) the standard for acceptance criteria and test cases, making them easier for all stakeholders to understand.    Step by Step style  GWT format        Open the Grab appNavigate to home feedTap on merchant entry point cardCheck that merchant landing page is shown    Given user opens the app And user navigates to the home feedWhen the user taps on the merchant entry point cardThen the user should see the merchant’s landing page  By enabling software engineers to conduct acceptance testing, we minimised back-and-forth discussions within the team regarding bug fixes and also, influenced a significant shift in perspective – quality is everyone’s responsibility.Another key aspect of shift-left testing is for teams to agree on a standard of quality in earlier stages of the SDLC. To do that, we started incorporating Definition of Ready (DoR) and Definition of Done (DoD) in our tasks.Incorporate Definition of Ready (DoR) and Definition of Done (DoD)As mentioned, quality checks can be done before development even begins and can start as early as backlog grooming and sprint planning. The team needs to agree on a standard for work products such as requirements, design, engineering solutions, and test cases. Having this alignment helps reduce the possibility of unclear requirements or misunderstandings that may lead to re-work or a low-quality feature.To enforce consistent quality of work products, everyone in the team should have access to these products and should follow DoRs and DoDs as standards in completing their tasks.  DoR: Explicit criteria that an epic, user story, or task must meet before it can be accepted into an upcoming sprint.   DoD: List of criteria to fulfill before we can mark the epic, user story, or task complete, or the entry or exit criteria for each story state transitions. Including DoRs and DoDs have proven to improve delivery pace and quality. One of the first teams to adopt this observed significant improvements in their delivery speed and app quality – consistently delivering over 90% of task commitments, minimising technical debt, and reducing manual testing times.Unfortunately, having these two changes alone were not sufficient – testing was still manually intensive and time consuming. To ease the load on our QA engineers, we needed to develop a balanced testing strategy.  Balanced testing strategy  Figure 4 - Test automation strategy  Our initial automation strategy only included unit testing, but we have since enhanced our testing strategy to be more balanced.  Unit testing  UI component testing  Backend integration testing  End-to-End (E2E) testingSimply having good coverage in one layer does not guarantee good quality of an app or new feature. It is important for teams to test vigorously with different types of testing to ensure that we cover all possible scenarios before a release.As you already know, unit tests are written and executed by software engineers during the development phases. Let’s look at what the remaining three layers mean.UI component testingThis type of testing focuses on individual components within the application and is useful for testing specific use cases of a service or feature. To reduce manual effort from QA engineers, teams started exploring automation and introduced a mobile testing framework for component testing.This UI component testing framework used mocked API responses to test screens and interactions on the elements. These UI component tests were automatically executed whenever the pipeline was run, which helped to reduce manual regression efforts. With shift-left testing, we also revised the DoD for new features to include at least 70% coverage of UI component tests.Backend integration testingBackend integration testing is especially important if your application regularly interacts with backend services, much like the Grab app. This means we need to ensure the quality and stability of these backend services. Since Grab started its journey toward becoming a superapp, more teams started performing backend integration tests like API integration tests.Our backend integration tests also covered positive and negative test cases to determine the happy and unhappy paths. At the moment, majority of Grab teams have complete test coverage for happy path use cases and are continuously improving coverage for other use cases.End-to-End (E2E) testingE2E tests are important because they simulate the entire user experience from start to end, ensuring that the system works as expected. We started exploring E2E testing frameworks, from as early as 2015, to automate tests for critical services like logging in and booking a ride.But as Grab introduced more services, off-the-shelf solutions were no longer a viable option, as we noticed issues like automation limitations and increased test flakiness. We needed a framework that is compatible with existing processes, stable enough to reduce flakiness, scalable, and easy to learn.With this criteria in mind, our QA engineering teams built an internal E2E framework that could make API calls, test different account-based scenarios, and provide many other features. Multiple pilot teams have started implementing tests with the E2E framework, which has helped to reduce regression efforts. We are continuously improving the framework by adding new capabilities to cover more test scenarios.Now that we’ve covered all the changes we implemented with shift-left testing, let’s take a look at how this changed our SDLC.Impact  Figure 5 - Updated SDLC process  Since the implementation of shift-left testing, we have improved our app quality without compromising our project delivery pace. Compared to 2019, we observed the following improvements within the Grab superapp in 2022:  Production issues with “Major and Critical” severity bugs found in production were reduced by 60%  Bugs found in development phase with “Major and Critical” severity were reduced by 40%What’s next?Through this journey, we recognise that there’s no such thing as a bug-free app – no matter how much we test, production issues still happen occasionally. To minimise the occurrence of bugs, we’re regularly conducting root cause analyses and writing postmortem reports for production incidents. These allow us to retrospect with other teams and come up with corrective actions and prevention plans. Through these continuous learnings and improvements, we can continue to shape the future of the Grab superapp.Special thanks to Sori Han for designing the images in this article.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/evolution-of-quality"
      }
      ,
    
      "determining-tech-stack": {
        "title": "How OVO determined the right technology stack for their web-based projects",
        "author": "george-matthewsandy-ys",
        "tags": "[&quot;Engineering&quot;, &quot;Technology stack&quot;, &quot;Exploration&quot;]",
        "category": "",
        "content": "In the current technology landscape, startups are developing rapidly. This usually leads to an increase in the number of engineers in teams, with the goal of increasing the speed of product development and delivery frequency. However, this growth often leads to a diverse selection of technology stacks being used by different teams within the same organisation.Having different technology stacks within a team could lead to a bigger problem in the future, especially if documentation is not well-maintained. The best course of action is to pick just one technology stack for your projects, but it begs the question, “How do I choose the best technology stack for my projects?”. One such example is OVO, which is an Indonesian payments, rewards, and financial services platform within Grab. We share our process and analysis to determine the best technology stack that complies with precise standards. By the end of the article, you may also learn to choose the best technology stack for your needs.BackgroundIn recent years, we have seen massive growth in modern web technologies, such as React, Angular, Vue, Svelte, Django, TypeScript, and many more. Each technology has its benefits. However, having so many choices can be confusing when you must determine which technologies are best for your projects. To narrow down the choices, a few aspects, such as scalability, stability, and usage in the market, must be considered.That’s the problem that we used to face. Most of our legacy services were not standardised and were written in different languages like PHP, React, and Vue. Also, the documentation for these legacy services is not well-structured or regularly updated.  Current technology stack usage in OVO  We realised that we had two main problems:  Various technology stacks (PHP, Vue, React, Nuxt, and Go) maintained simultaneously, with incomplete documentation, may consume a lot of time to understand the code, especially for engineers unfamiliar with the frameworks or even a new hire.  Context switching when reviewing code makes it hard to review other teammates’ merge requests on complex projects and quickly offer better code suggestions.To prevent these problems from recurring, teams must use one primary technology stack.After detailed comparisons, we narrowed our choices to two options – React and Vue – because we have developed projects in both technologies and already have the user interface (UI) library in each technology stack.  Taken from ulam.io  Next, we conducted a more detailed research and exploration for each technology. The main goals were to find the unique features, scalability, ease of migration, and compatibility for the UI library for React and Vue. To test the compatibility of each UI library, we also used a sample UI on one of our upcoming projects and sliced it.Here’s a quick summary of our exploration:  Metrics  Vue  React  UI Library Compatibility  Doesn’t require much component development  Doesn’t require much component development  Scalability  Easier to upgrade, slower in releasing major updates, clear migration guide  Quicker release of major versions, supports gradual updates  Others  Composition API, strong community (Vue Community)  Latest version (v18) of React gradual updates, doesn’t support IEFrom this table, we found that the differences between these frameworks are miniscule, making it tough for us to determine which to use. Ultimately, we decided to step back and see the Big Why. SolutionThe Big Why here was “Why do we need to standardise our technology stack?”. We wanted to ease the onboarding process for new hires and reduce the complexity, like context switching, during code reviews, which ultimately saves time.As Kleppmann (2017) states, “The majority of the cost of software is in its ongoing maintenance”. In this case, the biggest cost was time. Increasing the ease of maintenance would reduce the cost, so we decided to use maintainability as our north star metric.Kleppmann (2017) also highlighted three design principles in any software system:  Operability: Make it easy to keep the system running.  Simplicity: Easy for new engineers to understand the system by minimising complexity.  Evolvability: Make it easy for engineers to make changes to the system in the future.Keeping these design principles in mind, we defined three metrics that our selected tech stack must achieve:  Scalability          Keeping software and platforms up to date      Anticipating possible future problems        Stability of the library and documentation          Establishing good practices and tools for development        Usage in the market          The popularity of the library or framework and variety of coding best practices            Metrics    Vue    React        Scalability    FrameworkOperabilityEasier to update because there aren’t many approaches to writing Vue.EvolvabilitySince Vue is a framework, it needs fewer steps to upgrade.    LibrarySupports gradual updates but there will be many different approaches when upgrading React on our services.        Stability of the library and documentation    Has standardised documentation    Has many versions of documentation        Usage on Market    Smaller market share.SimplicityWe can reduce complexity for new hires, as the Vue standard in OVO remains consistent with standards in other companies.    Larger market share.    Many React variants are currently in the market, so different companies may have different folder structures/conventions.      Screenshot taken from https://www.statista.com/ on 2022-10-13  After conducting a detailed comparison between Vue and React, we decided to use Vue as our primary tech stack as it best aligns with Kleppmann’s three design principles and our north star metric of maintainability. Even though we noticed a few disadvantages to using Vue, such as smaller market share, we found that Vue is still the better option as it complies with all our metrics.Moving forward, we will only use one tech stack across our projects but we decided not to migrate technology for existing projects. This allows us to continue exploring and learning about other technologies’ developments. One of the things we need to do is ensure that our current projects are kept up-to-date.ImplementationAfter deciding on the primary technology stack, we had to do the following:  Define a boilerplate for future Vue projects, which will include items like a general library or dependencies, implementation for unit testing, and folder structure, to align with our north star metric.  Update our existing UI library with new components and the latest Vue version.  Perform periodic upgrades to existing React services and create a standardised code structure with proper documentation.With these practices in place, we can ensure that future projects will be standardised, making them easier for engineers to maintain.ImpactThere are a few key benefits of standardising our technology stack.  Scalability and maintainability: It’s much easier to scale and maintain projects using the same technology stack. For example, when implementing security patches on all projects due to certain vulnerabilities in the system or libraries, we will need one patch for each technology. With only one stack, we only need to implement one patch across all projects, saving a lot of time.  Faster onboarding process: The onboarding process is simplified for new hires because we have standardisation between all services, which will minimise the amount of context switching and lower the learning curve.  Faster deliveries: When it’s easier to implement a change, there’s a compounding impact where the delivery process is shortened and release to production is quicker. Ultimately, faster deliveries of a new product or feature will help increase revenue.Learnings/Conclusion    For every big decision, it is important to take a step back and understand the Big Why or the main motivation behind it, in order to remain objective. That’s why after we identified maintainability as our north star metric, it was easier to narrow down the choices and make detailed comparisons.The north star metric, or deciding factor, might differ vastly, but it depends on the problems you are trying to solve.Note: The OVO web team conducted this research in 2022 and was accurate at the time of publishing. The information here may only be applicable to the OVO web team.References  Kleppmann, M. (2017). Designing Data-Intensive Applications. Beijing: O’Reilly. ISBN: 978-1-4493-7332-0  Most used web frameworks 2022 - StatistaJoin usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/determining-tech-stack"
      }
      ,
    
      "migrating-to-abac": {
        "title": "Migrating from Role to Attribute-based Access Control",
        "author": "minhkhoi-nguyen",
        "tags": "[&quot;Engineering&quot;, &quot;Access control&quot;, &quot;Security&quot;]",
        "category": "",
        "content": "Grab has always regarded security as one of our top priorities; this is especially important for data platform teams. We need to control access to data and resources in order to protect our consumers and ensure compliance with various, continuously evolving security standards.Additionally, we want to keep the process convenient, simple, and easily scalable for teams. However, as Grab continues to grow, we have more services and resources to manage and it becomes increasingly difficult to keep the process frictionless. That’s why we decided to move from Role-Based Access Control (RBAC) to Attribute-Based Access Control (ABAC) for our Kafka Control Plane (KCP).In this article, you will learn how Grab’s streaming data platform team (Coban) deleted manual role and permission management of hundreds of roles and resources, and reduced operational overhead of requesting or approving permissions to zero by moving from RBAC to ABAC.IntroductionKafka is widely used across Grab teams as a streaming platform. For decentralised Kafka resource (e.g. topic) management, teams have the right to create, update, or delete based on their needs. As the data platform team, we implemented a KCP to ensure that these operations are only performed by authorised parties, especially on multi-tenant Kafka clusters.For internal access management, Grab uses its own Identity and Access Management (IAM) service, based on RBAC, to support authentication and authorisation processes:  Authentication verifies the identity of a user or service, for example, if the provided token is valid or expired.  Authorisation determines their access rights, for example, whether users can only update and/or delete their own Kafka topics.In RBAC, roles, permissions, actions, resources, and the relationships between them need to be defined in the IAM service. They are used to determine whether a user can access a certain resource.In the following example, we can see how IAM concepts come together. The Coban engineer role belongs to the Engineering-coban group and has permission to update the topic’s retention. Any engineer added to the Engineering-coban group will also be able to update the topic’s retention.    Following the same concept, each team using the KCP has its own roles, permissions, and resources created in the system. However, there are some disadvantages to this approach:  It leads to a significant growth in the number of access control artifacts both platform and user teams need to manage, and increased time and effort to debug access control issues. We start off by finding which group the engineer belongs to and locating the group that should be used for KCP, and then trace to role and permissions.  All group membership access requests of new joiners need to be reviewed and approved by their direct managers. This leads to a lot of backlog as new joiners might have multiple groups to join and managers might not be able to review them timely. In some cases, roles need to be re-applied or renewed every 90 days, which further adds to the delay.  Group memberships are not updated to reflect active members in the team, leaving some engineers with access they don’t need and others with access they should have but don’t.SolutionWith ABAC, access management becomes a lot easier. Any new joiner to a specific team gets the same access rights as everyone on that team – no need for manual approval from a manager. However, for ABAC to work, we need these components in place:  User attributes: Who is the subject (actor) of a request?  Resource attributes: Which object (resource) does the actor want to deal with?  Evaluation engine: How do we decide if the actor is allowed to perform the action on the resource?User attributesAll users have certain attributes depending on the department or team they belong to. This data is then stored and synced automatically with the human resource management system (HRMS) tool, which acts as a source of truth for Grab-wide data, every time a user switches teams, roles, or leaves the company.Resource attributesResource provisioning is an authenticated operation. This means that KCP knows who sent the requests and what each request/action is about. Similarly, resource attributes can be derived from their creators. For new resource provisioning, it is possible to capture the resource tags and store them after authentication. For existing resources, a major challenge was the need to backfill the tagging and ensure a seamless transition from the user’s perspective. In the past, all resource provisioning operations were done by a centralised platform team and most of the existing resource attributes are still under platform team’s ownership.Evaluation engineWe chose to use Open Policy Agent (OPA) as our policy evaluation engine mainly for its wide community support, applicable feature set, and extensibility to other tools and platforms in our system. This is also currently used by our team for Kafka authorisation. The policies are written in Rego, the default language supported by OPA.Architecture and implementationWith ABAC, the access control process looks like this:    User attributes    Authentication is handled by the IAM service. In the /generate_token call, a user requests an authentication token from KCP before calling an authenticated endpoint. KCP then calls IAM to generate a token and returns it to the user.In the /create_topic call, the user includes the generated token in the request header. KCP takes the token and verifies the token validity with IAM. User attributes are then extracted from the token payload for later use in request authorisation.Some of the common attributes we use for our policy are user identifier, department code, and team code, which provide details like a user’s department and work scope.When it comes to data governance and central platform and identity teams, one of the major challenges was standardising the set of attributes to be used for clear and consistent ABAC policies across platforms so that their lifecycle and changes could be governed. This was an important shift in the mental model for attribute management over the RBAC model.Resource attributesFor newly created resources, attributes will be derived from user attributes that are captured during the authentication process.    Previously with RBAC, existing resources did not have the required attributes. Since migrating to ABAC, the implementation has tagged newly created resources and ensured that their attributes are up to standard. IAM was also still doing the actual authorisation using RBAC.It is also important to note that we collaborated with data governance teams to backfill Kafka resource ownership. Having accurate ownership of resources like data lake or Kafka topics enabled us to move toward a self-service model and remove bottlenecks from centralised platform teams.After identifying most of the resource ownership, we started switching over to ABAC. The transition was smooth and had no impact on user experience. The remaining unidentified resources were tagged to lost-and-found and could be reclaimed by service teams when they needed permission to manage them.Open Policy AgentThe most common question when implementing the policy is “how do you define ownership by attributes?”. With respect to the principle of least privilege, each policy must be sufficiently strict to limit access to only the relevant parties. In the end, we aligned as an organisation on defining ownership by department and team.We created a simple example below to demonstrate how to define a policy:package authzimport future.keywordsdefault allow = falseallow if {        input.endpoint == \"updateTopic\"        is_owner(input.resource_attributes)}is_owner(md) if {        md.department == input.user_attributes.department        md.team == input.user_attributes.team}In this example, we start with denying access to everyone. If the updateTopic endpoint is called and the department and team attributes between user and resource are matched, access is allowed.With a similar scenario, we would need 1 role, 1 action, 1 resource, and 1 mapping (a.k.a permission) between action and resource. We will need to keep adding resources and permissions when we have new resources created. Compared to the policy above, no other changes are required.With ABAC, there are no further setup or access requests needed when a user changes teams. The user will be tagged to different attributes, automatically granted access to the new team’s resources, and excluded from the previous team’s resources.Another consideration we had was making sure that the policy is well-written and transparent in terms of change history. We decided to include this as part of our application code so every change is accounted for in the unit test and review process.AuthorisationThe last part of the ABAC process is authorisation logic. We added the logic to the middleware so that we could make a call to OPA for authorisation.    To ensure token validity after authentication, KCP extracts user attributes from the token payload and fetches resource attributes from the resource store. It combines the request metadata such as method and endpoint, along with the user and resource attributes into an OPA request. OPA then evaluates the request based on the redefined policy above and returns a response.AuditabilityFor ABAC authorisation, there are two key areas of consideration:  Who made changes to the policy, who deployed, and when the change was made  Who accessed what resource and whenWe manage policies in a dedicated GitLab repository and changes are submitted via merge requests. Based on the commit history, we can easily tell who made changes, reviewed, approved, and deployed the policy.     For resource access, OPA produces a decision log containing user attributes, resource attributes, and the authorisation decision for every call it serves. The log is kept for five days in Kibana for debugging purposes, then moved to S3 where it is kept for 28 days.ImpactThe move to ABAC authorisation has improved our controls as compared to the previous RBAC model, with the biggest impact being fewer resources to manage. Some other benefits include:  Optimised resource allocation: Discarded over 200 roles, 200 permissions, and almost 3000 unused resources from IAM services, simplifying our debugging process. Now, we can simply check the user and resource attributes as needed.  Simplified resource management: In the three months we have been using ABAC, about 600 resources have been added without any increase in complexity for authorisation, which is significantly lesser than the RBAC model.  Reduction in delays and waiting time: Engineers no longer have to wait for approval for KCP access.  Better governance over resource ownership and costs: ABAC allowed us to have a standardised and accurate tagging system of almost 3000 resources.LearningsAlthough ABAC does provide significant improvements over RBAC, it comes with its own caveats:  It needs a reliable and comprehensive attribute tagging system to function properly. This only became possible after roughly three months of identifying and tagging the ownership of existing resources by both automated and manual methods.  Tags should be kept up to date with the company’s growth. Teams could lose access to their resources if they are wrongly tagged. It needs a mechanism to keep up with changes, or people will unexpectedly lose access when user and resource attributes are changed.What’s next?  To keep up with organisational growth, KCP needs to start listening to the IAM stream, which is where all IAM changes are published. This will allow KCP to regularly update user attributes and refresh resource attributes when restructuring occurs, allowing authorisation to be done with the right data.  Constant collaboration with HR to ensure that we maintain sufficient user attributes (no extra unused information) that remain clean so ABAC works as expected.References  OPA  Rego  Zero trust with KafkaJoin usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/migrating-to-abac"
      }
      ,
    
      "securing-gitops-pipeline": {
        "title": "Securing GitOps pipelines",
        "author": "thang-le",
        "tags": "[&quot;Engineering&quot;, &quot;Open source&quot;, &quot;Pipelines&quot;, &quot;Continuous Delivery&quot;, &quot;Continuous Integration&quot;, &quot;Optimisation&quot;]",
        "category": "",
        "content": "IntroductionGrab’s real-time data platform team, Coban, has been managing infrastructure resources via Infrastructure-as-code (IaC). Through the IaC approach, Terraform is used to maintain infrastructure consistency, automation, and ease of deployment of our streaming infrastructure, notably:  Flink pipelines  Kafka topics  Kafka Connect connectorsWith Grab’s exponential growth, there needs to be a better way to scale infrastructure automatically. Moving towards GitOps processes benefits us in many ways:  Versioned and immutable: With our source code being stored in Git repositories, the desired state of infrastructure is stored in an environment that enforces immutability, versioning, and retention of version history, which helps with auditing and traceability.  Faster deployment: By automating the process of deploying resources after code is merged, we eliminate manual steps and improve overall engineering productivity while maintaining consistency.  Easier rollbacks: It’s as simple as making a revert for a Git commit as compared to creating a merge request (MR) and commenting Atlantis commands, which add extra steps and contribute to a higher mean-time-to-resolve (MTTR) for incidents.BackgroundOriginally, Coban implemented automation on Terraform resources using Atlantis, an application that operates based on user comments on MRs.  Fig. 1 User flow with Atlantis  We have come a long way with Atlantis. It has helped us to automate our workflows and enable self-service capabilities for our engineers. However, there were a few limitations in our setup, which we wanted to improve:  Coarse grained: There is no way to restrict the kind of Terraform resources users can create, which introduces security issues. For example, if a user is one of the Code owners, they can create another IAM role with Admin privileges with approval from their own team anywhere in the repository.  Limited automation: Users are still required to make comments in their MR such as atlantis apply. This requires the learning of Atlantis commands and is prone to human errors.  Limited capability: Having to rely entirely on Terraform and Hashicorp Configuration Language (HCL) functions to validate user input comes with limitations. For example, the ability to validate an input variable based on the value of another has been a requested feature for a long time.  Not adhering to Don’t Repeat Yourself (DRY) principle: Users need to create an entire Terraform project with boilerplate codes such as Terraform environment, local variables, and Terraform provider configurations to create a simple resource such as a Kafka topic.SolutionWe have developed an in-house GitOps solution named Khone. Its name was inspired by the Khone Phapheng Waterfall. We have evaluated some of the best and most widely used GitOps products available but chose not to go with any as the majority of them aim to support Kubernetes native or custom resources, and we needed infrastructure provisioning that is beyond Kubernetes. With our approach, we have full control of the entire user flow and its implementation, and thus we benefit from:  Security: The ability to secure the pipeline with many customised scripts and workflows.  Simple user experience (UX): Simplified user flow and prevents human errors with automation.  DRY: Minimise boilerplate codes. Users only need to create a single Terraform resource and not an entire Terraform project.  Fig. 2 User flow with Khone  With all types of streaming infrastructure resources that we support, be it Kafka topics or Flink pipelines, we have identified they all have common properties such as namespace, environment, or cluster name such as Kafka cluster and Kubernetes cluster. As such, using those values as file paths help us to easily validate users input and de-couple them from the resource specific configuration properties in their HCL source code. Moreover, it helps to remove redundant information to maintain consistency. If the piece of information is in the file path, it won’t be elsewhere in resource definition.  Fig. 3 Khone directory structure  With this approach, we can utilise our pipeline scripts, which are written in Python and perform validations on the types of resources and resource names using Regular Expressions (Regex) without relying on HCL functions. Furthermore, we helped prevent human errors and improved developers’ efficiency by deriving these properties and reducing boilerplate codes by automatically parsing out other necessary configurations such as Kafka brokers endpoint from the cluster name and environment.Pipeline stagesKhone’s pipeline implementation is designed with three stages. Each stage has different duties and responsibilities in verifying user input and securely creating the resources.  Fig. 4 An example of a Khone pipeline  Initialisation stageAt this stage, we categorise the changes into Deleted, Created or Changed resources and filter out unsupported resource types. We also prevent users from creating unintended resources by validating them based on resource path and inspecting the HCL source code in their Terraform module. This stage also prepares artefacts for subsequent stages.  Fig. 5 Terraform changes detected by Khone  Terraform stageThis is a downstream pipeline that runs either the Terraform plan or Terraform apply command depending on the state of the MR, which can either be pending review or merged. Individual jobs run in parallel for each resource change, which helps with performance and reduces the overall pipeline run time.For each individual job, we implemented multiple security checkpoints such as:  Code inspection: We use the python-hcl2 library to read HCL content of Terraform resources to perform validation, restrict the types of Terraform resources users can create, and ensure that resources have the intended configurations. We also validate whitelisted Terraform module source endpoint based on the declared resource type. This enables us to inherit the flexibility of Python as a programming language and perform validations more dynamically rather than relying on HCL functions.  Resource validation: We validate configurations based on resource path to ensure users are following the correct and intended directory structure.  Linting and formatting: Perform HCL code linting and formatting using Terraform CLI to ensure code consistency.Furthermore, our Terraform module independently validates parameters by verifying the working directory instead of relying on user input, acting as an additional layer of defence for validation.path = one(regexall(join(\"/\",[    \"^*\",    \"(?P&lt;repository&gt;khone|khone-dev)\",    \"resources\",    \"(?P&lt;namespace&gt;[^/]*)\",    \"(?P&lt;resource_type&gt;[^/]*)\",    \"(?P&lt;env&gt;[^/]*)\",    \"(?P&lt;cluster_name&gt;[^/]*)\",    \"(?P&lt;resource_name&gt;[^/]*)$\"]), path.cwd))Metric stageIn this stage, we consolidate previous jobs’ status and publish our pipeline metrics such as success or error rate.For our metrics, we identified actual users by omitting users from Coban. This helps us measure success metrics more consistently as we could isolate metrics from test continuous integration/continuous deployment (CI/CD) pipelines.For the second half of 2022, we achieved a 100% uptime for Khone pipelines.  Fig. 6 Khone's success metrics for the second half of 2022  Preventing pipeline config tamperingBy default, with each repository on GitLab that has CI/CD pipelines enabled, owners or administrators would need to have a pipeline config file at the root directory of the repository with the name .gitlab-ci.yml. Other scripts may also be stored somewhere within the repository.With this setup, whenever a user creates an MR, if the pipeline config file is modified as part of the MR, the modified version of the config file will be immediately reflected in the pipeline’s run. Users can exploit this by running arbitrary code on the privileged GitLab runner.In order to prevent this, we utilise GitLab’s remote pipeline config functionality. We have created another private repository, khone-admin, and stored our pipeline config there.  Fig. 7 Khone's remote pipeline config  In Fig. 7, our configuration is set to a file called khone-gitlab-ci.yml residing in the khone-admin repository under snd group.Preventing pipeline scripts tamperingWe had scripts that ran before the MR and they were approved and merged to perform preliminary checks or validations. They were also used to run the Terraform plan command. Users could modify these existing scripts to perform malicious actions. For example, they could bypass all validations and directly run the Terraform apply command to create unintended resources.This can be prevented by storing all of our scripts in the khone-admin repository and cloning them in each stage of our pipeline using the before_script clause.default:  before_script:    - rm -rf khone_admin    - git clone --depth 1 --single-branch https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.myteksi.net/snd/khone-admin.git khone_adminEven though this adds an overhead to each of our pipeline jobs and increases run time, the amount is insignificant as we have optimised the process by using shallow cloning. The Git clone command included in the above script with depth=1 and single-branch flag has reduced the time it takes to clone the scripts down to only 0.59 seconds.Testing our pipelineWith all the security measures implemented for Khone, this raises a question of how did we test the pipeline? We have done this by setting up an additional repository called khone-dev.  Fig. 8 Repositories relationship  Pipeline configWithin this khone-dev repository, we have set up a remote pipeline config file following this format:&lt;File Name&gt;@&lt;Repository Ref&gt;:&lt;Branch Name&gt;  Fig. 9 Khone-dev's remote pipeline config  In Fig. 9, our configuration is set to a file called khone-gitlab-ci.yml residing in the khone-admin repository under the snd group and under a branch named ci-test. With this approach, we can test our pipeline config without having to merge it to master branch that affects the main Khone repository. As a security measure, we only allow users within a certain GitLab group to push changes to this branch.Pipeline scriptsFollowing the same method for pipeline scripts, instead of cloning from the master branch in the khone-admin repository, we have implemented a logic to clone them from the branch matching our lightweight directory access protocol (LDAP) user account if it exists. We utilised the GITLAB_USER_LOGIN environment variable that is injected by GitLab to each individual CI job to get the respective LDAP account to perform this logic.default:  before_script:    - rm -rf khone_admin    - |      if git ls-remote --exit-code --heads \"https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.myteksi.net/snd/khone-admin.git\" \"$GITLAB_USER_LOGIN\" &gt; /dev/null; then        echo \"Cloning khone-admin from dev branch ${GITLAB_USER_LOGIN}\"        git clone --depth 1 --branch \"$GITLAB_USER_LOGIN\" --single-branch \"https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.myteksi.net/snd/khone-admin.git\" khone_admin      else        echo \"Dev branch ${GITLAB_USER_LOGIN} not found, cloning from master instead\"        git clone --depth 1 --single-branch \"https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.myteksi.net/snd/khone-admin.git\" khone_admin      fiWhat’s next?With security being our main focus for our Khone GitOps pipeline, we plan to abide by the principle of least privilege and implement separate GitLab runners for different types of resources and assign them with just enough IAM roles and policies, and minimal network security group rules to access our Kafka or Kubernetes clusters.Furthermore, we also plan to maintain high standards and stability by including unit tests in our CI scripts to ensure that every change is well-tested before being deployed.References  Specify a custom CI/CD configuration file  IAM roles for service accounts  GitLab code ownersSpecial thanks to Fabrice Harbulot for kicking off this project and building a strong foundation for it.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/securing-gitops-pipeline"
      }
      ,
    
      "geohash-plugin": {
        "title": "New zoom freezing feature for Geohash plugin",
        "author": "maria-mitisor",
        "tags": "[&quot;Engineering&quot;, &quot;Geohash&quot;, &quot;Maps&quot;, &quot;Open source&quot;]",
        "category": "",
        "content": "IntroductionGeohash is an encoding system with a unique identifier for each region on the planet. Therefore, all geohash units can be associated with an individual set of digits and letters.Geohash is a plugin built by Grab that is available in the Java OpenStreetMap Editor (JOSM) tool, which comes in handy for those who work on precise areas based on geohash units.BackgroundUp until recently, users of the Geohash JOSM plugin were unable to stop the displaying of new geohashes with every zoom-in or zoom-out. This meant that every time they changed the zoom, new geohashes would be displayed, and this became bothersome for many users when it was unneeded. The previous behaviour of the plugin when zooming in and out is depicted in the following short video:    This led to the implementation of the zoom freeze feature, which helps users toggle between Enable zoom freeze and Disable zoom freeze, based on their needs.SolutionAs you can see in the following image, a new label was created with the purpose of freezing or unfreezing the display of new geohashes with each zoom change:    By default, this label says “Enable zoom freeze”, and when zoom freezing is enabled, the label changes to “Disable zoom freeze”.In order to see how zoom freezing works, let’s consider the following example: a user wants to zoom inside the geohash with the code w886hu, without triggering the display of smaller geohashes inside of it. For this purpose, the user will enable the zoom freezing feature by clicking on the label, and then they will proceed with the zoom. The map will look like this:    It is apparent from the image that no new geohashes were created. Now, let’s say the user has finished what they wanted to do, and wants to go back to the “normal” geohash visualisation mode, which means disabling the zoom freeze option. After clicking on the label that now says ‘Disable zoom freeze’, new, smaller geohashes will be displayed, according to the current zoom level:    The functionality is illustrated in the following short video:    Another effect that enabling zoom freeze has is that it disables the ‘Display larger geohashes’ and ‘Display smaller geohashes’ options, since the geohashes are now fixed. The following images show how these options work before and after disabling zoom freeze:        To conclude, we believe that the release of this new feature will benefit users by making it more comfortable for them to zoom in and out of a map. By turning off the display of new geohashes when this is unwanted, map readability is improved, and this translates to a better user experience.Impact/LimitationsIn order to start using this new feature, users need to update the Geohash JOSM plugin.What’s next?Grab has come a long way in map-making, from using open source map-making software and developing its own suite of map-making tools to contributing to the open-source map community and building and launching GrabMaps. To find out more, read How KartaCam powers GrabMaps and KartaCam delivers comprehensive, cost-effective mapping data.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/geohash-plugin"
      }
      ,
    
      "graph-service-platform": {
        "title": "Graph service platform",
        "author": "wenxiang-lubruce-lijacob-yumuqi-lijia-chen",
        "tags": "[&quot;Engineering&quot;, &quot;Graph networks&quot;, &quot;Graphs&quot;, &quot;Graph visualisation&quot;, &quot;Security&quot;, &quot;Analytics&quot;, &quot;Fraud detection&quot;]",
        "category": "",
        "content": "IntroductionIn earlier articles of this series, we covered the importance of graph networks, graph concepts, how graph visualisation makes fraud investigations easier and more effective, and how graphs for fraud detection work. In this article, we elaborate on the need for a graph service platform and how it works.In the present age, data linkages can generate significant business value. Whether we want to learn about the relationships between users in online social networks, between users and products in e-commerce, or understand credit relationships in financial networks, the capability to understand and analyse large amounts of highly interrelated data is becoming more important to businesses.As the amount of consumer data grows, the GrabDefence team must continuously enhance fraud detection on mobile devices to proactively identify the presence of fraudulent or malicious users. Even simple financial transactions between users must be monitored for transaction loops and money laundering. To preemptively detect such scenarios, we need a graph service platform to help discover data linkages. BackgroundAs mentioned in an earlier article, a graph is a model representation of the association of entities and holds knowledge in a structured way by marginalising entities and relationships. In other words, graphs hold a natural interpretability of linked data and graph technology plays an important role. Since the early days, large tech companies started to create their own graph technology infrastructure, which is used for things like social relationship mining, web search, and sorting and recommendation systems with great commercial success.As graph technology was developed, the amount of data gathered from graphs started to grow as well, leading to a need for graph databases. Graph databases1 are used to store, manipulate, and access graph data on the basis of graph models. It is similar to the relational database with the feature of Online Transactional Processing (OLTP), which supports transactions, persistence, and other features.A key concept of graphs is the edge or relationship between entities. The graph relates the data items in the store to a collection of nodes and edges, the edges representing the relationships between the nodes. These relationships allow data in the store to be linked directly and retrieved with one operation.With graph databases, relationships between data can be queried fast as they are perpetually stored in the database. Additionally, relationships can be intuitively visualised using graph databases, making them useful for heavily interconnected data. To have real-time graph search capabilities, we must leverage the graph service platform and graph databases.Architecture detailsGraph services with graph databases are Platforms as a Service (PaaS) that encapsulate the underlying implementation of graph technology and support easier discovery of data association relationships with graph technologies.They also provide universal graph operation APIs and service management for users. This means that users do not need to build graph runtime environments independently and can explore the value of data with graph service directly.  Fig. 1 Graph service platform system architecture  As shown in Fig. 1, the system can be divided into four layers:  Storage backend - Different forms of data (for example, CSV files) are stored in Amazon S3, graph data stores in Neptune and meta configuration stores in DynamoDB.  Driver - Contains drivers such as Gremlin, Neptune, S3, and DynamoDB.  Service - Manages clusters, instances, databases etc, provides management API, includes schema and data load management, graph operation logic, and other graph algorithms.  RESTful APIs - Currently supports the standard and uniform formats provided by the system, the Management API, Search API for OLTP, and Analysis API for online analytical processing (OLAP).How it worksGraph flow  Fig. 2 Graph flow  CSV files stored in Amazon S3 are processed by extract, transform, and load (ETL) tools to generate graph data. This data is then managed by an Amazon Neptune DB cluster, which can only be accessed by users through graph service. Graph service converts user requests into asynchronous interactions with Neptune Cluster, which returns the results to users.When users launch data load tasks, graph service synchronises the entity and attribute information with the CSV file in S3, and the schema stored in DynamoDB. The data is only imported into Neptune if there are no inconsistencies.The most important component in the system is the graph service, which provides RESTful APIs for two scenarios: graph search for real-time streams and graph analysis for batch processing. At the same time, the graph service manages clusters, databases, instances, users, tasks, and meta configurations stored in DynamoDB, which implements features of service monitor and data loading offline or stream ingress online.Use case in fraud detectionIn Grab’s mobility business, we have come across situations where multiple accounts use shared physical devices to maximise their earning potential. With the graph capabilities provided by the graph service platform, we can clearly see the connections between multiple accounts and shared devices.Historical device and account data are stored in the graph service platform via offline data loading or online stream injection. If the device and account data exists in the graph service platform, we can find the adjacent account IDs or the shared device IDs by using the device ID or account ID respectively specified in the user request.In our experience, fraudsters tend to share physical resources to maximise their revenue. The following image shows a device that is shared by many users. With our Graph Visualisation platform based on graph service, you can see exactly what this pattern looks like.  Fig 3. Example of a device being shared with many users  Data injection  Fig. 4 Data injection  Graph service also supports data injection features, including data load by request (task with a type of data load) and real-time stream write by Kafka.  When connected to GrabDefence’s infrastructure, Confluent with Kafka is used as the streaming engine.  The purpose of using Kafka as a streaming write engine is two-fold: to provide primary user authentication and to relieve the pressure on Neptune.ImpactGraph service supports data management of Labelled Property Graphs and provides the capability to add, delete, update, and get vertices, edges, and properties for some graph models. Graph traversal and searching relationships with RESTful APIs are also more convenient with graph service.Businesses usually do not need to focus on the underlying data storage, just designing graph schemas for model definition according to their needs. With the graph service platform, platforms or systems can be built for personalised search, intelligent Q&amp;A, financial fraud, etc.For big organisations, extensive graph algorithms provide the power to mine various entity connectivity relationships in massive amounts of data. The growth and expansion of new businesses is driven by discovering the value of data.What’s next?  Fig. 5 Graph-centric ecosystems  We are building an integrated graph ecosystem inside and outside Grab. The infrastructure and service, or APIs are key components in graph-centric ecosystems; they provide graph arithmetic and basic capabilities of graphs in relation to search, computing, analysis etc. Besides that, we will also consider incorporating applications such as risk prediction and fraud detection in order to serve our current business needs.Speak to usGrabDefence is a proprietary fraud prevention platform built by Grab, Southeast Asia’s leading superapp. Since 2019, the GrabDefence team has shared its fraud management capabilities and platform with enterprises and startups to leverage Grab’s advanced AI/ML models, hyper local insights and patented device intelligence technologies.To learn more about GrabDefence or to speak to our fraud management experts, contact us at gd.contact@grabtaxi.com.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References            What is a Graph Database? - Developer Guides &#8617;      ",
        "url": "/graph-service-platform"
      }
      ,
    
      "zero-trust-with-kafka": {
        "title": "Zero trust with Kafka",
        "author": "fabrice-harbulotthanhtung-daoquangminh-tran",
        "tags": "[&quot;Engineering&quot;, &quot;Kafka&quot;, &quot;Performance&quot;, &quot;Zero trust&quot;, &quot;Access control&quot;]",
        "category": "",
        "content": "IntroductionGrab’s real-time data platform team, also known as Coban, has been operating large-scale Kafka clusters for all Grab verticals, with a strong focus on ensuring a best-in-class-performance and 99.99% availability.Security has always been one of Grab’s top priorities and as fraudsters continue to evolve, there is an increased need to continue strengthening the security of our data streaming platform. One of the ways of doing this is to move from a pure network-based access control to state-of-the-art security and zero trust by default, such as:  Authentication: The identity of any remote systems - clients and servers - is established and ascertained first, prior to any further communications.  Authorisation: Access to Kafka is granted based on the principle of least privilege; no access is given by default. Kafka clients are associated with the whitelisted Kafka topics and permissions - consume or produce - they strictly need. Also, granted access is auditable.  Confidentiality: All in-transit traffic is encrypted.SolutionWe decided to use mutual Transport Layer Security (mTLS) for authentication and encryption. mTLS enables clients to authenticate servers, and servers to reciprocally authenticate clients.Kafka supports other authentication mechanisms, like OAuth, or Salted Challenge Response Authentication Mechanism (SCRAM), but we chose mTLS because it is able to verify the peer’s identity offline. This verification ability means that systems do not need an active connection to an authentication server to ascertain the identity of a peer. This enables operating in disparate network environments, where all parties do not necessarily have access to such a central authority.We opted for Hashicorp Vault and its PKI engine to dynamically generate clients and servers’ certificates. This enables us to enforce the usage of short-lived certificates for clients, which is a way to mitigate the potential impact of a client certificate being compromised or maliciously shared. We said zero trust, right?For authorisation, we chose Policy-Based Access Control (PBAC), a more scalable solution than Role-Based Access Control (RBAC), and the Open Policy Agent (OPA) as our policy engine, for its wide community support.To integrate mTLS and the OPA with Kafka, we leveraged Strimzi, the Kafka on Kubernetes operator. In a previous article, we have alluded to Strimzi and hinted at how it would help with scalability and cloud agnosticism. Built-in security is undoubtedly an additional driver of our adoption of Strimzi.Server authentication  Figure 1 - Server authentication process for internal cluster communications  We first set up a single Root Certificate Authority (CA) for each environment (staging, production, etc.). This Root CA, in blue on the diagram, is securely managed by the Hashicorp Vault cluster. Note that the color of the certificates, keys, signing arrows and signatures on the diagrams are consistent throughout this article.To secure the cluster’s internal communications, like the communications between the Kafka broker and Zookeeper pods, Strimzi sets up a Cluster CA, which is signed by the Root CA (step 1). The Cluster CA is then used to sign the individual Kafka broker and zookeeper certificates (step 2). Lastly, the Root CA’s public certificate is imported into the truststores of both the Kafka broker and Zookeeper (step 3), so that all pods can mutually verify their certificates when authenticating one with the other.Strimzi’s embedded Cluster CA dynamically generates valid individual certificates when spinning up new Kafka and Zookeeper pods. The signing operation (step 2) is handled automatically by Strimzi.For client access to Kafka brokers, Strimzi creates a different set of intermediate CA and server certificates, as shown in the next diagram.  Figure 2 - Server authentication process for client access to Kafka brokers  The same Root CA from Figure 1 now signs a different intermediate CA, which the Strimzi community calls the Client CA (step 1). This naming is misleading since it does not actually sign any client certificates, but only the server certificates (step 2) that are set up on the external listener of the Kafka brokers. These server certificates are for the Kafka clients to authenticate the servers. This time, the Root CA’s public certificate will be imported into the Kafka Client truststore (step 3).Client authentication  Figure 3 - Client authentication process  For client authentication, the Kafka client first needs to authenticate to Hashicorp Vault and request an ephemeral certificate from the Vault PKI engine (step 1). Vault then issues a certificate and signs it using its Root CA (step 2). With this certificate, the client can now authenticate to Kafka brokers, who will use the Root CA’s public certificate already in their truststore, as previously described (step 3).CA treePutting together the three different authentication processes we have just covered, the CA tree now looks like this. Note that this is a simplified view for a single environment, a single cluster, and two clients only.  Figure 4 - Complete certificate authority tree  As mentioned earlier, each environment (staging, production, etc.) has its own Root CA. Within an environment, each Strimzi cluster has its own pair of intermediate CAs: the Cluster CA and the Client CA. At the leaf level, the Zookeeper and Kafka broker pods each have their own individual certificates.On the right side of the diagram, each Kafka client can get an ephemeral certificate from Hashicorp Vault whenever they need to connect to Kafka. Each team or application has a dedicated Vault PKI role in Hashicorp Vault, restricting what can be requested for its certificate (e.g., Subject, TTL, etc.).Strimzi deploymentWe heavily use Terraform to manage and provision our Kafka and Kafka-related components. This enables us to quickly and reliably spin up new clusters and perform cluster scaling operations.Under the hood, Strimzi Kafka deployment is a Kubernetes deployment. To increase the performance and the reliability of the Kafka cluster, we create dedicated Kubernetes nodes for each Strimzi Kafka broker and each Zookeeper pod, using Kubernetes taints and tolerations. This ensures that all resources of a single node are dedicated solely to either a single Kafka broker or a single Zookeeper pod.We also decided to go with a single Kafka cluster by Kubernetes cluster to make the management easier.Client setupCoban provides backend microservice teams from all Grab verticals with a popular Kafka SDK in Golang, to standardise how teams utilise Coban Kafka clusters. Adding mTLS support mostly boils down to upgrading our SDK.Our enhanced SDK provides a default mTLS configuration that works out of the box for most teams, while still allowing customisation, e.g., for teams that have their own Hashicorp Vault Infrastructure for compliance reasons. Similarly, clients can choose among various Vault auth methods such as AWS or Kubernetes to authenticate to Hashicorp Vault, or even implement their own logic for getting a valid client certificate.To mitigate the potential risk of a user maliciously sharing their application’s certificate with other applications or users, we limit the maximum Time-To-Live (TTL) for any given certificate. This also removes the overhead of maintaining a Certificate Revocation List (CRL). Additionally, our SDK stores the certificate and its associated private key in memory only, never on disk, hence reducing the attack surface.In our case, Hashicorp Vault is a dependency. To prevent it from reducing the overall availability of our data streaming platform, we have added two features to our SDK – a configurable retry mechanism and automatic renewal of clients’ short-lived certificates when two thirds of their TTL is reached. The upgraded SDK also produces new metrics around this certificate renewal process, enabling better monitoring and alerting.Authorisation  Figure 5 - Authorisation process before a client can access a Kafka record  For authorisation, we set up the Open Policy Agent (OPA) as a standalone deployment in the Kubernetes cluster, and configured Strimzi to integrate the Kafka brokers with that OPA.OPA policies - written in the Rego language - describe the authorisation logic. They are created in a GitLab repository along with the authorisation rules, called data sources (step 1). Whenever there is a change, a GitLab CI pipeline automatically creates a bundle of the policies and data sources, and pushes it to an S3 bucket (step 2). From there, it is fetched by the OPA (step 3).When a client - identified by its TLS certificate’s Subject - attempts to consume or produce a Kafka record (step 4), the Kafka broker pod first issues an authorisation request to the OPA (step 5) before processing the client’s request. The outcome of the authorisation request is then cached by the Kafka broker pod to improve performance.As the core component of the authorisation process, the OPA is deployed with the same high availability as the Kafka cluster itself, i.e. spread across the same number of Availability Zones. Also, we decided to go with one dedicated OPA by Kafka cluster instead of having a unique global OPA shared between multiple clusters. This is to reduce the blast radius of any OPA incidents.For monitoring and alerting around authorisation, we submitted an Open Source contribution in the opa-kafka-plugin project in order to enable the OPA authoriser to expose some metrics. Our contribution to the open source code allows us to monitor various aspects of the OPA, such as the number of authorised and unauthorised requests, as well as the cache hit-and-miss rates. Also, we set up alerts for suspicious activity such as unauthorised requests.Finally, as a platform team, we need to make authorisation a scalable, self-service process. Thus, we rely on the Git repository’s permissions to let Kafka topics’ owners approve the data source changes pertaining to their topics.Teams who need their applications to access a Kafka topic would write and submit a JSON data source as simple as this:{ \"example_topic\": {   \"read\": [     \"clientA.grab\",     \"clientB.grab\"   ],   \"write\": [     \"clientB.grab\"   ] }}GitLab CI unit tests and business logic checks are set up in the Git repository to ensure that the submitted changes are valid. After that, the change would be submitted to the topic’s owner for review and approval.What’s next?The performance impact of this security design is significant compared to unauthenticated, unauthorised, plaintext Kafka. We observed a drop in throughput, mostly due to the low performance of encryption and decryption in Java, and are currently benchmarking different encryption ciphers to mitigate this.Also, on authorisation, our current PBAC design is pretty static, with a list of applications granted access for each topic. In the future, we plan to move to Attribute-Based Access Control (ABAC), creating dynamic policies based on teams and topics’ metadata. For example, teams could be granted read and write access to all of their own topics by default. Leveraging a versatile component such as the OPA as our authorisation controller enables this evolution.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/zero-trust-with-kafka"
      }
      ,
    
      "kartacam-powers-grabmaps": {
        "title": "How KartaCam powers GrabMaps",
        "author": "shuangquan-houvictor-liangalex-iliseizhixin-yusuwei-yangan-tranameya-mannikar",
        "tags": "[&quot;Engineering&quot;, &quot;GrabMaps&quot;, &quot;KartaCam&quot;, &quot;Maps&quot;, &quot;Edge AI&quot;]",
        "category": "",
        "content": "IntroductionThe foundation for making any map is in imagery, but due to the complexity and dynamism of the real world, it is difficult for companies to collect high-quality, fresh images in an efficient yet low-cost manner. This is the case for Grab’s Geo team as well.Traditional map-making methods rely on professional-grade cameras that provide high resolution images to collect mapping imagery. These images are rich in content and detail, providing a good snapshot of the real world. However, we see two major challenges with this approach.The first is high cost. Professional cameras are too expensive to use at scale, especially in an emerging region like Southeast Asia. Apart from high equipment cost, operational cost is also high as local operation teams need professional training before collecting imagery.The other major challenge, related to the first, is that imagery will not be refreshed in a timely manner because of the high cost and operational effort required. It typically takes months or years before imagery is refreshed, which means maps get outdated easily.Compared to traditional collection methods, there are more affordable alternatives that some emerging map providers are using, such as crowdsourced collection done with smartphones or other consumer-grade action cameras. This allows more timely imagery refresh at a much lower cost.That said, there are several challenges with crowdsourcing imagery, such as:  Inconsistent quality in collected images.  Low operational efficiency as cameras and smartphones are not optimised for mapping.  Unreliable location accuracy.In order to solve the challenges above, we started building our own artificial intelligence (AI) camera called KartaCam.What is KartaCam?Designed specifically for map-making, KartaCam is a lightweight camera that is easy to operate. It is everything you need for accurate and efficient image collection. KartaCam is powered by edge AI, and mainly comprises a camera module, a dual-band Global Navigation Satellite System (GNSS) module, and a built-in 4G Long-Term Evolution (LTE) module.  KartaCam  Camera moduleThe camera module or optical design of KartaCam focuses on several key features:  Wide field of vision (FOV): A wide FOV to capture as many scenes and details as possible without requiring additional trips. A single KartaCam has a wide lens FOV of &gt;150° and when we use four KartaCams together, each facing a different direction, we increase the FOV to 360°.  High image quality: A combination of high-definition optical lens and a high-resolution pixel image sensor can help to achieve better image quality. KartaCam uses a high-quality 12MP image sensor.  Ease of use: Portable and easy to start using for people with little to no photography training. At Grab, we can easily deploy KartaCam to our fleet of driver-partners to map our region as they regularly travel these roads while ferrying passengers or making deliveries.Edge AI for smart capturing on edgeEach KartaCam device is also equipped with edge AI, which enables AI computations to operate closer to the actual data – in our case, imagery collection. With edge AI, we can make decisions about imagery collection (i.e. upload, delete or recapture) at the device-level.To help with these decisions, we use a series of edge AI models and algorithms that are executed immediately after each image capture such as:      Scene recognition model: For efficient map-making, we ensure that we make the right screen verdicts, meaning we only upload and process the right scene images. Unqualified images such as indoor, raining, and cloudy images are deleted directly on the KartaCam device. Joint detection algorithms are deployed in some instances to improve the accuracy of scene verdicts. For example, to detect indoor recording we look at a combination of driver moving speed, Inertial Measurement Units (IMU) data and edge AI image detection.        Image quality (IQ) checking AI model: The quality of the images collected is paramount for map-making. Only qualified images judged by our IQ classification algorithm will be uploaded while those that are blurry or considered low-quality will be deleted. Once an unqualified image is detected (usually within the next second), a new image is captured, improving the success rate of collection.        Object detection AI model: Only roadside images that contain relevant map-making content such as traffic signs, lights, and Point of Interest (POI) text are uploaded.        Privacy information detection: Edge AI also helps protect privacy when collecting street images for map-making. It automatically blurs private information such as pedestrians’ faces and car plate numbers before uploading, ensuring adequate privacy protection.      Better positioning with a dual-band GNSS moduleThe Global Positioning System (GPS) mainly uses two frequency bands: L1 and L5. Most traditional phone or GPS modules only support the legacy GPS L1 band, while modern GPS modules support both L1 and L5. KartaCam leverages the L5 band which provides improved signal structure, transmission capabilities, and a wider bandwidth that can reduce multipath error, interference, and noise impacts. In addition, KartaCam uses a fine-tuned high-quality ceramic antenna that, together with the dual frequency band GPS module, greatly improves positioning accuracy.Keeping KartaCam connectedKartaCam has a built-in 4G LTE module that ensures it is always connected and can be remotely managed. The KartaCam management portal can monitor camera settings like resolution and capturing intervals, even in edge AI machine learning models. This makes it easy for Grab’s map ops team and drivers to configure their cameras and upload captured images in a timely manner.Enhancing KartaCamKartaCam 360: Capturing a panorama viewTo improve single collection trip efficiency, we group four KartaCams together to collect 360° images. The four cameras can be synchronised within milliseconds and the collected images are stitched together in a panoramic view.With KartaCam 360, we can increase the number of images collected in a single trip. According to Grab’s benchmark testing in Singapore and Jakarta, the POI information collected by KartaCam 360 is comparable to that of professional cameras, which cost about 20x more.  KartaCam 360 &amp; Scooter mount    Image sample from KartaCam 360  KartaCam and the image collection workflowKartaCam, together with other GrabMaps imagery tools, provides a highly efficient, end-to-end, low-cost, and edge AI-powered smart solution to map the region. KartaCam is fully integrated as part of our map-making workflow.Our map-making solution includes the following components:  Collection management tool - Platform that defines map collection tasks for our driver-partners.  KartaView application - Mobile application provides map collection tasks and handles crowdsourced imagery collection.  KartaCam - Camera device connected to KartaView via Bluetooth and equipped with edge automatic processing for imagery capturing according to the task accepted.  Camera management tool - Handles camera parameters and settings for all KartaCam devices and can remotely control the KartaCam.  Automatic processing - Collected images are processed for quality check, stitching, and personal identification information (PII) blurring.  KartaView imagery platform - Processed images are then uploaded and the driver-partner receives payment.    In a future article, we will dive deeper into the technology behind KartaView and its role in GrabMaps.ImpactAt the moment, Grab is rolling out thousands of KartaCams to all locations across Southeast Asia where Grab operates. This saves operational costs while improving the efficiency and quality of our data collection.    Better data quality and more map attributesDue to the excellent image quality, wide FOV coverage, accurate GPS positioning, and sensor data, the 360° images captured by KartaCam 360 also register detailed map attributes like POIs, traffic signs, and address plates. This will help us build a high quality map with rich and accurate content.    Reducing operational costsBased on our research, the hardware cost for KartaCam 360 is significantly lower compared to similar professional cameras in the market. This makes it a more feasible option to scale up in Southeast Asia as the preferred tool for crowdsourcing imagery collection.With image quality checks and detection conducted at the edge, we can avoid re-collections and also ensure that only qualified images are uploaded. These result in saving time as well as operational and upload costs.Upholding privacy standardsKartaCam automatically blurs captured images that contain PII, like faces and licence plates directly from the edge devices. This means that all sensitive information is removed at this stage and is never uploaded to Grab servers.  On-the-edge blurring example  What’s next?Moving forward, Grab will continue to enhance KartaCam’s performance in the following aspects:  Further improve image quality with better image sensors, unique optical components, and state-of-art Image Signal Processor (ISP).  Make KartaCam compatible with Light Detection And Ranging (LIDAR) for high-definition collection and indoor use cases.  Improve GNSS module performance with higher sampling frequency and accuracy, and integrate new technology like Real-Time Kinematic (RTK) and Precise Point Positioning (PPP) solutions to further improve the positioning accuracy. When combined with sensor fusion from IMU sensors, we can improve positioning accuracy for map-making further.  Improve usability, integration, and enhance imagery collection and portability for KartaCam so driver-partners can easily capture mapping data.   Explore new product concepts for future passive street imagery collection.To find out more about how KartaCam delivers comprehensive cost-effective mapping data, check out this article.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References",
        "url": "/kartacam-powers-grabmaps"
      }
      ,
    
      "graph-for-fraud-detection": {
        "title": "Graph for fraud detection",
        "author": "min-chenadvitiya-vashistjenn-ngjia-chen",
        "tags": "[&quot;Analytics&quot;, &quot;Data Science&quot;, &quot;Security&quot;, &quot;Graphs&quot;, &quot;Graph visualisation&quot;, &quot;Graph networks&quot;, &quot;Fraud detection&quot;]",
        "category": "",
        "content": "In earlier articles of this series, we’ve covered the importance of graph networks, graph concepts and how graph visualisation makes fraud investigations easier and more effective. In this article, we will explore how we use graph-based models to tackle fraud detection as fraud patterns increase and diversify.Grab has grown rapidly in the past few years. It has expanded its business from ride hailing to food and grocery delivery, financial services, and more. Fraud detection is challenging in Grab, because new fraud patterns always arise whenever we introduce a new business product. We cannot afford to develop a new model whenever a new fraud pattern appears as it is time consuming and introduces a cold start problem, that is no protection at the early stage. We need a general fraud detection framework to better protect Grab from various unknown fraud risks.Our key observation is that although Grab has many different business verticals, the entities within those businesses are connected to each other (Figure 1. Left), for example, two passengers may be connected by a Wi-Fi router or phone device, a merchant may be connected to a passenger by a food order, and so on. A graph provides an elegant way to capture the spatial correlation among different entities in the Grab ecosystem. A common fraud shows clear patterns on a graph, for example, a fraud syndicate tends to share physical devices, and collusion happens between a merchant and an isolated set of passengers (Figure 1. Right).  Figure 1. Left: The graph captures different correlations in the Grab ecosystem.  Right: The graph shows that common fraud has clear patterns.  We believe graphs can help us discover subtle traces and complicated fraud patterns more effectively. Graph-based solutions will be a sustainable foundation for us to fight against known and unknown fraud risks.Why graph?The most common fraud detection methods include the rule engine and the decision tree-based models, for example, boosted tree, random forest, and so on. Rules are a set of simple logical expressions designed by human experts to target a particular fraud problem. They are good for simple fraud detection, but they usually do not work well in complicated fraud or unknown fraud cases.            Fraud detection methods       Utilises correlations  (Higher is better)      Detects unknown fraud  (Higher is better)      Requires feature engineering  (Lower is better)      Depends on labels  (Lower is better)                  Rule engine      Low      N/A      N/A      Low              Decision tree      Low      Low      High      High              Graph model      High      High      Low      Low      Table 1. Graph vs. common fraud detection methods.Decision tree-based models have been dominating fraud detection and Kaggle competitions for structured or tabular data in the past few years. With that said, the performance of a tree-based model is highly dependent on the quality of labels and feature engineering, which is often hard to obtain in real life. In addition, it usually does not work well in unknown fraud which has not been seen in the labels.On the other hand, a graph-based model requires little amount of feature engineering and it is applicable to unknown fraud detection with less dependence on labels, because it utilises the structural correlations on the graph.In particular, fraudsters tend to show strong correlations on a graph, because they have to share physical properties such as personal identities, phone devices, Wi-Fi routers, delivery addresses, and so on, to reduce cost and maximise revenue as shown in Figure 2 (left). An example of such strong correlations is shown in Figure 2 (right), where the entities on the graph are densely connected, and the known fraudsters are highlighted in red. Those strong correlations on the graph are the key reasons that make the graph based approach a sustainable foundation for various fraud detection tasks.  Figure 2. Fraudsters tend to share physical properties to reduce cost (left), and they are densely connected as shown on a graph (right).  Semi-supervised graph learningUnlike traditional decision tree-based models, the graph-based machine learning model can utilise the graph’s correlations and achieve great performance even with few labels. The semi-supervised Graph Convolutional Network model has been extremely popular in recent years 1. It has proven its success in many fraud detection tasks across industries, for example, e-commerce fraud, financial fraud, internet traffic fraud, etc.We apply the Relational Graph Convolutional Network (RGCN) 2 for fraud detection in Grab’s ecosystem. Figure 3 shows the overall architecture of RGCN. It takes a graph as input, and the graph passes through several graph convolutional layers to get node embeddings. The final layer outputs a fraud probability for each node. At each graph convolutional layer, the information is propagated along the neighbourhood nodes within the graph, that is nodes that are close on the graph are similar to each other.  Fig 3. A semi-supervised Relational Graph Convolutional Network model.  We train the RGCN model on a graph with millions of nodes and edges, where only a few percentages of the nodes on the graph have labels. The semi-supervised graph model has little dependency on the labels, which makes it a robust model for tackling various types of unknown fraud.Figure 4 shows the overall performance of the RGCN model. On the left is the Receiver Operating Characteristic (ROC) curve on the label dataset, in particular, the Area Under the Receiver Operating Characteristic (AUROC) value is close to 1, which means the RGCN model can fit the label data quite well. The right column shows the low dimensional projections of the node embeddings on the label dataset. It is clear that the embeddings of the genuine passenger are well separated from the embeddings of the fraud passenger. The model can distinguish between a fraud and a genuine passenger quite well.  Fig 4. Left: ROC curve of the RGCN model on the label dataset.  Right: Low dimensional projections of the graph node embeddings.  Finally, we would like to share a few tips that will make the RGCN model work well in practice.  Use less than three convolutional layers: The node feature will be over-smoothed if there are many convolutional layers, that is all the nodes on the graph look similar.  Node features are important: Domain knowledge of the node can be formulated as node features for the graph model, and rich node features are likely to boost the model performance.Graph explainabilityUnlike other deep network models, graph neural network models usually come with great explainability, that is why a user is classified as fraudulent. For example, fraudulent accounts are likely to share hardware devices and form dense clusters on the graph, and those fraud clusters can be easily spotted on a graph visualiser 3.Figure 5 shows an example where graph visualisation helps to explain the model prediction scores. The genuine passenger with a low RGCN score does not share devices with other passengers, while the fraudulent passenger with a high RGCN score shares devices with many other passengers, that is, dense clusters.  Figure 5. Upper left: A genuine passenger with a low RGCN score has no device sharing with other passengers. Bottom right: A fraudulent user with a high RGCN score shares devices with many other passengers.  Closing thoughtsGraphs provide a sustainable foundation for combating many different types of fraud risks. Fraudsters are evolving very fast these days, and the best traditional rules or models can do is to chase after those fraudsters given that a fraud pattern has already been discovered. This is suboptimal as the damage has already been done on the platform. With the help of graph models, we can potentially detect those fraudsters before any fraudulent activity has been conducted, thus reducing the fraud cost.The graph structural information can significantly boost the model performance without much dependence on labels, which is often hard to get and might have a large bias in fraud detection tasks. We have shown that with only a small percentage of labelled nodes on the graph, our model can already achieve great performance.With that said, there are also many challenges to making a graph model work well in practice. We are working towards solving the following challenges we are facing.  Feature initialisation: Sometimes, it is hard to initialise the node feature, for example, a device node does not carry many semantic meanings. We have explored self-supervised pre-training 4 to help the feature initialisation, and the preliminary results are promising.  Real-time model prediction: Realtime graph model prediction is challenging because real-time graph updating is a heavy operation in most cases. One possible solution is to do batch real-time prediction to reduce the overhead.  Noisy connections: Some connections on the graph are inherently noisy on the graph, for example, two users sharing the same IP address does not necessarily mean they are physically connected. The IP might come from a mobile network. One possible solution is to use the attention mechanism in the graph convolutional kernel and control the message passing based on the type of connection and node profiles.Speak to usGrabDefence is a proprietary fraud prevention platform built by Grab, Southeast Asia’s leading superapp. Since 2019, the GrabDefence team has shared its fraud management capabilities and platform with enterprises and startups to leverage Grab’s advanced AI/ML models, hyper local insights and patented device intelligence technologies.To learn more about GrabDefence or to speak to our fraud management experts, contact us at gd.contact@grabtaxi.com.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References            T. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in ICLR, 2017 &#8617;              Schlichtkrull, Michael, et al. “Modeling relational data with graph convolutional networks.” European semantic web conference. Springer, Cham, 2018. &#8617;              Fujiao Liu, Shuqi Wang, et al.. “Graph Networks - 10X investigation with Graph Visualisations”. Grab Tech Blog. &#8617;              Wang, Chen, et al.. “Deep Fraud Detection on Non-attributed Graph.” IEEE Big Data conference, PSBD, 2021. &#8617;      ",
        "url": "/graph-for-fraud-detection"
      }
      ,
    
      "query-expansion-based-on-user-behaviour": {
        "title": "Query expansion based on user behaviour",
        "author": "shuailong-liangweilun-wuyuan-mengsimone-wong",
        "tags": "[&quot;Analytics&quot;, &quot;Data Science&quot;]",
        "category": "",
        "content": "IntroductionOur consumers used to face a few common pain points while searching for food with the Grab app. Sometimes, the results would include merchants that were not yet operational or locations that were out of the delivery radius. Other times, no alternatives were provided. The search system would also have difficulties handling typos, keywords in different languages, synonyms, and even word spacing issues, resulting in a suboptimal user experience.Over the past few months, our search team has been building a query expansion framework that can solve these issues. When a user query comes in, it expands the query to a few related keywords based on semantic relevance and user intention. These expanded words are then searched with the original query to recall more results that are high-quality and diversified. Now let’s take a deeper look at how it works.Query expansion frameworkBuilding the query expansion corpusWe used two different approaches to produce query expansion candidates: manual annotation for top keywords and data mining based on user rewrites.Manual annotation for top keywordsSearch has a pronounced fat head phenomenon. The most frequent thousand of keywords account for more than 70% of the total search traffic. Therefore, handling these keywords well can improve the overall search quality a lot. We manually annotated the possible expansion candidates for these common keywords to cover the most popular merchants, items and alternatives. For instance, “McDonald’s” is annotated with {“burger”, “western”}.Data mining based on user rewritesWe observed that sometimes users tend to rewrite their queries if they are not satisfied with the search result. As a pilot study, we checked the user rewrite records within some user search sessions and found several interesting samples:{Ya Kun Kaya Toast,Starbucks}{healthy,Subway}{Muni,Muji}{奶茶,koi}{Roti,Indian}We can see that besides spelling corrections, users’ rewrite behaviour also reveals deep semantic relations between these pairs that cannot be easily captured by lexical similarity, such as similar merchants, merchant attributes, language differences, cuisine types, and so on. We can leverage the user’s knowledge to build a query expansion corpus to improve the diversity of the search result and user experience. Furthermore, we can use the wisdom of the crowd to find some common patterns with higher confidence.Based on this intuition, we leveraged the high volume of search click data available in Grab to generate high-quality expansion pairs at the user session level. To augment the original queries, we collected rewrite pairs that happened to multiple users and multiple times in a time period. Specifically, we used the heuristic rules below to collect the rewrite pairs:  Select the sessions where there are at least two distinct queries (rewrite session)  Collect adjacent query pairs in the search session where the second query leads to a click but the first does not (effective rewrite)  Filter out the sample pairs with time interval longer than 30 seconds in between, as users are more likely to change their mind on what to look for in these pairs (single intention)  Count the occurrences and filter out the low-frequency pairs (confidence management)After we have the mining pairs, we categorised and annotated the rewrite types to gain a deeper understanding of the user’s rewrite behaviour. A few samples mined from the Singapore area data are shown in the table below.      Original query    Rewrite query    Frequency in a month    Distinct user count    Type        playmade by 丸作    playmade    697    666    Drop keywords        mcdonald's    burger    573    535    Merchant -&gt; Food        Bubble tea    koi    293    287    Food -&gt; Merchant        Kfc    McDonald's    238    234    Merchant -&gt; Merchant        cake    birthday cake    206    205    Add words        麦当劳    mcdonald's    205    199    Locale change        4 fingers    4fingers    165    162    Space correction        krc    kfc    126    124    Spelling correction        5 guys    five guys    120    120    Number synonym        koi the    koi thé    45    44    Tone change  We further computed the percentages of some categories, as shown in the figure below.    Figure 1. The donut chart illustrates the percentages of the distinct user counts for different types of rewrites.  Apart from adding words, dropping words and spelling corrections, a significant portion of the rewrites are in the category of Other. It is more semantic driven, such as merchant to merchant, or merchant to cuisine. Those rewrites are useful for capturing deeper connections between queries and can be a powerful diversifier to query expansion.GroupingAfter all the rewrite pairs were discovered offline through data mining, we grouped the query pairs by the original query to get the expansion candidates of each query. For serving efficiency, we limited the max number of expansion candidates to three.Query expansion servingExpansion matching architectureThe expansion matching architecture benefits from the recent search architecture upgrade, where the system flow is changed to a query understanding, multi-recall and result fusion flow. In particular, a query goes through the query understanding module and gets augmented with additional information. In this case, the query understanding module takes in the keyword and expands it to multiple synonyms, for example, KFC will be expanded to fried chicken. The original query together with its expansions are sent together to the search engine under the multi-recall framework. After that, results from multiple recallers with different keywords are fused together.Continuous monitoring and feedback loopIt’s important to make sure the expansion pairs are relevant and up-to-date. We run the data mining pipeline periodically to capture the new user rewrite behaviours. Meanwhile, we also monitor the expansion pairs’ contribution to the search result by measuring the net contribution of recall or user interaction that the particular query brings, and eliminate the obsolete pairs in an automatic way. This reflects our effort to build an adaptive system.ResultsWe conducted online A/B experiments across 6 countries in Southeast Asia to evaluate the expanded queries generated by our system. We set up 3 groups:  Control group, where no query is expanded.  Treatment group 1, where we expanded the queries based on manual annotations only.  Treatment group 2, where we expanded the queries using the data mining approach.We observed decent uplift in click-through rate and conversion rate from both treatment groups. Furthermore, in treatment group 2, the data mining approach produced even better results.Future workData mining enhancementCurrently, the data mining approach can only identify the pairs from the same search session by one user. This limits the number of linked pairs. Some potential enhancements include:  Augment expansion pairs by associating queries from different users who click on the same merchant/item, for example, using a click graph. This can capture relevant queries across user sessions.  Build a probabilistic model on top of the current transition pairs. Currently, all the transition pairs are equally weighted but apparently, the transitions that happen more often should carry higher probability/weights.Ads applicationQuery expansion can be applied to advertising and would increase ads fill rate. With “KFC” expanded to “fried chicken”, the sponsored merchants who buy the keyword “fried chicken” would be eligible to show up when the user searches “KFC”. This would enable Grab to provide more relevant sponsored content to our users, which helps not only the consumers but also the merchants.Special thanks to Zhengmin Xu and Daniel Ng for proofreading this article.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/query-expansion-based-on-user-behaviour"
      }
      ,
    
      "using-mobile-sensor-data-to-encourage-safer-driving": {
        "title": "Using mobile sensor data to encourage safer driving",
        "author": "laiyi-lin",
        "tags": "[&quot;Analytics&quot;, &quot;Driving patterns&quot;, &quot;Data Science&quot;, &quot;GPS&quot;, &quot;Security&quot;]",
        "category": "",
        "content": "“Telematics”, a cross between the words telecommunications and informatics, was coined in the late 1970s to refer to the use of communication technologies in facilitating exchange of information. In the modern day, such technologies may include cloud platforms, mobile networks, and wireless transmissions (e.g., Bluetooth). Although the initial intention is for a more general scope, telematics is now specifically used to refer to vehicle telematics where details of vehicle movements are tracked for use cases such as driving safety, driver profiling, fleet optimisation, and productivity improvements.We’ve previously published this article to share how Grab uses telematics to improve driver safety. In this blog post, we dive deeper into how telematics technology is used at Grab to encourage safer driving for our driver and delivery partners.BackgroundAt Grab, the safety of our users and their experience on our platform is our highest priority. By encouraging safer driving habits from our driver and delivery partners, road traffic accidents can be minimised, potentially reducing property damage, injuries, and even fatalities. Safe driving also helps ensure smoother rides and a more pleasant experience for consumers using our platform.To encourage safer driving, we should:  Have a data-driven approach to understand how our driver and delivery partners are driving.  Help partners better understand how to improve their driving by summarising key driving history into a personalised Driving Safety Report.Understanding driving behaviourOne of the most direct forms of driving assessment is consumer feedback or complaints. However, the frequency and coverage of this feedback is not very high as they are only applicable to transport verticals like JustGrab or GrabBike and not delivery verticals like GrabFood or GrabExpress. Plus, most driver partners tend not to receive any driving-related feedback (whether positive or negative), even for the transport verticals.A more comprehensive method of assessing driving behaviour is to use the driving data collected during Grab bookings. To make sense of these data, we focus on selected driving manoeuvres (e.g., braking, acceleration, cornering, speeding) and detect the number of instances where our data shows unsafe driving in each of these areas.We acknowledge that the detected instances may be subjected to errors and may not provide the complete picture of what’s happening on the ground (e.g., partners may be forced to do an emergency brake due to someone swerving into their lane).To address this, we have incorporated several fail-safe checks into our detection logic to minimise erroneous detection. Also, any assessment of driving behaviour will be based on an aggregation of these unsafe driving instances over a large amount of driving data. For example, individual harsh braking instances may be inconclusive but if a driver partner displays multiple counts consistently across many bookings, it is likely that the partner may be used to unsafe driving practices like tailgating or is distracted while driving.Telematics for detecting unsafe drivingFor Grab to consistently ensure our consumers’ safety, we need to proactively detect unsafe driving behaviour before an accident occurs. However, it is not feasible for someone to be with our driver and delivery partners all the time to observe their driving behaviour. We should leverage sensor data to monitor these driving behaviour at scale.Traditionally, a specialised “black box” inertial measurement unit (IMU) equipped with sensors such as accelerometers, gyroscopes, and GPS needs to be installed in alignment with the vehicle to directly measure vehicular acceleration and speed. In this manner, it would be straightforward to detect unsafe driving instances using this data. Unfortunately, the cost of purchasing and installing such devices for all our partners is prohibitively high and it would be hard to scale.Instead, we can leverage a device that all partners already have: their mobile phone. Modern smartphones already contain similar sensors to those in IMUs and data can be collected through the telematics SDK. More details on telematics data collection can be found in a recently published Grab tech blog article1.It’s important to note that telematics data are collected at a sufficiently high sampling frequency (much more than 1 Hz) to minimise inaccuracies in detecting unsafe driving instances characterised by sharp acceleration impulses.Processing mobile sensor data to detect unsafe drivingUnlike specialised IMUs installed in vehicles, mobile sensor data have added challenges to detecting unsafe driving.Accounting for orientation: Phone vs. vehicleThe phone is usually in a different orientation compared to the vehicle. Strictly speaking, the phone accelerometer sensor measures the accelerations of the phone and not the vehicle acceleration. To infer vehicle acceleration from phone sensor data, we developed a customised processing algorithm optimised specifically for Grab’s data.First, the orientation offset of the phone with respect to the vehicle is defined using Euler angles: roll, pitch and yaw. In data windows with no net acceleration of the vehicle (e.g., no braking, turning motion), the only acceleration measured by the accelerometer is gravitational acceleration. Roll and pitch angles can then be determined through trigonometric manipulation. The complete triaxial accelerations of the phone are then rotated to the horizontal plane and the yaw angle is determined by principal component analysis (PCA).An assumption here is that there will be sufficient braking and acceleration manoeuvring for PCA to determine the correct forward direction. This Euler angles determination is done periodically to account for any movement of phones during the trip. Finally, the raw phone accelerations are rotated to the vehicle orientation through a matrix multiplication with the rotation matrix derived from the Euler angles (see Figure 1).    Figure 1: Inference of vehicle acceleration from the phone sensor data. Smartphone and car images modified from designs found in Freepik.com.  Handling variations in data qualityOur processing algorithm is optimised to be highly robust and handle large variations in data quality that is expected from bookings on the Grab platform. There are many reported methods for processing mobile data to reorientate telematics data for four wheel vehicles23.However, with the prevalent use of motorcycles on our platform, especially for delivery verticals, we observed that data collected from two wheel vehicles tend to be noisier due to differences in phone stability and vehicular vibrations. Data noise can be exacerbated if partners hold the phone in their hand or place it in their pockets while driving.In addition, we also expect a wide variation in data quality and sensor availability from different phone models, such as older, low-end models to the newest, flagship models. A good example to illustrate the robustness of our algorithm is having different strategies to handle different degrees of data noise. For example, a simple low-pass filter is used for low noise data, while more complex variational decomposition and Kalman filter approaches are used for high noise data.Detecting behaviour anomalies with thresholdsOnce the vehicular accelerations are inferred, we can use a thresholding approach (see Figure 2) to detect unsafe driving instances.For unsafe acceleration and braking, a peak finding algorithm is used to detect acceleration peaks beyond a threshold in the longitudinal (forward/backward) direction. For unsafe cornering, older and lower end phones are usually not equipped with gyroscope sensors, so we should look for peaks of lateral (sidewards) acceleration (which constitutes the centripetal acceleration during the turn) beyond a threshold. GPS bearing data that coarsely measures the orientation of the vehicle is then used to confirm that a cornering and not lane change instance is being detected. The thresholds selected are fine-tuned on Grab’s data using initial values based on published literature4 and other sources.To reduce false positive detection, no unsafe driving instances will be flagged when:  Large discrepancies are observed between speeds derived from integrating the longitudinal (forward/backward) acceleration and speeds directly measured by the GPS sensor.  Large phone motions are detected. For example, when the phone falls to the seat from the dashboard, accelerations recorded on the phone sensor will deviate significantly from the vehicle accelerations.  GPS speed is very low before and after the unsafe driving instance is detected. This is limited to data collected from motorcycles which is usually used by delivery partners. It implies that the partner is walking and not in a vehicle. For example, a GrabFood delivery partner may be collecting the food from the merchant partner on foot, so no unsafe driving instances should be detected.    Figure 2: Animation showing unsafe driving detection by thresholding. Dotted lines in acceleration charts indicate selected thresholds. Map tiles by stamen design.  Detecting speeding instances from GPS speeds and map dataTo define speeding along a stretch of road, we used a rule-based method by comparing raw speeds from GPS pings with speeding thresholds for that road. Although GPS speeds are generally accurate (subjected to minimal GPS errors), we need to take more precautions to ensure the right speeding thresholds are determined.These thresholds are set using known speed limits from available map data or hourly aggregated speed statistics where speed limits are not available. The coverage and accuracy of known speed limits is continuously being improved by our in-house mapping initiatives and  validated comprehensively by the respective local ground teams in selected cities.Aggregating GPS pings from Grab driver and delivery partners can be a helpful proxy to actual speed limits by defining speeding violations as outliers from socially acceptable speeds derived from partners collectively. To reliably compute aggregated speed statistics, a representative speed profile for each stretch of road must first be inferred from raw GPS pings (see Figure 3).As ping sampling intervals are fixed, more pings tend to be recorded for slower speeds. To correct the bias in the speed profile, we reweigh ping counts by using speed values as weights. Furthermore, to minimise distortions in the speed profile from vehicles driving at lower-than-expected speeds due to high traffic volumes, only pings from free-flowing traffic are used when inferring the speed profile.Free-flowing traffic is defined by speeds higher than the median speed on each defined road category (e.g., small residential roads, normal primary roads, large expressways). To ensure extremely high speeds are flagged regardless of the speed of other drivers, maximum threshold values for aggregated speeds are set for each road category using heuristics based on the maximum known speed limit of that road category.    Figure 3: Steps to infer a representative speed profile for computing aggregated speed statistics.  Besides a representative speed profile, hourly aggregation should also include data from a sufficient number of unique drivers depending on speed variability. To obtain enough data, hourly aggregations are performed on the same day of the week over multiple weeks. This way, we have a comprehensive time-specific speed profile that accounts for traffic quality (e.g., peak hour traffic, traffic differences between weekdays/weekends) and driving conditions (e.g., visibility difference between day/night).When detecting speeding violations, the GPS pings used are snapped-to-road and stationary pings, pings with unrealistic speeds, while pings with low GPS accuracy (e.g., when the vehicle is in a tunnel) are excluded. A speeding violation is defined as a sequence of consecutive GPS pings that exceed the speeding threshold. The following checks were put in place to minimise erroneous flagging of speeding violations:  Removal of duplicated (or stale) GPS pings.  Sufficient speed buffer given to take into account GPS errors.  Sustained speeding for a prolonged period of time is required to exclude transient speeding events (e.g., during lane change).Driving safety reportThe driving safety report is a platform safety product that driver and delivery partners can access via their driver profile page on the Grab Driver Application (see Figure 4). It is updated daily and aims to create awareness regarding driving habits by summarising key information from the processed data into a personalised report that can be easily consumed.Individual reports of each driving manoeuvre (e.g., braking, acceleration, cornering and speeding) are available for daily and weekly views. Partners can also get more detailed information of each individual instance such as when these unsafe driving instances were detected.    Figure 4: Driving safety report for driver and delivery partners using four wheel vehicles. a) Actionable insights feature circled by red dotted lines. b) Daily view of various unsafe driving instances where more details of each instance can be viewed by tapping on “See details”.  Actionable insightsBesides compiling the instances of unsafe driving in a report to create awareness, we are also using these data to provide some actionable recommendations for our partners to improve their driving.With unsafe driving feedback from consumers and reported road traffic accident data from our platform, we also train machine learning models to identify patterns in the detected unsafe driving instances and estimate the likelihood of partners receiving unsafe driving feedback or getting into accidents. One use case is to compute a safe driving score that equates a four-wheel partner’s driving behaviour to a numerical value where a higher score indicates a safer driver.Additionally, we use Shapley additive explanation (SHAP) approaches to determine which driving manoeuvre contributes the most to increasing the likelihood of partners receiving unsafe driving feedback or getting into accidents. This information is included as an actionable insight in the driving safety report and helps partners to identify the key area to improve their driving.What’s next?At the moment, Grab performs telematics processing and unsafe driving detections after the trip and updates the report the next day. One of the biggest improvements would be to share this information with partners faster. We are actively working on developing a real-time processing algorithm that addresses this and also, satisfies the robustness requirements such that partners are immediately aware after an unsafe driving instance is detected.Besides detecting typical unsafe driving manoeuvres, we are also exploring other use cases for mobile sensor data in road safety such as detection of poor road conditions, counterflow driving against traffic, and phone usage leading to distracted driving.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References            Burhan, W. (2022). How telematics helps Grab to improve safety. Grab Tech Blog. https://engineering.grab.com/telematics-at-grab &#8617;              Mohan, P., Padmanabhan, V.N. and Ramjee, R. (2008).Nericell: rich monitoring of road and traffic conditions using mobile smartphones. SenSys ‘08: Proceedings of the 6th ACM conference on Embedded network sensor systems, 312-336. https://doi.org/10.1145/1460412.1460444 &#8617;              Sentiance (2016). Driving behavior modeling using smart phone sensor data. Sentiance Blog. https://sentiance.com/2016/02/11/driving-behavior-modeling-using-smart-phone-sensor-data/ &#8617;              Yarlagadda, J. and Pawar, D.S. (2022). Heterogeneity in the Driver Behavior: An Exploratory Study Using Real-Time Driving Data. Journal of Advanced Transportation. vol. 2022, Article ID 4509071. https://doi.org/10.1155/2022/4509071 &#8617;      ",
        "url": "/using-mobile-sensor-data-to-encourage-safer-driving"
      }
      ,
    
      "automatic-rule-backtesting": {
        "title": "Automatic rule backtesting with large quantities of data",
        "author": "chao-wangclemens-valientejun-liudaniel-wang",
        "tags": "[&quot;Testing&quot;, &quot;Automation&quot;, &quot;Backtesting&quot;, &quot;Data science&quot;]",
        "category": "",
        "content": "IntroductionAnalysts need to analyse and simulate a rule on historical data to check the performance and accuracy of the rule. Backtesting enables analysts to run simulations of the rules and manage the results from the rule engine UI.Backtesting helps analysts to:  Define the desired impact of the rule for our business and users.  Evaluate the accuracy of the rule based on historical data.  Compare and analyse results with data points, such as known false positives, user segments, risk profile of a user or transaction, and so on.Currently, the analytics process to test performance of a rule is not standardised, and is inaccurate and inefficient. Analysts from different teams have different approaches:  Offline process using Presto tables. This process is lengthy and inaccurate.  Offline process based on the rule engine payload. The setup takes time, and the process is not streamlined.  Running rules in shadow mode. This process takes days to get the desired result.  A team in Grab uses different rule engines to manage rules and do backtesting. This doubles the effort for analysts and engineers.In our vision for backtesting, it should allow analysts to:  Efficiently run and manage their jobs.  Create custom metrics, reports and dimensions for backtesting.  Add external data points and metrics to do a deep dive.For the purpose of establishing a minimum viable product (MVP), backtesting will support basic capabilities and enable analysts to access required metrics and data points. Thus, analysts can:  Run backtesting jobs from the rule engine UI.  Get fixed reports and dimensions for every checkpoint.  Get access to relevant data to analyse backtesting results.BackgroundAssume a simple use case: A rule to detect the transaction risk. Each transaction has a transaction_id, user_id, currency, amount, timestamp. The rule engine also provides a treatment (Approve or Decline) based on the rule logic for the transaction.In this specific use case, we would like to see what will be the aggregation number of the total transactions, total distinct users, and the sum of the amount, based on the dimensions of date, treatment, and currency in the last couple of weeks.The result may look like the following data:            Dimension          Dimension          Dimension          metric            metric                metric                       Date      Treatment      Currency      Total tx      Distinct user          Total amount              2020-05-1      Approve      SGD      100      80      10020              2020-05-1      Decline      SGD      50      40      450              2020-05-1      Approve      MYR      110      100      1200              2020-05-1      Decline      MYR      30      15      400      * This data does not reflect actual Grab data and is for illustrative purposes only.Solution  Use a cloud-agnostic Spark-based data pipeline to replay any existing or proposed rule to check performance.  Use a Web Portal to:          Create or select a rule to replay, with replay time range.      Display and download the result, such as total events and hit counts.        Replay any existing or proposed rule for checking performance.  Allow users to create or select a rule to replay in the rule engine UI, with provided replay time range.  Display the replay result in the rule engine UI, such as total events and hit counts.  Provide a way to download all testing results in the rule engine UI (for example, all rule responses).  Remove dependency on the specific cloud provider stack, so other teams in Grab can use it instead of Google Cloud Platform (GCP).Architecture details    The rule editor UI reacts to the user input. Its engine sends a job command to the Amazon Simple Queue Service (SQS) to initialise the job. After that, the rule editor also performs the following processes in the background:  Lambda listens to the request SQS queue and invokes a job via the Spark jobs API.  The job fetches the executable artifacts, data source. After the job is completed, the job script saves the result sheet as required to S3.  The Spark script pushes the job final status (success, failure, timeout) through the shutdown hook to respond to the SQS queue.  The rule editor engine listens to response callback messages, and processes the job metadata to the database, or sends notifications.  The rule editor displays the job metadata on the UI.  The package pipeline builds and deploys the executable artifacts to S3 as a manageable structure.  The Spark script takes the filter logic as its input parameters.WorkflowHistorical data preparationThe historical events are published by the rule engine through Kafka, and stored into the S3 bucket based on time. The Backtesting system then fetches these data for testing based on the time range requested.By using a Kubernetes stream pipeline, we also save the trust inference stream to Trust AWS subaccount. With the customer bucket and file format, we can improve the efficiency of the data processing, and also avoid any delay from the data lake.Engineering specifications  Target location:    s3a://stg-trust-inference-event/&lt;engine-name&gt;/&lt;predict-name&gt;/&lt;YYYY&gt;/MM/DD/hh/mm/ss/&lt;000001&gt;.snappy.parquet    s3a://prd-trust-inference-event/&lt;engine-name&gt;/&lt;predict-name&gt;/&lt;YYYY&gt;/MM/DD/hh/mm/ss/&lt;000001&gt;.snappy.parquetDescription: Following the fields of steam definition, the engine name would be ruleengine, or catwalk. The predict-name would be preride (checkpoint name), or cnpu (model name).  File Format: avro  File Compression: Snappy  There is no auto retention on sub-account S3. We will implement the archive process in the future.   The default pipeline and the new pipeline will run in parallel until the Data Engineering team is ready to retire the default pipeline.Backtesting  Upon scheduling, the Backtesting Portal sends a message to SQS, which is then captured by the listening Lambda.  Lambda invokes a Spark job over the AWS elastic mapreduce engine (EMR).  The EMR engine fetches the executable artifacts containing the rule script and historical data from S3, and starts a Spark job to apply the rule script over historical data. Depending on the size of data, the Spark cluster will scale automatically to ensure timely completion.  Once completed, a report file is generated and available on Backtesting UI.UI    Learnings and conclusionsAfter the release, here’s what our data analysers had to say:  For trust analysts, testing a rule on historical data happens outside the rule engine UI and is not user-friendly, leading to analysts wasting significant time.  For financial analysts, as analysts migrate to the rule engine UI, the existing solution will be deprecated with no other solution.  An alternative to simulate a rule;  we no longer need to run a rule in shadow mode because we can use historical data to determine the outcome. This new approach saves us weeks of effort on the rule onboarding process.What’s next?The underlying Spark jobs in this tool were developed by knowledgeable data engineers, which is a disadvantage because it requires a high level of expertise to modify the analytics. To mitigate this restriction, we are looking into using domain-specific language (DSL) to allow users to input desired attributes and dimensions, and provide the job release pipeline for self-serving jobs.Thanks to Jia Long Loh for the support on the offline infrastructure engineering.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/automatic-rule-backtesting"
      }
      ,
    
      "how-we-store-millions-orders": {
        "title": "How we store and process millions of orders daily",
        "author": "xi-chensiliang-cao",
        "tags": "[&quot;Database&quot;, &quot;Storage&quot;, &quot;Distributed Systems&quot;, &quot;Platform&quot;]",
        "category": "",
        "content": "IntroductionIn the real world, after a passenger places a GrabFood order from the Grab App, the merchant-partner will prepare the order. A driver-partner will then collect the food and deliver it to the passenger. Have you ever wondered what happens in the backend system? The Grab Order Platform is a distributed system that processes millions of GrabFood or GrabMart orders every day. This post aims to share the journey of how we designed the database solution that powers the order platform.BackgroundWhat are the design goals when building the database solution? We collected the requirements by analysing query patterns and traffic patterns.Query patternsHere are some important query examples that the Order Platform supports:      Write queries:    a.  Create an order.    b.  Update an order.        Read queries:    a.  Get order by id.    b.  Get ongoing orders by passenger id.    c.  Get historical orders by various conditions.    d.  Get order statistics (for example, get the number of orders)  We can break down queries into two categories: transactional queries and analytical queries. Transactional queries are critical to online order creation and completion, including the write queries and read queries such as 2a or 2b. Analytical queries like 2c and 2d retrieves historical orders or order statistics on demand. Analytical queries are not essential to the oncall order processing.Traffic patternsGrab’s Order Platform processes a significant amount of transaction data every month.During peak hours, the write Queries per Second (QPS) is three times of primary key reads; whilst the range Queries per Second are four times of the primary key reads.Design goalsFrom the query and traffic patterns, we arrived at the following three design goals:  Stability - the database solution must be able to handle high read and write QPS. Online order processing queries must have high availability. Even when some part of the system is down, we must be able to provide a degraded experience to the end users allowing them to still be able to create and complete an order.  Scalability and cost - the database solution must be able to support fast evolution of business requirements, given now we handle up to a million orders per month. The solution must also be cost effective at a large scale.  Consistency - strong consistency for transactional queries, and eventually consistency for analytical queries.SolutionThe first design principle towards a stable and scalable database solution is to use different databases to serve transactional and analytical queries, also known as OLTP and OLAP queries. An OLTP database serves queries critical to online order processing. This table keeps data for only a short period of time. Meanwhile, an OLAP database has the same set of data, but serves our historical and statistical queries. This database keeps data for a longer time.What are the benefits from this design principle? From a stability point of view, we can choose different databases which can better fulfil our different query patterns and QPS requirements. An OLTP database is the single source of truth for online order processing; any failure in the OLAP database will not affect online transactions. From a scalability and cost point of view, we can choose a flexible database for OLAP to support our fast evolution of business requirements. We can maintain less data in our OLTP database while keeping some older data in our OLAP database.To ensure that the data in both databases are consistent, we introduced the second design principle - data ingestion pipeline. In Figure 1, Order Platform writes data to the OLTP database to process online orders and asynchronously pushes the data into the data ingestion pipeline. The data ingestion pipeline ensures that the OLAP database data is eventually consistent.  Figure 1: Order Platform database solution overview  Architecture detailsOLTP databaseThere are two categories of OLTP queries, the key-value queries (for example, load by order id) and the batch queries (for example, Get ongoing orders by passenger id). We use DynamoDB as the database to support these OLTP queries.Why DynamoDB?  Scalable and highly available: the tables of DynamoDB are partitioned and each partition is three-way replicated.  Support for strong consistent reads by primary key.  DynamoDB has a mechanism called adaptive capacity to handle hotkey traffic. Internally, DynamoDB will distribute higher capacity to high-traffic partitions, and isolate frequently accessed items to a dedicated partition. This way, the hotkey can utilise the full capacity of an entire partition, which is up to 3000 read capacity units and 1000 write capacity units.  Figure 2: DynamoDB table structure overview. Source:  Amazon Web Services (2019, 28 April)  In each DynamoDB table, it has many items with attributes. In each item, it has a partition key and sort key. The partition key is used for key-value queries, and the sort key is used for range queries. In our case, the table contains multiple order items. The partition key is order ID. We can easily support key-value queries by the partition key.            order_id (PK)      state      pax_id      created_at      pax_id_gsi                  order1      Ongoing      Alice      9:00am              order2      Ongoing      Alice      9:30am              order3      Completed      Alice      8:30am      Batch queries like ‘Get ongoing orders by passenger id’ are supported by DynamoDB Global Secondary Index (GSI). A GSI is like a normal DynamoDB table, which also has keys and attributes.In our case, we have a GSI table where the partition key is the pax_id_gsi. The attribute pax_id_gsi is linked to the main table. It is eventually consistent with the main table that is maintained by DynamoDB. If the Order Platform queries ongoing orders for Alice, two items will be returned from the GSI table.            pax_id_gsi (PK)      created_at (SK)      order_id                  Alice      9:00am      order1              Alice      9:30am      order2      We also make use of an advanced feature of GSI named sparse index to support ongoing order queries. When we update order status from ongoing to completed, at the same time, we set the pax_id_gsi to empty, so that the linked item in the GSI will be automatically deleted by DynamoDB. At any time, the GSI table only stores the ongoing orders. We use a sparse index mechanism to control our table size for better performance and to be more cost effective.The next problem is data retention. This is achieved with the DynamoDB Time To Live (TTL) feature. DynamoDB will auto-scan expired items and delete them. But the challenge is when we add TTL to big tables, it will bring a heavy load to the background scanner and might result in an outage. Our solution is to only add a TTL attribute to the new items in the table. Then, we manually delete the items without TTL attributes, and run a script to delete items with TTL attributes that are too old. After this process, the table size will be quite small, so we can enable the TTL feature on the TTL attribute that we previously added without any concern. The retention period of our DynamoDB data is three months.Costwise, DynamoDB is charged by storage size and the provision of the read write capability. The provision capability is actually auto scalable. The cost is on-demand. So it’s generally cheaper than RDS.OLAP databaseWe use MySQL RDS as the database to support historical and statistical OLAP queries.Why not Aurora? We choose RDS mainly because it is a mature database solution. Even if Aurora can provide better high-availability, RDS is enough to support our less critical use cases. Costwise, Aurora charges by data storage and the number of requested Input/Output Operations per Second (IOPS). RDS charges only by data storage. As we are using General Purpose (SSD) storage, IOPS is free and supports up to 16k IOPS.We use MySQL partitioning for data retention. The order table is partitioned by creation time monthly. Since the data access pattern is mostly by month, the partition key can reduce cross-partition queries. Partitions older than six months are dropped at the beginning of each month.Data ingestion pipeline  Figure 3: Data Ingestion Pipeline Architecture.  A Kafka stream is used to process data in the data ingestion pipeline. We choose the Kafka stream, because it has 99.95% SLA. It is not restricted by the OLTP and OLAP database types.Even if Kafka can provide 99.95% SLA, there is still the chance of stream producer failures. When the producer fails, we will store the message in an Amazon Simple Queue Service (SQS) and retry. If the retry also fails, it will be moved to the SQS dead letter queue (DLQ), to be consumed at a later time.On the stream consumer side, we use back-off retry at both stream and database levels to ensure consistency. In a worst-case scenario, we can rewind the stream events from Kafka.It is important for the data ingestion pipeline to handle duplicate messages and out-of-order messages.Duplicate messages are handled by the database level unique key (for example, order ID + creation time).For the out-of-order messages, we implemented the following two mechanisms:  Version update: we only update the most recently updated data. The precision of the update time is in microseconds, which is enough for most of the use cases.  Upsert: if the update events occur before the create events, we simulate an upsert operation.ImpactAfter launching our solution this year, we have saved significantly on cloud costs. In the earlier solution, Order Platform synchronously writes to DynamoDB and Aurora and the data is kept forever.ConclusionIn terms of stability, we use DynamoDB as the critical OLTP database to ensure high availability for online order processing. Scalability wise, we use RDS as the OLAP database to support our quickly evolving business requirements by using a rich, multiple index. Cost efficiency is achieved by data retention in both databases. For consistency, we built a single source of truth OLTP database and an OLAP database that is eventually consistent with the help of the data ingestion pipeline.What’s next?Currently, the database solution is running on the production environment. Even though the database solution is proven to be stable, scalable and consistent, we still see some potential areas of improvement.We use MySQL RDS for OLAP data storage. Even though MySQL is stable and cost effective, it is difficult to serve more complicated queries like free text search. Hence, we plan to explore other NoSQL databases like ElasticSearch.We hope this post helps you understand how we store Grab orders and fulfil the queries from the Grab Order Platform.ReferencesAmazon Web Services. (2019, 28 April) Build with DynamoDB: S1 E1 – Intro to Amazon DynamoDB [Video]. YouTube.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/how-we-store-millions-orders"
      }
      ,
    
      "automated-faq": {
        "title": "How we automated FAQ responses at Grab",
        "author": "preeti-karkera",
        "tags": "[&quot;Automation&quot;, &quot;Knowledge management&quot;, &quot;Productivity&quot;]",
        "category": "",
        "content": "Overview and initial analysisKnowledge management is often one of the biggest challenges most companies face internally. Teams spend several working hours trying to either inefficiently look for information or constantly asking colleagues about information already documented somewhere. A lot of time is spent on the internal employee communication channels (in our case, Slack) simply trying to figure out answers to repetitive questions. On our journey to automate the responses to these repetitive questions, we needed first to figure out exactly how much time and effort is spent by on-call engineers answering such repetitive questions.We soon identified that many of the internal engineering tools’ on-call activities involve answering users’ (internal users) questions on various Slack channels. Many of these questions have already been asked or documented on the wiki. These inquiries hinder on-call engineers’ productivity and affect their ability to focus on operational tasks. Once we figured out that on-call employees spend a lot of time answering Slack queries, we decided on a journey to determine the top questions.We considered smaller groups of teams for this study and found out that:  The topmost user queries are “How do I do ABC?” or “Is XYZ broken?”.  The second most commonly asked questions revolve around access requests, approvals, or other permissions. The answer to such questions is often URLs to existing documentation.These findings informed us that we didn’t just need an artificial intelligence (AI) based autoresponder to repetitive questions. We must, in fact, also leverage these channels’ chat histories to identify patterns.Gathering user votes for shortlisted vendorsIn light of saving costs and time and considering the quality of existing solutions already available in the market, we decided not to reinvent the wheel and instead purchase an existing product. And to figure out which product to purchase, we needed to do a comparative analysis. And thus began our vendor comparison journey!While comparing the feature sets offered by different vendors, we understood that our users need to play a part in this decision-making process. However, sharing our vendor analysis with our users and allowing them to choose the bot of their choice posed several challenges:  Users could be biased towards known bots (from previous experiences).  Users could be biased towards big brands with a preconceived notion that big brands mean better features and better user support.  Users may likely pick the most expensive vendor, assuming that a higher cost means higher efficiency.To ensure that we receive unbiased feedback, here’s how we opened users up to voting. We highlighted the top features of each vendor’s bot compared to other shortlisted bots. We hid the names of the bots to avoid brand attraction. At a high level, here’s what the categorisation looked like:      Features    Vendor 1 (name  hidden)    Vendor 2 (name  hidden)    Vendor 3 (name  hidden)        Enables crowdsourcing, everyone is incentivised to participate.  Participants/SME names are visible.  Everyone can access the web UI and see how the responses configured on the bot.          -    -        Lowers discussions on channels by providing easy ways to raise tickets to the team instead of discussing on Slack.    -                   Only a specific set of admins (or oncall engineers) feed and maintain the bot thus ensuring information authenticity and reliability.                        Easy bot feeding mechanism/web UI to update FAQs.          -             Superior natural language processing capabilities.               -            Please vote    Vendor 1    Vendor 2    Vendor 3      Although none of the options had all the features our users wanted, about 60% chose Vendor 1 (OneBar). From this, we discovered the core features that our users needed while keeping them involved in the decision-making process.Matching our requirements with available vendors’ feature setsAlthough our users made their preferences clear, we still needed to ensure that the feature sets available in the market suited our internal requirements in terms of the setup and the features available in portals that we envisioned replacing. As part of our requirements gathering process, here are some of the critical conditions that became more and more prominent:  An ability to crowdsource Slack discussions/conclusions and save them directly from Slack (preferably with a single command).  An ability to auto-respond to Slack queries without calling the bot manually.  The bot must be able to respond to queries only on the preconfigured Slack channel (not a Slack-wide auto-responder that is already available).  Ability to auto-detect frequently asked questions on the channels would mean less work for platform engineers to feed the bot manually and periodically.  A trusted and secured data storage setup and a responsive customer support team.Proof of conceptWe considered several tools (including some of the tools used by our HR for auto-answering employee questions). We then decided to do a complete proof of concept (POC) with OneBar to check if it fulfils our internal requirements.These were the phases in which we conducted the POC for the shortlisted vendor (OneBar):Phase 1: Study the traffic, see what insights OneBar shows and what it could/should potentially show. Then think about how an ideal oncall or support should behave in such an environment. i.e. we could identify specific messages in history and describe what should’ve happened to each one of them.Phase 2: Create required records in OneBar and configure it to match the desired behaviour as closely as possible.Phase 3: Let the tool run for a couple of weeks and then evaluate how well it responds to questions, how often people search directly, how much information they add, etc. Onebar adds all these metrics in the app making it easier to monitor activity.In addition to the Onebar POC, we investigated other solutions and did a thorough vendor comparison and analysis. After running the POC and investigating other vendors, we decided to use OneBar as its features best meet our needs.Prioritising Slack channelsWhile we had multiple Slack channels that we’d love to have enabled the shortlisted bot on, our initial contract limited our use of the bot to only 20 channels. We could not use OneBar to auto-scan more than 20 Slack channels.Users could still chat directly with the bot to get answers to FAQs based on what was fed to the bot’s knowledge base (KB). They could also access the web login, which displays its KB, other valuable features, and additional features for admins/experts.Slack channels that we enabled the licensed features on were prioritised based on:  Most messages sent on the channel per month, i.e. most active channels.  Most members impacted, i.e. channels with a large member count.To do this, we used Slack analytics reports and identified the channels that fit our prioritisation criteria.Change is difficult but often essentialOnce we’d onboarded the vendor, we began training and educating employees on using this new Knowledge Management system for all their FAQs. It was a challenge as change is always complex but essential for growth.A series of tech talks and training conducted across the company and at more minor scales also helped guide users about the bot’s features and capabilities.At the start, we suffered from a lack of data resulting in incorrect responses from the bot. But as the team became increasingly aware of the features and learned more about its capabilities, the bot’s number of KB items grew, resulting in a much more efficient experience. It took us around one quarter to feed the bot consistently to see accurate and frequent responses from it.Crowdsourcing our internal glossaryWith an increasing number of acronyms and company-specific words emerging each year, the number of acronyms and company-specific abbreviations that new joiners face is immense.We solved this issue by using the bot’s channel-specific KB feature. We created a specific Slack channel dedicated to storing and retrieving definitions of acronyms and other words. This solution turned out to be a big hit with our users.And who fed the bot with the terms and glossary items? Who better than our onboarding employees to train the bot to help other onboarders. A targeted campaign dedicated to feeding the bot excited many of our onboarders. They began to play around with the bot’s features and provide it with as many glossary items as possible, thus winning swags!In a matter of weeks, the user base grew from a couple of hundred to around 3000. This effort was also called out in one of our company-wide All Hands meetings, a big win for our team!Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/automated-faq"
      }
      ,
    
      "graph-visualisation": {
        "title": "Graph Networks - 10X investigation with Graph Visualisations",
        "author": "fujiao-liushuqi-wangmuqi-lijia-chen",
        "tags": "[&quot;Security&quot;, &quot;Graphs concepts&quot;, &quot;Graph technology&quot;, &quot;Graph visualisation&quot;]",
        "category": "",
        "content": "IntroductionDetecting fraud schemes used to require investigations using large amounts and varying types of data that come from many different anti-fraud systems. Investigators then need to combine the different types of data and use statistical methods to uncover suspicious claims, which is time consuming and inefficient in most cases.    We are always looking for ways to improve fraud investigation methods and stay one step ahead of our ever-growing fraudsters. In the introductory blog and graph concepts articles of this series, we’ve covered experimenting with a set of Graph Network technologies, including Graph Visualisation, and the basics of graph concepts.In this post, we will introduce our Graph Visualisation Platform and briefly illustrate how it makes fraud investigations easier and more effective.Why visualise a graph?If you’re a fan of crime shows, you would have come across scenes like a detective putting together evidence, such as pictures, notes and articles, on a board and connecting them with thumb tacks and yarn. When you look at the board, it’s easy to see the relationships between the different pieces of evidence. That’s what graphs do, especially in fraud detection.    In the same way, while graph data is the raw material of an investigation, some of the most interesting relationships are often inferred rather than modelled directly in the data. Visualising these relationships can give a unique “big picture” of the data that is difficult or impossible to obtain with traditional relational tables and business intelligence tools.On the other hand, graph visualisation enhances the quick identification of relationships and significant structures because it is an intuitive way to help detect patterns. Plus, the human brain processes visual information much faster; that’s where our Graph Visualisation platform comes in.What is the Graph Visualisation platform?Graph Visualisation platform is a full-featured investigation platform that can reveal hidden connections and context in data by transforming raw records into highly visual and interactive maps. From there, investigators can grab any data point and quickly see relationships, patterns, and anomalies, and if necessary, drill down to investigate further.This is all done without writing a manual query, switching between anti-fraud systems, or having to think about data science! These are some of the interactions on the platform that easily make anomalies or relevant patterns stand out.Expanding the dataTo date, we have over three billion nodes and edges in our storage system. It is not possible (nor necessary) to show all of the data at once. The platform allows the user to grab any data point and easily expand to view the relationships.    Timeline tracking and history replayThe Graph Visualisation platform’s interactive time filter lets you see temporal relationships within your data and clearly reveals the chronological progression of events. You can start with a specific time of interest, track everything that happens after, then quickly focus on the time and relationships that matter most.    10X investigationsHere are a few examples of how the Graph Visualisation platform facilitates fraud investigations.Appeal confirmationThe following image shows the difference between a true fraudster and a falsely identified one. On the left, we have a Grab rental corporate account that was falsely detected by a fraud rule. Upon review, we discovered that there is no suspicious connection to this account, thus the account got unblocked.On the right, we have a passenger that was blocked by the system and they appealed. Investigations showed that the passenger is, in fact, part of an extremely dense device-sharing network, so we maintained our decision to block.    Modus operandi discoveryPassenger sharing deviceFraudsters tend to share physical resources to maximise their revenue. With our Graph Visualisation platform, you can see exactly how this pattern looks like. The image below shows a device that is shared by a lot of fraudsters.    Anti-money laundering (AML)On the left, we see a pattern of healthy spending on Grab. However, on the right, we can see that passengers are highly connected, and it has frequent large amount transfers to other payment providers.    Closing thoughtsGraph Visualisation is an intuitive way to investigate suspicious connections and potential patterns of crime. Investigators can directly interact with any data point to get the details they need and literally view the relationships in the data to make fast, accurate, and defensible decisions.While fraud detection is a good use case for Graph Visualisation, it’s not the only possibility. Graph Visualisation can help make anything more efficient and intelligent, especially if you have highly connected data.Speak to usGrabDefence is a proprietary fraud prevention platform built by Grab, Southeast Asia’s leading superapp. Since 2019, the GrabDefence team has shared its fraud management capabilities and platform with enterprises and startups to leverage Grab’s advanced AI/ML models, hyper local insights and patented device intelligence technologies.To learn more about GrabDefence or to speak to our fraud management experts, contact us at gd.contact@grabtaxi.com.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/graph-visualisation"
      }
      ,
    
      "facial-recognition": {
        "title": "How facial recognition technology keeps you safe",
        "author": "kaifeng-teewentao-xie",
        "tags": "[&quot;Security&quot;, &quot;Facial recognition&quot;]",
        "category": "",
        "content": "Facial recognition technology is one of the many modern technologies that previously only appeared in science fiction movies. The roots of this technology can be traced back to the 1960s and have since grown dramatically due to the rise of deep learning techniques and accelerated digital transformation in recent years.In this blog post, we will talk about the various applications of facial recognition technology in Grab, as well as provide details of the technical components that build up this technology.Application of facial recognition technology  At Grab, we believe in prevention, protection, and action to create a safer every day for our consumers, partners, and the community as a whole. All selfies collected by Grab are handled according to Grab’s Privacy Policy and securely protected under privacy legislation in the countries in which we operate. We will elaborate in detail in a section further below.One key incident prevention method is to verify the identity of both our consumers and partners:  From the perspective of protecting the safety of passengers, having a reliable driver authentication process can avoid unauthorized people from delivering a ride. This ensures that trips on Grab are only completed by registered licensed driver-partners that have passed our comprehensive background checks.  From the perspective of protecting the safety of driver-partners, verifying the identity of new passengers using facial recognition technology helps to deter crimes targeting our driver-partners and make incident investigations easier.        Safety incidents that arise from lack of identity verificationFacial recognition technology is also leveraged to improve Grab digital financial services, particularly in facilitating the “electronic Know Your Customer” (e-KYC) process. KYC is a standard regulatory requirement in the financial services industry to verify the identity of customers, which commonly serves to deter financial crime, such as money laundering.Traditionally, customers are required to visit a physical counter to verify their government-issued ID as proof of identity. Today, with the widespread use of mobile devices, coupled with the maturity of facial recognition technologies, the process has become much more seamless and can be done entirely digitally.  Figure 1: GrabPay wallet e-KYC regulatory requirements in the Philippines  Overview of facial recognition technology  Figure 2: Face recognition flow  The typical facial recognition pipeline involves multiple stages, which starts with image preprocessing, face anti-spoof, followed by feature extraction, and finally the downstream applications - face verification or face search.The most common image preprocessing techniques for face recognition tasks are face detection and face alignment. The face detection algorithm locates the face region in an image, and is usually followed by face alignment, which identifies the key facial landmarks (e.g. left eye, right eye, nose, etc.) and transforms them into a standardised coordinate space. Both of these preprocessing steps aim to ensure a consistent quality of input data for downstream applications.Face anti-spoof refers to the process of ensuring that the user-submitted facial image is legitimate. This is to prevent fraudulent users from stealing identities (impersonating someone else by using a printed photo or replaying videos from mobile screens) or hiding identities (e.g. wearing a mask). The main approach here is to extract low-level spoofing cues, such as the moiré pattern, using various machine learning techniques to determine whether the image is spoofed.After passing the anti-spoof checks, the user-submitted images are sent for face feature extraction, where important features that can be used to distinguish one person from another are extracted. Ideally, we want the feature extraction model to produce embeddings (i.e. high-dimensional vectors) with small intra-class distance (i.e. faces of the same person) and large inter-class distance (i.e. faces of different people), so that the aforementioned downstream applications (i.e. face verification and face search) become a straightforward task - thresholding the distance between embeddings.Face verification is one of the key applications of facial recognition and it answers the question, “Is this the same person?”. As previously alluded to, this can be achieved by comparing the distance between embeddings generated from a template image (e.g. government-issued ID or profile picture) and a query image submitted by the user. A short distance indicates that both images belong to the same person, whereas a large distance indicates that these images are taken from different people.Face search, on the other hand, tackles the question, “Who is this person?”, which can be framed as a vector/embedding similarity search problem. Image embeddings belonging to the same person would be highly similar, thus ranked higher, in search results. This is particularly useful for deterring criminals from re-onboarding to our platform by blocking new selfies that match a criminal profile in our criminal denylist database.Face anti-spoofFor face anti-spoof, the most common methods used to attack the facial recognition system are screen replay and printed paper. To distinguish these spoof attacks from genuine faces, we need to solve two main challenges.The first challenge is to obtain enough data of spoof attacks to enable the training of models. The second challenge is to carefully train the model to focus on the subtle differences between spoofed and genuine cases instead of overfitting to other background information.  Figure 3: Original face (left), screen replay attack (middle), synthetic data with a moiré pattern (right)  Source 1Collecting large volumes of spoof data is naturally hard since spoof cases in product flows are very rare. To overcome this problem, one option is to synthesise large volumes of spoof data instead of collecting the real spoof data. More specifically, we synthesise moiré patterns on genuine face images that we have, and use the synthetic data as the screen replay attack data. This allows our model to use small amounts of real spoof data and sufficiently identify spoofing, while collecting more data to train the model.  Figure 4: Data preparation with patch data  On the other hand, a spoofed face image contains lots of information with subtle spoof cues such as moiré patterns that cannot be detected by the naked eye. As such, it’s important to train the model to identify spoof cues instead of focusing on the possible domain bias between the spoof data and genuine data. To achieve this, we need to change the way we prepare the training data.Instead of using the entire selfie image as the model input, we firstly detect and crop the face area, then evenly split the cropped face area into several patches. These patches are used as input to train the model. During inference, images are also split into patches the same way and the final result will be the average of outputs from all patches. After this data preprocessing, the patches will contain less global semantic information and more local structure features, making it easier for the model to learn and distinguish spoofed and genuine images.Face verification  “Data is food for AI.” - Andrew Ng, founder of Google BrainThe key success factors of artificial intelligence (AI) models are undoubtedly driven by the volume and quality of data we hold. At Grab, we have one of the largest and most comprehensive face datasets, covering a wide range of demographic groups in Southeast Asia. This gives us a strong advantage to build a highly robust and unbiased facial recognition model that serves the region better.As mentioned earlier, all selfies collected by Grab are securely protected under privacy legislation in the countries in which we operate. We take reasonable legal, organisational and technical measures to ensure that your Personal Data is protected, which includes measures to prevent Personal Data from getting lost, or used or accessed in an unauthorised way. We limit access to these Personal Data to our employees on a need to know basis. Those processing any Personal Data will only do so in an authorised manner and are required to treat the information with confidentiality.Also, selfie data will not be shared with any other parties, including our driver, delivery partners or any other third parties without proper authorisation from the account holder. They are strictly used to improve and enhance our products and services, and not used as a means to collect personal identifiable data. Any disclosure of personal data will be handled in accordance with Grab Privacy Policy.  Figure 5: Semi-Siamese architecture (source)  Other than data, model architecture also plays an important role, especially when handling less common face verification scenarios, such as ”selfie to ID photo” and “selfie to masked selfie” verifications.  The main challenge of “selfie to ID photo” verification is the shallow nature of the dataset, i.e. a large number of unique identities, but a low number of image samples per identity. This type of dataset lacks representation in intra-class diversity, which would commonly lead to model collapse during model training. Besides, “selfie to ID photo” verification also poses numerous challenges that are different from general facial recognition, such as aging (old ID photo), attrited ID card (normal wear and tear), and domain difference between printed ID photo and real-life selfie photo.To address these issues, we leveraged a novel training method named semi-Siamese training (SST) 2, which is proposed by Du et al. (2020). The key idea is to enlarge intra-class diversity by ensuring that the backbone Siamese networks have similar parameters, but are not entirely identical, hence the name “semi-Siamese”.Just like typical Siamese network architecture, feature vectors generated by the subnetworks are compared to compute the loss functions, such as Arc-softmax, Triplet loss, and Large margin cosine loss, all of which aim to reduce intra-class distance while increasing the inter-class distances. With the usage of the semi-Siamese backbone network, intra-class diversity is further promoted as it is guaranteed by the difference between the subnetworks, making the training convergence more stable.  Figure 6: Masked face verification  Another type of face verification problem we need to solve these days is the “selfie to masked selfie” verification. To pass this type of face verification, users are required to take off their masks as previous face verification models are unable to verify people with masks on. However, removing face masks to do face verification is inconvenient and risky in a crowded environment, which is a pain for many of our driver-partners who need to do verification from time to time.To help ease this issue, we developed a face verification model that can verify people even while they are wearing masks. This is done by adding masked selfies into the training data and training the model with both masked and unmasked selfies. This not only enables the model to perform verification for people with masks on, but also helps to increase the accuracy of verifying those without masks. On top of that, masked selfies act as data augmentation and help to train the model with stronger ability of extracting features from the face.Face search    As previously mentioned, once embeddings are produced by the facial recognition models, face search is fundamentally no different from face verification. Both processes use the distance between embeddings to decide whether the faces belong to the same person. The only difference here is that face search is more computationally expensive, since face verification is a 1-to-1 comparison, whereas face search is a 1-to-N comparison (N=size of the database).In practice, there are many ways to significantly reduce the complexity of the search algorithm from O(N), such as using Inverted File Index (IVF) and Hierarchical Navigable Small World (HNSW) graphs. Besides, there are also various methods to increase the query speed, such as accelerating the distance computation using GPU, or approximating the distances using compressed vectors. This problem is also commonly known as Approximate Nearest Neighbor (ANN). Some of the great open-sourced vector similarity search libraries that can help to solve this problem are ScaNN3 (by Google), FAISS4(by Facebook), and Annoy (by Spotify).What’s next?In summary, facial recognition technology is an effective crime prevention and reduction tool to strengthen the safety of our platform and users. While the enforcement of selfie collection by itself is already a strong deterrent against fraudsters misusing our platform, leveraging facial recognition technology raises the bar by helping us to quickly and accurately identify these offenders.As technologies advance, face spoofing patterns also evolve. We need to continuously monitor spoofing trends and actively improve our face anti-spoof algorithms to proactively ensure our users’ safety.With the rapid growth of facial recognition technology, there is also a growing concern regarding data privacy issues. At Grab, consumer privacy and safety remain our top priorities and we continuously look for ways to improve our existing safeguards.In May 2022, Grab was recognised by the Infocomm Media Development Authority in Singapore for its stringent data protection policies and processes through the award of Data Protection Trustmark (DPTM) certification. This recognition reinforces our belief that we can continue to draw the benefits from facial recognition technology, while avoiding any misuse of it. As the saying goes, “Technology is not inherently good or evil. It’s all about how people choose to use it”.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References            Niu, D., Guo R., and Wang, Y. (2021). Moiré Attack (MA): A New Potential Risk of Screen Photos. Advances in Neural Information Processing Systems. https://papers.nips.cc/paper/2021/hash/db9eeb7e678863649bce209842e0d164-Abstract.html &#8617;              Du, H., Shi, H., Liu, Y., Wang, J., Lei, Z., Zeng, D., &amp; Mei, T. (2020). Semi-Siamese Training for Shallow Face Learning. European Conference on Computer Vision, 36–53. Springer. &#8617;              Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., &amp; Kumar, S. (2020). Accelerating Large-Scale Inference with Anisotropic Vector Quantization. International Conference on Machine Learning. https://arxiv.org/abs/1908.10396 &#8617;              Johnson, J., Douze, M., &amp; Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3), 535–547. &#8617;      ",
        "url": "/facial-recognition"
      }
      ,
    
      "graph-concepts": {
        "title": "Graph concepts and applications",
        "author": "wenxiang-lumuqi-lijia-chen",
        "tags": "[&quot;Security&quot;, &quot;Graphs concepts&quot;, &quot;Graph technology&quot;]",
        "category": "",
        "content": "IntroductionIn an introductory article, we talked about the importance of Graph Networks in fraud detection. In this article, we will be adding some further context on graphs, graph technology and some common use cases.Connectivity is the most prominent feature of today’s networks and systems. From molecular interactions, social networks and communication systems to power grids, shopping experiences or even supply chains, networks relating to real-world systems are not random. This means that these connections are not static and can be displayed differently at different times. Simple statistical analysis is insufficient to effectively characterise, let alone forecast, networked system behaviour.As the world becomes more interconnected and systems become more complex, it is more important to employ technologies that are built to take advantage of relationships and their dynamic properties. There is no doubt that graphs have sparked a lot of attention because they are seen as a means to get insights from related data. Graph theory-based approaches show the concepts underlying the behaviour of massively complex systems and networks.What are graphs?Graphs are mathematical models frequently used in network science, which is a set of technological tools that may be applied to almost any subject. To put it simply, graphs are mathematical representations of complex systems.Origin of graphsThe first graph was produced in 1736 in the city of Königsberg, now known as Kaliningrad, Russia. In this city, there were two islands with two mainland sections that were connected by seven different bridges.Famed mathematician Euler wanted to plot a journey through the entire city by crossing each bridge only once. Euler proceeded to abstract the four regions of the city and the seven bridges into edges but he demonstrated that the problem was unsolvable. A simplified abstract graph is shown in Fig 1.  Fig 1 Abstraction graph  The graph’s four dots represent Königsberg’s four zones, while the lines represent the seven bridges that connect them. Zones connected by an even number of bridges is clearly navigable because several paths to enter and exit are available. Zones connected by an odd number of bridges can only be used as starting or terminating locations because the same route can only be taken once.The number of edges associated with a node is known as the node degree. If two nodes have odd degrees and the rest have even degrees, the Königsberg problem could be solved. For example, exactly two regions must have an even number of bridges while the rest have an odd number of bridges. However, as illustrated in Fig 1, no Königsberg location has an even number of bridges, rendering this problem unsolvable.Definition of graphsA graph is a structure that consists of vertices and edges. Vertices, or nodes, are the objects in a problem, while edges are the links that connect vertices in a graph.  Vertices are the fundamental elements that a graph requires to function; there should be at least one in a graph. Vertices are mathematical abstractions that refer to objects that are linked by a condition.On the other hand, edges are optional as graphs can still be defined without any edges. An edge is a link or connection between any two vertices in a graph, including a connection between a vertex and itself. The idea is that if two vertices are present, there is a relationship between them.We usually indicate V={v1, v2, …, vn} as the set of vertices, and E = {e1, e2, …, em} as the set of edges. From there, we can define a graph G as a structure G(V, E) which models the relationship between the two sets:  Fig 2 Graph structure  It is worth noting that the order of the two sets within parentheses matters, because we usually express the vertices first, followed by the edges. A graph H(X, Y) is therefore a structure that models the relationship between the set of vertices X and the set of edges Y, not the other way around.Graph data modelNow that we have covered graphs and their typical components, let us move on to graph data models, which help to translate a conceptual view of your data to a logical model. Two common graph data formats are Resource Description Framework (RDF) and Labelled Property Graph (LPG).Resource Description Framework (RDF)RDF is typically used for metadata and facilitates standardised exchange of data based on their relationships. RDFs typically consist of a triple: a subject, a predicate, and an object. A collection of such triples is an RDF graph. This can be depicted as a node and a directed edge diagram, with each triple representing a node-edge-node graph, as shown in Fig 3.  Fig 3 RDF graph  The three types of nodes that can exist are:  Internationalised Resource Identifiers (IRI) - online resource identification code.  Literals - data type value, i.e. text, integer, etc.  Blank nodes - have no identification; similar to anonymous or existential variables.Let us use an example to illustrate this. We have a person with the name Art and we want to plot all his relationships. In this case, the IRI is http://example.org/art and this can be shortened by defining a prefix like ex.In this example, the IRI http://xmlns.com/foaf/0.1/knows defines the relationship knows. We define foaf as the prefix for http://xmlns.com/foaf/0.1/. The following code snippet shows how a graph like this will look.@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt;@prefix ex: &lt;http://example.org/&gt;ex:art foaf:knows ex:bobex:art foaf:knows ex:beaex:bob foaf:knows ex:calex:bob foaf:knows ex:camex:bea foaf:knows ex:coeex:bea foaf:knows ex:coryex:bea foaf:age 23ex:bea foaf:based_near_:o1In the last two lines, you can see how a literal and blank node would be depicted in an RDF graph. The variable foaf:age is a literal node with the integer value of 23, while foaf:based_near is an anonymous spatial entity with a node identifier of underscore. Outside the context of this graph, o1 is a data identifier with no meaning.Multiple IRIs, intended for use in RDF graphs, are typically stored in an RDF vocabulary. These IRIs often begin with a common substring known as a namespace IRI. In some cases, namespace IRIs are also associated with a short name known as a namespace prefix. In the example above, http://xmlns.com/foaf/0.1/ is the namespace IRI and foaf and ex are namespace prefixes.Note: RDF graphs are considered atemporal as they provide a static snapshot of data. They can use appropriate language extensions to communicate information about events or other dynamic properties of entities.An RDF dataset is a set of RDF graphs that includes one or more named graphs as well as exactly one default graph. A default graph is one that can be empty, and has no associated IRI or name, while each named graph has an IRI or a blank node corresponding to the RDF graph and its name. If there is no named graph specified in a query, the default graph is queried (hence its name).Labelled Property Graph (LPG)A labelled property graph is made up of nodes, links, and properties. Each node is given a label and a set of characteristics in the form of arbitrary key-value pairs. The keys are strings, and the values can be any data type. A relationship is then defined by adding a directed edge that is labelled and connects two nodes with a set of properties.In Fig 4, we have an LPG that shows two nodes: art and bea. The bea node has two characteristics, age and proximity, that are connected by a known edge. This edge has the attribute since because it commemorates the year that art and bea first met.  Fig 4 Labelled Property Graph: Example 1  Nodes, edges and properties must be defined when designing an LPG data model. In this scenario, based_near might not be applicable to all vertices, but they should be defined. You might be wondering, why not represent the city Seattle as a node and add an edge marked as based_near that connects a person and the city?In general, if there is a value linked to a large number of other nodes in the network and it requires additional properties to correlate  with other nodes, it should be represented as a node. In this scenario, the architecture defined in Fig 5 is more appropriate for traversing based_near connections. It also gives us the ability to link any new attributes to the based_near relationship.  Fig 5 Labelled Property Graph: Example 2  Now that we have the context of graphs, let us talk about graph databases, how they help with large data queries and the part they play in Graph Technology.Graph databaseA graph database is a type of NoSQL database that stores data using network topology. The idea is derived from LPG, which represents data sets with vertices, edges, and attributes.  Vertices are instances or entities of data that represent any object to be tracked, such as people, accounts, locations, etc.  Edges are the critical concepts in graph databases which represent relationships between vertices. The connections have a direction that can be unidirectional (one-way) or bidirectional (two-way).  Properties represent descriptive information associated with vertices. In some cases, edges have properties as well.Graph databases provide a more conceptual view of data that is closer to reality. Modelling complex linkages becomes simpler because interconnections between data points are given the same weight as the data itself.Graph database vs. relational databaseRelational databases are currently the industry norm and take a structured approach to data, usually in the form of tables. On the other hand, graph databases are agile and focus on immediate relationship understanding. Neither type is designed to replace the other, so it is important to know what each database type has to offer.  Fig 6 Graph database vs relational database  There is a domain for both graph and relational databases. Graph databases outperform typical relational databases, especially in use cases involving complicated relationships, as they take a more naturalistic and flowing approach to data.The key distinctions between graph and relational databases are summarised in the following table:            Type      Graph      Relational                  Format      Nodes and edges with properties      Tables with rows and columns              Relationships      Represented with edges between nodes      Created using foreign keys between tables              Flexibility      Flexible      Rigid              Complex queries      Quick and responsive      Requires complex joins              Use case      Systems with highly connected relationships      Transaction focused systems with more straightforward relationships      Table. 1 Graph vs. Relational DatabasesAdvantages and disadvantagesEvery database type has its advantages and disadvantages; knowing the distinctions as well as potential options for specific challenges is crucial. Graph databases are a rapidly evolving technology with improved functions compared with other database types.AdvantagesSome advantages of graph databases include:  Agile and flexible structures.  Explicit relationship representation between entities.  Real-time query output - speed depends on the number of relationships.DisadvantagesThe general disadvantages of graph databases are:  No standardised query language; depends on the platform used.  Not suitable for transactional-based systems.  Small user base, making it hard to find troubleshooting support.Graph technologyGraph technology is the next step in improving analytics delivery. Traditional analytics is insufficient to meet complicated business operations, distribution, and analytical concerns as data quantities expand.Graph technology aids in the discovery of unknown correlations in data that would otherwise go undetected or unanalysed. When the term graph is used to describe a topic, three distinct concepts come to mind: graph theory, graph analytics, and graph data management.  Graph theory - A mathematical notion that uses stack ordering to find paths, linkages, and networks of logical or physical objects, as well as their relationships. Can be used to model molecules, telephone lines, transport routes, manufacturing processes, and many other things.  Graph analytics - The application of graph theory to uncover nodes, edges, and data linkages that may be assigned semantic attributes. Can examine potentially interesting connections in data found in traditional analysis solutions, using node and edge relationships.  Graph database - A type of storage for data generated by graph analytics. Filling a knowledge graph, which is a model in data that indicates a common usage of acquired knowledge or data sets expressing a frequently held notion, is a typical use case for graph analytics output.While the architecture and terminology are sometimes misunderstood, graph analytics’ output can be viewed through visualisation tools, knowledge graphs, particular applications, and even some advanced dashboard capabilities of business intelligence tools. All three concepts above are frequently used to improve system efficiency and even to assist in dynamic data management. In this approach, graph theory and analysis are inextricably linked, and analysis may always rely on graph databases.Graph-centric user storiesFraud detectionTraditional fraud prevention methods concentrate on discrete data points such as individual accounts, devices, or IP addresses. However, today’s sophisticated fraudsters avoid detection by building fraud rings using stolen and fake identities. To detect such fraud rings, we need to look beyond individual data points to the linkages that connect them.Graph technology greatly transcends the capabilities of a relational database, by revealing hard-to-find patterns. Enterprise businesses also employ Graph technology to supplement their existing fraud detection skills to tackle a wide range of financial crimes, including first-party bank fraud, fraud, and money laundering.Real-time recommendationsAn online business’s success depends on systems that can generate meaningful recommendations in real time. To do so, we need the capacity to correlate product, customer, inventory, supplier, logistical, and even social sentiment data in real time. Furthermore, a real-time recommendation engine must be able to record any new interests displayed during the consumer’s current visit in real time, which batch processing cannot do.Graph databases outperform relational and other NoSQL data stores in terms of delivering real-time suggestions. Graph databases can easily integrate different types of data to get insights into consumer requirements and product trends, making them an increasingly popular alternative to traditional relational databases.Supply chain managementWith complicated scenarios like supply chains, there are many different parties involved and companies need to stay vigilant in detecting issues like fraud, contamination, high-risk areas or unknown product sources. This means that there is a need to efficiently process large amounts of data and ensure transparency throughout the supply chain.To have a transparent supply chain, relationships between each product and party need to be mapped out, which means there will be deep linkages. Graph databases are great for these as they are designed to search and analyse data with deep links. This means they can process enormous amounts of data without performance issues.Identity and access managementManaging multiple changing roles, groups, products and authorisations can be difficult, especially in large organisations. Graph technology integrates your data and allows quick and effective identity and access control. It also allows you to track all identity and access authorisations and inheritances with significant depth and real-time insights.Network and IT operationsBecause of the scale and complexity of network and IT infrastructure, you need a configuration management database (CMDB) that is far more capable than relational databases. Neptune is an example of a CMDB and graph database that allows you to correlate your network, data centre, and IT assets to aid troubleshooting, impact analysis, and capacity or outage planning.A graph database allows you to integrate various monitoring tools and acquire important insights into the complicated relationships that exist between various network or data centre processes. Possible applications of graphs in network and IT operations range from dependency management to automated microservice monitoring.Risk assessment and monitoringRisk assessment is crucial in the fintech business. With multiple sources of credit data such as ecommerce sites, mobile wallets and loan repayment records, it can be difficult to accurately assess an individual’s credit risk. Graph Technology makes it possible to combine these data sources, quantify an individual’s fraud risk and even generate full credit reviews.One clear example of this is IceKredit, which employs artificial intelligence (AI) and machine learning (ML) techniques to make better risk-based decisions. With Graph technology, IceKredit has also successfully detected unreported links and increased efficiency of financial crime investigations.Social networkWhether you’re using stated social connections or inferring links based on behaviour, social graph databases like Neptune introduce possibilities for building new social networks or integrating existing social graphs into commercial applications.Having a data model that is identical to your domain model allows you to better understand your data, communicate more effectively, and save time. By decreasing the time spent data modelling, graph databases increase the quality and speed of development for your social network application.Artificial intelligence (AI) and machine learning (ML)AI and ML use statistical and analytical approaches to find patterns in data and provide insights. However, there are two prevalent concerns that arise - the quality of data and effectiveness of the analytics. Some AI and ML solutions have poor accuracy because there is not enough training data or variants that have a high correlation to the outcome.These ML data issues can be solved with graph databases as it’s possible to connect and traverse links, as well as supplement raw data. With Graph technology, ML systems can recognise each column as a “feature” and each connection as a distinct characteristic, and then be able to identify data patterns and train themselves to recognise these relationships.ConclusionGraphs are a great way to visually represent complex systems and can be used to easily detect patterns or relationships between entities. To help improve graphs’ ability to detect patterns early, businesses should consider using Graph technology, which is the next step in improving analytics delivery.Graph technology typically consists of:  Graph theory - Used to find paths, linkages and networks of logical or physical objects.  Graph analytics - Application of graph theory to uncover nodes, edges, and data linkages.  Graph database - Storage for data generated by graph analytics.Although predominantly used in fraud detection, Graph technology has many other use cases such as making real-time recommendations based on consumer behaviour, identity and access control, risk assessment and monitoring, AI and ML, and many more.In our next blog article, we will be talking about how our Graph Visualisation Platform enhances Grab’s fraud detection methods.Speak to usGrabDefence is a proprietary fraud prevention platform built by Grab, Southeast Asia’s leading superapp. Since 2019, the GrabDefence team has shared its fraud management capabilities and platform with enterprises and startups to leverage Grab’s advanced AI/ML models, hyper local insights and patented device intelligence technologies.To learn more about GrabDefence or to speak to our fraud management experts, contact us at gd.contact@grabtaxi.com.References  https://www.baeldung.com/cs/graph-theory-intro  https://web.stanford.edu/class/cs520/2020/notes/What_Are_Graph_Data_Models.htmlJoin usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/graph-concepts"
      }
      ,
    
      "automated-experiment-analysis": {
        "title": "Automated Experiment Analysis - Making experimental analysis scalable",
        "author": "albert-chengankit-sinhasaubhagya-awaneeshkenneth-rithvikruike-zhang",
        "tags": "[&quot;Experiment&quot;, &quot;Experimental analysis&quot;, &quot;Azure Databricks&quot;]",
        "category": "",
        "content": "IntroductionTrustworthy experiments are key to making sound decisions, so analysts and data scientists put a lot of effort into analysing them and making business impacts. An extension of Grab’s Experimentation (GrabX) platform, Automated Experiment Analysis is one of Grab’s data products that helps automate statistical analyses of experiments. It also provides automatic experimental data pipelines and customised tests for different types of experiments.Designed to help Grab in its journey of innovation and data-driven decision making, the data product helps to:  Standardise and automate the basic experiment analysis process on Grab experiments.  Ensure post-experiment results are reproducible under a company-wide standard, and easily reviewed by each other.  Democratise the institutional knowledge of experimentation across functions.BackgroundToday, the GrabX platform provides the ability to define, configure, and execute online controlled experiments (OCEs), often called A/B tests, to gather trustworthy data and make data-driven decisions about how to improve our products.Before the automated analysis, each experiment was analysed manually on an ad-hoc basis. This manual and federated model brings in several challenges at the company level:  Inefficiency: Repetitive nature of data pipeline building and basic post-experiment analyses incur large costs and deplete the analysts’ bandwidth from running deeper analyses.  Lack of quality control: Risk of unstandardised, inaccurate or late results as the platform cannot exercise data-governance/control or extend offerings to Grab’s other entities.  Lack of scalability and availability: GrabX users have varied backgrounds and skills, making their approaches to experiments different and not easily transferable/shared. E.g. Some teams may use more advanced techniques to speed up their experiments without using too much resources but these techniques are not transferable without considerable training.SolutionArchitecture details  Architecture diagram  When users set up experiments on GrabX, they can configure the success metrics they are interested in. These metrics configurations are then stored in the metadata as “bronze”, “silver”, and “gold” datasets depending on the corresponding step in the automated data pipeline process.Metrics configuration and “bronze” datasetsIn this project, we have developed a metrics glossary that stores information about what the metrics are and how they are computed. The metrics glossary is stored in CosmoDB and serves as an API Endpoint for GrabX so users can pick from the list of available metrics. If a metric is not available, users can input their custom metrics definition.This metrics selection, as an analysis configuration, is then stored as a “bronze” dataset in Azure Data Lake as metadata, together with the experiment configurations. Once the experiment starts, the data pipeline gathers all experiment subjects and their assigned experiment groups from our clickstream tracking system.In this case, the experiment subject refers to the facets of the experiment. For example, if the experiment subject is a user, then the user will go through the same experience throughout the entire experimentation period.Metrics computation and “silver” datasetsIn this step, the metrics engine gathers all metrics data based on the metrics configuration and computes the metrics for each experiment subject. This computed data is then stored as a “silver” dataset and is the foundation dataset for all statistical analyses.“Silver” datasets are then passed through the “Decision Engine” to get the final “gold” datasets, which contain the experiment results.Results visualisation and ”gold” datasetsIn “gold” datasets, we have the result of the experiment, along with some custom messages we want to show our users. These are saved in sets of fact and dim tables (typically used in star schemas).For users to visualise the result on GrabX, we leverage the embedded Power BI visualisation. We build the visualisation using a “gold” dataset and embed it to each experiment page with a fixed filter. By doing so, users can experience the end-to-end flow directly from GrabX.ImplementationThe implementation consists of four key engineering components:  Analysis configuration setup  A data pipeline  Automatic analysis  Results visualisationAnalysis configuration is part of the experiment setup process where users select success metrics they are interested in. This is an essential configuration for post-experiment analysis, in addition to the usual experiment configurations (e.g. sampling strategies).It ensures that the reported experiment results will align with the hypothesis setup, which helps avoid one of the common pitfalls in OCEs 1.There are three types of metrics available:  Pre-defined metrics: These metrics are already defined in the Scribe datamart, e.g. Gross Merchandise Value (GMV) per pax.  Event-based metrics: Users can specify an ad-hoc metric in the form of a funnel with event names for funnel start and end.  Build your own metrics: Users have the flexibility to define a metric in the form of a SQL query.A data pipeline here mainly consists of data sourcing and data processing. We use Azure Data Factory to schedule ETL pipelines so we can calculate the metrics and statistical analysis. ETL jobs are written in spark and run using Databricks.Data pipelines are streamlined to the following:  Load experiments and metrics metadata, defined at the experiment creation stage.  Load experiment and clickstream events.  Load experiment assignments. An experiment assignment maps a randomisation unit ID to the corresponding experiment or variant IDs.  Merge the data mentioned above for each experiment variant, and obtain sufficient data to do a deeper results analysis.Automatic analysis uses an internal python package “Decision Engine”, which decouples the dataset and statistical tests, so that we can incrementally improve applications of advanced techniques. It provides a comprehensive set of test results at the variant level, which include statistics, p-values, confidence intervals, and the test choices that correspond to the experiment configurations. It’s a crowdsourced project which allows all to contribute what they believe should be included in fundamental post-experiment analysis.Results visualisation leverages PowerBI, which is embedded in the GrabX UI, so users can run the experiments and review the results on a single platform. ImpactAt the individual user level, Automated Experiment Analysis is designed to enable analysts and data scientists to associate metrics with experiments, and present the experiment results in a standardised and comprehensive manner. It speeds up the decision-making process and frees up the bandwidths of analysts and data scientists to conduct deeper analyses.At the user community level, it improves the efficiency of running experimental analysis by capturing all experiments, their results, and the launch decision within a single platform.Learnings/ConclusionAutomated Experiment Analysis is the first building block to boost the trustworthiness of OCEs in Grab. Not all types of experiments are fully onboard, and they might not need to be. Through this journey, we believe these key learnings would be useful for experimenters and platform teams:  To standardise and simplify several experimental analysis steps, there needs to be automation data pipelines, analytics tools, and a metrics store in the infrastructure.  The “Decision Engine” analytics tool should be decoupled from the other engineering components, so that it can be incrementally improved in future.  To democratise knowledge and ensure service coverage, many components need to have a crowdsourcing feature, e.g. the metrics store has a BYOM function, and “Decision Engine” is an open-sourced internal python package.  Tracking implementation is important. To standardise data pipelines and achieve scalability, we need to standardise the way we implement tracking.What’s next?A centralised metric store -  We built a metric calculation dictionary, which currently contains around 30-40 basic business metrics, but its functionality is limited to GrabX Experimentation use case.If the metric store is expected to serve more general uses, it needs to be further enriched by allowing some “smarts”, e.g. fabric-agnostic metrics computations 2, other types of data slicing, and some considerations with real-time metrics or signals.An end-to-end experiment guide rail - Currently, we provide automatic data analysis after an experiment is done, but no guardrail features at multiple experiment stages, e.g. sampling strategy choices, sample size recommendation from the planning stage, and data quality check during/after the experiment windows. Without the end-to-end guardrails, running experiments will be very prone to pitfalls. We therefore plan to add some degree of automation to ensure experiments adhere to the standards used by the post-experimental analysis.A more comprehensive analysis toolbox - The current state of the project mainly focuses on infrastructure development, so it starts with basic frequentist’s A/B testing approaches. In future versions, it can be extended to include sequential testing, CUPED 3, attribution analysis, Causal Forest, heterogeneous treatment effects, etc.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References            Dmitriev, P., Gupta, S., Kim, D. W., &amp; Vaz, G. (2017, August). A dirty dozen: twelve common metric interpretation pitfalls in online controlled experiments. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1427-1436). &#8617;              Metric computation for multiple backends, Craig Boucher, Ulf Knoblich, Dan Miller, Sasha Patotski, Amin Saied, Microsoft Experimentation Platform &#8617;              Deng, A., Xu, Y., Kohavi, R., &amp; Walker, T. (2013, February). Improving the sensitivity of online controlled experiments by utilising pre-experiment data. In Proceedings of the sixth ACM international conference on Web search and data mining (pp. 123-132). &#8617;      ",
        "url": "/automated-experiment-analysis"
      }
      ,
    
      "search-architecture-revamp": {
        "title": "Search architecture revamp",
        "author": "lipeng-zhangtao-houweilun-wu",
        "tags": "[&quot;Architecture&quot;, &quot;Optimisation&quot;, &quot;Search&quot;]",
        "category": "",
        "content": "BackgroundPrior to 2021, Grab’s search architecture was designed to only support textual matching, which takes in a user query and looks for exact matches within the ecosystem through an inverted index. This legacy system meant that only textual matching results could be fetched.In the second half of 2021, the Deliveries search team worked on improving this architecture to make it smarter, more scalable and also unlock future growth for different search use cases at Grab. The figure below shows a simplified overview of the legacy architecture.  Legacy architecture  Problem statementWith the legacy system, we noticed several problems.Search results were textually matched without considering intention and contextIf a user types in a query “Roti Prata” (flatbread), he is likely looking for Roti Prata dishes and those matches with the dish name should be prioritised compared with matches with the merchant-partner’s name or matches with other entities.In the legacy system, all entities whose names partially matched “Roti Prata” were displayed and ranked according to hard coded weights, and matches with merchant-partner names were always prioritised, even if the user intention was clearly to search for the “Roti Prata” dish itself.  This problem was more common in Mart, as users often intended to search for items instead of shops. Besides the lack of intention recognition, the search system was also unable to take context into consideration; users searching the same keyword query at different times and locations could have different objectives. E.g. if users search for “Bread” in the day, they may be likely to look for cafes while searches at night could be for breakfast the next day.Search results from multiple business verticals were not blended effectivelyIn Grab’s context, results from multiple verticals were often merged. For example, in Mart searches, Ads and Mart organic search results were displayed together; in Food searches, Ads, Food and Mart organic results were blended together.In the legacy architecture, multiple business verticals were merged on the Deliveries API layer, which resulted in the leak of abstraction and loss of useful data as data from the search recall stage was also not taken into account during the merge stage.Inability to quickly scale to new search use cases and difficulty in reusing existing componentsThe legacy code base was not written in a structured way that could scale to new use cases easily. If new search use cases cannot be built on top of an existing system, it can be rather tedious to keep rebuilding the function every time there is a new search use case.SolutionIn this section, solutions from both architecture and implementation perspectives are presented to address the above problem statements.ArchitectureIn the new architecture, the flow is extended from lexical recall only to multi-layer including boosting, multi-recall, and ranking. The addition of boosting enables capabilities like intent recognition and query expansion, while the change from single lexical recall to multi-recall opens up the potential for other recall methods, e.g. embedding based and graph based.These help address the first problem statement. Furthermore, the multi-recall framework enables fetching results from multiple business verticals, addressing the second problem statement. In the new framework, results from different verticals and different recall methods were grouped and ranked together without any leak of abstraction or loss of useful data from search recall stage in ranking.  Upgraded architecture  ImplementationWe believe that the key to a platform’s success is modularisation and flexible assembling of plugins to enable quick product iteration. That is why we implemented a combination of a framework defined by the platform and plugins provided by service teams. In this implementation, plugins are assembled through configurations, which addresses the third problem statement and has two advantages:  Separation of concern. With the main flow abstracted and maintained by the platform, service team developers could focus on the application logic by writing plugins and fitting them into the main flow. In this case, developers without search experience could quickly enable new search flows.  Reusing plugins and economies of scale. With more use cases onboarded, more plugins are written by service teams and these plugins are reusable assets, resulting in scale effect. For example, an Ads recall plugin could be reused in Food keyword or non-keyword searches, Mart keyword or non-keyword searches and universal search flows as all these searches contain non-organic Ads. Similarly, a Mart recall plugin could be reused in Mart keyword or non-keyword searches, universal search and Food keyword search flows, as all these flows contain Mart results. With more plugins accumulated on our platform, developers might be able to ship a new search flow by just reusing and assembling the existing plugins.ConclusionOur platform now has a smart search with intent recognition and semantic (embedding-based) search. The process of adding new modules is also more straightforward and adds intention recognition to the boosting step as well as embedding as an additional recall to the multi-recall step. These modules can be easily reused by other use cases.On top of that, we also have a mixed Ads and an organic framework. This means that data in the recall stage is taken into consideration and Ads can now be ranked together with organic results, e.g. text relevance.With a modularised design and plugins provided by the platform, it is easier for clients to use our platform with a simple onboarding process. Furthermore, plugins can be reused to cater to new use cases and achieve a scale effect.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/search-architecture-revamp"
      }
      ,
    
      "doc-as-code": {
        "title": "Embracing a Docs-as-Code approach",
        "author": "shujuan-cheong",
        "tags": "[&quot;Docs-as-Code&quot;, &quot;Documentation&quot;, &quot;Technical documentation&quot;, &quot;Engineering practices&quot;]",
        "category": "",
        "content": "The Docs-as-Code concept has been gaining traction in the past few years as more tech companies start implementing this approach. One of the most widely-known examples is Spotify, that ​​uses Docs-as-Code to publish documentation in an internal developer portal.Since the start of 2021, Grab has also adopted a Docs-as-Code approach to improve our technical documentation. Before we talk about how this is done at Grab, let’s explain what this concept really means.What is Docs-as-Code?Docs-as-Code is a mindset of creating and maintaining technical documentation. The goal is to empower engineers to write technical documentation frequently and keep it up to date by integrating with their tools and processes.This means that technical documentation is placed in the same repository as the code, making it easier for engineers to write and update. Next, we’ll go through the motivations behind this initiative.Why embark on this journey?After speaking to Grab engineers, we found that some of their biggest challenges are around finding and writing documentation. Like many other companies on the same journey, Grab is rather big and our engineers are split into many different teams. Within each team, technical documentation can be stored on different platforms and in different formats, e.g. Google drive documents, text files, etc. This makes it hard to find relevant information, especially if you are trying to find another team’s documentation.On top of that, we realised that the documentation process is disconnected from an engineer’s everyday activities, making technical documentation an awkward afterthought. This means that even if people could find the information, there was a good chance that it would not be up to date.To address these issues, we need a centralised platform, a single source of truth, so that people can find and discover technical documentation easily. But first, we need to change how we write technical documentation. This is where Docs-as-Code comes in.  How does Docs-as-Code solve the problem?With Docs-as-Code, technical documentation is:  Written in plaintext.  Editable in a code editor.  Stored in the same repository as the source code so it’s easier to update docs whenever a code change is committed.  Published on a central platform.The idea is to consolidate all technical documentation on a central platform, making it easier to discover and find content by using an easy-to-navigate information architecture and targeted search.How is Grab embracing Docs-as-Code?We’ve developed an internal developer portal that simplifies the process of writing, reviewing and publishing technical documentation.Here’s a brief overview of the process:  Create a dedicated docs folder in a Git repository.  Push Markdown files into the docs folder.  Configure the developer portal to publish docs from the respective code repository.The latest version of the documentation will automatically be built and published in the developer portal.  Simplified documentation process  This way, technical documentation is closer to the source code and integrated into the code development process. Writing and updating technical documentation becomes part of writing code, and this encourages engineers to keep documentation updated.Measuring successWhenever there’s a change throughout big organisations like Grab, it can be tough to implement. But thankfully, our engineers recognised the importance of improving documentation and making it easier to maintain or update.We surveyed our users and here’s what some have said about our Docs-as-Code initiative:  “[W]ith the doc and source code in one place, test backend engineers can now make doc changes via standard code review process and re-use the same content for CLI helper message and documentation.” - Kang Yaw Ong, Test Automation - Engineering Manager  “[Docs-as-Code] is a great initiative, as it keeps documentation in line and up-to-date with the development of a project. Managing documentation using a version control system and the same tools to handle merges and conflicts reduces overhead and friction in an engineer’s workflow.” - Eugene Chiang, Foundations - Engineering ManagerProgress and future optimisationsSince we first started the Docs-as-Code initiative in Grab, we’ve made a lot of progress in terms of adoption - approximately 80% of Grab services will have their technical documentation on the internal portal by April 2022.We’ve also improved overall user experience by enhancing stability and performance, improving navigation and content formatting, and enabling feedback. But it doesn’t stop there; we are continuously improving the internal portal and providing more features for our engineers.Apart from technical documentation, we are also applying the Docs-as-Code approach to our technical training content. This means moving both self-paced and workshop training content to a centralised repository and providing engineers a single platform for all their learning needs.Special thanks to the Tech Learning - Documentation team for their contributions to this blog post.We are hiring!We are looking for more technical content developers to join the team. If you’re keen on joining our Docs-as-Code journey and improving developer experience, check out our open listings in Singapore and Malaysia.Join us in driving this initiative forward and making documentation more approachable for everyone!Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/doc-as-code"
      }
      ,
    
      "graph-networks": {
        "title": "Graph Networks - Striking fraud syndicates in the dark",
        "author": "muqi-li",
        "tags": "[&quot;Graph networks&quot;, &quot;Graphs&quot;, &quot;Fraud detection&quot;, &quot;Security&quot;]",
        "category": "",
        "content": "    As a leading superapp in Southeast Asia, Grab serves millions of consumers daily. This naturally makes us a target for fraudsters and to enhance our defences, the Integrity team at Grab has launched several hyper-scaled services, such as the Griffin real-time rule engine and Advanced Feature Engineering. These systems enable data scientists and risk analysts to develop real-time scoring, and take fraudsters out of our ecosystems.Apart from individual fraudsters, we have also observed the fast evolution of the dark side over time. We have had to evolve our defences to deal with professional syndicates that use advanced equipment such as device farms and GPS spoofing apps to perform fraud at scale. These professional fraudsters are able to camouflage themselves as normal users, making it significantly harder to identify them with rule-based detection.Since 2020, Grab’s Integrity team has been advancing fraud detection with more sophisticated techniques and experimenting with a range of graph network technologies such as graph visualisations, graph neural networks and graph analytics. We’ve seen a lot of progress in this journey and will be sharing some key learnings that might help other teams who are facing similar issues.What are Graph-based Prediction Platforms?  “You can fool some of the people all of the time, and all of the people some of the time, but you cannot fool all of the people all of the time.” - Abraham LincolnA Graph-based Prediction Platform connects multiple entities through one or more common features. When such entities are viewed as a macro graph network, we uncover new patterns that are otherwise unseen to the naked eye. For example, when investigating if two users are sharing IP addresses or devices, we might not be able to tell if they are fraudulent or just family members sharing a device.However, if we use a graph system and look at all users sharing this device or IP address, it could show us if these two users are part of a much larger syndicate network in a device farming operation. In operations like these, we may see up to hundreds of other fake accounts that were specifically created for promo and payment fraud. With graphs, we can identify fraudulent activity more easily.Grab’s Graph-based Prediction PlatformLeveraging the power of graphs, the team has primarily built two types of systems:  Graph Database Platform: An ultra-scalable storage system with over one billion nodes that powers:                  Graph Visualisation: Risk specialists and data analysts can review user connections real-time and are able to quickly capture new fraud patterns with over 10 dimensions of features (see Fig 1).          Fig 1: Graph visualisation                      Network-based feature system: A configurable system for engineers to adjust machine learning features based on network connectivity, e.g. number of hops between two users, numbers of shared devices between two IP addresses.                  Graph-based Machine Learning: Unlike traditional fraud detection models, Graph Neural Networks (GNN) are able to utilise the structural correlations on the graph and act as a sustainable foundation to combat many different kinds of fraud. The data science team has built large-scale GNN models for scenarios like anti-money laundering and fraud detection.    Fig 2 shows a Money Laundering Network where hundreds of accounts coordinate the placement of funds, layering the illicit monies through a complex web of transactions making funds hard to trace, and consolidate funds into spending accounts.    Fig 2: Money Laundering Network  What’s next?In a future article of our Graph Network blog series, we will dive deeper into how we develop the graph infrastructure and database using AWS Neptune.Check out the other articles in this series:  Graph concepts and applications  Graph Networks - 10X investigation with Graph Visualisations  Graph for fraud detection  Graph service platformSpeak to usGrabDefence is a proprietary fraud prevention platform built by Grab, Southeast Asia’s leading superapp. Since 2019, the GrabDefence team has shared its fraud management capabilities and platform with enterprises and startups to leverage Grab’s advanced AI/ML models, hyper local insights and patented device intelligence technologies.To learn more about GrabDefence or to speak to our fraud management experts, contact us at gd.contact@grabtaxi.com.Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/graph-networks"
      }
      ,
    
      "how-we-reduced-our-ci-yaml": {
        "title": "How we reduced our CI YAML files from 1800 lines to 50 lines",
        "author": "jialong-lohoscar-cassettiwenbo-wei",
        "tags": "[&quot;CI&quot;, &quot;Machine Learning&quot;, &quot;Pipelines&quot;, &quot;Continuous Integration&quot;, &quot;Continuous Delivery&quot;, &quot;Optimisation&quot;, &quot;Rust&quot;]",
        "category": "",
        "content": "This article illustrates how the Cauldron Machine Learning (ML) Platform team uses GitLab parent-child pipelines to dynamically generate GitLab CI files to solve several limitations of GitLab for large repositories, namely:  Limitations to the number of includes (100 by default).  Simplifying the GitLab CI file from 1800 lines to 50 lines.  Reducing the need for nested gitlab-ci yml files.IntroductionCauldron is the Machine Learning (ML) Platform team at Grab. The Cauldron team provides tools for ML practitioners to manage the end to end lifecycle of ML models, from training to deployment. GitLab and its tooling are an integral part of our stack, for continuous delivery of machine learning.One of our core products is MerLin Pipelines. Each team has a dedicated repo to maintain the code for their ML pipelines. Each pipeline has its own subfolder. We rely heavily on GitLab rules to detect specific changes to trigger deployments for the different stages of different pipelines (for example, model serving with Catwalk, and so on).BackgroundApproach 1: Nested child filesOur initial approach was to rely heavily on static code generation to generate the child gitlab-ci.yml files in individual stages. See Figure 1 for an example directory structure. These nested yml files are pre-generated by our cli and committed to the repository.    Figure 1: Example directory structure with nested gitlab-ci.yml files. Child `gitlab-ci.yml` files are added by using the include keyword.&nbsp;    Figure 2: Example root .gitlab-ci.yml file, and include clauses.&nbsp;    Figure 3: Example child `.gitlab-ci.yml` file for a given stage (Deploy Model) in a pipeline (pipeline 1).&nbsp;As teams add more pipelines and stages, we soon hit a limitation in this approach:  There was a soft limit in the number of includes that could be in the base .gitlab-ci.yml file.It became evident that this approach would not scale to our use-cases.Approach 2: Dynamically generating a big CI fileOur next attempt to solve this problem was to try to inject and inline the nested child gitlab-ci.yml contents into the root gitlab-ci.yml file, so that we no longer needed to rely on the in-built GitLab “include” clause.To achieve it, we wrote a utility that parsed a raw gitlab-ci file, walked the tree to retrieve all “included” child gitlab-ci files, and to replace the includes to generate a final big gitlab-ci.yml file.Figure 4 illustrates the resulting file is generated from Figure 3.    Figure 4: “Fat” YAML file generated through this approach, assumes the original raw file of Figure 3.&nbsp;This approach solved our issues temporarily. Unfortunately, we ended up with GitLab files that were up to 1800 lines long. There is also a soft limit to the size of gitlab-ci.yml files. It became evident that we would eventually hit the limits of this approach.SolutionOur initial attempt at using static code generation put us partially there. We were able to pre-generate and infer the stage and pipeline names from the information available to us. Code generation was definitely needed, but upfront generation of code had some key limitations, as shown above. We needed a way to improve on this, to somehow generate GitLab stages on the fly. After some research, we stumbled upon Dynamic Child Pipelines.Quoting the official website:  Instead of running a child pipeline from a static YAML file, you can define a job that runs your own script to generate a YAML file, which is then used to trigger a child pipeline.  This technique can be very powerful in generating pipelines targeting content that changed or to build a matrix of targets and architectures.We were already on the right track. We just needed to combine code generation with child pipelines, to dynamically generate the necessary stages on the fly.Architecture details    Figure 5: Flow diagram of how we use dynamic yaml generation. The user raises a merge request in a branch, and subsequently merges the branch to master.&nbsp;ImplementationThe user Git flow can be seen in Figure 5, where the user modifies or adds some files in their respective Git team repo. As a refresher, a typical repo structure consists of pipelines and stages (see Figure 1). We would need to extract the information necessary from the branch environment in Figure 5, and have a stage to programmatically generate the proper stages (for example, Figure 3).In short, our requirements can be summarized as:  Detecting the files being changed in the Git branch.  Extracting the information needed from the files that have changed.  Passing this to be templated into the necessary stages.Let’s take a very simple example, where a user is modifying a file in stage_1 in pipeline_1 in Figure 1. Our desired output would be:    Figure 6: Desired output that should be dynamically generated.&nbsp;Our template would be in the form of:    Figure 7: Example template, and information needed. Let’s call it template_file.yml.&nbsp;First, we need to detect the files being modified in the branch. We achieve this with native git diff commands, checking against the base of the branch to track what files are being modified in the merge request. The output (let’s call it diff.txt) would be in the form of:M        pipelines/pipeline_1/stage_1/modelserving.yaml Figure 8: Example diff.txt generated from git diff. We must extract the yellow and green information from the line, corresponding to pipeline_name and stage_name.    Figure 9: Information that needs to be extracted from the file.&nbsp;We take a very simple approach here, by introducing a concept called stop patterns.Stop patterns are defined as a comma separated list of variable names, and the words to stop at. The colon (:) denotes how many levels before the stop word to stop.For example, the stop pattern:pipeline_name:pipelinestells the parser to look for the folder pipelines and stop before that, extracting pipeline_1 from the example above tagged to the variable name pipeline_name.The stop pattern with two colons (::):stage_name::pipelinestells the parser to stop two levels before the folder pipelines, and extract stage_1 as stage_name.Our cli tool allows the stop patterns to be comma separated, so the final command would be:cauldron_repo_util diff.txt template_file.ymlpipeline_name:pipelines,stage_name::pipelines &gt; generated.ymlWe elected to write the util in Rust due to its high performance, and its rich templating libraries (for example, Tera) and decent cli libraries (clap).Combining all these together, we are able to extract the information needed from git diff, and use stop patterns to extract the necessary information to be passed into the template. Stop patterns are flexible enough to support different types of folder structures.    Figure 10: Example Rust code snippet for parsing the Git diff file.&nbsp;When triggering pipelines in the master branch (see right side of Figure 5), the flow is the same, with a small caveat that we must retrieve the same diff.txt file from the source branch. We achieve this by using the rich GitLab API, retrieving the pipeline artifacts and using the same util above to generate the necessary GitLab steps dynamically.ImpactAfter implementing this change, our biggest success was reducing one of the biggest ML pipeline Git repositories from 1800 lines to 50 lines. This approach keeps the size of the .gitlab-ci.yaml file constant at 50 lines, and ensures that it scales with however many pipelines are added.Our users, the machine learning practitioners, also find it more productive as they no longer need to worry about GitLab yaml files.Learnings and conclusionWith some creativity, and the flexibility of GitLab Child Pipelines, we were able to invest some engineering effort into making the configuration re-usable, adhering to DRY principles.Special thanks to the Cauldron ML Platform team.What’s nextWe might open source our solution.References      Parent-child pipelines        Backend: The gitlab-ci.yml is limited to 100 includes  Join usGrab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/how-we-reduced-our-ci-yaml"
      }
      ,
    
      "kafka-connect": {
        "title": "How Kafka Connect helps move data seamlessly",
        "author": "wenli-wankaran-kamaththanhtung-dao",
        "tags": "[&quot;Kafka&quot;, &quot;Data processing&quot;, &quot;Real-Time&quot;]",
        "category": "",
        "content": "Grab’s real-time data platform team a.k.a. Coban has written about Plumbing at scale, Optimally scaling Kakfa consumer applications, and Exposing Kafka via VPCE. In this article, we will cover the importance of being able to easily move data in and out of Kafka in a low-code way and how we achieved this with Kafka Connect.To build a NoOps managed streaming platform in Grab, the Coban team has:  Engineered an ecosystem on top of Apache Kafka.  Successfully adopted it to production for both transactional and analytical use cases.  Made it a battle-tested industrial-standard platform.In 2021, the Coban team embarked on a new journey (Kafka Connect) that enables and empowers Grabbers to move data in and out of Apache Kafka seamlessly and conveniently.Kafka Connect stack in GrabThis is what Coban’s Kafka Connect stack looks like today. Multiple data sources and data sinks, such as MySQL, S3 and Azure Data Explorer, have already been supported and productionised.    The Coban team has been using Protobuf as the serialisation-deserialisation (SerDes) format in Kafka. Therefore, the role of Confluent schema registry (shown at the top of the figure) is crucial to the Kafka Connect ecosystem, as it serves as the building block for conversions such as Protobuf-to-Avro, Protobuf-to-JSON and Protobuf-to-Parquet.What problems are we trying to solve?Problem 1: Change Data Capture (CDC)In a big organisation like Grab, we handle large volumes of data and changes across many services on a daily basis, so it is important for these changes to be reflected in real time.In addition, there are other technical challenges to be addressed:  As shown in the figure below, data is written twice in the code base - once into the database (DB) and once as a message into Kafka. In order for the data in the DB and Kafka to be consistent, the two writes have to be atomic in a two-phase commit protocol (or other atomic commitment protocols), which is non-trivial and impacts availability.  Some use cases require data both before and after a change.    Problem 2: Message mirroring for disaster recoveryThe Coban team has done some research on Kafka MirrorMaker, an open-source solution. While it can ensure better data consistency, it takes significant effort to adopt it onto existing Kubernetes infrastructure hosted by the Coban team and achieve high availability.Another major challenge that the Coban team faces is offset mirroring and translation, which is a known challenge in Kafka communities. In order for Kafka consumers to seamlessly resume their work with a backup Kafka after a disaster, we need to cater for offset translation.Data ingestion into Azure Event HubsAzure Event Hubs has a Kafka-compatible interface and natively supports JSON and Avro schema. The Coban team uses Protobuf as the SerDes framework, which is not supported by Azure Event Hubs. It means that conversions have to be done for message ingestion into Azure Event Hubs.SolutionTo tackle these problems, the Coban team has picked Kafka Connect because:  It is an open-source framework with a relatively big community that we can consult if we run into issues.  It has the ability to plug in transformations and custom conversion logic.Let us see how Kafka Connect can be used to resolve the previously mentioned problems.Kafka Connect with Debezium connectorsDebezium is a framework built for capturing data changes on top of Apache Kafka and the Kafka Connect framework. It provides a series of connectors for various databases, such as MySQL, MongoDB and Cassandra.Here are the benefits of MySQL binlog streams:  They not only provide changes on data, but also give snapshots of data before and after a specific change.  Some producers no longer have to push a message to Kafka after writing a row to a MySQL database. With Debezium connectors, services can choose not to deal with Kafka and only handle MySQL data stores.Architecture    In case of DB upgrades and outagesDB Data Definition Language (DDL) changes, migrations, splits and outages are common in database operations, and each operation type has a systematic resolution.The Debezium connector has built-in features to handle DDL changes made by DB migration tools, such as pt-online-schema-change, which is used by the Grab DB Ops team.To deal with MySQL instance changes and database splits, the Coban team leverages on the Kafka Connect framework’s ability to change the offsets of connectors. By changing the offsets, Debezium connectors can properly function after DB migrations and resume binlog synchronisation from any position in any binlog file on a MySQL instance.    Refer to the Debezium documentation for more details.Success storiesThe CDC project on MySQL via Debezium connectors has been greatly successful in Grab. One of the biggest examples is its adoption in the Elasticsearch optimisation carried out by GrabFood, which has been published in another blog.MirrorMaker2 with offset translationKafka MirrorMaker2 (MM2), developed in and shipped together with the Apache Kafka project, is a utility to mirror messages and consumer offsets. However, in the Coban team, the MM2 stack is deployed on the Kafka Connect framework per connector because:  A few Kafka Connect clusters have already been provisioned.  Compared to launching three connectors bundled in MM2, Coban can have finer controls on MirrorSourceConnector and MirrorCheckpointConnector, and manage both of them in an infrastructure-as-code way via Hashicorp Terraform.    Success storiesEnsuring business continuity is a key priority for Grab and this includes the ability to recover from incidents quickly. In 2021H2, there was a campaign that ran across many teams to examine the readiness and robustness of various services and middlewares. Coban’s Kafka is one of these services that proved to be robust after rounds of chaos engineering. With MM2 on Kafka Connect to mirror both messages and consumer offsets, critical services and pipelines could safely be replicated and launched across AWS regions if outages occur.Because the Coban team has proven itself as the battle-tested Kafka service provider in Grab, other teams have also requested to migrate streams from self-managed Kafka clusters to ones managed by Coban. MM2 has been used in such migrations and brought zero downtime to the streams’ producers and consumers.Mirror to Azure Event Hubs with an in-house converterThe Analytics team runs some real time ingestion and analytics projects on Azure. To support this cross-cloud use case, the Coban team has adopted MM2 for message mirroring to Azure Event Hubs.Typically, Event Hubs only accept JSON and Avro bytes, which is incompatible with the existing SerDes framework. The Coban team has developed a custom converter that converts bytes serialised in Protobuf to JSON bytes at runtime.These steps explain how the converter works:  Deserialise bytes in Kafka to a Protobuf DynamicMessage according to a schema retrieved from the Confluent™ schema registry.  Perform a recursive post-order depth-first-search on each field descriptor in the DynamicMessage.  Convert every Protobuf field descriptor to a JSON node.  Serialise the root JSON node to bytes.The converter has not been open sourced yet.Deployment    Docker containers are the Coban team’s preferred infrastructure, especially since some production Kafka clusters are already deployed on Kubernetes. The long-term goal is to provide Kafka in a software-as-a-service (SaaS) model, which is why Kubernetes was picked. The diagram below illustrates how Kafka Connect clusters are built and deployed.    What’s next?The Coban team is iterating on a unified control plane to manage resources like Kafka topics, clusters and Kafka Connect. In the foreseeable future, internal users should be able to provision Kafka Connect connectors via RESTful APIs and a graphical user interface (GUI).At the same time, the Coban team is closely working with the Data Engineering team to make Kafka Connect the preferred tool in Grab for moving data in and out of external storages (S3 and Apache Hudi).Coban is hiring!The Coban (Real-time Data Platform) team at Grab in Singapore is hiring software and site reliability engineers at all levels as we double down on growing our platform capabilities.Join us in building state-of-the-art, mission critical, TB/hour scale data platforms that enable thousands of engineers, data scientists, and analysts to serve millions of consumers, businesses, and partners across Southeast Asia!Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/kafka-connect"
      }
      ,
    
      "supporting-large-campaigns-at-scale": {
        "title": "Supporting large campaigns at scale",
        "author": "jie-zhangabdullah-mamun",
        "tags": "[&quot;Kafka&quot;, &quot;Scheduling&quot;, &quot;Stream processing&quot;, &quot;Batch processing&quot;, &quot;Scheduled job&quot;]",
        "category": "",
        "content": "IntroductionAt Grab, we run large marketing campaigns every day. A typical campaign may require executing multiple actions for millions of users all at once. The actions may include sending rewards, awarding points, and sending messages. Here is what a campaign may look like: On 1st Jan 2022, send two ride rewards to all the users in the “heavy users” segment. Then, send them a congratulatory message informing them about the reward.Years ago, Grab’s marketing team used to stay awake at midnight to manually trigger such campaigns. They would upload a file at 12 am and then wait for a long time for the campaign execution to complete. To solve this pain point and support more capabilities down this line, we developed a “batch job” service, which is part of our in-house real-time automation engine, Trident.The following are some services we use to support Grab’s marketing teams:  Rewards: responsible for managing rewards.  Messaging: responsible for sending messages to users. For example, push notifications.  Segmentation: responsible for storing and retrieving segments of users based on certain criteria.For simplicity, only the services above will be referenced for this article. The “batch job” service we built uses rewards and messaging services for executing actions, and uses the segmentation service for fetching users in a segment.System requirementsFunctional requirements  Apply a sequence of actions targeting a large segment of users at a scheduled time, display progress to the campaign manager and provide a final report.          For each user, the actions must be executed in sequence; the latter action can only be executed if the preceding action is successful.      Non-functional requirements  Quick execution and high turnover rate.          Definition of turnover rate: the number of scheduled jobs completed per unit time.        Maximise resource utilisation and balance server load.For the sake of brevity, we will not cover the scheduling logic, nor the generation of the report. We will focus specifically on executing actions.Naive approachLet’s start thinking from the most naive solution, and improve from there to reach an optimised solution.Here is the pseudocode of a naive action executor.def executeActionOnSegment(segment, actions):   for user in fetchUsersInSegment(segment):       for action in actions:           success := doAction(user, action)           if not success:               break           recordActionResult(user, action)def doAction(user, action):   if action.type == \"awardReward\":       rewardService.awardReward(user, action.meta)   elif action.type == \"sendMessage\":       messagingService.sendMessage(user, action.meta)   else:       # other action types ...One may be able to quickly tell that the naive solution does not satisfy our non-functional requirements for the following reasons:  Execution is slow:          The programme is single-threaded.      Actions are executed for users one by one in sequence.      Each call to the rewards and messaging services will incur network trip time, which impacts time cost.        Resource utilisation is low: The actions will only be executed on one server. When we have a cluster of servers, the other servers will sit idle.Here are our alternatives for fixing the above issues:  Actions for different users should be executed in parallel.  API calls to other services should be minimised.  Distribute the work of executing actions evenly among different servers.Note: Actions for the same user have to be executed in sequence. For example, if a sequence of required actions are (1) award a reward, (2) send a message informing the user to use the reward, then we can only execute action (2) after action (1) is successfully done for logical reasons and to avoid user confusion.Our approachA message queue is a well-suited solution to distribute work among multiple servers. We selected Kafka, among numerous message services, due to its following characteristics:  High throughput: Kafka can accept reads and writes at a very high speed.  Robustness: Events in Kafka are distributedly stored with redundancy, without a need to worry about data loss.  Pull-based consumption: Consumers can consume events at their own speed. This helps to avoid overloading our servers.When a scheduled campaign is triggered, we retrieve the users from the segment in batches; each batch comprises around 100 users. We write the batches into a Kafka stream, and all our servers consume from the stream to execute the actions for the batches. The following diagram illustrates the overall flow.    Data in Kafka is stored in partitions. The partition configuration is important to ensure that the batches are evenly distributed among servers:  Number of partitions: Ensure that the number of stream partitions is greater than or equal to the max number of servers we will have in our cluster. This is because one Kafka partition can only be consumed by one consumer. If we have more consumers than partitions, some consumers will not receive any data.  Partition key: For each batch, assign a hash value as the partition key to randomly allocate batches into different partitions.Now that work is distributed among servers in batches, we can consider how to process each batch faster. If we follow the naive logic, for each user in the batch, we need to call the rewards or messaging service to execute the actions. This will create very high QPS (queries per second) to those services, and incur significant network round trip time.To solve this issue, we decided to build batch endpoints in rewards and messaging services. Each batch endpoint takes in a list of user IDs and action metadata as input parameters, and returns the action result for each user, regardless of success or failure. With that, our batch processing logic looks like the following:def processBatch(userBatch, actions):   users = userBatch   for action in actions:       successUsers, failedUsers = doAction(users, action)       recordFailures(failedUsers, action)       users = successUsersdef doAction(users, action):   resp = {}   if action.type == \"awardReward\":       resp = rewardService.batchAwardReward(users, action.meta)   elif action.type == \"sendMessage\":       resp = messagingService.batchSendMessage(users, action.meta)   else:   # other action types ...   return getSuccessUsers(resp), getFailedUsers(resp)In the implementation of batch endpoints, we also made optimisations to reduce latency. For example, when awarding rewards, we need to write the records of a reward being given to a user in multiple database tables. If we make separate DB queries for each user in the batch, it will cause high QPS to DB and incur high network time cost. Therefore, we grouped all the users in the batch into one DB query for each table update instead.Benchmark tests show that using the batch DB query reduced API latency by up to 85%.Further optimisationsAs more campaigns started running in the system, we came across various bottlenecks. Here are the optimisations we implemented for some major examples.Shard stream by action typeTwo widely used actions are awarding rewards and sending messages to users. We came across situations where the sending of messages was blocked because a different campaign of awarding rewards had already started. If millions of users were targeted for rewards, this could result in significant waiting time before messages are sent, ultimately leading them to become irrelevant.We found out the API latency of awarding rewards is significantly higher than sending messages. Hence, to make sure messages are not blocked by long-running awarding jobs, we created a dedicated Kafka topic for messages. By having different Kafka topics based on the action type, we were able to run different types of campaigns in parallel.    Shard stream by countryGrab operates in multiple countries. We came across situations where a campaign of awarding rewards to a small segment of users in one country was delayed by another campaign that targeted a huge segment of users in another country. The campaigns targeting a small set of users are usually more time-sensitive.Similar to the above solution, we added different Kafka topics for each country to enable the processing of campaigns in different countries in parallel.Remove unnecessary waitingWe observed that in the case of chained actions, messaging actions are generally the last action in the action list. For example, after awarding a reward, a congratulatory message would be sent to the user.We realised that it was not necessary to wait for a sending message action to complete before processing the next batch of users. Moreover, the latency of the sending messages API is lower than awarding rewards. Hence, we adjusted the sending messages API to be asynchronous, so that the task of awarding rewards to the next batch of users can start while messages are being sent to the previous batch.ConclusionWe have architected our batch jobs system in such a way so that it can be enhanced and optimised without redoing its work. For example, although we currently obtain the list of targeted users from a segmentation service, in the future, we may obtain this list from a different source, for example, all Grab Platinum tier members.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/supporting-large-campaigns-at-scale"
      }
      ,
    
      "telematics-at-grab": {
        "title": "How telematics helps Grab to improve safety",
        "author": "wilson-burhan",
        "tags": "[&quot;Engineering&quot;, &quot;Data Science&quot;, &quot;Driving patterns&quot;, &quot;Safety&quot;, &quot;Analytics&quot;]",
        "category": "",
        "content": "Telematics is a collection of sensor data such as accelerometer data, gyroscope data, and GPS data that a driver’s mobile phone provides, and we collect, during the ride. With this information, we apply data science logic to detect traffic events such as harsh braking, acceleration, cornering, and unsafe lane changes, in order to help improve our consumers’ ride experience.IntroductionAs Grab grows to meet our consumers’ needs, the number of driver-partners has also grown. This requires us to ensure that our consumers’ safety continues to remain the highest priority as we scale. We developed an in-house telematics engine which uses mobile phone sensors to determine, evaluate, and quantify the driving behaviour of our driver-partners. This telemetry data is then evaluated and gives us better insights into our driver-partners’ driving patterns.Through our data, we hope to improve our driver-partners’ driving habits and reduce the likelihood of driving-related incidents on our platform. This telemetry data also helps us determine optimal insurance premiums for driver-partners with risky driving patterns and reward driver-partners who have better driving habits.In addition, we also merge telematics data with spatial data to further identify areas where dangerous driving manoeuvres happen frequently. This data is used to inform our driver-partners to be alert and drive more safely in such areas.BackgroundWith more consumers using the Grab app, we realised that purely relying on passenger feedback is not enough; we had no definitive way to tell which driver-partners were actually driving safely, when they deviated from their routes or even if they had been involved in an accident.To help address these issues, we developed an in-house telematics engine that analyses telemetry data, identifies driver-partners’ driving behaviour and habits, and provides safety reports for them.Architecture details    As shown in the diagram, our telematics SDK receives raw sensor data from our driver-partners’ devices and processes it in two ways:  On-device processing for crash detection: Used to determine situations such as if the driver-partner has been in an accident.  Raising traffic events and generating safety reports after each job: Useful for detecting events like speeding and harsh braking.Note: Safety reports are generated by our backend service using sensor data that is only uploaded as a text file after each ride.ImplementationOur telematics framework relies on accelerometer, gyroscope and GPS sensors within the mobile device to infer the vehicle’s driving parameters. Both accelerometer and gyroscope are triaxial sensors, and their respective measurements are in the mobile device’s frame of reference.That being said, the data collected from these sensors have no fixed sample rate, so we need to implement sensor data time synchronisation. For example, there will be temporal misalignment between gyroscope and accelerometer data if they do not share the same timestamp. The sample rate that comes from the accelerometer and gyroscope also varies independently. Therefore, we need to uniformly sample the sensor data to be at the same frequency rate.This synchronisation process is done in two steps:  Interpolation to uniform time grid at a reasonably higher frequency.  Decimation from the higher frequency to the output data rate for accelerometer and gyroscope data.We then use the Fourier Transform to transform a signal from time domain to frequency domain for compression. These components are then written to a text file on the mobile device, compressed, and uploaded after the end of each ride.Learnings/ConclusionThere are a few takeaways that we learned from this project:  Sensor data frequency: There are many device manufacturers out there for Android and each one of them has a different sensor chipset. The frequency of the sensor data may vary from device to device.  Four-wheel (4W) vs two-wheel (2W): The behaviour is different for a driver-partner on 2W vs 4W, so we need different rules for each.  Hardware axis-bias: The device may not be aligned with the vehicle during the ride. It cannot be assumed that the phone will remain in a fixed orientation throughout the trip, so the mobile device sensors might not accurately measure the acceleration/braking or sharp turning of the vehicle.  Sensor noise: There are artifacts in sensor readings, which are basically a single outlier event that represents an error and is not a valid sensor reading.  Time-synchronisation: GPS, accelerometer, and gyroscope events are captured independently by three different sensors and have different time formats. These events will need to be transformed into the same time grid in order to work together. For example, the GPS location from 30 seconds prior to the gyroscope event will not work as they are out of sync.  Data compression and network consumption: Longer rides will contain more telematics data.  It will result in a bigger upload size and increase in time for file compression.What’s next?There are a few milestones that we want to accomplish with our telematics framework in the future. However, our number one goal is to extend telematics to all bookings across Grab verticals. We are also planning to add more on-device rules and data processing for event detections to further eliminate future delays from backend communication for crash detection.With the data from our telematics framework, we can improve our passengers’ experience and improve safety for both passengers and driver-partners.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/telematics-at-grab"
      }
      ,
    
      "real-time-data-ingestion": {
        "title": "Real-time data ingestion in Grab",
        "author": "shuguang-xiangirfan-haniffeng-cheng",
        "tags": "[&quot;Engineering&quot;, &quot;Data ingestion&quot;]",
        "category": "",
        "content": "Typically, modern applications use various database engines for their service needs; within Grab, these would be MySQL, Aurora and DynamoDB. Lately, the Caspian team has observed an increasing need to consume real-time data for many service teams. These real-time changes in database records help to support online and offline business decisions for hundreds of teams.Because of that, we have invested time into synchronising data from MySQL, Aurora and Dynamodb to the message queue, i.e. Kafka. In this blog, we share how real-time data ingestion has helped since it was launched.IntroductionOver the last few years, service teams had to write all transactional data twice: once into Kafka and once into the database. This helped to solve the inter-service communication challenges and obtain audit trail logs. However, if the transactions fail, data integrity becomes a prominent issue. Moreover, it is a daunting task for developers to maintain the schema of data written into Kafka.With real-time ingestion, there is a notably better schema evolution and guaranteed data consistency; service teams no longer need to write data twice.You might be wondering, why don’t we have a single transaction that spans the services’ databases and Kafka, to make data consistent? This would not work as Kafka does not support being enlisted in distributed transactions. In some situations, we might end up having new data persisting into the services’ databases, but not having the corresponding message sent to Kafka topics.Instead of registering or modifying the mapped table schema in Golang writer into Kafka beforehand, service teams tend to avoid such schema maintenance tasks entirely. In such cases, real-time ingestion can be adopted where data exchange among the heterogeneous databases or replication between source and replica nodes is required.While reviewing the key challenges around real-time data ingestion, we realised that there were many potential user requirements to include. To build a standardised solution, we identified several points that we felt were high priority:  Make transactional data readily available in real time to drive business decisions at scale.  Capture audit trails of any given database.  Get rid of the burst read on databases caused by SQL-based query ingestion.To empower Grabbers with real-time data to drive their business decisions, we decided to take a scalable event-driven approach, which is being facilitated with a bunch of internal products, and designed a solution for real-time ingestion.  Anatomy of architectureThe solution for real-time ingestion has several key components:  Stream data storage  Event producer  Message queue  Stream processor  Figure 1. Real time ingestion architecture  Stream storageStream storage acts as a repository that stores the data transactions in order with exactly-once guarantee. However, the level of order in stream storage differs with regards to different databases.For MySQL or Aurora, transaction data is stored in binlog files in sequence and rotated, thus ensuring global order. Data with global order assures that all MySQL records are ordered and reflects the real life situation. For example, when transaction logs are replayed or consumed by downstream consumers, consumer A’s Grab food order at 12:01:44 pm will always appear before consumer B’s order at 12:01:45 pm.However, this does not necessarily hold true for DynamoDB stream storage as DynamoDB streams are partitioned. Audit trails of a given record show that they go into the same partition in the same order, ensuring consistent partitioned order. Thus when replay happens, consumer B’s order might appear before consumer A’s.Moreover, there are multiple formats to choose from for both MySQL binlog and DynamoDB stream records. We eventually set ROW for binlog formats and NEW_AND_OLD_IMAGES for DynamoDB stream records. This depicts the detailed information before and after modifying any given table record. The binlog and DynamoDB stream main fields are tabulated in Figures 2 and 3 respectively.  Figure 2. Binlog record schema    Figure 3. DynamoDB stream record schema  Event producerEvent producers take in binlog messages or stream records and output to the message queue. We evaluated several technologies for the different database engines.For MySQL or Aurora, three solutions were evaluated: Debezium, Maxwell, and Canal. We chose to onboard Debezium as it is deeply integrated with the Kafka Connect framework. Also, we see the potential of extending solutions among other external systems whenever moving large collections of data in and out of the Kafka cluster.One such example is the open source project that attempts to build a custom DynamoDB connector extending the Kafka Connect (KC) framework. It self manages checkpointing via an additional DynamoDB table and can be deployed on KC smoothly.However, the DynamoDB connector fails to exploit the fundamental nature of storage DynamoDB streams: dynamic partitioning and auto-scaling based on the traffic. Instead, it spawns only a single thread task to process all shards of a given DynamoDB table. As a result, downstream services suffer from data latency the most when write traffic surges.In light of this, the lambda function becomes the most suitable candidate as the event producer. Not only does the concurrency of lambda functions scale in and out based on actual traffic, but the trigger frequency is also adjustable at your discretion.KafkaThis is the distributed data store optimised for ingesting and processing data in real time. It is widely adopted due to its high scalability, fault-tolerance, and parallelism. The messages in Kafka are abstracted and encoded into Protobuf. Stream processorThe stream processor consumes messages in Kafka and writes into S3 every minute. There are a number of options readily available in the market; Spark and Flink are the most common choices. Within Grab, we deploy a Golang library to deal with the traffic.Use casesNow that we’ve covered how real-time data ingestion is done in Grab, let’s look at some of the situations that could benefit from real-time data ingestion.1. Data pipelinesWe have thousands of pipelines running hourly in Grab. Some tables have significant growth and generate workload beyond what a SQL-based query can handle. An hourly data pipeline would incur a read spike on the production database shared among various services, draining CPU and memory resources. This deteriorates other services’ performance and could even block them from reading. With real-time ingestion, the query from data pipelines would be incremental and span over a period of time.Another scenario where we switch to real-time ingestion is when a missing index is detected on the table. To speed up the query, SQL-based query ingestion requires indexing on columns such as created_at, updated_at and id. Without indexing, SQL based query ingestion would either result in high CPU and memory usage, or fail entirely.Although adding indexes for these columns would resolve this issue, it comes with a cost, i.e. a copy of the indexed column and primary key is created on disk and the index is kept in memory. Creating and maintaining an index on a huge table is much costlier than for small tables. With performance consideration in mind, it is not recommended to add indexes to an existing huge table.Instead, real-time ingestion overshadows SQL-based ingestion. We can spawn a new connector, archiver (Coban team’s Golang library that dumps data from Kafka at minutes-level frequency) and compaction job to bubble up the table record from binlog to the destination table in the Grab data lake.  Figure 4. Using real-time ingestion for data pipelines  2. Drive business decisionsA key use case of enabling real-time ingestion is driving business decisions at scale without even touching the source services. Saga pattern is commonly adopted in the microservice world. Each service has its own database, splitting an overarching database transaction into a series of multiple database transactions. Communication is established among services via message queue i.e. Kafka.In an earlier tech blog published by the Grab Search team, we talked about how real-time ingestion with Debezium optimised and boosted search capabilities. Each MySQL table is mapped to a Kafka topic and one or multiple topics build up a search index within Elasticsearch.With this new approach, there is no data loss, i.e. changes via MySQL command line tool or other DB management tools can be captured. Schema evolution is also naturally supported; the new schema defined within a MySQL table is inherited and stored in Kafka. No producer code change is required to make the schema consistent with that in MySQL. Moreover, the database read has been reduced by 90 percent including the efforts of the Data Synchronisation Platform.  Figure 5. Grab Search team use case  The GrabFood team exemplifies mostly similar advantages in the DynamoDB area. The only differences compared to MySQL are that the frequency of the lambda functions is adjustable and parallelism is auto-scaled based on the traffic. By auto-scaling, we mean that more lambda functions will be auto-deployed to cater to a sudden spike in traffic, or destroyed as the traffic falls.  Figure 6. Grab Food team use case  3. Database replicationAnother use case we did not originally have in mind is incremental data replication for disaster recovery. Within Grab, we enable DynamoDB streams for tier 0 and critical DynamoDB tables. Any insert, delete, modify operations would be propagated to the disaster recovery table in another availability zone.When migrating or replicating databases, we use the strangler fig pattern, which offers an incremental, reliable process for migrating databases. This is a method whereby a new system slowly grows on top of an old system and is gradually adopted until the old system is “strangled” and can simply be removed. Figure 7 depicts how DynamoDB streams drive real-time synchronisation between tables in different regions.  Figure 7. Data replication among DynamoDB tables across different regions in DBOps team  4. Deliver audit trailsReasons for maintaining data audit trails are manifold in Grab: regulatory requirements might mandate businesses to keep complete historical information of a consumer or to apply machine learning techniques to detect fraudulent transactions made by consumers. Figure 8 demonstrates how we deliver audit trails in Grab.  Figure 8. Deliver audit trails in Grab  SummaryReal time ingestion is playing a pivotal role in Grab’s ecosystem. It:  boosts data pipelines with less read pressure imposed on databases shared among various services;  empowers real-time business decisions with assured resource efficiency;  provides data replication among tables residing in various regions; and  delivers audit trails that either keep complete history or help unearth fraudulent operations.Since this project launched, we have made crucial enhancements to facilitate daily operations with several in-house products that are used for data onboarding, quality checking, maintaining freshness, etc.We will continuously improve our platform to provide users with a seamless experience in data ingestion, starting with unifying our internal tools. Apart from providing a unified platform, we will also contribute more ideas to the ingestion, extending it to Azure and GCP, supporting multi-catalogue and offering multi-tenancy.In our next blog, we will drill down to other interesting features of real-time ingestion, such as how ordering is achieved in different cases and custom partitioning in real-time ingestion. Stay tuned!Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/real-time-data-ingestion"
      }
      ,
    
      "abacus-issuing-points-for-multiple-sources": {
        "title": "Abacus - Issuing points for multiple sources",
        "author": "chandrakanth",
        "tags": "[&quot;Engineering&quot;, &quot;Event processing&quot;, &quot;Optimisation&quot;, &quot;Stream Processing&quot;]",
        "category": "",
        "content": "IntroductionEarlier in 2021 we published an article on Trident, Grab’s in-house real-time if this, then that (IFTTT) engine which manages campaigns for the Grab Loyalty Programme. The Grab Loyalty Programme encourages consumers to make Grab transactions by rewarding points when transactions are made. Grab rewards two types of points namely OVOPoints and GrabRewards Points (GRP). OVOPoints are issued for transactions made in Indonesia and GRP are for the transactions that are made in all other markets. In this article, the term GRP will be used to refer to both OVOPoints and GrabRewards Points.Rewarding GRP is one of the main components of the Grab Loyalty Programme. By rewarding GRP, our consumers are incentivised to transact within the Grab ecosystem. Consumers can then redeem their GRP for a range of exciting items on the GrabRewards catalogue or to offset the cost of their spendings.As we continue to grow our consumer base and our product offerings, a more robust platform is needed to ensure successful points transactions. In this post, we will share the challenges in rewarding GRP and how Abacus, our Point Issuance platform helps to overcome these challenges while managing various use cases.ChallengesGrowing number of productsThe number of Grab’s product offerings has grown as part of Grab’s goal in becoming a superapp. The demand for rewarding GRP increased as each product team looked for ways to retain consumer loyalty. For this, we needed a platform which could support the different requirements from each product team.External partnershipsGrab’s external partnerships consist of both one- and two-way point exchanges. With selected partners, Grab users are able to convert their GRP for the partner’s loyalty programme points, and the other way around.Use casesBesides the need to cater for the growing number of products and external partnerships, Grab needed a centralised points management system which could cater to various use cases of points rewarding. Let’s take a look at the use cases.Any product, any pointsThere are many products in Grab and each product should be able to reward different GRP for different scenarios. Each product rewards GRP based on the goal they are trying to achieve.The following examples illustrate the different scenarios:GrabCar: Reward 100 GRP for when a driver cancels a booking as a form of compensation or to reward GRP for every ride a consumer makes.GrabFood: Reward consumers for each meal order.GrabPay: Reward consumers three times the number of GRP for using GrabPay instead of cash as the mode of payment.More points for loyal consumersAnother use case is to reward loyal consumers with more points. This incentivises consumers to transact within the Grab ecosystem. One example are membership tiers granted based on the number of GRP a consumer has accumulated. There are four membership tiers: Member, Silver, Gold and Platinum.  Point multiplier  There are different points multipliers for different membership tiers. For example, a Gold member would earn 2.25 GRP for every dollar spent while a Silver member earns only 1.5 GRP for the same amount spent. A consumer can view their membership tier and GRP information from the account page on the Grab app.  GrabRewards Points and membership tier information  Growing number of transactionsTeams within Grab and external partners use GRP in their business. There is a need for a platform that can process millions of transactions every day with high availability rates. Errors can easily impact the issuance of points which may affect our consumers’ trust.Our solution - AbacusTo overcome the challenges and cater for various use cases, we developed a Points Management System known as Abacus. It offers an interface for external partners with the capability to handle millions of daily transactions without significant downtime.Points rewardingThere are seven main components of Abacus as shown in the following architectural diagram. Details of each component are explained in this section.  Abacus architecture  Transaction input sourceThe points rewarding process begins when a transaction is complete. Abacus listens to streams for completed transactions on the Grab platform. Each transaction that abacus receives in the stream carries the data required to calculate the GRP to be rewarded such as country ID, product ID, and payment ID etc.Apart from computing the number of GRP to be rewarded for a transaction and then rewarding the points, Abacus also allows clients from within the Grab platform and outside of the Grab platform to make an API call to reward GRP to consumers. The client who wants to reward their consumers with GRP will call Abacus with either a specific point value (for example 100 points) or will provide the necessary details like transaction amount and the relevant multipliers for Abacus to compute the points and then reward them.Point Calculation moduleThe Point Calculation module calculates the GRP using the data and multipliers that are unique to each transaction.Point Calculation dependencies for internal servicesPoint Calculation dependencies are the multipliers needed to calculate the number of points. The Point Calculation module fetches the correct point multipliers for each transaction. The multipliers are configured by specific country teams when the product is launched. They may vary by country to allow country teams the flexibility to achieve their growth and retention targets. There are different types of multipliers.Vertical multiplier: The multiplier for each vertical. A vertical is a service or product offered by Grab. Examples of verticals are GrabCar and GrabFood. The multiplier can be different for each vertical.EPPF multiplier: The effective price per fare multiplier. EPPF is the reference conversion rate per point. For example:      EPPF = 1.0; if you are issuing X points per SGD1        EPPF = 0.1; if you are issuing X points per THB10        EPPF = 0.0001; if you are issuing X points per IDR10,000  Payment Type multiplier: The multiplier for different modes of payments.Tier multiplier: The multiplier for each tier.Point Calculation formula for internal clientsThe Point Calculation module uses a formula to calculate GRP. The formula is the product of all the multipliers and the transaction amount.GRP = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplierThe following are examples for calculating GRP:Example 1:Bob is a platinum member of Grab. He orders lunch in Singapore for SGD15 using GrabPay as the payment method. Let’s assume the following:Vertical multiplier = 2EPPF multiplier = 1Cashless multiplier = 2Tier multiplier = 3GRP = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier= 15 * 2 * 1 * 2 * 3= 180From this transaction, Bob earns 180 GRP.Example 2:Jane is a Gold member of Grab. She orders lunch in Indonesia for Rp150000 using GrabPay as the payment method. Let’s assume the following:Vertical multiplier = 2EPPF multiplier = 0.00005Cashless multiplier = 2Tier multiplier = 2GRP = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier= 150000 * 2 * 0.00005 * 2 * 2= 60From this transaction, Jane earns 60 GRP.  Example of multipliers for payment options and tiers  Point Calculation dependencies for external clientsExternal partners supply the Point Calculation dependencies which are then configured in our backend at the time of integration. These external partners can set their own multipliers instead of using the above mentioned multipliers which are specific to Grab. This document details the APIs which are used to award points for external clients.Simple Queue ServiceAbacus uses Amazon Simple Queue Service (SQS) to ensure that the points system process is robust and fault tolerant.Point Awarding SQSIf there are no errors during the Point Calculation process, the Point Calculation module will send a message containing the points to be awarded to the Point Awarding SQS.Retry SQSThe Point Calculation module may not receive the required data when there is a downtime in the Point Calculation dependencies. If this occurs,  an error is triggered and the Point Calculation module will send a message to Retry SQS. Messages sent to the Retry SQS will be re-processed by the Point Calculation module. This ensures that the points are properly calculated despite having outages on dependencies. Every message that we push to either the Point Awarding SQS or Retry SQS will have a field called Idempotency key which is used to ensure that we reward the points only once to a particular transaction.Point Awarding moduleThe successful calculation of GRP triggers a message to the Point Awarding module via the Point SQS. The Point Awarding module tries to reward GRP to the consumer’s account. Upon successful completion, an ACK is sent back to the Point SQS signalling that the message was successfully processed and triggers deletion of the message. If Point SQS does not receive an ACK, the message is redelivered after an interval. This process ensures that the points system is robust and fault tolerant.LedgerGRP is rewarded to the consumer once it is updated in the Ledger. The Ledger tracks how many GRP a consumer has accumulated, what they were earned for, and the running total number of GRP.Notification serviceOnce the Ledger is updated, the Notification service sends the consumer a message about the GRP they receive.Point Kafka streamFor all successful GRP transactions, Abacus sends a message to the Point Kafka stream. Downstream services listen to this stream to identify the consumer’s behaviour and take the appropriate actions. Services of this stream can listen to events they are interested in and execute their business logic accordingly. For example, a service can use the information from the Point Kafka stream to determine a consumer’s membership tier.Points expiryFurther addition to Abacus is the handling of points expiry. The Expiry Extension module enables activity-based points expiry. This enables GRP to not expire as long as the consumer makes one Grab transaction within the next three or six months from their last transaction.The Expiry Extension module updates the point expiry date to the database after successfully rewarding GRP to the consumer. At the end of each month, a process loads all consumers whose points will expire in that particular month and sends it to the Point Expiry SQS. The Point Expiry Consumer will then expire all the points for the consumers and this data is updated in the Ledger. This process repeats on a monthly basis.  Expiry Extension module  Points expiry date is always the last day of the third or sixth month. For example, Adam makes a transaction on 10 January. His points expiry date is 31 July which is six months from the month of his last transaction. Adam then makes a transaction on 28 February. His points expiry period is shifted by one month to 31 August.  Points expiry  ConclusionThe Abacus platform enables us to perform millions of GRP transactions on a daily basis. Being able to curate rewards for consumers increases the value proposition of our products and consumer retention. If you have any comments or questions about Abacus, feel free to leave a comment below.Special thanks to Arianto Wibowo and Vaughn Friesen.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/abacus-issuing-points-for-multiple-sources"
      }
      ,
    
      "exposing-kafka-cluster": {
        "title": "Exposing a Kafka Cluster via a VPC Endpoint Service",
        "author": "fabrice-harbulot",
        "tags": "[&quot;Engineering&quot;, &quot;Cloud&quot;, &quot;Kafka&quot;]",
        "category": "",
        "content": "In large organisations, it is a common practice to isolate the cloud resources of different verticals. Amazon Web Services (AWS) Virtual Private Cloud (VPC) is a convenient way of doing so. At Grab, while our core AWS services reside in a main VPC, a number of Grab Tech Families (TFs) have their own dedicated VPC. One such example is GrabKios. Previously known as “Kudo”, GrabKios was acquired by Grab in 2017 and has always been residing in its own AWS account and dedicated VPC.In this article, we explore how we exposed an Apache Kafka cluster across multiple Availability Zones (AZs) in Grab’s main VPC, to producers and consumers residing in the GrabKios VPC, via a VPC Endpoint Service. This design is part of Coban unified stream processing platform at Grab.There are several ways of enabling communication between applications across distinct VPCs; VPC peering is the most straightforward and affordable option. However, it potentially exposes the entire VPC networks to each other, needlessly increasing the attack surface.Security has always been one of Grab’s top concerns and with Grab’s increasing growth, there is a need to deprecate VPC peering and shift to a method of only exposing services that require remote access. The AWS VPC Endpoint Service allows us to do exactly that for TCP/IPv4 communications within a single AWS region.Setting up a VPC Endpoint Service compared to VPC peering is already relatively complex. On top of that, we need to expose an Apache Kafka cluster via such an endpoint, which comes with an extra challenge. Apache Kafka requires clients, called producers and consumers, to be able to deterministically establish a TCP connection to all brokers forming the cluster, not just any one of them.Last but not least, we need a design that optimises performance and cost by limiting data transfer across AZs.Note: All variable names, port numbers and other details used in this article are only used as examples.Architecture overviewAs shown in this diagram, the Kafka cluster resides in the service provider VPC (Grab’s main VPC) while local Kafka producers and consumers reside in the service consumer VPC (GrabKios VPC).In Grab’s main VPC, we created a Network Load Balancer (NLB) and set it up across all three AZs, enabling cross-zone load balancing. We then created a VPC Endpoint Service associated with that NLB.Next, we created a VPC Endpoint Network Interface in the GrabKios VPC, also set up across all three AZs, and attached it to the remote VPC endpoint service in Grab’s main VPC. Apart from this, we also created a Route 53 Private Hosted Zone .grab and a CNAME record kafka.grab that points to the VPC Endpoint Network Interface hostname.Lastly, we configured producers and consumers to use kafka.grab:10000 as their Kafka bootstrap server endpoint, 10000/tcp being an arbitrary port of our choosing. We will explain the significance of these in later sections.    Network Load Balancer setupOn the NLB in Grab’s main VPC, we set up the corresponding bootstrap listener on port 10000/tcp, associated with a target group containing all of the Kafka brokers forming the cluster. But this listener alone is not enough.As mentioned earlier, Apache Kafka requires producers and consumers to be able to deterministically establish a TCP connection to all brokers. That’s why we created one listener for every broker in the cluster, incrementing the TCP port number for each new listener, so each broker endpoint would have the same name but with different port numbers, e.g. kafka.grab:10001 and kafka.grab:10002.We then associated each listener with a dedicated target group containing only the targeted Kafka broker, so that remote producers and consumers could differentiate between the brokers by their TCP port number.The following listeners and associated target groups were set up on the NLB:  10000/tcp (bootstrap) -&gt; 9094/tcp @ [broker 101, broker 201, broker 301]  10001/tcp -&gt; 9094/tcp @ [broker 101]  10002/tcp -&gt; 9094/tcp @ [broker 201]  10003/tcp -&gt; 9094/tcp @ [broker 301]Security Group rulesIn the Kafka brokers’ Security Group (SG), we added an ingress SG rule allowing 9094/tcp traffic from each of the three private IP addresses of the NLB. As mentioned earlier, the NLB was set up across all three AZs, with each having its own private IP address.On the GrabKios VPC (consumer side), we created a new SG and attached it to the VPC Endpoint Network Interface. We also added ingress rules to allow all producers and consumers to connect to tcp/10000-10003.Kafka setupKafka brokers typically come with a listener on port 9092/tcp, advertising the brokers by their private IP addresses. We kept that default listener so that local producers and consumers in Grab’s main VPC could still connect directly.$ kcat -L -b 10.0.0.1:9092 3 brokers: broker 101 at 10.0.0.1:9092 (controller)   broker 201 at 10.0.0.2:9092 broker 301 at 10.0.0.3:9092... truncated output ...We also configured all brokers with an additional listener on port 9094/tcp that advertises the brokers by:  Their shared private name kafka.grab.  Their distinct TCP ports previously set up on the NLB’s dedicated listeners.$ kcat -L -b 10.0.0.1:9094 3 brokers: broker 101 at kafka.grab:10001 (controller)   broker 201 at kafka.grab:10002 broker 301 at kafka.grab:10003... truncated output ...Note that there is a difference in how the broker’s endpoints are advertised in the two outputs above. The latter enables connection to any particular broker from the GrabKios VPC via the VPC Endpoint Service.It would definitely be possible to advertise the brokers directly with the remote VPC Endpoint Interface hostname instead of kafka.grab, but relying on such a private name presents at least two advantages.First, it decouples the Kafka deployment in the service provider VPC from the infrastructure deployment in the service consumer VPC. Second, it makes the Kafka cluster easier to expose to other remote VPCs, should we need it in the future.Limiting data transfer across Availability ZonesAt this stage of the setup, our Kafka cluster is fully reachable from producers and consumers in the GrabKios VPC. Yet, the design is not optimal.When a producer or a consumer in the GrabKios VPC needs to connect to a particular broker, it uses its individual endpoint made up of the shared name kafka.grab and the broker’s dedicated TCP port.The shared name arbitrarily resolves into one of the three IP addresses of the VPC Endpoint Network Interface, one for each AZ.Hence, there is a fair chance that the obtained IP address is neither in the client’s AZ nor in that of the target Kafka broker. The probability of this happening can be as high as 2/3 when both client and broker reside in the same AZ and 1/3 when they do not.While that is of little concern for the initial bootstrap connection, it becomes a serious drawback for actual data transfer, impacting the performance and incurring unnecessary data transfer cost.For this reason, we created three additional CNAME records in the Private Hosted Zone in the GrabKios VPC, one for each AZ, with each pointing to the VPC Endpoint Network Interface zonal hostname in the corresponding AZ:  kafka-az1.grab  kafka-az2.grab  kafka-az3.grabNote that we used az1, az2, az3 instead of the typical AWS 1a, 1b, 1c suffixes, because the latter’s mapping is not consistent across AWS accounts.We also reconfigured each Kafka broker in Grab’s main VPC by setting their 9094/tcp listener to advertise brokers by their new zonal private names.$ kcat -L -b 10.0.0.1:9094 3 brokers: broker 101 at kafka-az1.grab:10001 (controller)   broker 201 at kafka-az2.grab:10002 broker 301 at kafka-az3.grab:10003... truncated output ...Our private zonal names are shared by all brokers in the same AZ while TCP ports remain distinct for each broker. However, this is not clearly shown in the output above because our cluster only counts three brokers, one in each AZ.The previous common name kafka.grab remains in the GrabKios VPC’s Private Hosted Zone and allows connections to any broker via an arbitrary, likely non-optimal route. GrabKios VPC producers and consumers still use that highly-available endpoint to initiate bootstrap connections to the cluster.    Future improvementsFor this setup, scalability is our main challenge. If we add a new broker to this Kafka cluster, we would need to:  Assign a new TCP port number to it.  Set up a new dedicated listener on that TCP port on the NLB.  Configure the newly spun up Kafka broker to advertise its service with the same TCP port number and the private zonal name corresponding to its AZ.  Add the new broker to the target group of the bootstrap listener on the NLB.  Update the network SG rules on the service consumer side to allow connections to the newly allocated TCP port.We rely on Terraform to dynamically deploy all AWS infrastructure and on Jenkins and Ansible to deploy and configure Apache Kafka. There is limited overhead but there are still a few manual actions due to a lack of integration. These include transferring newly allocated TCP ports and their corresponding EC2 instances’ IP addresses to our Ansible inventory, commit them to our codebase and trigger a Jenkins job deploying the new Kafka broker.Another concern of this setup is that it is only applicable for AWS. As we are aiming to be multi-cloud, we may need to port it to Microsoft Azure and leverage the Azure Private Link service.In both cases, running Kafka on Kubernetes with the Strimzi operator would be helpful in addressing the scalability challenge and reducing our adherence to one particular cloud provider. We will explain how this solution has helped us address these challenges in a future article.Special thanks to David Virgil Naranjo whose blog post inspired this work.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/exposing-kafka-cluster"
      }
      ,
    
      "scalable-ads-server": {
        "title": "How Grab built a scalable, high-performance ad server",
        "author": "anthony-mccallumjeremy-wangwenxin-liaoashish-agarwalxiuqi-chenyangandy-lineliot-li",
        "tags": "[&quot;Engineering&quot;, &quot;Ads&quot;, &quot;Design&quot;]",
        "category": "",
        "content": "Why ads?GrabAds is a service that provides businesses with an opportunity to market their products to Grab’s consumer base. During the pandemic, as the demand for food delivery grew, we realised that ads could be a service we offer to our small restaurant merchant-partners to expand their reach. This would allow them to not only mitigate the loss of in-person traffic but also grow by attracting more customers.Many of these small merchant-partners had no experience with digital advertising and we provided an easy-to-use, scalable option that could match their business size. On the other side of the equation, our large network of merchant-partners provided consumers with more choices. For hungry consumers stuck at home, personalised ads and promotions helped them satisfy their cravings, thus fulfilling their intent of opening the Grab app in the first place!Why build our own ad server?Building an ad server is an ambitious undertaking and one might rightfully ask why we should invest the time and effort to build a technically complex distributed system when there are several reasonable off-the-shelf solutions available.The answer is we didn’t, at least not at first. We used one of these off-the-shelf solutions to move fast and build a minimally viable product (MVP). The result of this experiment was a resounding success; we were providing clear value to our merchant-partners, our consumers and Grab’s overall business.However, to take things to the next level meant scaling the ads business up exponentially. Apart from being one of the few companies with the user engagement to support an ads business at scale, we also have an ecosystem that combines our network of merchant-partners, an understanding of our consumers’ interactions across multiple services in the Grab superapp, and a payments solution, GrabPay, to close the loop. Furthermore, given the hyperlocal nature of our business, the in-app user experience is highly customised by location. In order to integrate seamlessly with this ecosystem, scale as Grab’s overall business grows and handle personalisation using machine learning (ML), we needed an in-house solution.What we builtWe designed and built a set of microservices, streams and pipelines which orchestrated the core ad serving functionality, as shown below.      Targeting - This is the first step in the ad serving flow. We fetch a set of candidate ads specifically targeted to the request based on keywords the user searched for, the user’s location, the time of day, and the data we have about the user’s preferences or other characteristics. We chose ElasticSearch as the data store for our ads repository as it allows us to query based on a disparate set of targeting criteria.  Capping - In this step, we filter out candidate ads which have exceeded various caps. This includes cases where an advertising campaign has already reached its budget goal, as well as custom requirements about the frequency an ad is allowed to be shown to the same user. In order to make this decision, we need to know how much budget has already been spent and how many times an ad has already been shown. We chose ScyllaDB to store these “stats”, which is scalable, low-cost and can handle the large read and write requirements of this process (more on how this data gets written to ScyllaDB in the Tracking step).  Pacing - In this step, we alter the probability that a matching ad candidate can be served, based on a specific campaign goal. For example, in some cases, it is desirable for an ad to be shown evenly throughout the day instead of exhausting the entire ad budget as soon as possible. Similar to Capping, we require access to information on how many times an ad has already been served and use the same ScyllaDB stats store for this.  Scoring - In this step, we score each ad. There are a number of factors that can be used to calculate this score including predicted clickthrough rate (pCTR), predicted conversion rate (pCVR) and other heuristics that represent how relevant an ad is for a given user.  Ranking - This is where we compare the scored candidate ads with each other and make the final decision on which candidate ads should be served. This can be done in several ways such as running a lottery or performing an auction. Having our own ad server allows us to customise the ranking algorithm in countless ways, including incorporating ML predictions for user behaviour. The team has a ton of exciting ideas on how to optimise this step and now that we have our own stack, we’re ready to execute on those ideas.  Pricing - After choosing the winning ads, the final step before actually returning those ads in the API response is to determine what price we will charge the advertiser. In an auction, this is called the clearing price and can be thought of as the minimum bid price required to outbid all the other candidate ads. Depending on how the ad campaign is set up, the advertiser will pay this price if the ad is seen (i.e. an impression occurs), if the ad is clicked, or if the ad results in a purchase.  Tracking - Here, we close the feedback loop and track what users do when they are shown an ad. This can include viewing an ad and ignoring it, watching a video ad, clicking on an ad, and more. The best outcome is for the ad to trigger a purchase on the Grab app. For example, placing a GrabFood order with a merchant-partner; providing that merchant-partner with a new consumer. We track these events using a series of API calls, Kafka streams and data pipelines. The data ultimately ends up in our ScyllaDB stats store and can then be used by the Capping and Pacing steps above.PrinciplesIn addition to all the usual distributed systems best practices, there are a few key principles that we focused on when building our system.  Latency - Latency is important for ads. If the user scrolls faster than an ad can load, the ad won’t be seen. The longer an ad remains on the screen, the more likely the user will notice it, have their interest piqued and click on it. As such, we set strict limits on the latency of the ad serving flow. We spent a large amount of effort tuning ElasticSearch so that it could return targeted ads in the shortest amount of time possible. We parallelised parts of the serving flow wherever possible and we made sure to A/B test all changes both for business impact and to ensure they did not increase our API latency.  Graceful fallbacks - We need user-specific information to make personalised decisions about which ads to show to a given user. This data could come in the form of segmentation of our users, attributes of a single user or scores derived from ML models. All of these require the ad server to make dependency calls that could add latency to the serving flow. We followed the principle of setting strict timeouts and having graceful fallbacks when we can’t fetch the data needed to return the most optimal result. This could be due to network failures or dependencies operating slower than usual. It’s often better to return a non-personalised result than no result at all.  Global optimisation - Predicting supply (the amount of users viewing the app) and demand (the amount of advertisers wanting to show ads to those users) is difficult. As a superapp, we support multiple types of ads on various screens. For example, we have image ads, video ads, search ads, and rewarded ads. These ads could be shown on the home screen, when booking a ride, or when searching for food delivery. We intentionally decided to have a single ad server supporting all of these scenarios. This allows us to optimise across all users and app locations. This also ensures that engineering improvements we make in one place translate everywhere where ads or promoted content are shown.What’s next?Grab’s ads business is just getting started. As the number of users and use cases grow, ads will become a more important part of the mix. We can help our merchant-partners grow their own businesses while giving our users more options and a better experience.Some of the big challenges ahead are:  Optimising our real-time ad decisions, including exciting work on using ML for more personalised results. There are many factors that can be considered in ad personalisation such as past purchase history, the user’s location and in-app browsing behaviour. Another area of optimisation is improving our auction strategy to ensure we have the most efficient ad marketplace possible.  Expanding the types of ads we support, including experimenting with new types of content, finding the best way to add value as Grab expands its breadth of services.  Scaling our services so that we can match Grab’s velocity and handle growth while maintaining low latency and high reliability.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/scalable-ads-server"
      }
      ,
    
      "biometrics-authentication": {
        "title": "Biometric authentication - Why do we need it?",
        "author": "chad-burgessrachel-fong",
        "tags": "[&quot;Engineering&quot;, &quot;Security&quot;]",
        "category": "",
        "content": "In recent years, Identity and Access Management has gained importance within technology industries as attackers continue to target large corporations in order to gain access to private data and services. To address this issue, the Grab Identity team has been using a 6-digit PIN to authenticate a user during a sensitive transaction such as accessing a GrabPay Wallet. We also use SMS one-time passwords (OTPs) to log a user into the application.We look at existing mechanisms that Grab uses to authenticate its users and how biometric authentication helps strengthen application security and save costs. We also look at the various technical decisions taken to ensure the robustness of this feature as well as some key learnings.IntroductionThe mechanisms we use to authenticate our users have evolved as the Grab Identity team consistently refines our approach. Over the years, we have observed several things:  OTP and Personal Identification Number (PIN) are susceptible to hacking and social engineering.  These methods have high user friction (e.g. delay or failure to receive SMS, need to launch Facebook/Google).  Shared/rented driver accounts cause safety concerns for passengers and increases potential for fraud.  High OTP costs at $0.03/SMS.Social engineering efforts have gotten more advanced - attackers could pretend to be your friends and ask for your OTP or even post phishing advertisements that prompt for your personal information.                            With more sophisticated social engineering attacks on the rise, we need solutions that can continue to protect our users and Grab in the long run.BackgroundWhen we looked into developing solutions for these problems, which was mainly about cost and security, we went back to basics and looked at what a secure system meant.  Knowledge Factor: Something that you know (password, PIN, some other data)  Possession Factor: Something physical that you have (device, keycards)  Inherent Factor: Something that you are (face ID, fingerprint, voice)We then compared the various authentication mechanisms that the Grab app currently uses, as shown in the following table:   Authentication factor   1. Something that you know    2. Something physical that you have    3. Something that you are   OTP  ✔️  ✔️    Social  ✔️      PIN  ✔️      Biometrics    ✔️  ✔️With methods based on the knowledge and possession factors, it is still possible for attackers to get users to reveal sensitive account information. On the other hand, biometrics are something you are born with and that makes it more complex to mimic. Hence, we have added biometrics as an additional layer to enhance Grab’s existing authentication methods and build a more secure platform for our users.SolutionBiometric authentication powered by device biometrics provides a robust platform to enhance trust. This is because modern phones provide a few key features that allow client server trust to be established:  Biometric sensor (fingerprint or face ID).  Advent of devices with secure enclaves.A secure enclave, being a part of the device, is separate from the main operating system (OS) at the kernel level. The enclave is used to store private keys that can be unlocked only by the biometrics on the device.Any changes to device security such as changing a PIN or adding another fingerprint will invalidate all prior access to this secure enclave. This means that when we enroll a user in biometrics this way, we can be sure that any payload from said device that matches the public part of said private key is authorised by the user that created it.        Architecture detailsThe important part of the approach lies in the enrollment flow. The process is quite simple and can be described in the following steps:  Create an elevated public/private key pair that requires users authentication.  Ask users to authenticate in order to prove they are the device holders.  Sign payload with confirmed unlocked private key and send public key to finish enrolling.  Store returned reference id in the encrypted shared preferences/keychain.    ImplementationThe key implementation details is as follows:  Grab’s HellfireSDK confirms if the device is not rooted.  Uses SHA512withECDSA for hashing algorithm.  Encrypted shared preferences/keychain to store data.  Secure enclave to store private keys.These key technologies allow us to create trust between devices and services. The raw biometric data stays within the device and instead sends an encrypted signature of biometry data to Grab for verification purposes.ImpactBiometric login aims to resolve the many problems highlighted earlier in this article such as reducing user friction and saving SMS OTP costs.We are still experimenting with this feature so we do not have insights on business impact yet. However, from early experiment runs, we estimate over 90% adoption rate and a success rate of nearly 90% for biometric logins.Learnings/ConclusionAs methods of executing identity theft or social engineering get more creative, simply using passwords and PINs is not enough. Grab, and many other organisations, are realising that it’s important to augment existing security measures with methods that are inherent and unique to users.By using biometrics as an added layer of security in a multi-factor authentication strategy, we can keep our users safe and decrease the probability of successful attacks. Not only do we ensure that the user is a legitimate entity, we also ensure that we protect their privacy by ensuring that the biometric data remains on the user’s device.What’s next?  IdentitySDK - this feature will be moved into an SDK so other teams integrate it via plug and play.  Standalone biometrics - biometric authentication is currently tightly coupled with PIN i.e. biometric authentication happens in place of PIN if biometric authentication is set up. Therefore, users would never see both PIN and biometric in the same session, which limits our robustness in terms of multi-factor authentication.  Integration with DAX and beyond - We plan to enable this feature for all teams who need to use biometric authentication.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/biometrics-authentication"
      }
      ,
    
      "using-real-world-patterns-to-improve-matching": {
        "title": "Using real-world patterns to improve matching in theory and practice",
        "author": "tenindra-avictor-liang",
        "tags": "[&quot;Data Science&quot;, &quot;Research&quot;]",
        "category": "",
        "content": "A research publication authored by Tenindra Abeywickrama (Grab), Victor Liang (Grab) and Kian-Lee Tan (NUS) based on their work, which was awarded the Best Scalable Data Science Paper Award for 2021.Matching the right passengers to the right driver-partners is a critically important task in ride-hailing services. Doing this suboptimally can lead to passengers taking longer to reach their destinations and drivers losing revenue. Perhaps, the most challenging of all is that this is a continuous process with a constant stream of new ride requests and new driver-partners becoming available. This makes computing matchings a very computationally expensive task requiring high throughput.We discovered that one component of the typically used algorithm to find matchings has a significant impact on efficiency that has hitherto gone unnoticed. However, we also discovered a useful property of real-world optimal matchings that allows us to improve the algorithm, in an interesting scenario of practice informing theory.A real-world exampleLet us consider a simple matching algorithm as depicted in Figure 1, where passengers and driver-partners are matched by travel time. In the figure, we have three driver-partners (D1, D2, and D3) and three passengers (P1, P2, and P3).Finding the travel time involves computing the fastest route from each driver-partner to each passenger, for example the dotted routes from D1 to P1, P2 and P3 respectively. Finding the assignment of driver-partners to passengers that minimise the overall travel time involves representing the problem in a more abstract way as a bipartite graph shown below.In the bipartite graph, the set of passengers and the set of driver-partners form the two bipartite sets, respectively. The edges connecting them represent the travel time of the fastest routes, and their costs are shown in the cost matrix on the right.   Figure 1. Example driver-to-passenger matching scenario  Finding the optimal assignment is known as solving the minimum weight bipartite matching problem (also known as the assignment problem). This problem is often solved using a technique called the Kuhn-Munkres (KM) algorithm1 (also known as the Hungarian Method).If we were to run the algorithm on the scenario shown in Figure 1, we would find the optimal matching highlighted in red on the cost matrix shown in the figure. However, there is an important step that we have not paid great attention to so far, and that is the computation of the cost matrix. As it turns out, this step has quite a significant impact on performance in real-world settings.Impact of the cost matrixPast work that solves the assignment problem assumes the cost matrix is given as input, but we observe that the time taken to compute the cost matrix is not always trivial. This is especially true in our real-world scenario. Firstly, matching driver-partners and passengers is a continuous process, as we mentioned earlier. Costs are not fixed; they change over time as driver-partners move and new passenger requests are received.This means the matrix must be recomputed each time we attempt a matching (for example every X seconds). Not only is finding the shortest path between a single passenger and driver-partner computationally expensive, we must do this for all pairs of passengers and driver-partners. In fact, in the real world, the time taken to compute the matrix is longer than the time taken to compute the optimal assignment! A simple consideration of time complexity suggests that this is true.If m is the number of driver-partners/passengers we are trying to match, the KM algorithm typically runs in O(m^3). If n is the number of nodes in the road network, then computing the cost matrix runs in O(m x n log n) using Dijkstra’s algorithm2.We know that n is around 400,000 for Singapore’s road network (and much larger for bigger cities), thus we can reasonably expect O(m x n log n) to dominate O(m^3) for m &lt; 1500, which is the kind of value for m we expect in the real-world. We ran experiments on Singapore’s road network to verify this, as shown in Figure 2.                          Figure 2. Proportion of time to compute the matrix vs. assignment for varying m on the Singapore road network  In Figure 2a, we can see that m must be greater than 2500, before the assignment time overtakes the matrix computation time. Even if we use a modern and advanced technique like Contraction Hierarchies3 to compute the fastest path, the observation holds, as shown in Figure 2b. This shows we can significantly improve overall matching performance if we can reduce the matrix computation time.A redeeming intuition: Spatial locality of matchingWhile studying real-world locations of passengers and driver-partners, we observed an interesting property, which we dubbed “spatial locality of matching”. We find that the passenger assigned to each driver-partner in an optimal matching is one of the nearest passengers to the driver-partner (it might not be the nearest). This makes intuitive sense as passengers and driver-partners will be distributed throughout a city and it’s unlikely that the best match for a particular driver-partner is on the other side of the city.In Figure 3, we see an example scenario exhibiting spatial locality of matching. While this is an idealised case to demonstrate the principle, it is not a significant departure from the real-world. From the cost matrix shown, it is very easy to see which assignment will give the lowest total travel time.   Figure 3. Example driver-partner to passenger matching scenario exhibiting spatial locality of matching  Now, it begs the question, do we even need to compute the other costs to find the optimal matching? For example, can we avoid computing the cost from D3 to P1, which are very far apart and unlikely to be matched?Incremental Kuhn-MunkresAs it turns out, there is a way to take advantage of spatial locality of matching to reduce cost computation time. We propose an Incremental KM algorithm that computes costs only when they are required, and (hopefully) avoids computing all of them. Our modified KM algorithm incorporates an inexpensive lower-bounding technique to achieve this without adding significant overhead, as we will elaborate in the next section.   Figure 4. System overview of Incremental Kuhn-Munkres implementation  Retrieving objects nearest to a query point by their fastest route is a very well studied problem (commonly referred to as k-Nearest Neighbour search)4. We employ this concept to implement a priority queue Qi for each driver ui, as displayed in Figure 4. These priority queues allow retrieving the nearest passengers by a lower-bound on the travel time. The top of a priority queue implies a lower-bound on the travel time for all passengers that have not been retrieved yet. We can then use this minimum lower-bound as a lower-bound edge cost for all bipartite edges associated with that driver-partner for which we have not computed the exact cost so far.Now, the KM algorithm can proceed as usual, using the virtual edge cost implied by the relevant priority queue, to avoid computing the exact edge cost. Of course, there may be circumstances where the virtual edge cost is insufficiently accurate for KM to compute the optimal matching. To solve this, we propose refinement rules that detect when a virtual edge cost is insufficient.If a rule is triggered, we refine the queue by retrieving the top element and computing its exact edges; this is where the “incremental” part comes from. In almost all cases, this will also increase the minimum key (lower-bound) in the priority queue.If you’re interested in finding out more, you can delve deeper into the pruning rules, inner workings of the algorithm and mathematical proofs of correctness by reading our research paper5.For now, it suffices to say that the Incremental KM algorithm produces the exact same result as the original KM algorithm. It just does so in an optimistic incremental way, hoping that we can find the result without computing all possible costs. This is perfectly suited to take advantage of spatial locality of matching. Moreover, not only do we save time by avoiding computing exact costs, we avoid computing longer fastest paths/travel times to further away passengers that are more computationally expensive than those for nearby passengers.Experimental investigationCompetitionWe conducted a thorough experimental investigation to verify the practical performance of the proposed techniques. We implemented two variants of our Incremental KM technique, differing in the implementation of the priority queue and the shortest path technique used.  IKM-DIJK: Uses Dijkstra’s algorithm to compute shortest paths. Priority queues are simply the priority queue of the Dijkstra’s search from each driver-partner. This adds no overhead over the regular KM algorithm, so any speedup comes for free.  IKM-GAC: Uses state-of-the-art lower-bound technique COLT6 to implement the priority queues and G-tree4, a fast technique to compute shortest paths. The COLT index must be built for each assignment, and this overhead is included in all running times.We compared our proposed variants against the regular KM algorithm using Dijkstra and G-tree, respectively, to compute the entire cost matrix up front. Thus, we can make an apples-to-apples comparison to see how effective our techniques are.DatasetsWe ran experiments using the real-world road network for Singapore. For the Singapore dataset, we also use a real production workload consisting of Grab bookings over a 7-day period from December 2018.Performance evaluationTo test our technique on the Singapore workload, we created an assignment problem by first choosing the window size W in seconds. Then, we batched all the bookings in a randomly selected window of that size and used the passenger and driver-partner locations from these bookings to create the bipartite sets. Next, we found an optimal matching using each technique and reported the results averaged over several randomly selected windows for several metrics.   Figure 5. Average percentage of the cost matrix computed by each technique vs. batching window size  In Figure 5, we verify that our proposed techniques are indeed computing fewer exact costs compared to their counterparts. Naturally, the original KM variants compute 100% of the matrix.   Figure 6. Average running time to find an optimal assignment by each technique vs. batching window size  In Figure 6, we can see the running times of each technique. The results in the figure confirm that the reduced computation of exact costs translates to a significant reduction of running time by over an order of magnitude. This verifies that the time saved is greater than any overhead added. Remember, the improvement of IKM-DIJK comes essentially for free! On the other hand, using IKM-GAC can achieve very low running times.   Figure 7. Maximum throughput supported by each technique vs. batching window size  In Figure 7, we report a slightly different metric. We measure m, the maximum number of passengers/driver-partners that can be batched within the time window W. This can be considered as the maximum throughput of each technique. Our technique supports significantly higher throughput.Note that the improvement is smaller than in other cases because real-world values of m rarely reach these levels, where the assignment time starts to take up a greater proportion of the overall computation time.ConclusionIn summary, computing assignment costs do indeed have a significant impact on the running time of finding optimal assignments. However, we show that by utilising the spatial locality of matching inherent in real-world assignment problems, we can avoid computing exact costs, unless absolutely necessary, by modifying the KM algorithm to work incrementally.We presented an interesting case where practice informs the theory, with our novel modifications to the classical KM algorithm. Moreover, our technique can be potentially applied beyond driver-partner and passenger matching in ride-hailing services.For example, the Route Inspection algorithm also uses shortest path edge costs to find a minimum-weight bipartite matching, and our technique could be a drop-in replacement. It would also be interesting to see if these principles can be generalised and applied to other domains where the assignment problem is used.AcknowledgementsThis research was jointly conducted between Grab and the Grab-NUS AI Lab within the Institute of Data Science at the National University of Singapore (NUS). Tenindra Abeywickrama was previously a postdoctoral fellow at the lab and now a data scientist with Grab.Special thanks to Kian-Lee Tan from NUS for co-authoring this paper.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References            H. W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly 2, 1-2 (1955), 83–97 &#8617;              Dijkstra, E.W. A note on two problems in connexion with graphs. Numer. Math. 1, 269–271 (1959) &#8617;              Robert Geisberger, Peter Sanders, Dominik Schultes, and Daniel Delling. 2008. Contraction Hierarchies: Faster and Simpler Hierarchical Routing in Road Networks. In WEA. 319–333 &#8617;              Ruicheng Zhong, Guoliang Li, Kian-Lee Tan, Lizhu Zhou, and Zhiguo Gong. 2015. G-Tree: An Efficient and Scalable Index for Spatial Search on Road Networks. IEEE Trans. Knowl. Data Eng. 27, 8 (2015), 2175–2189 &#8617; &#8617;2              Tenindra Abeywickrama, Victor Liang, and Kian-Lee Tan. 2021. Optimizing bipartite matching in real-world applications by incremental cost computation. Proc. VLDB Endow. 14, 7 (March 2021), 1150–1158 &#8617;              Tenindra Abeywickrama, Muhammad Aamir Cheema, and Sabine Storandt. 2020. Hierarchical Graph Traversal for Aggregate k Nearest Neighbors Search in Road Networks. In ICAPS. 2–10 &#8617;      ",
        "url": "/using-real-world-patterns-to-improve-matching"
      }
      ,
    
      "designing-products-and-services-based-on-jtbd": {
        "title": "Designing products and services based on Jobs to be Done",
        "author": "tim-langesoon-hau-chuasherizan-sheikh",
        "tags": "[&quot;Design&quot;, &quot;Product&quot;, &quot;Database&quot;, &quot;User Research&quot;]",
        "category": "",
        "content": "IntroductionIn 2016, Clayton Christensen, a Harvard Business School professor, wrote a book called Competing Against Luck. In his book, he talked about the kind of jobs that exist in our everyday life and how we can uncover hidden jobs through the act of non-consumption. Non-consumption is the inability for a consumer to fulfil an important Job to be Done (JTBD).JTBD is a framework; it is a different way of looking at consumer goals and is based on the notion that people buy products and services to get a job done. In this article, we will walk through what the JTBD framework is, look at an example of a popular JTBD, and look at how we use the JTBD framework in one of Grab’s services.JTBD frameworkIn his book, Clayton Christensen gives the example of the milkshake, as a JTBD example. In the mid-90s, a fast food chain was trying to understand how to improve the milkshakes they were selling and how they could sell more milkshakes. To sell more, they needed to improve the product. To understand the job of the milkshake, they interviewed their customers. They asked their customers why they were buying the milkshakes, and what progress the milkshake would help them make.Job 1: To fill their stomachsOne of the key insights was the first job, the customers wanted something that could fill their stomachs during their early morning commute to the office. Usually, these car drives would take one to two hours, so they needed something to keep them awake and to keep themselves full.In this scenario, the competition could be a banana, but think about the properties of a banana. A banana could fill your stomach but your hands get dirty and sticky after peeling it. Bananas cannot do a good job here. Another competitor could be a Snickers bar, but it is rather unhealthy, and depending on how many bites you take, you could finish it in one minute.By understanding the job the milkshake was performing, the restaurant now had a specific way of improving the product. The milkshake could be made milkier so it takes time to drink through a straw. The customer can then enjoy the milkshake throughout the journey; the milkshake is optimised for the job.   Milkshake  Job 2: To make children happyAs part of the study, they also interviewed parents who came to buy milkshakes in the afternoon, around 3:00 PM. They found out that the parents were buying the milkshakes to make their children happy.By knowing this, they were able to optimise the job by offering a smaller version of the milkshake which came in different flavours like strawberry and chocolate. From this milkshake example, we learn that multiple jobs can exist for one product. From that, we can make changes to a product to meet those different jobs.JTBD at GrabFoodA team at GrabFood wanted to prioritise which features or products to build, and performed a prioritisation exercise. However, there was a lack of fundamental understanding of why our consumers were using GrabFood or any other food delivery services. To gain deeper insights on this, we conducted a JTBD study.We applied the JTBD framework in our research investigation. We used the force diagram framework to find out what job a consumer wanted to achieve and the corresponding push and pull factors driving the consumer’s decision. A job here is defined as the progress that the consumer is trying to make in a particular context.   Force diagram  There were four key points in the force diagram:  What jobs are people using GrabFood for?  What did people use prior to GrabFood to get the jobs done?  What pushed them to seek a new solution? What is attractive about this new solution?  What are the things that will make them go back to the old product? What are the anxieties of the new product?By applying this framework, we progressively asked these questions in our interview sessions:  Can you remind us of the last time you used GrabFood? — This was to uncover the situation or the circumstances.  Why did you order this food? — This was to get down to the core of the need.  Can you tell us, before GrabFood, what did you use to get the same job done?From the interview sessions, we were able to uncover a number of JTBDs, one example was working parents buying food for their families. Before GrabFood, most of them were buying from food vendors directly, but that is a time consuming activity and it adds additional friction to an already busy day. This led them in search of a new solution and GrabFood provided that solution.Let’s look at this JTBD in more depth. One anxiety that parents had when ordering GrabFood was the sheer number of choices they had to make in order to check out their order:   Force diagram - inertia, anxiety  There was already a solution for this problem: bundles! Food bundles is a well-known concept from the food and beverage industry; items that complement each other are bundled together for a more efficient checkout experience.   Force diagram - pull, push  However, not all GrabFood merchants created bundles to solve this problem for their consumers. This was an untapped opportunity for the merchants to solve a critical problem for their consumers. Eureka! We knew that we needed to help merchants create bundles in an efficient way to solve for the consumer’s JTBD.We decided to add a functionality to the GrabMerchant app that allowed merchants to create bundles. We built an algorithm that matched complementary items and automatically suggested these bundles to merchants. The merchant only had to tap a button to create a bundle instantly.   Bundle  The feature was released and thousands of restaurants started adding bundles to their menu. Our JTBD analysis proved to be correct: food and beverage entrepreneurs were now equipped with an essential tool to drive growth and we removed an obstacle for parents to choose GrabFood to solve for their JTBD.ConclusionAt Grab, we understand the importance of research. We educate designers and other non-researcher employees to conduct research studies. We also encourage the sharing of research findings, and we ensure that research insights are consumable. By using the JTBD framework and asking questions specifically to understand the job of our consumers and partners, we are able to gain fundamental understanding of why our consumers are using our products and services. This helps us improve our products and services, and optimise it for the jobs that need to be done throughout Southeast Asia.This article was written based on an episode of the Grab Design Podcast - a conversation with Grab Lead Researcher Soon Hau Chua. Want to listen to the Grab Design Podcast? Join the team, we’re hiring!Special thanks to Amira Khazali and Irene from Tech Learning.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/designing-products-and-services-based-on-jtbd"
      }
      ,
    
      "search-indexing-optimisation": {
        "title": "Search indexing optimisation",
        "author": "weilun-wuyanguang-hu",
        "tags": "[&quot;Engineering&quot;, &quot;Data&quot;, &quot;Database&quot;, &quot;Optimisation&quot;]",
        "category": "",
        "content": "Modern applications commonly utilise various database engines, with each serving a specific need. At Grab Deliveries, MySQL database (DB) is utilised to store canonical forms of data, and Elasticsearch to provide advanced search capabilities. MySQL serves as the primary data storage for raw data, and Elasticsearch as the derived storage.   Search data flow  Efforts have been made to synchronise data between MySQL and Elasticsearch. In this post, a series of techniques will be introduced on how to optimise incremental search data indexing.BackgroundThe synchronisation of data from the primary data storage to the derived data storage is handled by Food-Puxian, a Data Synchronisation Platform (DSP). In a search service context, it is the synchronisation of data between MySQL and Elasticsearch.The data synchronisation process is triggered on every real-time data update to MySQL, which will streamline the updated data to Kafka. DSP consumes the list of Kafka streams and incrementally updates the respective search indexes in Elasticsearch. This process is also known as Incremental Sync.Kafka to DSPDSP uses Kafka streams to implement Incremental Sync. A stream represents an unbounded, continuously updating data set, which is ordered, replayable and fault-tolerant.   Data synchronisation process using Kafka  The above diagram depicts the process of data synchronisation using Kafka. The Data Producer creates a Kafka stream for every operation done on MySQL and sends it to Kafka in real-time. DSP creates a stream consumer for each Kafka stream and the consumer reads data updates from respective Kafka streams and synchronises them to Elasticsearch.MySQL to ElasticsearchIndexes in Elasticsearch correspond to tables in MySQL. MySQL data is stored in tables, while Elasticsearch data is stored in indexes. Multiple MySQL tables are joined to form an Elasticsearch index. The below snippet shows the Entity-Relationship mapping in MySQL and Elasticsearch. Entity A has a one-to-many relationship with entity B. Entity A has multiple associated tables in MySQL, table A1 and A2, and they are joined into a single Elasticsearch index A.   ER mapping in MySQL and Elasticsearch  Sometimes a search index contains both entity A and entity B. In a keyword search query on this index, e.g. “Burger”, objects from both entity A and entity B whose name contains “Burger” are returned in the search response.Original Incremental SyncOriginal Kafka streamsThe Data Producers create a Kafka stream for every MySQL table in the ER diagram above. Every time there is an insert, update, or delete operation on the MySQL tables, a copy of the data after the operation executes is sent to its Kafka stream. DSP creates different stream consumers for every Kafka stream since their data structures are different.Stream Consumer infrastructureStream Consumer consists of 3 components.  Event Dispatcher: Listens and fetches events from the Kafka stream, pushes them to the Event Buffer and starts a goroutine to run Event Handler for every event whose ID does not exist in the Event Buffer.  Event Buffer: Caches events in memory by the primary key (aID, bID, etc). An event is cached in the Buffer until it is picked by a goroutine or replaced when a new event with the same primary key is pushed into the Buffer.  Event Handler: Reads an event from the Event Buffer and the goroutine started by the Event Dispatcher handles it.   Stream consumer infrastructure  Event Buffer procedureEvent Buffer consists of many sub buffers, each with a unique ID which is the primary key of the event cached in it. The maximum size of a sub buffer is 1. This allows the Event Buffer to deduplicate events having the same ID in the buffer.The below diagram shows the procedure of pushing an event to the Event Buffer. When a new event is pushed to the buffer, the old event sharing the same ID will be replaced. The replaced event is therefore not handled.   Pushing an event to the Event Buffer  Event Handler procedureThe below flowchart shows the procedures executed by the Event Handler. It consists of the common handler flow (in white), and additional procedures for object B events (in green). After creating a new Elasticsearch document by data loaded from the database, it will get the original document from Elasticsearch to compare if any field is changed and decide whether it is necessary to send the new document to Elasticsearch.When object B event is being handled, on top of the common handler flow, it also cascades the update to the related object A in the Elasticsearch index. We name this kind of operation Cascade Update.   Procedures executed by the Event Handler  Issues in the original infrastructureData in an Elasticsearch index can come from multiple MySQL tables as shown below.   Data in an Elasticsearch index  The original infrastructure came with a few issues.  Heavy DB load: Consumers read from Kafka streams, treat stream events as notifications then use IDs to load data from the DB to create a new Elasticsearch document. Data in the stream events are not well utilised. Loading data from the DB every time to create a new Elasticsearch document results in heavy traffic to the DB. The DB becomes a bottleneck.  Data loss: Producers send data copies to Kafka in application code. Data changes made via MySQL command-line tool (CLT) or other DB management tools are lost.  Tight coupling with MySQL table structure: If producers add a new column to an existing table in MySQL and this column needs to be synchronised to Elasticsearch, DSP is not able to capture the data changes of this column until the producers make the code change and add the column to the related Kafka Stream.  Redundant Elasticsearch updates: Elasticsearch data is a subset of MySQL data. Producers publish data to Kafka streams even if changes are made on fields that are not relevant to Elasticsearch. These stream events that are irrelevant to Elasticsearch would still be picked up.  Duplicate cascade updates: Consider a case where the search index contains both object A and object B. A large number of updates to object B are created within a short span of time. All the updates will be cascaded to the index containing both objects A and B. This will bring heavy traffic to the DB.Optimised Incremental SyncMySQL BinlogMySQL binary log (Binlog) is a set of log files that contain information about data modifications made to a MySQL server instance. It contains all statements that update data. There are two types of binary logging:  Statement-based logging: Events contain SQL statements that produce data changes (inserts, updates, deletes).  Row-based logging: Events describe changes to individual rows.The Grab Caspian team (Data Tech) has built a Change Data Capture (CDC) system based on MySQL row-based Binlog. It captures all the data modifications made to MySQL tables.Current Kafka streamsThe Binlog stream event definition is a common data structure with three main fields: Operation, PayloadBefore and PayloadAfter. The Operation enums are Create, Delete, and Update. Payloads are the data in JSON string format. All Binlog streams follow the same stream event definition. Leveraging PayloadBefore and PayloadAfter in the Binlog event, optimisations of incremental sync on DSP becomes possible.   Binlog stream event main fields  Stream Consumer optimisationsEvent Handler optimisationsOptimisation 1Remember that there was a redundant Elasticsearch updates issue mentioned above where the Elasticsearch data is a subset of the MySQL data. The first optimisation is to filter out irrelevant stream events by checking if the fields that are different between PayloadBefore and PayloadAfter are in the Elasticsearch data subset.Since the payloads in the Binlog event are JSON strings, a data structure only with fields that are present in Elasticsearch data is defined to parse PayloadBefore and PayloadAfter. By comparing the parsed payloads, it is easy to know whether the change is relevant to Elasticsearch.The below diagram shows the optimised Event Handler flows. As shown in the blue flow, when an event is handled, PayloadBefore and PayloadAfter are compared first. An event will be processed only if there is a difference between PayloadBefore and PayloadAfter. Since the irrelevant events are filtered, it is unnecessary to get the original document from Elasticsearch.   Event Handler optimisation 1  Achievements  No data loss. Changes made via MySQL CLT or other DB manage tools can be captured.  No dependency on MySQL table definition. All the data is in JSON string format.  No redundant Elasticsearch updates and DB reads.  Elasticsearch reads traffic reduced by 90%: Not a need to get the original document from Elasticsearch to compare with the newly created document anymore.  55% of irrelevant stream events are filtered out.  The DB load is reduced by 55%   Elasticsearch event updates for optimisation 1  Optimisation 2The PayloadAfter in the event provides updated data. This makes us think about whether a completely new Elasticsearch document is needed each time, with its data read from several MySQL tables. The second optimisation is to change to a partial update using data differences from the Binlog event.The below diagram shows the Event Handler procedure flow with a partial update. As shown in the red flow, instead of creating a new Elasticsearch document for each event, a check on whether the document exists will be performed first. If the document exists, which happens for the majority of the time, the data is changed in this event, provided the comparison between PayloadBefore and PayloadAfter is updated to the existing Elasticsearch document.   Event Handler optimisation 2  Achievements  Change most Elasticsearch relevant events to partial update: Use data in stream events to update Elasticsearch.  Elasticsearch load reduced: Only fields that have been changed will be sent to Elasticsearch.  DB load reduced: DB load reduced by 80% based on Optimisation 1.   Elasticsearch event updates for optimisation 2  Event Buffer optimisationInstead of replacing the old event, we merge the new event with the old event when the new event is pushed to the Event Buffer.The size of each sub buffer in Event Buffer is 1. In this optimisation, the stream event is not treated as a notification anymore. We use the Payloads in the event to perform Partial Updates. The old procedure of replacing old events is no longer suitable for the Binlog stream.When the Event Dispatcher pushes a new event to a non-empty sub buffer in the Event Buffer, it will merge event A in the sub buffer and the new event B into a new Binlog event C, whose PayloadBefore is from Event A and PayloadAfter is from Event B.   Merge operation for Event Buffer optimisation  Cascade Update optimisationOptimisationWe used a new stream to handle cascade update events. When the producer sends data to the Kafka stream, data sharing the same ID will be stored at the same partition. Every DSP service instance has only one stream consumer. When Kafka streams are consumed by consumers, one partition will be consumed by only one consumer. So the Cascade Update events sharing the same ID will be consumed by one stream consumer on the same EC2 instance. With this special mechanism, the in-memory Event Buffer is able to deduplicate most of the Cascade Update events sharing the same ID.The flowchart below shows the optimised Event Handler procedure. Highlighted in green is the original flow while purple highlights the current flow with Cascade Update events.When handling an object B event, instead of cascading update the related object A directly, the Event Handler will send a Cascade Update event to the new stream. The consumer of the new stream will handle the Cascade Update event and synchronise the data of object A to the Elasticsearch.   Event Handler with Cascade Update events  Achievements  Cascade Update events deduplicated by 80%.  DB load introduced by cascade update is reduced.   Cascade Update events  SummaryIn this article four different DSP optimisations are explained. After switching to MySQL Binlog streams provided by the Coban team and optimising Stream Consumer, DSP has saved about 91% DB reads and 90% Elasticsearch reads, and the average queries per second (QPS) of stream traffic processed by Stream Consumer increased from 200 to 800. The max QPS at peak hours could go up to 1000+. With a higher QPS, the duration of processing data and the latency of synchronising data from MySQL to Elasticsearch was reduced. The data synchronisation ability of DSP has greatly improved after optimisation.Special thanks to Jun Ying Lim and Amira Khazali for proofreading this article.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/search-indexing-optimisation"
      }
      ,
    
      "multi-armed-bandit-system-recommendation": {
        "title": "Automating Multi-Armed Bandit testing during feature rollout",
        "author": "weicheng-zhuzhuolun-liweilun-wuda-huang",
        "tags": "[&quot;Engineering&quot;, &quot;Testing&quot;, &quot;Optimisation&quot;]",
        "category": "",
        "content": "A/B testing is an experiment where a random e-commerce platform user is given two versions of a variable: a control group and a treatment group, to discover the optimal version that maximizes conversion. When running A/B testing, you can take the Multi-Armed Bandit optimisation approach to minimise the loss of conversion due to low performance.In the traditional software development process, Multi-Armed Bandit (MAB) testing and rolling out a new feature are usually separate processes. The novel Multi-Armed Bandit System for Recommendation solution, hereafter the Multi-Armed Bandit Optimiser, proposes automating the Multi-Armed Bandit testing simultaneously while rolling out the new feature.Advantages  Automates the MAB testing process during new feature rollouts.  Selects the optimal parameters based on predefined metrics of each use case, which results in an end-to-end solution without the need for user intervention.  Uses the Batched Multi-Armed Bandit and Monte Carlo Simulation, which enables it to process large-scale business scenarios.  Uses a feedback loop to automatically collect recommendation metrics from user event logs and to feed them to the Multi-Armed Bandit Optimiser.  Uses an adaptive rollout method to automatically roll out the best model to the maximum distribution capacity according to the feedback metrics.ArchitectureThe following diagram illustrates the system architecture.    System architecture&nbsp;The novel Multi-Armed Bandit System for Recommendation solution contains three building blocks.  Stream processing frameworkA lightweight system that performs basic operations on Kafka Streams, such as aggregation, filtering, and mapping. The proposed solution relies on this framework to pre-process raw events published by mobile apps and backend processes into the proper format that can be fed into the feedback loop.  Feedback loopA system that calculates the goal metrics and optimises the model traffic distribution. It runs a metrics server which pulls the data from Stalker, which is a time series database that stores the processed events in the last one hour. The metrics server invokes a Spark Job periodically to run the SQL queries that computes the pre-defined goal metrics: the Clickthrough Rate, Conversion Rate and so on, provided by users. The output of the job is dumped into an S3 bucket, and is picked up by optimiser runtime. It runs the Multi-Armed Bandit Optimiser to optimise the model traffic distribution based on the latest goal metrics.  Dynamic value receiver, or the GrabX variableMulti-Armed Bandit Optimiser modulesThe Multi-Armed Bandit Optimiser consists of the following modules:  Reward Update  Batched Multi-Armed Bandit Agent  Monte-Carlo Simulation  Adaptive Rollout    Multi-Armed Bandit Optimiser modules&nbsp;The goal of the Multi-Armed Bandit Optimisation is to find the optimal Arm that results in the best predefined metrics, and then allocate the maximum traffic to that Arm.The solution can be illustrated in the following problem. For K Arm, in which the action space A={1,2,…,K}, the Multi-Arm-Bandit Optimiser goal is to solve the one-shot optimisation problem of .&nbsp;Reward Update moduleThe Reward Update module collects a batch of the metrics. It calculates the Success and Failure counts, then updates the Beta distribution of each Arm with the Batched Multi-Armed Bandit algorithm.Multi-Armed Bandit Agent moduleIn the Multi-Armed Bandit Agent module, each Arm’s metrics are modelled as a Beta distribution which is sampled with Thompson Sampling. The Beta distribution formula is:  .&nbsp;The Batched Multi-Armed Bandit algorithm updates the Beta distribution with the batch metrics. The optimisation algorithm can be described in the following method.    Batched Multi-Armed Bandit algorithm&nbsp;Monte-Carlo Simulation moduleThe Monte-Carlo Simulation module runs the simulation for N repeated times to find the best Arm over a configurable simulation window. Then, it applies the simulated results as each Arm’s distribution percentage for the next round.To handle different scenarios, we designed two strategies.  Max strategy: We count each Arm’s Success count’s result in Monte-Carlo Simulation, and then compute the next round distribution according to the success rate.  Mean strategy: We average each Arm’s Beta distribution probabilities’s result in Monte-Carlo Simulation, and then compute the next round distribution according to the averaged probabilities of each Arm.Adaptive Rollout moduleThe Adaptive Rollout module rolls out the sampled distribution of each Multi-Armed Bandit Arm, in the form of Multi-Armed Bandit Arm Model ID and distribution, to the experimentation platform’s configuration variable. The resulting variable is then read from the online service. The process repeats as it collects feedback from the Adaptive Rollout metrics’ results in the feedback loop.Multi-Armed Bandit for Recommendation SolutionIn the GrabFood Recommended for You widget, there are several food recommendation models that categorise lists of merchants. The choice of the model is controlled through experiments at rollout, and the results of the experiments are analysed offline. After the analysis, data scientists and product managers rectify the model choice based on the experiment results.The Multi-Armed Bandit System for Recommendation solution improves the process by speeding up the feedback loop with the Multi-Armed Bandit system. Instead of depending on offline data which comes out at T+N, the solution responds to minute-level metrics, and adjusts the model faster.This results in an optimal solution faster. The proposed Multi-Armed Bandit for Recommendation solution workflow is illustrated in the following diagram.     Multi-Armed Bandit for Recommendation solution workflow&nbsp;Optimisation metricsThe GrabFood recommendation uses the Effective Conversion Rate metrics as the optimisation objective. The Effective Conversion Rate is defined as the total number of checkouts through the Recommended for You widget, divided by the total widget viewed and multiplied by the coverage rate.The events of views, clicks, and checkouts are collected over a 30-minute aggregation window and the coverage. A request with a checkout is considered as a success event, while a non-converted request is considered as a failure event.Multi-Armed Bandit strategyWith the Multi-Armed Bandit Optimiser, the Beta distribution is selected to model the Effective Conversion Rate. The use of the mean strategy in the Monte-Carlo Simulation results in a more stable distribution.Rollout policyThe Multi-Armed Bandit Optimiser uses the eater ID as the unique entity, applies a policy and assigns different percentages of eaters to each model, based on computed distribution at the beginning of each loop.Fallback logicThe Multi-Armed Bandit Optimiser first runs model validation to ensure all candidates are suitable for rolling out. If the scheduled MAB job fails, it falls back to a default distribution that is set to 50-50% for each model.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/multi-armed-bandit-system-recommendation"
      }
      ,
    
      "grabfood-bundle-size": {
        "title": "How We Cut GrabFood.com’s Page JavaScript Asset Sizes by 3x",
        "author": "gibson-cheng",
        "tags": "[&quot;Product&quot;, &quot;Asset Size&quot;, &quot;Cloud&quot;, &quot;Optimisation&quot;]",
        "category": "",
        "content": "IntroductionEvery week, GrabFood.com’s cloud infrastructure serves over &gt;1TB network egress and 175 million requests, which increased our costs. To minimise cloud costs, we had to look at optimising (and reducing) GrabFood.com’s bundle size.Any reduction in bundle size helps with:  Faster site loads! (especially for locations with lower mobile broadband speeds)  Cost savings for users: Less data required for each site load  Cost savings for Grab: Less network egress required to serve users  Faster build times: Fewer dependencies -&gt; less code for webpack to bundle -&gt; faster builds  Smaller builds: Fewer dependencies -&gt; less code -&gt; smaller buildsAfter applying the 7 webpack bundle optimisations, we were able to yield the following improvements:  7% faster page load time from 2600ms to 2400ms  66% faster JS static asset load time from 180ms to 60ms  3x smaller JS static assets from 750KB to 250KB  1.5x less network egress from 1800GB to 1200GB  20% less for CloudFront costs from $1750 to $1400  1.4x smaller bundle from 40MB to 27MB  3.6x faster build time from ~2000s to ~550sSolutionOne of the biggest factors influencing bundle size is dependencies. As mentioned earlier, fewer dependencies mean fewer lines of code to compile, which result in a smaller bundle size. Thus, to optimise GrabFood.com’s bundle size, we had to look into our dependencies.Tldr;Jump to Step C: Reducing your Dependencies to see the 7 strategies we used to cut down our bundle size.Step A: Identify Your DependenciesIn this step, we need to ask ourselves ‘what are our largest dependencies?’. We used the webpack-bundle-analyzer to inspect GrabFood.com’s bundles. This gave us an overview of all our dependencies and we could easily see which bundle assets were the largest.   Our grabfood.com bundle analyzer output    For Next.js, you should use @next/bundle-analyze instead.  Bundle analysis output allows us to easily inspect what’s in our bundle.What to look out for:I: Large dependencies (fairly obvious, because the box size will be large)II: Duplicate dependencies (same library that is bundled multiple times across different assets)III: Dependencies that look like they don’t belong (e.g. Why is ‘elliptic’ in my frontend bundle?)What to avoid:  Isolating dependencies that are very small (e.g. &lt;20kb). Not worth focusing on this due to very meagre returns.          E.g. Business logic like your React code      E.g. Small node dependencies      Step B: Investigate the Usage of Your Dependencies (Where are my Dependencies Used?)In this step, we are trying to answer this question: “Given a dependency, which files and features are making use of it?”.   Image source  There are two broad approaches that can be used to identify how our dependencies are used:I: Top-down approach: “Where does our project use dependency X?”  Conceptually identify which feature(s) requires the use of dependency X.  E.g. Given that we have ‘jwt-simple’ as a dependency, which set of features in my project requires JWT encoding/decoding?II: Bottom-up approach: “How did dependency X get used in my project?”  Trace dependencies by manually tracing import() and require() statements  Alternatively, use dependency visualisation tools such as dependency-cruiser to identify file interdependencies. Note that output can quickly get noisy for any non-trivial project, so use it for inspecting small groups of files (e.g. single domains).Our recommendation is to use a mix of both Top-down and Bottom-up approaches to identify and isolate dependencies.Dos:  Be methodical when tracing dependencies: Use a document to track your progress as you manually trace inter-file dependencies.  Use dependency visualisation tools like dependency-cruiser to quickly view a given file’s dependencies.  Consult Dr. Google if you get stuck somewhere, especially if the dependencies are buried deep in a dependency tree i.e. non-1st-degree dependencies (e.g. “Why webpack includes elliptic bn.js modules in bundle”)Don’ts:  Stick to a single approach - Know when to switch between Top-down and Bottom-up approaches to narrow down the search space.Step C: Reducing Your DependenciesNow that you know what your largest dependencies are and where they are used, the next step is figuring out how you can shrink your dependencies.   Image source  Here are 7 strategies that you can use to reduce your dependencies:  Lazy load large dependencies and less-used dependencies  Unify instances of duplicate modules  Use libraries that are exported in ES Modules format  Replace libraries whose features are already available on the Browser Web API  Avoid large dependencies by changing your technical approach  Avoid using node dependencies or libraries that require node dependencies  Optimise your external dependenciesNote: These strategies have been listed in ascending order of difficulty - focus on the easy wins first 🙂1. Lazy Load Large Dependencies and Less-used Dependencies   “When a file adds +2MB worth of dependencies”, Image source  Similar to how lazy loading is used to break down large React pages to improve page performance, we can also lazy load libraries that are rarely used, or are not immediately used until prior to certain user actions.Before:const crypto = require(‘crypto’)const computeHash = (value, secret) =&gt; { return crypto.createHmac(value, secret)}After:const computeHash = async (value, secret) =&gt; { const crypto = await import(‘crypto’) return crypto.createHmac(value, secret)}Example:  Scenario: Use of Anti-abuse library prior to sensitive API calls  Action: Instead of bundling the anti-abuse library together with the main page asset, we opted to lazy load the library only when we needed to use it (i.e. load the library just before making certain sensitive API calls).  Results: Saved 400KB on the main page asset.Notes:  Any form of lazy loading will incur some latency on the user, since the asset must be loaded with XMLHttpRequest.2. Unify Instances of Duplicate Modules   Image source  If you see the same dependency appearing in multiple assets, consider unifying these duplicate dependencies under a single entrypoint.Before:// ComponentOne.jsximport GrabMaps from ‘grab-maps’// ComponentTwo.jsximport GrabMaps, { Marker } from ‘grab-maps’After:// grabMapsImportFn.jsconst grabMapsImportFn = () =&gt; import(‘grab-maps’)// ComponentOne.tsxconst grabMaps = await grabMapsImportFn()const GrabMaps = grabMaps.default// ComponentTwo.tsxconst grabMaps = await grabMapsImportFn()const GrabMaps = grabMaps.defaultconst Marker = grabMaps.MarkerExample:  Scenario: Duplicate ‘grab-maps’ dependencies in bundle  Action: We observed that we were bundling the same ‘grab-maps’ dependency in 4 different assets so we refactored the application to use a single entrypoint, ensuring that we only bundled one instance of ‘grab-maps’.  Results: Saved 2MB on total bundle size.Notes:  Alternative approach: Manually define a new cacheGroup to target a specific module (see more) with ‘enforce:true’, in order to force webpack to always create a separate chunk for the module. Useful for cases where the single dependency is very large (i.e. &gt;100KB), or when asynchronously loading a module isn’t an option.  Certain libraries that appear in multiple assets (e.g. antd) should not be mistaken as identical dependencies. You can verify this by inspecting each module with one another. If the contents are different, then webpack has already done its job of tree-shaking the dependency and only importing code used by our code.  Webpack relies on the import() statement to identify that a given module is to be explicitly bundled as a separate chunk (see more).3. Use Libraries that are Exported in ES Modules Format“Did you say ‘tree-shaking’?”, Image source    If a given library has a variant with an ES Module distribution, use that variant instead.  ES Modules allows webpack to perform  tree-shaking automatically, allowing you to save on your bundle size because unused library code is not bundled.  Use  bundlephobia to quickly ascertain if a given library is tree-shakeable (e.g. ‘lodash-es’ vs  ‘lodash’)Before:import { get } from ‘lodash’After:import { get } from ‘lodash-es’Example:  Use Case: Using Lodash utilities  Action: Instead of using the standard ‘lodash’ library, you can swap it out with ‘lodash-es’, which is bundled using ES Modules and is functionally equivalent.  Results: Saved 0KB - We were already directly importing individual Lodash functions (e.g. ‘lodash/get’), therefore importing only the code we need. Still, ES Modules is a more convenient way to go about this 👍.Notes:  Alternative approach: Use babel plugins (e.g. ‘babel-plugin-transform-imports’) to transform your import statements at build time to selectively import specific code for a given library.4. Replace Libraries whose Features are Already Available on the Browser Web API“When you replace axios with fetch”, Image source  If you are relying on libraries for functionality that is available on the Web API, you should revise your implementation to leverage on the Web API, allowing you to skip certain libraries when bundling, thus saving on bundle size.Before:import axios from ‘axios’const getEndpointData = async () =&gt; { const response = await axios.get(‘/some-endpoint’) return response}After:const getEndpointData = async () =&gt; { const response = await fetch(‘/some-endpoint’) return response}Example:  Use Case: Replacing axios with fetch() in the anti-abuse library  Action: We observed that our anti-abuse library was relying on axios to make web requests. Since our web app is only targeting modern browsers - most of which support fetch() (with the notable exception of IE) - we refactored the library’s code to use fetch() exclusively.  Results: Saved 15KB on anti-abuse library size.5. Avoid Large Dependencies by Changing your Technical ApproachImage source  If it is acceptable to change your technical approach, we can avoid using certain dependencies altogether.Before:import jwt from ‘jwt-simple’const encodeCookieData = (data) =&gt; { const result = jwt.encode(data, ‘some-secret’) return result}After:const encodeCookieData = (data) =&gt; { const result = JSON.stringify(data) return result}Example:  Scenario: Encoding for browser cookie persistence  Action: As we needed to store certain user preferences in the user’s browser, we previously opted to use JWT encoding; this involved signing JWTs on the client side, which has a hard dependency on ‘crypto’. We revised the implementation to use plain JSON encoding instead, removing the need for ‘crypto’.  Results: Saved 250KB per page asset, 13MB in total bundle size.6. Avoid Using Node Dependencies or Libraries that Require Node Dependencies“When someone does require(‘crypto’)”, Image source  You should not need to use node-related dependencies, unless your application relies on a node dependency directly or indirectly.Examples of node dependencies: ‘Buffer’, ‘crypto’, ‘https’ (see more)Before:import jwt from ‘jsonwebtoken’const decodeJwt = async (value) =&gt; { const result = await new Promise((resolve) =&gt; { jwt.verify(token, 'some-secret', (err, decoded) =&gt; resolve(decoded)) }) return result}After:import jwt_decode from ‘jwt-decode’const decodeJwt = (value) =&gt; { const result = jwt_decode(value) return result}Example:  Scenario: Decoding JWTs on the client side  Action: In terms of JWT usage on the client side, we only need to decode JWTs - we do not need any logic related to encoding JWTs. Therefore, we can opt to use libraries that perform just decoding (e.g. ‘jwt-decode’) instead of libraries (e.g. ‘jsonwebtoken’) that performs the full suite of JWT-related operations (e.g. signing, verifying).  Results: Same as in Point 5: Example. (i.e. no need to decode JWTs anymore, since we aren’t using JWT encoding for browser cookie persistence)7. Optimise your External Dependencies“Team: Can you reduce the bundle size further? You: (nervous grin)“, Image source  We can do a deep-dive into our dependencies to identify possible size optimisations by applying all the aforementioned techniques. If your size optimisation changes get accepted, regardless of whether it’s publicly (e.g. GitHub) or privately hosted (own company library), it’s a win-win for everybody! 🥳Example:  Scenario: Creating custom ‘node-forge’ builds for our Anti-abuse library  Action: Our Anti-abuse library only uses certain features of ‘node-forge’. Thankfully, the ‘node-forge’ maintainers have provided an easy way to make custom builds that only bundle selective features (see more).  Results: Saved 85KB in Anti-abuse library size and reduced bundle size for all other dependent projects.Step D: Verify that You have Modified the Dependencies“Now… where did I put that needle?”, Image source  So, you’ve found some opportunities for major bundle size savings, that’s great!But as always, it’s best to be methodical to measure the impact of your changes, and to make sure no features have been broken.  Perform your code changes  Build the project again and open the bundle analysis report  Verify the state of a given dependency          Deleted dependency - you should not be able to find the dependency      Lazy-loaded dependency - you should see the dependency bundled as a separate chunk      Non-duplicated dependency - you should only see a single chunk for the non-duplicated dependency        Run tests to make sure you didn’t break anything (i.e. unit tests, manual tests)Other ConsiderationsPreventive Measures  Periodically monitor your bundle size to identify increases in bundle size  Periodically monitor your site load times to identify increases in site load timesWebpack Configuration Options  Disable bundling node modules with ‘node: false’          Only if your project doesn’t already include libraries that rely on node modules.      Allows for fast detection when someone tries to use a library that requires node modules, as the build will fail        Experiment with ‘cacheGroups’          Most default configurations of webpack do a pretty good job of identifying and bundling the most commonly used dependencies into a single chunk (usually called vendor.js)      You can experiment with webpack  optimisation options to see if you get better results        Experiment with import() ‘Magic Comments’          You may experiment with import() magic comments to modify the behaviour of specific import() statements, although the default setting will do just fine for most cases.      If you can’t remove the dependency:  For all dependencies that must be used, it’s probably best to lazy load all of them so you won’t block the page’s initial rendering (see more).ConclusionImage source  To summarise, here’s how you can go about this business of reducing your bundle size.Namely…  Identify Your Dependencies  Investigate the Usage of Your Dependencies  Reduce Your Dependencies  Verify that You have Modified the DependenciesAnd by using these 7 strategies…  Lazy load large dependencies and less-used dependencies  Unify instance of duplicate modules  Use libraries that are exported in ES Modules format  Replace libraries whose features are already available on the Browser Web API  Avoid large dependencies by changing your technical approach  Avoid using node dependencies  Optimise your external dependenciesYou can have…  Faster page load time (smaller individual pages)  Smaller bundle (fewer dependencies)  Lower network egress costs (smaller assets)  Faster builds (fewer dependencies to handle)Now armed with this information, may your eyes be keen, your bundles be lean, your sites be fast, and your cloud costs be low! 🚀 ✌️Special thanks to Han Wu, Melvin Lee, Yanye Li, and Shujuan Cheong for proofreading this article. 🙂Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/grabfood-bundle-size"
      }
      ,
    
      "protecting-personal-data-in-grabs-imagery": {
        "title": "Protecting Personal Data in Grab's Imagery",
        "author": "adrian-popovicizsolt-vadasziadrian-marginhannes-kruppa",
        "tags": "[&quot;Engineering&quot;, &quot;Machine Learning&quot;, &quot;Data&quot;, &quot;Datasets&quot;, &quot;Data Science&quot;]",
        "category": "",
        "content": "Image Collection Using KartaViewA few years ago, we realised a strong demand to better understand the streets where our driver-partners and consumers go, with the purpose to better fulfil their needs and also, to quickly adapt ourselves to the rapidly changing environment in the Southeast Asian cities.One way to fulfil that demand was to create an image collection platform named KartaView which is Grab Geo’s platform for geotagged imagery. It empowers collection, indexing, storage, retrieval of imagery, and map data extraction.KartaView is a public, partially open-sourced product, used both internally and externally by the OpenStreetMap community and other users. As of 2021, KartaView has public imagery in over 100 countries with various coverage degrees, and 60+ cities of Southeast Asia. Check it out at here.   Figure 1 - KartaView platform  Why Image Blurring is ImportantIncidentally, many people and licence plates are in the collected images, whose privacy is a serious concern. We deeply respect all of them and consequently, we are using image obfuscation as the most effective anonymisation method for ensuring privacy protection.Because manually annotating the regions in the picture where faces and licence plates are located is impractical, this problem should be solved using machine learning and engineering techniques. Hence we detect and blur all faces and licence plates which could be considered as personal data.   Figure 2 - Sample blurred picture  In our case, we have a wide range of picture types: regular planar, very wide and 360 pictures in equirectangular format collected with 360 cameras. Also, because we are collecting imagery globally, the vehicle types, licence plates, and human environments are quite diverse in appearance, and are not handled well by off-the-shelf blurring software. So we built our own custom blurring solution which yielded higher accuracy and better cost efficiency overall with respect to blurring of personal data.   Figure 3 - Example of equirectangular image where personal data has to be blurred  Behind the scenes, in KartaView, there are a set of cool services which can derive useful information from the pictures like image quality, traffic signs, roads, etc. A big part of them are using deep learning algorithms which potentially can be negatively affected by running them over blurred pictures. In fact, based on the assessment we have done so far, the impact is extremely low, similar to the one reported in a well known study of face obfuscation in ImageNet 1.Outline of Grab’s Blurring ProcessAt a high level, this is how Grab goes about the blurring process:  Transform each picture into a set of planar images. In this way, we further process all pictures, whatever the format they had, in the same way.  Use an object detector able to detect all faces and licence plates in a planar image having a standard field of view.  Transform the coordinates of the detected regions into original coordinates and blur those regions.   Figure 4 - Picture’s processing steps2In the following section, we are going to describe in detail the interesting aspects of the second step, sharing the challenges and how we were solving them. Let’s start with the first and most important part, the dataset.DatasetOur current dataset consists of images from a wide range of cameras, including normal perspective cameras from mobile phones, wide field of view cameras and also 360 degree cameras.It is the result of a series of data collections contributed by Grab’s data tagging teams, which may contain 2 classes of dataset that are of interest for us: FACE and LICENSE_PLATE.The data was collected using Grab internal tools, stored in queryable databases, making it a system that gives the possibility to revisit and correct the data if necessary, but also making it possible for data engineers to select and filter the data of interest.Dataset EvolutionEach iteration of the dataset was made to address certain issues discovered while having models used in a production environment and observing situations where the model lacked in performance.                  Dataset v1      Dataset v2      Dataset v3                  Nr. images      15226      17636      30538              Nr. of labels      64119      86676      242534      If the first version was basic, containing a rough tagging strategy we quickly noticed that it was not detecting some special situations that appeared due to the pandemic situation: people wearing masks.This led to another round of data annotation to include those scenarios.The third iteration addressed a broader range of issues:  Small regions of interest (objects far away from the camera)    Objects in very dark backgrounds    Rotated objects or even upside down    Variation of the licence plate design due to images from different countries and regions    People wearing masks    Faces in the mirror - see below the mirror of the motorcycle    But the main reason was because of a scenario where the recording had at the start or end (but not only) close-ups of the operator who was checking the camera. This led to images with large regions of interest containing the camera operator’s face - too large to be detected by the model.  We investigated the dataset structure by splitting the data into bins based on the bbox sizes (in pixels). This made something clear to us: the dataset was unbalanced.  We made bins for tag sizes with a stride of 100 pixels and went up to the maximum value present in the dataset which accounted for 1 sample of size 2000 pixels. The majority of the labels were small in size and the higher we would go with the size, the fewer tags we would have. This made it clear that we would need more targeted annotations for our dataset to try to balance it.All these scenarios required the tagging team to revisit the data multiple times and also change the tagging strategy by including more tags that were considered at a certain limit. It also required them to pay more attention to small details that may have been missed in a previous iteration.Data SplittingTo better understand the strategy chosen for splitting the data, we also need to understand the source of the data. The images come from different devices that are used in different geographical locations (different countries) and are from a continuous trip recording. The annotation team used an internal tool to visualise the trips image by image and mark the faces and licence plates present in them. We would then have access to all those images and their respective metadata.The chosen ratios for splitting are:  Train 70%  Validation 10%  Test 20%            Number of train images      12733              Number of validation images      1682              Number of test images      3221              Number of labelled classes in train set      60630              Number of labelled classes in validation set      7658              Number of of labelled classes in test set      18388      The split is not so trivial as we have some requirements and need to complete some conditions:  An image can have multiple tags from one or both classes but must belong to just one subset.  The tags should be split as close as possible to the desired ratios.  As different images can belong to the same trip in a close geographical relation, we need to force them in the same subset. By doing so, we avoid similar tags in train and test subsets, resulting in incorrect evaluations.Data AugmentationThe application of data augmentation plays a crucial role while training the machine learning model. There are mainly three ways in which data augmentation techniques can be applied:  Offline data augmentation - enriching a dataset by physically multiplying some of its images and applying modifications to them.  Online data augmentation - on the fly modifications of the image during train time with configurable probability for each modification.  Combination of both offline and online data augmentation.In our case, we are using the third option which is a combination of both.The first method that contributes to offline augmentation is a method called image view splitting. This is necessary for us due to different image types: perspective camera images, wide field of view images, 360 degree images in equirectangular format. All these formats and field of views with their respective distortions would complicate the data and make it hard for the model to generalise it and also handle different image types that could be added in the future.For this, we defined the concept of image views which are an extracted portion (view) of an image with some predefined properties. For example, the perspective projection of 75 by 75 degrees field of view patches from the original image.Here we can see a perspective camera image and the image views generated from it:   Figure 5 - Original image     Figure 6 - Two image views generated  The important thing here is that each generated view is an image on its own with the associated tags. They also have an overlapping area so we have a possibility to contain the same tag in two views but from different perspectives. This brings us to an indirect outcome of the first offline augmentation.The second method for offline augmentation is the oversampling of some of the images (views). As mentioned above, we faced the problem of an unbalanced dataset, specifically we were missing tags that occupied high regions of the image, and even though our tagging teams tried to annotate as many as they could find, these were still scarce.As our object detection model is an anchor-based detector, we did not even have enough of them to generate the anchor boxes correctly. This could be clearly seen in the accuracy of the previous trained models, as they were performing poorly on bins of big sizes.By randomly oversampling images that contained big tags, up to a minimum required number, we managed to have better anchors and increase the recall for those scenarios. As described below, the chosen object detector for blurring was YOLOv4 which offers a large variety of online augmentations. The online augmentations used are saturation, exposure, hue, flip and mosaic.ModelAs of summer of 2021, the “go to” solution for object detection in images are convolutional neural networks (CNN), being a mature solution able to fulfil the needs efficiently.ArchitectureMost CNN based object detectors have three main parts: Backbone, Neck and (Dense or Sparse Prediction) Heads. From the input image, the backbone extracts features which can be combined in the neck part to be used by the prediction heads to predict object bounding-boxes and their labels.   Figure 7 - Anatomy of one and two-stage object detectors  3The backbone is usually a CNN classification network pretrained on some dataset, like ImageNet-1K. The neck combines features from different layers in order to produce rich representations for both large and small objects. Since the objects to be detected have varying sizes, the topmost features are too coarse to represent smaller objects, so the first CNN based object detectors were fairly weak in detecting small sized objects. The multi-scale, pyramid hierarchy is inherent to CNNs so Tsung-Yi Lin et al 4 introduced the Feature Pyramid Network which at marginal costs combines features from multiple scales and makes predictions on them. This or improved variants of this technique is used by most detectors nowadays. The head part does the predictions for bounding boxes and their labels.YOLO is part of the anchor-based one-stage object detectors family being developed originally in Darknet, an open source neural network framework written in C and CUDA. Back in 2015, it was the first end-to-end differentiable network of this kind that offered a joint learning of object bounding boxes and their labels.One reason for the big success of newer YOLO versions is that the authors carefully merged new ideas into one architecture, the overall speed of the model being always the north star.YOLOv4 introduces several changes to its v3 predecessor:  Backbone - CSPDarknet53: YOLOv3 Darknet53 backbone was modified to use Cross Stage Partial Network (CSPNet 5) strategy, which aims to achieve richer gradient combinations by letting the gradient flow propagate through different network paths.  Multiple configurable augmentation and loss function types, so called “Bag of freebies”, which by changing the training strategy can yield higher accuracy without impacting the inference time.  Configurable necks and different activation functions, they call “Bag of specials”.InsightsFor this task, we found that YOLOv4 gave a good compromise between speed and accuracy as it has doubled the speed of a more accurate two-stage detector while maintaining a very good overall precision/recall. For blurring, the main metric for model selection was the overall recall, while precision and intersection over union (IoU) of the predicted box comes second as we want to catch all personal data even if some are wrong. Having a multitude of possibilities to configure the detector architecture and train it on our own dataset we conducted several experiments with different configurations for backbones, necks, augmentations and loss functions to come up with our current solution.We faced challenges in training a good model as the dataset posed a large object/box-level scale imbalance, small objects being over-represented in the dataset. As described in 6 and 4, this affects the scales of the estimated regions and the overall detection performance. In 6 several solutions are proposed for this out of which the SPP 7 blocks and PANet 8 neck used in YOLOv4 together with heavy offline data augmentation increased the performance of the actual model in comparison to the former ones.As we have evaluated, the model still has some issues:  Occlusion of the object, either by the camera view, head accessories or other elements:  These cases would need extra annotations in the dataset, just like the faces or licence plates that are really close to the camera and occupy a large region of interest in the image.  As we have a limited number of annotations of close objects to the camera view, the model has incorrectly learnt this, sometimes producing false positives in these situations:  Again, one solution for this would be to include more of these scenarios in the dataset.What’s Next?Grab spends a lot of effort ensuring privacy protection for its users so we are always looking for ways to further improve our related models and processes.As far as efficiency is concerned, there are multiple directions to consider for both the dataset and the model. There are two main factors that drive the costs and the quality: further development of the dataset for additional edge cases (e.g. more training data of people wearing masks) and the operational costs of the model.As the vast majority of current models require a fully labelled dataset, this puts a large work effort on the Data Entry team before creating a new model. Our dataset increased 4x for its third version, but still there is room for improvement as described in the Dataset section.As Grab extends its operations in more cities, new data is collected that has to be processed, this puts an increased focus on running detection models more efficiently.Directions to pursue to increase our efficiency could be the following:  As plenty of unlabelled data is available from imagery collection, a natural direction to explore is self-supervised visual representation learning techniques to derive a general vision backbone with superior transferring performance for our subsequent tasks as detection, classification.  Experiment with optimisation techniques like pruning and quantisation to get a faster model without sacrificing too much on accuracy.  Explore new architectures: YOLOv5, EfficientDet or Swin-Transformer for Object Detection.  Introduce semi-supervised learning techniques to improve our model performance on the long tail of the data.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!References  Bharat Singh, Larry S. Davis. An Analysis of Scale Invariance in Object Detection - SNIP. arXiv:1711.08189v2  Zhenda Xie et al. Self-Supervised Learning with Swin Transformers.  arXiv:2105.04553v2Footnotes            Kaiyu Yang et al. Study of Face Obfuscation in ImageNet: arxiv.org/abs/2103.06191 &#8617;              Nitish S. Mutha How to map Equirectangular projection to Rectilinear projection &#8617;              Alexey Bochkovskiy et al.. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv:2004.10934v1 &#8617;              Tsung-Yi Lin et al. Feature Pyramid Networks for Object Detection. arXiv:1612.03144v2 &#8617; &#8617;2              Chien-Yao Wang et al. CSPNet: A New Backbone that can Enhance Learning Capability of CNN. arXiv:1911.11929v1 &#8617;              Kemal Oksuz et al.. Imbalance Problems in Object Detection: A Review. arXiv:1909.00169v3 &#8617; &#8617;2              Kaiming He et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. arXiv:1406.4729v4 &#8617;              Shu Liu et al. Path Aggregation Network for Instance Segmentation. arXiv:1803.01534v4 &#8617;      ",
        "url": "/protecting-personal-data-in-grabs-imagery"
      }
      ,
    
      "processing-etl-tasks-with-ratchet": {
        "title": "Processing ETL tasks with Ratchet",
        "author": "amar-prakash",
        "tags": "[&quot;Pipelines&quot;, &quot;Data&quot;, &quot;ETL&quot;, &quot;Engineering&quot;]",
        "category": "",
        "content": "OverviewAt Grab, the Lending team is focused towards building products that help finance various segments of users, such as Passengers, Drivers, or Merchants, based on their needs. The team builds products that enable users to avail funds in a seamless and hassle-free way. In order to achieve this, multiple lending microservices continuously interact with each other. Each microservice handles different responsibilities, such as providing offers, storing user information, disbursing availed amounts to a user’s account, and many more.In this tech blog, we will discuss what Data and Extract, Transform and Load (ETL) pipelines are and how they are used for processing multiple tasks in the Lending Team at Grab. We will also discuss Ratchet, which is a Go library, that helps us in building data pipelines and handling ETL tasks. Let’s start by covering the basis of Data and ETL pipelines.What is a Data Pipeline?A Data pipeline is used to describe a system or a process that moves data from one platform to another. In between platforms, data passes through multiple steps based on defined requirements, where it may be subjected to some kind of modification. All the steps in a Data pipeline are automated, and the output from one step acts as an input for the next step.   Data Pipeline (Source: Hazelcast)  What is an ETL Pipeline?An ETL pipeline is a type of Data pipeline that consists of 3 major steps, namely extraction of data from a source, transformation of that data into the desired format, and finally loading the transformed data to the destination. The destination is also known as the sink.   Extract-Transform-Load (Source: TatvaSoft)  The combination of steps in an ETL pipeline provides functions to assure that the business requirements of the application are achieved.Let’s briefly look at each of the steps involved in the ETL pipeline.Data ExtractionData extraction is used to fetch data from one or multiple sources with ease. The source of data can vary based on the requirement. Some of the commonly used data sources are:  Database  Web-based storage (S3, Google cloud, etc)  Files  User Feeds, CRM, etc.The data format can also vary from one use case to another. Some of the most commonly used data formats are:  SQL  CSV  JSON  XMLOnce data is extracted in the desired format, it is ready to be fed to the transformation step.Data TransformationData transformation involves applying a set of rules and techniques to convert the extracted data into a more meaningful and structured format for use. The extracted data may not always be ready to use. In order to transform the data, one of the following techniques may be used:  Filtering out unnecessary data.  Preprocessing and cleaning of data.  Performing validations on data.  Deriving a new set of data from the existing one.  Aggregating data from multiple sources into a single uniformly structured format.Data LoadingThe final step of an ETL pipeline involves moving the transformed data to a sink where it can be accessed for its use. Based on requirements, a sink can be one of the following:  Database  File  Web-based storage (S3, Google cloud, etc)An ETL pipeline may or may not have a loadstep based on its requirements. When the transformed data needs to be stored for further use, the loadstep is used to move the transformed data to the storage of choice. However, in some cases, the transformed data may not be needed for any further use and thus, the loadstep can be skipped.Now that you understand the basics, let’s go over how we, in the Grab Lending team, use an ETL pipeline.Why Use Ratchet?At Grab, we use Golang for most of our backend services. Due to Golang’s simplicity, execution speed, and concurrency support, it is a great choice for building data pipeline systems to perform custom ETL tasks.Given that Ratchet is also written in Go, it allows us to easily build custom data pipelines.Go channels are connecting each stage of processing, so the syntax for sending data is intuitive for anyone familiar with Go. All data being sent and received is in JSON, providing a nice balance of flexibility and consistency.Utilising Ratchet for ETL TasksWe use Ratchet for multiple ETL tasks like batch processing, restructuring and rescheduling of loans, creating user profiles, and so on. One of the backend services, named Azkaban, is responsible for handling various ETL tasks.Ratchet uses Data Processors for building a pipeline consisting of multiple stages. Data Processors each run in their own goroutine so all of the data is processed concurrently. Data Processors are organised into stages, and those stages are run within a pipeline. For building an ETL pipeline, each of the three steps (Extract, Transform and Load) use a Data Processor for implementation. Ratchet provides a set of built-in, useful Data Processors, while also providing an interface to implement your own. Usually, the transform stage uses a Custom Data Processor.   Data Processors in Ratchet (Source: Github)  Let’s take a look at one of these tasks to understand how we utilise Ratchet for processing an ETL task.Whitelisting Merchants Through ETL PipelinesWhitelisting essentially means making the product available to the user by mapping an offer to the user ID. If a merchant in Thailand receives an option to opt for Cash Loan, it is done by whitelisting that merchant. In order to whitelist our merchants, our Operations team uses an internal portal to upload a CSV file with the user IDs of the merchants and other required information. This CSV file is generated by our internal Data and Risk team and handed over to the Operations team. Once the CSV file is uploaded, the user IDs present in the file are whitelisted within minutes. However, a lot of work goes in the background to make this possible.Data ExtractionOnce the Operations team uploads the CSV containing a list of merchant users to be whitelisted, the file is stored in S3 and an entry is created on the Azkaban service with the document ID of the uploaded file.   File upload by Operations team  The data extraction step makes use of a Custom CSV Data Processor that uses the document ID to first create a PreSignedUrl and then uses it to fetch the data from S3. The data extracted is in bytes and we use commas as the delimiter to format the CSV data.Data TransformationIn order to transform the data, we define a Custom Data Processor that we call a Transformer for each ETL pipeline. Transformers are responsible for applying all necessary transformations to the data before it is ready for loading. The transformations applied in the merchant whitelisting transformers are:  Convert data from bytes to struct.  Check for presence of all mandatory fields in the received data.  Perform validation on the data received.  Make API calls to external microservices for whitelisting the merchant.As mentioned earlier, the CSV file is uploaded manually by the Operations team. Since this is a manual process, it is prone to human errors. Validation of data in the data transformation step helps avoid these errors and not propagate them further up the pipeline. Since CSV data consists of multiple rows, each row passes through all the steps mentioned above.Data LoadingWhenever the merchants are whitelisted, we don’t need to store the transformed data. As a result, we don’t have a loadstep for this ETL task, so we just use an Empty Data Processor. However, this is just one of many use cases that we have. In cases where the transformed data needs to be stored for further use, the loadstep will have a Custom Data Processor, which will be responsible for storing the data.Connecting All StagesAfter defining our Data Processors for each of the steps in the ETL pipeline, the final piece is to connect all the stages together. As stated earlier, the ETL tasks have different ETL pipelines and each ETL pipeline consists of 3 stages defined by their Data Processors.In order to connect these 3 stages, we define a Job Processor for each ETL pipeline. A Job Processor represents the entire ETL pipeline and encompasses Data Processors for each of the 3 stages. Each Job Processor implements the following methods:  SetSource: Assigns the Data Processor for the Extraction stage.  SetTransformer: Assigns the Data Processor for the Transformation stage.  SetDestination: Assigns the Data Processor for the Load stage.  Execute: Runs the ETL pipeline.   Job processors containing Data Processor for each stage in ETL  When the Azkaban service is initialised, we run the SetSource(), SetTransformer() and SetDestination() methods for each of the Job Processors defined. When an ETL task is triggered, the Execute() method of the corresponding Job Processor is run. This triggers the ETL pipeline and gradually runs the 3 stages of ETL pipeline. For each stage, the Data Processor assigned during initialisation is executed.ConclusionETL pipelines help us in streamlining various tasks in our team. As showcased through the example in the above section, an ETL pipeline breaks a task into multiple stages and divides the responsibilities across these stages.In cases where a task fails in the middle of the process, ETL pipelines help us determine the cause of the failure quickly and accurately. With ETL pipelines, we have reduced the manual effort required for validating data at each step and avoiding propagation of errors towards the end of the pipeline.Through the use of ETL pipelines and schedulers, we at Lending have been able to automate the entire pipeline for many tasks to run at scheduled intervals without any manual effort involved at all. This has helped us tremendously in reducing human errors, increasing the throughput of the system and making the backend flow more reliable. As we continue to automate more and more of our tasks that have tightly defined stages, we foresee a growth in our ETL pipelines usage.Referenceshttps://www.alooma.com/blog/what-is-a-data-pipelinehttp://rkulla.blogspot.com/2016/01/data-pipeline-and-etl-tasks-in-go-usinghttps://medium.com/swlh/etl-pipeline-and-data-pipeline-comparison-bf89fa240ce9Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/processing-etl-tasks-with-ratchet"
      }
      ,
    
      "app-modularisation-at-scale": {
        "title": "App Modularisation at Scale",
        "author": "amar-jain",
        "tags": "[&quot;App&quot;, &quot;Build Time&quot;, &quot;Engineering&quot;, &quot;Monorepo&quot;]",
        "category": "",
        "content": "Grab a coffee ☕️, sit back and enjoy reading. 😃Wanna know how we improved our app’s build time performance and developer experience at Grab? Continue reading…Where it all beganImagine you are working on an app that grows continuously as more and more features are added to it, it becomes challenging to manage the code at some point. Code conflicts increase due to coupling, development slows down, releases take longer to ship, collaboration becomes difficult, and so on.Grab superapp is one such app that offers many services like booking taxis, ordering food, payments using an e-wallet, transferring money to friends/families, paying at merchants, and many more, across Southeast Asia.Grab app followed a monolithic architecture initially where the entire code was held in a single module containing all the UI and business logic for almost all of its features. But as the app grew, new developers were hired, and more features were built, it became difficult to work on the codebase. We had to think of better ways to maintain the codebase, and that’s when the team decided to modularise the app to solve the issues faced.What is Modularisation?Breaking the monolithic app module into smaller, independent, and interchangeable modules to segregate functionality so that every module is responsible for executing a specific functionality and will contain everything necessary to execute that functionality.Modularising the Grab app was not an easy task as it brought many challenges along with it because of its complicated structure due to the high amount of code coupling.Approach and DesignWe divided the task into the following sub-tasks to ensure that only one out of many functionalities in the app was impacted at a time.  Setting up the infrastructure by creating Base/Core modules for Networking, Analytics, Experimentation, Storage, Config, and so on.  Building Shared Library modules for Styling, Common-UI, Utils, etc.  Incrementally building Feature modules for user-facing features like Payments Home, Wallet Top Up, Peer-to-Merchant (P2M) Payments, GrabCard and many others.  Creating Kit modules to enable inter-module communication. This step helped us in building the feature modules in parallel.  Finally, the App module was used as a hub to connect all the other modules together using dependency injection (Dagger).   Modularised app structure  In the above diagram, payments-home, wallet top-up, and grabcard are different features provided by the Grab app. top-up-kit and grabcard-kit are bridges that expose functionalities from topup and grabcard modules to the payments-home module, respectively.In the process of modularising the Grab app, we ensured that a feature module did not directly depend on other feature modules so that they could be built in parallel using the available CPU cores of the machine, hence reducing the overall build time of the app.With the Kit module approach, we separated our code into independent layers by depending only on abstractions instead of concrete implementation.Modularisation Benefits  Faster build times and hence faster CI: Gradle build system compiles only the changed modules and uses the binaries of all the non-affected modules from its cache. So the compilation becomes faster as independent modules are run in parallel on different threads.  Fine dependency graph: Dependencies of a module are well defined.  Reusability across other apps: Modules can be used across different apps by converting them into an AAR/SDK.  Scale and maintainability: Teams can work independently on the modules owned by them without blocking each other.  Well-defined code ownership: Easier to define ownership per module in the codebase.Limitations  Requires more effort and time to modularise an app.  Separate configuration files to be maintained for each module.  Gradle sync time starts to grow.  IDE becomes very slow and its memory usage goes up a lot.  Parallel execution of the module depends on the machine’s capabilities.Where we are nowThere are more than 1,000 modules in the Grab app and are still counting.At Grab, we have many sub-teams which take care of different features available in the app. Grab Financial Group (GFG) is one such sub-team that handles everything related to payments in the app. For example: P2P &amp; P2M money transfers, e-Wallet activation, KYC, and so on.We started modularising payments further in July 2020 as it was already bombarded with too many features and it was difficult for the team to work on the single payments module. The result of payments modularisation is shown in the following chart.   Build time graph of payments module  As of today, we have about 200+ modules in GFG and more than 95% of the modules take less than 15s to build.ConclusionModularisation has helped us a lot in reducing the overall build time of the app and also, in improving the developer experience by breaking dependencies and allowing us to define code ownership. Having said that, modularisation is not an easy or a small task, especially for large projects with legacy code. However, with careful planning and the right design, modularisation can help in forming a well-structured and maintainable project.Hope you enjoyed reading. Don’t forget to 👏.References:  https://proandroiddev.com/build-a-modular-android-app-architecture-25342d99de82  https://medium.com/google-developer-experts/modularizing-android-applications-9e2d18f244a0  https://medium.com/@mydogtom/modularization-part-1-application-structure-overview-9e465909a9bcJoin usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/app-modularisation-at-scale"
      }
      ,
    
      "reshaping-chat-support": {
        "title": "Reshaping Chat Support for Our Users",
        "author": "elisa-monacchiwan-ling-guaisuman-anand",
        "tags": "[&quot;Product&quot;, &quot;Design&quot;, &quot;Chat support&quot;]",
        "category": "",
        "content": "IntroductionThe Grab support team plays a key role in ensuring our users receive support when things don’t go as expected or whenever there are questions on our products and services.In the past, when users required real-time support, their only option was to call our hotline and wait in the queue to talk to an agent. But voice support has its downsides: sometimes it is complex to describe an issue in the app, and it requires the user’s full attention on the call.With chat messaging apps growing massively in the last years, chat has become the expected support channel users are familiar with. It offers real-time support with the option of multitasking and easily explaining the issue by sharing pictures and documents. Compared to voice support, chat provides access to the conversation for future reference.With chat growth, building a chat system tailored to our support needs and integrated with internal data, seemed to be the next natural move.In our previous articles, we covered the tech challenges of building the chat platform for the web, our workforce routing system and improving agent efficiency with machine learning. In this article, we will explain our approach and key learnings when building our in-house chat for support from a Product and Design angle.    A glimpse at agent and user experienceWhy Reinvent the WheelWe wanted to deliver a product that would fully delight our users. That’s why we decided to build an in-house chat tool that can:  Prevent chat disconnections and ensure a consistent chat experience: Building a native chat experience allowed us to ensure a stable chat session, even when users leave the app. Besides, leveraging on the existing Grab chat infrastructure helped us achieve this fast and ensure the chat experience is consistent throughout the app. You can read more about the chat architecture here.  Improve productivity and provide faster support turnarounds: By building the agent experience in the CRM tool, we could reduce the number of tools the support team uses and build features tailored to our internal processes. This helped to provide faster help for our users.  Allow integration with internal systems and services: Chat can be easily integrated with in-house AI models or chatbot, which helps us personalise the user experience and improve agent productivity.  Route our users to the best support specialist available: Our newly built routing system accounts for all the use cases we were wishing for such as prioritising certain requests, better distribution of the chat load during peak hours, making changes at scale and ensuring each chat is routed to the best support specialist available.Fail Fast with an MVPBefore building a full-fledged solution, we needed to prove the concept, an MVP that would have the key features and yet, would not take too much effort if it fails. To kick start our experiment, we established the success criteria for our MVP; how do we measure its success or failure?Defining What Success Looks LikeAny experiment requires a hypothesis - something you’re trying to prove or disprove and it should relate to your final product. To tailor the final product around the success criteria, we need to understand how success is measured in our situation. In our case, disconnections during chat support was one of the key challenges faced so our hypothesis was:Starting with Design SprintOur design sprint aimed to solutionise a series of problem statements, and generate a prototype to validate our hypothesis. To spark ideation, we run sketching exercises such as Crazy 8, Solution sketch and end off with sharing and voting.        Some of the prototypes built during the Design sprintDefining MVP Scope to Run the ExperimentTo test our hypothesis quickly, we had to cut the scope by focusing on the basic functionality of allowing chat message exchanges with one agent.Here is the main flow and a sneak peek of the design:      Accepting chats       Handling concurrent chats What We Learnt from the ExperimentDuring the experiment, we had to constantly put ourselves in our users’ shoes as ‘we are not our users’. We decided to shadow our chat support agents and get a sense of the potential issues our users actually face. By doing so, we learnt a lot about how the tool was used and spotted several problems to address in the next iterations.In the end, the experiment confirmed our hypothesis that having a native in-app chat was more stable than the previous chat in use, resulting in a better user experience overall.Starting with the End in MindOnce the experiment was successful, we focused on scaling. We defined the most critical jobs to be done for our users so that we could scale the product further. When designing solutions to tackle each of them, we ensured that the product would be flexible enough to address future pain points. Would this work for more channels, more users, more products, more countries?Before scaling, the problems to solve were:  Monitoring the performance of the system in real-time, so that swift operational changes can be made to ensure users receive fast support;  Routing each chat to the best agent available, considering skills, occupancy, as well as issue prioritisation. You can read more about the our routing system design here;  Easily communicate with users and show empathy, for which we built file-sharing capabilities for both users and agents, as well as allowing emojis, which create a more personalised experience.Scaling EfficientlyWe broke down the chat support journey to determine what areas could be improved.Reducing Waiting TimeWhen analysing the current wait time, we realised that when there was a surge in support requests, the average waiting time increased drastically. In these cases, most users would be unresponsive by the time an agent finally attends to them.To solve this problem, the team worked on a dynamic queue limit concept based on Little’s law. The idea is that considering the number of incoming chats and the agents’ capacity, we can forecast the number of users we can handle in a reasonable time, and prevent the remaining from initiating a chat. When this happens, we ensure there’s a backup channel for support so that no user is left unattended.This allowed us to reduce chat waiting time by ~30% and reduce unresponsive users by ~7%.Reducing Time to ReplyA big part of the chat time is spent typing the message to send to the user. Although the previous tool had templated messages, we observed that 85% of them were free-typed. This is because agents felt the templates were impersonal and wanted to add their personal style to the messages.With this information in mind, we knew we could help by providing autocomplete suggestions  while the agents are typing. We built a machine learning based feature that considers several factors such as user type, the entry point to support, and the last messages exchanged, to suggest how the agent should complete the sentence. When this feature was first launched, we reduced the average chat time by 12%!Read this to find out more about how we built this machine learning feature, from defining the problem space to its implementation.  Reducing the Overall Chat TimeLooking at the average chat time, we realised that there was still room for improvement. How can we help our agents to manage their time better so that we can reduce the waiting time for users in the queue?We needed to provide visibility of chat durations so that our agents could manage their time better. So, we added a timer at the top of each chat window to indicate how long the chat was taking.      Timer in the minimised chat We also added nudges to remind agents that they had other users to attend to while they were in the chat.    Timer in the maximised chatBy providing visibility via prompts and colour-coded indicators to prevent exceeding the expected chat duration, we reduced the average chat time by 22%!What We Learnt from this Project  Start with the end in mind. When you embark on a big project like this, have a clear vision of how the end state looks like and plan each step backwards. How does success look like and how are we going to measure it? How do we get there?  Data is king. Data helped us spot issues in real-time and guided us through all the iterations following the MVP. It helped us prioritise the most impactful problems and take the right design decisions. Instrumentation must be part of your MVP scope!  Remote user testing is better than no user testing at all. Ideally, you want to do user testing in the exact environment your users will be using the tool but a pandemic might make things a bit more complex. Don’t let this stop you! The qualitative feedback we received from real users, even with a prototype on a video call, helped us optimise the tool for their needs.  Address the root cause, not the symptoms. Whenever you are tasked with solving a big problem, break it down into its components by asking “Why?” until you find the root cause. In the first phases, we realised the tool had a longer chat time compared to 3rd party softwares. By iteratively splitting the problem into smaller ones, we were able to address the root causes instead of the symptoms.  Shadow your users whenever you can. By looking at the users in action, we learned a ton about their creative ways to go around the tool’s limitations. This allowed us to iterate further on the design and help them be more efficient.Of course, this would not have been possible without the incredible work of several teams: GS TF, GS, Comms platform, Driver and Merchant teams.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/reshaping-chat-support"
      }
      ,
    
      "debugging-high-latency-market-store": {
        "title": "Debugging High Latency Due to Context Leaks",
        "author": "sourabh-sumanadarsh-koyyachandankumar-agarwal",
        "tags": "[&quot;Engineering&quot;, &quot;Latency&quot;, &quot;Debugging&quot;, &quot;Memory Leak&quot;]",
        "category": "",
        "content": "BackgroundMarket-Store is an in-house developed general purpose feature store that is used to serve real-time computed machine learning (ML) features. Market-Store has a stringent SLA around latency, throughput, and availability as it empowers ML models, which are used in Dynamic Pricing and Consumer Experience.ProblemAs Grab continues to grow, introducing new ML models and handling increased traffic, Market-Store started to experience high latency. Market-Store’s SLA states that 99% of transactions should be within 200ms, but our latency increased to 2 seconds. This affected the availability and accuracy of our models that rely on Market-Store for real-time features.Latency IssueWe used different metrics and logs to debug the latency issue but could not find any abnormalities that directly correlated to the API’s performance. We discovered that the problem went away temporarily when we restarted the service. But during the next peak period, the service began to struggle once again and the problem became more prominent as Market-Store’s query per second (QPS) increased.The following graph shows an increase in the memory used with time over 12 hours. Even as the system load receded, memory usage continued to increase.The continuous increase in memory consumption indicated the possibility of a memory leak, which occurs when memory is allocated but not returned after its use is over. This results in consistently increasing consumed memory until the service runs out of memory and crashes.Although we could restart the service and resolve the issue temporarily, the increasing memory use suggested a deeper underlying root cause. This meant that we needed to conduct further investigation with tools that could provide deeper insights into the memory allocations.Debugging Using Go ToolsPPROF is a profiling tool by Golang that helps to visualise and analyse profiles from Go programmes. A profile is a collection of stack traces showing the call sequences in your programme that eventually led to instances of a particular event i.e. allocation. It also provides details such as Heap and CPU information, which could provide insights into the bottlenecks of the Go programme.By default, PPROF is enabled on all Grab Go services, making it the ideal tool to use in our scenario. To understand how memory is allocated, we used PPROF to generate Market-Store’s Heap profile, which can be used to understand how inuse memory was allocated for the programme.You can collect the Heap profile by running this command:go tool pprof 'http://localhost:6060/debug/pprof/heap'The command then generates the Heap profile information as shown in the diagram below: From this diagram, we noticed that a lot of memory was allocated and held by the child context created from Async Library even after the tasks were completed.In Market-Store, we used the Async Library, a Grab open-source library, which typically used to run concurrent tasks. Any contexts created by the Async Library should be cleaned up after the background tasks are completed. This way, memory would be returned to the service.However, as shown in the diagram, memory was not being returned, resulting in a memory leak, which explains the increasing memory usage even as Market-Store’s system load decreased.Uncovering the Real IssueSo we knew that Market-Store’s latency was affected, but we didn’t know why. From the first graph, we saw that memory usage continued to grow even as Market-Store’s system load decreased. Then, PPROF showed us that the memory held by contexts was not cleaned up, resulting in a memory leak.Through our investigations, we drew a correlation between the increase in memory usage and a degradation in the server’s API latency. In other words, the memory leak resulted in a high memory consumption and eventually, caused the latency issue.However, there was no change in our service that would have impacted how contexts are created and cleaned up. So what caused the memory leak?Debugging the Memory LeakWe needed to look into the Async Library and how it worked. For Market-Store, we updated the cache asynchronously for the write-around caching mechanism. We use the Async Library for running the update tasks in the background.The following code snippet explains how the Async Library works:async.Consume(context.Background(), runtime.NumCPU()*4, buffer)// Consume runs the tasks with a specific max concurrencyfunc Consume(ctx context.Context, concurrency int, tasks chan Task) Task {   // code...   return Invoke(ctx, func(context.Context) (interface{}, error) {       workers := make(chan int, concurrency)       concurrentTasks := make([]Task, concurrency)       // code ...       t.Run(ctx).ContinueWith(ctx, func(interface{}, error) (interface{}, error) {       // code...      })    }}func Invoke(ctx context.Context, action Work) Task {    return NewTask(action).Run(ctx)}func(t *task) Run(ctx context.Context) Task {    ctx, t.cancel = context.WithCancel(ctx)    go t.run(ctx)    return t}Note: Code that is not relevant to this article was replaced with code.As seen in the code snippet above, the Async Library initialises the Consume method with a background context, which is then passed to all its runners. Background contexts are empty and do not track or have links to child contexts that are created from them.In Market-Store, we use background contexts because they are not bound by request contexts and can continue running even after a request context is cleaned up. This means that once the task has finished running, the memory consumed by child contexts would be freed up, avoiding the issue of memory leaks altogether.Identifying the Cause of the Memory LeakUpon further digging, we discovered an MR that was merged into the library to address a task cancellation issue. As shown in the code snippet below, the Consume method had been modified such that task contexts were being passed to the runners, instead of the empty background contexts.func Consume(ctx context.Context, concurrency int, tasks chan Task) Task {     // code...     return Invoke(ctx, func(taskCtx context.Context) (interface{}, error) {         workers := make(chan int, concurrency)         concurrentTasks := make([]Task, concurrency)         // code ...         t.Run(taskCtx).ContinueWith(ctx, func(interface{}, error) (interface{}, error) {            // code...        })     }}Before we explain the code snippet, we should briefly explain what Golang contexts are. A context is a standard Golang package that carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes. We should always remember to cancel contexts after using them.Importance of Context CancellationWhen a context is cancelled, all contexts derived from it are also cancelled. This means that there will be no unaccounted contexts or links and it can be achieved by using the Async Library’s CancelFunc.The Async Library’s CancelFunc method will:  Cancel the created child context and its children  Remove the parent reference from the child context  Stop any associated timersWe should always make sure to call the CancelFunc method after using contexts, to ensure that contexts and memory are not leaked.Explaining the Impact of the MRIn the previous code snippet, we see that task contexts are passed to runners and they are not being cancelled. The Async Library created task contexts from non-empty contexts, which means the task contexts are tracked by the parent contexts. So, even if the work associated with these task contexts is complete, they will not be cleaned up by the system (garbage collected).As we started using task contexts instead of background contexts and did not cancel them, the memory used by these contexts was never returned, thus resulting in a memory leak.It took us several tries to debug and investigate the root cause of Market-Store’s high latency issue and through this incident, we learnt several important things that would help prevent a memory leak from recurring.      Always cancel the contexts you’ve created. Leaving it to garbage collection (system cleanup) may result in unexpected memory leaks.        Go profiling can provide plenty of insights about your programme, especially when you’re not sure where to start troubleshooting.        Always benchmark your dependencies when integrating or updating the versions to ensure they don’t have any performance bottlenecks.  Special thanks to Chip Dong Lim for his contributions and for designing the GIFs included in this article.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/debugging-high-latency-market-store"
      }
      ,
    
      "building-hyper-self-service-distributed-tracing-feedback-system": {
        "title": "Building a Hyper Self-Service, Distributed Tracing and Feedback System for Rule &amp; Machine Learning (ML) Predictions",
        "author": "warren-zhouwenhui-wuyongguo-meimuqi-livarun-kansal",
        "tags": "[&quot;Engineering&quot;, &quot;Machine Learning&quot;, &quot;Statistics&quot;, &quot;Distributed Tracing&quot;]",
        "category": "",
        "content": "IntroductionIn Grab, the Trust, Identity, Safety, and Security (TISS) is a team of software engineers and AI developers working on fraud detection, login identity check, safety issues, etc. There are many TISS services, like grab-fraud, grab-safety, and grab-id. They make billions of business decisions daily using the Griffin rule engine, which determines if a passenger can book a trip, get a food promotion, or if a driver gets a delivery booking.There is a natural demand to log down all these important business decisions, store them and query them interactively or in batches. Data analysts and scientists need to use the data to train their machine learning models. RiskOps and customer service teams can query the historical data and help consumers.That’s where Archivist comes in; it is a new tracing, statistics and feedback system for rule and machine learning-based predictions. It is reliable and performant. Its innovative data schema is flexible for storing events from different business scenarios. Finally, it provides a user-friendly UI, which has access control for classified data.Here are the impacts Archivist has already made:  Currently, there are 2 teams with a total of 5 services and about 50 business scenarios using Archivist. The scenarios include fraud prevention (e.g. DriverBan, PassengerBan), payment checks (e.g. PayoutBlockCheck, PromoCheck), and identity check events like PinTrigger.  It takes only a few minutes to onboard a new business scenario (event type), by using the configuration page on the user portal. Previously, it took at least 1 to 2 days.  Each day, Archivist logs down 80 million logs to the ElasticSearch cluster, which is about 200GB of data.  Each week, Grab Support (GS)/Risk Ops goes to the user portal and checks Archivist logs for about 2,000 distinct customers. They can search based on numerous dimensions such as the Passenger/DriverID, phone number, request ID, booking code and payment fingerprint.BackgroundEach day, TISS services make billions of business decisions (predictions), based on the Griffin rule engine and ML models.After the predictions are made, there are still some tough questions for these services to answer.  If Risk Ops believes a prediction is false-positive, a consumer could be banned. If this happens, how can consumers or Risk Ops report or feedback this information to the new rule and ML model training quickly?  As CustomService/Data Scientists investigating any tickets opened due to TISS predictions/decisions, how do you know which rules and data were used? E.g. why the passenger triggered a selfie, or why a booking was blocked.  After Data Analysts/Data Scientists (DA/DS) launch a new rule/model, how can they track the performance in fine-granularity and in real-time? E.g. week-over-week rule performance in a country or city.  How can DA/DS access all prediction data for data analysis or model training?  How can the system keep up with Grab’s business launch speed, with maximum self-service?ProblemTo answer the questions above, TISS services previously used company-wide Kibana to log predictions.  For example, a log looks like: PassengerID:123,Scenario:PinTrigger,Decision:Trigger,.... This logging method had some obvious issues:  Logs in plain text don’t have any structure and are not friendly to ML model training as most ML models need processed data to make accurate predictions.  Furthermore, there is no fine-granularity access control for developers in Kibana.  Developers, DA and DS have no access control while GS has no access at all. So GS cannot easily see the data and DA/DS cannot easily process the data.To address all the Kibana log issues, we developed ActionTrace, a code library with a well-structured data schema. The logs, also called documents, are stored in a dedicated ElasticSearch cluster with access control implemented. However, after using it for a while, we found that it still needed some improvements.  Each business scenario involves different types of entities and ActionTrace is not fully self-service. This means that a lot of development work was needed to support fast-launching business scenarios. Here are some examples:                  The main entities in the taxi business are Driver and Passenger,                    The main entities in the food business can be Merchant, Driver and Consumer.              All these entities will need to be manually added into the ActionTrace data schema.     Each business scenario may have their own custom information logged. Because there is no overlap, each of them will correspond to a new field in the data schema. For example:          For any scenario involving payment, a valid payment method and expiration date is logged.      For the taxi business, the geohash is logged.              To store the log data from ActionTrace, different teams need to set up and manage their own ElasticSearch clusters. This increases hardware and maintenance costs.    There was a simple Web UI created for viewing logs from ActionTrace, but there was still no access control in fine granularity.SolutionWe developed Archivist, a new tracing, statistics, and feedback system for ML/rule-based prediction events. It’s centralised, performant and flexible. It answers all the issues mentioned above, and it is an improvement over all the existing solutions we have mentioned previously.The key improvements are:  User-defined entities and custom fields          There are no predefined entity types. Users can define up to 5 entity types (E.g. PassengerId, DriverId, PhoneNumber, PaymentMethodId, etc.).      Similarly, there are a limited number of custom data fields to use, in addition to the common data fields shared by all business scenarios.        A dedicated service shared by all other services          Each service writes its prediction events to a Kafka stream. Archivist then reads the stream and writes to the ElasticSearch cluster.      The data writes are buffered, so it is easy to handle traffic surges in peak time.      Different services share the same Elastic Cloud Enterprise (ECE) cluster, but they create their own daily file indices so the costs can be split fairly.        Better support for data mining, prediction stats and feedback          Kafka stream data are simultaneously written to AWS S3. DA/DS can use the PrestoDB SQL query engine to mine the data.      There is an internal web portal for viewing Archivist logs. Customer service teams and Ops can use no-risk data to address GS tickets, while DA, DS and developers can view high-risk data for code/rule debugging.        A reduction of development days to support new business launches          Previously, it took a week to modify and deploy the ActionTrace data schema. Now, it only takes several minutes to configure event schemas in the user portal.        Saves time in RiskOps/GS investigations          With the new web UI which has access control in place, the different roles in the company, like Customer service and Data analysts, can access the Archivist events with different levels of permissions.      It takes only a few clicks for them to find the relevant events that impact the drivers/passengers.      Architecture DetailsArchivist’s system architecture is shown in the diagram below.   Archivist system architecture    Different services (like fraud-detection, safety-allocation, etc.) use a simple SDK to write data to a Kafka stream (the left side of the diagram).  In the centre of Archivist is an event processor. It reads data from Kafka, and writes them to ElasticSearch (ES).  The Kafka stream writes to the Amazon S3 data lake, so DA/DS can use the Presto SQL query engine to query them.  The user portal (bottom right) can be used to view the Archivist log and update configurations. It also sends all the web requests to the API Handler in the centre.The following diagram shows how internal and external users use Archivist as well as the interaction between the Griffin rule engine and Archivist.   Archivist use cases  Flexible Event SchemaIn Archivist, a prediction/decision is called an event. The event schema can be divided into 3 main parts conceptually.  Data partitioning: Fields like service_name and event_type categorise data by services and business scenarios.                   Field name       Type       Example       Notes                       service_name       string       GrabFraud       Name of the Service                 event_type       string       PreRide       PaxBan/SafeAllocation             Business decision making: request_id, decisions, reasons, event_content are used to record the business decision, the reason and the context (E.g. The input features of machine learning algorithms).                   Field name       Type       Example       Notes                       request_id       string       a16756e8-efe2-472b-b614-ec6ae08a5912       a 32-digit id for web requests                 event_content       string              Event context                 decisions       [string]       [\"NotAllowBook\", \"SMS\"]       A list                 reasons       string              json payload string of the response from engine.             Customisation: Archivist provides user-defined entities and custom fields that we feel are sufficient and flexible for handling different business scenarios.                   Field name       Type       Example       Notes                       entity_type_1       string       Passenger                        entity_id_1       string       12151                        entity_type_2       string       Driver                        entity_id_2       string       341521-rdxf36767                        ...       string                               entity_id_5       string                               custom_field_type_1       string       “MessageToUser”                        custom_field_1       string       \"please contact Ops\"       User defined fields                 custom_field_type_2              “Prediction rule:”                        custom_field_2       string       “ML rule: 123, version:2”                        ...       string                               custom_field_6       string                         A User Portal to Support Querying, Prediction Stats and FeedbackDA, DS, Ops and GS can access the internal user portal to see the prediction events, individually and on an aggregated city level.   A snapshot of the Archivist logs showing the aggregation of the data in each city  There are graphs on the portal, showing the rule/model performance on individual customers over a period of time.   Rule performance on a customer over a period of time  How to Use Archivist for Your ServiceIf you want to get onboard Archivist, the coding effort is minimal. Here is an example of a code snippet to log an event:   Code snippet to log an event  LessonsDuring the implementation of Archivist, we learnt some things:  A good system needs to support multi-tenants from the beginning. Originally, we thought we could use just one Kafka stream, and put all the documents from different teams into one ElasticSearch (ES) index. But after one team insisted on keeping their data separately from others, we created more Kafka streams and ES indexes. We realised that this way, it’s easier for us to manage data and share the cost fairly.  Shortly after we launched Archivist, there was an incident where the ES data writes were choked. Because each document write is a goroutine, the number of goroutines increased to 400k and the memory usage reached 100% within minutes. We added a patch (2 lines of code) to limit the maximum number of goroutines in our system. Since then, we haven’t had any more severe incidents in Archivist.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/building-hyper-self-service-distributed-tracing-feedback-system"
      }
      ,
    
      "our-journey-to-continuous-delivery-at-grab-part2": {
        "title": "Our Journey to Continuous Delivery at Grab (Part 2)",
        "author": "sylvain-bougerel",
        "tags": "[&quot;Deployment&quot;, &quot;CI&quot;, &quot;Continuous Integration&quot;, &quot;Continuous Deployment&quot;, &quot;Deployment Process&quot;, &quot;Continuous Delivery&quot;, &quot;Multi Cloud&quot;, &quot;Hermetic Deployments&quot;, &quot;Automation&quot;]",
        "category": "",
        "content": "In the first part of this blog post, you’ve read about the improvements made to our build and staging deployment process, and how plenty of manual tasks routinely taken by engineers have been automated with Conveyor: an in-house continuous delivery solution.This new post begins with the introduction of the hermeticity principle for our deployments, and how it improves the confidence with promoting changes to production. Changes sent to production via Conveyor’s deployment pipelines are then described in detail.   Overview of Grab delivery process  Finally, looking back at the engineering efficiency improvements around velocity and reliability over the last 2 years, we answer the big question - was the investment on a custom continuous delivery solution like Conveyor the right decision for Grab?Improving Confidence in our Production Deployments with HermeticityThe term deployment hermeticity is borrowed from build systems. A build system is called hermetic if builds always produce the same artefacts regardless of changes in the environment they run on. Similarly, we call our deployments hermetic if they always result in the same deployed artefacts regardless of the environment’s change or the number of times they are executed.The behaviour of a service is rarely controlled by a single variable. The application that makes up your service is an important driver of its behaviour, but its configuration is an important contributor, for example. The behaviour for traditional microservices at Grab is dictated mainly by 3 versioned artefacts: application code, static and dynamic configuration.  Conveyor has been integrated with the systems that operate changes in each of these parameters. By tracking all 3 parameters at every deployment, Conveyor can reproducibly deploy microservices with similar behaviour: its deployments are hermetic.Building upon this property, Conveyor can ensure that all deployments made to production have been tested before with the same combination of parameters. This is valuable to us:  An outcome of staging deployments for a specific set of parameters is a good predictor of outcomes in production deployments for the same set of parameters and thus it makes testing in staging more relevant.  Rollbacks are hermetic; we never rollback to a combination of parameters that has not been used previously.In the past, incidents had resulted from an application rollback not compatible with the current dynamic configuration version; this was aggravating since rollbacks are expected to be a safe recovery mechanism. The introduction of hermetic deployments has largely eliminated this category of problems.Hermeticity is maintained by registering the deployment parameters as artefacts after each successfully completed pipeline. Users must then select one of the registered deployment metadata to promote to production.At this point, you might be wondering: why not use a single pipeline that includes both staging and production deployments? This was indeed how it started, with a single pipeline spanning multiple environments. However, engineers soon complained about it.The most obvious reason for the complaint was that less than 20% of changes deployed in staging will make their way to production. This meant that engineers would have toil associated with each completed staging deployment since the pipeline must be manually cancelled rather than continued to production.The other reason is that this multi-environment pipeline approach reduced flexibility when promoting changes to production. There are different ways to apply changes to a cluster. For example, lengthy pipelines that refresh instances can be used to deploy any combination of changes, while there are quicker pipelines restricted to dynamic configuration changes (such as feature flags rollouts). Regardless of the order in which the changes are made and how they are applied, Conveyor tracks the change.Eventually, engineers promote a deployment artefact to production. However they do not need to apply changes in the same sequence with which were applied to staging. Furthermore, to prevent erroneous actions, Conveyor presents only changes that can be applied with the requested pipeline (and sometimes, no changes are available). Not being forced into a specific method of deploying changes is one of added benefits of hermetic deployments.Returning to Our Journey Towards Engineering EfficiencyIf you can recall, the first part of this blog post series ended with a description of staging deployment. Our deployment to production starts with a verification that we uphold our hermeticity principle, as explained above.Our production deployment pipelines can run for several hours for large clusters with rolling releases (few run for days), so we start by acquiring locks to ensure there are no concurrent deployments for any given cluster.Before making any changes to the environment, we automatically generate release notes, giving engineers a chance to abort if the wrong set of changes are sent to production.The pipeline next waits for a deployment slot. Early on, engineers adopted deployment windows that coincide with office hours, such that if anything goes wrong, there is always someone on hand to help. Prior to the introduction of Conveyor, however, engineers would manually ask a Slack bot for approval. This interaction is now automated, and the only remaining action left is for the engineer to approve that the deployment can proceed via a single click, in line with our hands-off deployment principle.When the canary is in production, Conveyor automates monitoring it. This process is similar to the one already discussed in the first part of this blog post: Engineers can configure a set of alerts that Conveyor will keep track of. As soon as any one of the alerts is triggered, Conveyor automatically rolls back the service.If no alert is raised for the duration of the monitoring period, Conveyor waits again for a deployment slot. It then publishes the release notes for that deployment and completes the deployments for the cluster. After the lock is released and the deployment registered, the pipeline finally comes to its successful completion.Benefits of Our Journey Towards Engineering EfficiencyAll these improvements made over the last 2 years have reduced the effort spent by engineers on deployment while also reducing the failure rate of our deployments.If you are an engineer working on DevOps in your organisation, you know how hard it can be to measure the impact you made on your organisation. To estimate the time saved by our pipelines, we can model the activities that were previously done manually with a rudimentary weighted graph. In this graph, each edge carries a probability of the activity being performed (100% when unspecified), while each vertex carries the time taken for that activity.Focusing on our regular staging deployments only, such a graph would look like this:The overall amount of effort automated by the staging pipelines () is represented in the graph above. It can be converted into the equation below:This equation shows that for each staging deployment, around 16 minutes of work have been saved. Similarly, for regular production deployments, we find that 67 minutes of work were saved for each deployment:Moreover, efficiency was not the only benefit brought by the use of deployment pipelines for our traditional microservices. Surprisingly perhaps, the rate of failures related to production changes is progressively reducing while the amount of production changes that were made with Conveyor increased across the organisation (starting at 1.5% of failures per deployments, and finishing at 0.3% on average over the last 3 months for the period of data collected):Keep Calm and AutomateSince the first draft for this post was written, we’ve made many more improvements to our pipelines. We’ve begun automating Database Migrations; we’ve extended our set of hermetic variables to Amazon Machine Image (AMI) updates; and we’re working towards supporting container deployments.Through automation, all of Conveyor’s deployment pipelines have contributed to save more than 5,000 man-days of efforts in 2020 alone, across all supported teams. That’s around 20 man-years worth of effort, which is around 3 times the capacity of the team working on the project! Investments in our automation pipelines have more than paid for themselves, and the gains go up every year as more workflows are automated and more teams are onboarded.If Conveyor has saved efforts for engineering teams, has it then helped to improve velocity? I had opened the first part of this blog post with figures on the deployment funnel for microservice teams at Grab, towards the end of 2018. So where do the figures stand today for these teams?In the span of 2 years, the average number of build and staging deployment performed each day has not varied much. However, in the last 3 months of 2020, engineers have sent twice more changes to production than they did for the same period in 2018.Perhaps the biggest recognition received by the team working on the project, was from Grab’s engineers themselves. In the 2020 internal NPS survey for engineering experience at Grab, Conveyor received the highest score of any tools (built in-house or not).All these improvements in efficiency for our engineers would never have been possible without the hard work of all team members involved in the project, past and present: Tanun Chalermsinsuwan, Aufar Gilbran, Deepak Ramakrishnaiah, Repon Kumar Roy (Kowshik), Su Han, Voislav Dimitrijevikj, Stanley Goh, Htet Aung Shine, Evan Sebastian, Qijia Wang, Oscar Ng, Jacob Sunny, Subhodip Mandal and many others who have contributed and collaborated with them.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/our-journey-to-continuous-delivery-at-grab-part2"
      }
      ,
    
      "how-we-improved-agent-chat-efficiency-with-ml": {
        "title": "How We Improved Agent Chat Efficiency with Machine Learning",
        "author": "suman-anandelisa-monacchidarrell-tayyun-zouwan-ling-guai",
        "tags": "[&quot;Engineering&quot;, &quot;Machine Learning&quot;, &quot;Consumer Support&quot;]",
        "category": "",
        "content": "In previous articles (see Grab’s in-house chat platform, workforce routing), we shared how chat has grown to become one of the primary channels for support in the last few years.With continuous chat growth and a new in-house tool, helping our agents be more efficient and productive was key to ensure a faster support time for our users and scale chat even further.Starting from the analysis on the usage of another third-party tool as well as some shadowing sessions, we realised that building a templated-based feature wouldn’t help. We needed to offer personalisation capabilities, as our consumer support specialists care about their writing style and tone, and using templates often feels robotic.We decided to build a machine learning model, called SmartChat, which offers contextual suggestions by leveraging several sources of internal data, helping our chat specialists type much faster, and hence serving more consumers.In this article, we are going to explain the process from problem discovery to design iterations, and share how the model was implemented from both a data science and software engineering perspective.How SmartChat WorksDiving Deeper into the ProblemAgent productivity became a key part in the process of scaling chat as a channel for support.After splitting chat time into all its components, we noted that agent typing time represented a big portion of the chat support journey, making it the perfect problem to tackle next.After some analysis on the usage of the third-party chat tool, we found out that even with functionalities such as canned messages, 85% of the messages were still free typed.Hours of shadowing sessions also confirmed that the consumer support specialists liked to add their own flair. They would often use the template and adjust it to their style, which took more time than just writing it on the spot. With this in mind, it was obvious that templates wouldn’t be too helpful, unless they provided some degree of personalisation.We needed something that reduces typing time and also:  Allows some degree of personalisation, so that answers don’t seem robotic and repeated.  Works with multiple languages and nuances, considering Grab operates in 8 markets, even some of the English markets have some slight differences in commonly used words.  It’s contextual to the problem and takes into account the user type, issue reported, and even the time of the day.  Ideally doesn’t require any maintenance effort, such as having to keep templates updated whenever there’s a change in policies.Considering the constraints, this seemed to be the perfect candidate for a machine learning-based functionality, which predicts sentence completion by considering all the context about the user, issue and even the latest messages exchanged.Usability is KeyTo fulfil the hypothesis, there are a few design considerations:  Minimising the learning curve for agents.  Avoiding visual clutter if recommendations are not relevant.To increase the probability of predicting an agent’s message, one of the design explorations is to allow agents to select the top 3 predictions (Design 1). To onboard agents, we designed a quick tip to activate SmartChat using keyboard shortcuts.By displaying the top 3 recommendations, we learnt that it slowed agents down as they started to read all options even if the recommendations were not helpful. Besides, by triggering this component upon every recommendable text, it became a distraction as they were forced to pause.In our next design iteration, we decided to leverage and reuse the interaction of SmartChat from a familiar platform that agents are using - Gmail’s Smart Compose. As agents are familiar with Gmail, the learning curve for this feature would be less steep. For first time users, agents will see a “Press tab” tooltip, which will activate the text recommendation. The tooltip will disappear after 5 times of use.To relearn the shortcut, agents can hover over the recommended text.How We Track ProgressKnowing that this feature would come in multiple iterations, we had to find ways to track how well we were doing progressively, so we decided to measure the different components of chat time.We realised that the agent typing time is affected by:  Percentage of characters saved. This tells us that the model predicted correctly, and also saved time. This metric should increase as the model improves.  Model’s effectiveness. The agent writes the least number of characters possible before getting the right suggestion, which should decrease as the model learns.  Acceptance rate. This tells us how many messages were written with the help of the model. It is a good proxy for feature usage and model capabilities.  Latency. If the suggestion is not shown in about 100-200ms, the agent would not notice the text and keep typing.ArchitectureThe architecture involves support specialists initiating the fetch suggestion request, which is sent for evaluation to the machine learning model through API gateway. This ensures that only authenticated requests are allowed to go through and also ensures that we have proper rate limiting applied.We have an internal platform called Catwalk, which is a microservice that offers the capability to execute machine learning models as a HTTP service. We used the Presto query engine to calculate and analyse the results from the experiment.Designing the Machine Learning ModelI am sure all of us can remember an experiment we did in school when we had to catch a falling ruler. For those who have not done this experiment, feel free to try it at home! The purpose of this experiment is to define a ballpark number for typical human reaction time (equations also included in the video link).Typically, the human reaction time ranges from 100ms to 300ms, with a median of about 250ms (read more here). Hence, we decided to set the upper bound for SmartChat response time to be 200ms while deciding the approach. Otherwise, the experience would be affected as the agents would notice a delay in the suggestions. To achieve this, we had to manage the model’s complexity and ensure that it achieves the optimal time performance.Taking into consideration network latencies, the machine learning model would need to churn out predictions in less than 100ms, in order for the entire product to achieve a maximum 200ms refresh rate.As such, a few key components were considered:  Model Tokenisation          Model input/output tokenisation needs to be implemented along with the model’s core logic so that it is done in one network request.      Model tokenisation needs to be lightweight and cheap to compute.        Model Architecture          This is a typical sequence-to-sequence (seq2seq) task so the model needs to be complex enough to account for the auto-regressive nature of seq2seq tasks.      We could not use pure attention-based models, which are usually state of the art for seq2seq tasks, as they are bulky and computationally expensive.        Model Service          The model serving platform should be executed on a low-level, highly performant framework.      Our proposed solution considers the points listed above. We have chosen to develop in Tensorflow (TF), which is a well-supported framework for machine learning models and application building.For Latin-based languages, we used a simple whitespace tokenizer, which is serialisable in the TF graph using the tensorflow-text package.import tensorflow_text as texttokenizer = text.WhitespaceTokenizer()For the model architecture, we considered a few options but eventually settled for a simple recurrent neural network architecture (RNN), in an Encoder-Decoder structure:  Encoder          Whitespace tokenisation      Single layered Bi-Directional RNN      Gated-Recurrent Unit (GRU) Cell            Decoder          Single layered Uni-Directional RNN      Gated-Recurrent Unit (GRU) Cell        Optimisation          Teacher-forcing in training, Greedy decoding in production      Trained with a cross-entropy loss function      Using ADAM (Kingma and Ba) optimiser      FeaturesTo provide context for the sentence completion tasks, we provided the following features as model inputs:  Past conversations between the chat agent and the user  Time of the day  User type (Driver-partners, Consumers, etc.)  Entrypoint into the chat (e.g. an article on cancelling a food order)These features give the model the ability to generalise beyond a simple language model, with additional context on the nature of contact for support. Such experiences also provide a better user experience and a more customised user experience.For example, the model is better aware of the nature of time in addressing “Good {Morning/Afternoon/Evening}” given the time of the day input, as well as being able to interpret meal times in the case of food orders. E.g. “We have contacted the driver, your {breakfast/lunch/dinner} will be arriving shortly”.Typeahead Solution for the User InterfaceWith our goal to provide a seamless experience in showing suggestions to accepting them, we decided to implement a typeahead solution in the chat input area. This solution had to be implemented with the ReactJS library, as the internal web-app used by our support specialist for handling chats is built in React.There were a few ways to achieve this:  Modify the Document Object Model (DOM) using Javascript to show suggestions by positioning them over the input HTML tag based on the cursor position.  Use a content editable div and have the suggestion span render conditionally.After evaluating the complexity in both approaches, the second solution seemed to be the better choice, as it is more aligned with the React way of doing things: avoid DOM manipulations as much as possible.However, when a suggestion is accepted we would still need to update the content editable div through DOM manipulation. It cannot be added to React’s state as it creates a laggy experience for the user to visualise what they type.Here is a code snippet for the implementation:import React, { Component } from 'react';import liveChatInstance from './live-chat';export class ChatInput extends Component { constructor(props) {   super(props);   this.state = {     suggestion: '',   }; } getCurrentInput = () =&gt; {   const { roomID } = this.props;   const inputDiv = document.getElementById(`input_content_${roomID}`);   const suggestionSpan = document.getElementById(     `suggestion_content_${roomID}`,   );   // put the check for extra safety in case suggestion span is accidentally cleared   if (suggestionSpan) {     const range = document.createRange();     range.setStart(inputDiv, 0);     range.setEndBefore(suggestionSpan);     return range.toString(); // content before suggestion span in input div   }   return inputDiv.textContent; }; handleKeyDown = async e =&gt; {   const { roomID } = this.props;   // tab or right arrow for accepting suggestion   if (this.state.suggestion &amp;&amp; (e.keyCode === 9 || e.keyCode === 39)) {     e.preventDefault();     e.stopPropagation();     this.insertContent(this.state.suggestion);     this.setState({ suggestion: '' });   }   const parsedValue = this.getCurrentInput();   // space   if (e.keyCode === 32 &amp;&amp; !this.state.suggestion &amp;&amp; parsedValue) {     // fetch suggestion     const prediction = await liveChatInstance.getSmartComposePrediction(       parsedValue.trim(), roomID);     this.setState({ suggestion: prediction })   } }; insertContent = content =&gt; {   // insert content behind cursor   const { roomID } = this.props;   const inputDiv = document.getElementById(`input_content_${roomID}`);   if (inputDiv) {     inputDiv.focus();     const sel = window.getSelection();     const range = sel.getRangeAt(0);     if (sel.getRangeAt &amp;&amp; sel.rangeCount) {       range.insertNode(document.createTextNode(content));       range.collapse();     }   } }; render() {   const { roomID } = this.props;   return (     &lt;div className=\"message_wrapper\"&gt;       &lt;div         id={`input_content_${roomID}`}         role={'textbox'}         contentEditable         spellCheck         onKeyDown={this.handleKeyDown}       &gt;         {!!this.state.suggestion.length &amp;&amp; (           &lt;span             contentEditable={false}             id={`suggestion_content_${roomID}`}           &gt;             {this.state.suggestion}           &lt;/span&gt;         )}       &lt;/div&gt;     &lt;/div&gt;   ); }}The solution uses the spacebar as the trigger for fetching the suggestion from the ML model and stores them in a React state. The ML model prediction is then rendered in a dynamically rendered span.We used the window.getSelection() and range APIs to:  Find the current input value  Insert the suggestion  Clear the input to type a new messageThe implementation has also considered the following:  Caching. API calls are made on every space character to fetch the prediction. To reduce the number of API calls, we also cached the prediction until it differs from the user input.  Recover placeholder. There are data fields that are specific to the agent and consumer, such as agent name and user phone number, and these data fields are replaced by placeholders for model training. The implementation recovers the placeholders in the prediction before showing it on the UI.  Control rollout. Since rollout is by percentage per country, the implementation has to ensure that only certain users can access predictions from their country chat model.  Aggregate and send metrics. Metrics are gathered and sent for each chat message.ResultsThe initial experiment results suggested that we managed to save 20% of characters, which improved the efficiency of our agents by 12% as they were able to resolve the queries faster. These numbers exceeded our expectations and as a result, we decided to move forward by rolling SmartChat out regionally.What’s Next?In the upcoming iteration, we are going to focus on non-Latin language support, caching, and continuous training.Non-Latin Language Support and CachingThe current model only works with Latin languages, where sentences consist of space-separated words. We are looking to provide support for non-Latin languages such as Thai and Vietnamese. The result would also be cached in the frontend to reduce the number of API calls, providing the prediction faster for the agents.Continuous TrainingThe current machine learning model is built with training data derived from historical chat data. In order to teach the model and improve the metrics mentioned in our goals, we will enhance the model by letting it learn from data gathered in day-to-day chat conversations. Along with this, we are going to train the model to give better responses by providing more context about the conversations.Seeing how effective this solution has been for our chat agents, we would also like to expose this to the end consumers to help them express their concerns faster and improve their overall chat experience.Special thanks to Kok Keong Matthew Yeow, who helped to build the architecture and implementation in a scalable way.—-Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/how-we-improved-agent-chat-efficiency-with-ml"
      }
      ,
    
      "learn-how-grab-leveraged-performance-marketing-automation": {
        "title": "How Grab Leveraged Performance Marketing Automation to Improve Conversion Rates by 30%",
        "author": "sc-ngherman-khooaudrey-jeevaibhav-vijmilhad-miah",
        "tags": "[&quot;Automation&quot;, &quot;Engineering&quot;, &quot;Marketing&quot;]",
        "category": "",
        "content": "Grab, Southeast Asia’s leading superapp, is a hyperlocal three-sided marketplace that operates across hundreds of cities in Southeast Asia. Grab started out as a taxi hailing company back in 2012 and in less than a decade, the business has evolved tremendously and now offers a diverse range of services for consumers’ everyday needs.To fuel our business growth in newer service offerings such as GrabFood, GrabMart and GrabExpress, user acquisition efforts play a pivotal role in ensuring we create a sustainable Grab ecosystem that balances the marketplace dynamics between our consumers, driver-partners and merchant-partners.Part of our user growth strategy is centred around our efforts in running direct-response app campaigns to increase trials on our superapp offerings. Executing these campaigns brings about a set of unique challenges against the diverse cultural backdrop present in Southeast Asia, challenging the team to stay hyperlocal in our strategies while driving user volumes at scale. To address these unique challenges, Grab’s performance marketing team is constantly seeking ways to leverage automation and innovate on our operations, improving our marketing efficiency and effectiveness.Managing Grab’s Ever-expanding Business, Geographical Coverage and New User AcquisitionGrab’s ever-expanding services, extensive geographical coverage and hyperlocal strategies result in an extremely dynamic, yet complex ad account structure. This also means that whenever there is a new business vertical launch or hyperlocal campaign, the team would spend valuable hours rolling out a large volume of new ads across our accounts in the region.   A sample of our Google Ads account structure.The granular structure of our Google Ads account provided us with flexibility to execute hyperlocal strategies, but this also resulted in thousands of ad groups that had to be individually maintained.In 2019, Grab’s growth was simply outpacing our team’s resources and we finally hit a bottleneck. This challenged the team to take a step back and make the decision to pursue a fully automated solution built on the following principles for long term sustainability:      Building ad-tech solutions in-house instead of acquiring off-the-shelf solutions    Grab’s unique business model calls for several tailor-made features, none of which the existing ad tech solutions were able to provide.        Shifting our mindset to focus on the infinite game    In order to sustain the exponential volume in the ads we run, we had to seek the path of automation.  For our very first automation project, we decided to look into automating creative refresh and upload for our Google Ads account. With thousands of ad groups running multiple creatives each, this had become a growing problem for the team. Overtime, manually monitoring these creatives and refreshing them on a regular basis had become impossible.The Automation FundamentalsGrab’s superapp nature means that any automation solution fundamentally needs to be robust:  Performance-driven - to maintain and improve conversion efficiency over time  Flexibility -  to fit needs across business verticals and hyperlocal execution  Inclusivity - to account for future service launches and marketing tech (e.g. product feeds and more)  Scalability - to account for future geography/campaign coverageWith these in mind, we incorporated them in our requirements for the custom creative automation tool we planned to build.      Performance-driven - while many advertising platforms, such as Google’s App Campaigns, have built-in algorithms to prevent low-performing creatives from being served, the fundamental bottleneck lies in the speed in which these low-performing creatives can be replaced with new assets to improve performance. Thus, solving this bottleneck would become the primary goal of our tool.        Flexibility - to accommodate our broad range of services, geographies and marketing objectives, a mapping logic would be required to make sure the right creatives are added into the right campaigns.    To solve this, we relied on a standardised creative naming convention, using key attributes in the file name to map an asset to a specific campaign and ad group based on:          Market      City      Service type      Language      Creative theme      Asset type      Campaign optimisation goal            Inclusivity - to address coverage of future service offerings and interoperability with existing ad-tech vendors, we designed and built our tool conforming to many industry API and platform standards.        Scalability - to ensure full coverage of future geographies/campaigns, the in-house solution’s frontend and backend had to be robust enough to handle volume. Working hand in glove with Google, the solution was built by leveraging multiple APIs including Google Ads and Youtube to host and replace low-performing assets across our ad groups. The solution was then deployed on AWS’ serverless compute engine.  Enter CARACARA is an automation tool that scans for any low-performing creatives and replaces them with new assets from our creative library:  A sneak peek of how CARA worksIn a controlled experimental launch, we saw nearly 2,000 underperforming assets automatically replaced across more than 8,000 active ad groups, translating to an 18-30% increase in clickthrough and conversion rates.  A subset of results from CARA's experimental launchThrough automation, Grab’s performance marketing team has been able to significantly improve clickthrough and conversion rates while saving valuable man-hours. We have also established a scalable foundation for future growth. The best part? We are just getting started.Authored on behalf of the performance marketing team @ Grab. Special thanks to the CRM data analytics team, particularly Milhad Miah and Vaibhav Vij for making this a reality.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/learn-how-grab-leveraged-performance-marketing-automation"
      }
      ,
    
      "reducing-your-go-binary-size": {
        "title": "One Small Step Closer to Containerising Service Binaries",
        "author": "stan-halkasamuel-thomas",
        "tags": "[&quot;Back End&quot;, &quot;Engineering&quot;, &quot;Golang&quot;, &quot;Cloud-Native Transformations&quot;, &quot;Containerisation&quot;, &quot;Kubernetes&quot;]",
        "category": "",
        "content": "Grab’s engineering teams currently own and manage more than 250+ microservices. Depending on the business problems that each team tackles, our development ecosystem ranges from Golang, Java, and everything in between.Although there are centralised systems that help automate most of the build and deployment tasks, there are still some teams working on different technologies that manage their own build, test and deployment systems at different maturity levels. Managing a varied build and deploy ecosystems brings their own challenges.Build challenges  Broken external dependencies.  Non-reproducible builds due to changes in AMI, configuration keys and other build parameters.  Missing security permissions between different repositories.Deployment challenges  Varied deployment environments necessitating a bigger learning curve.  Managing the underlying infrastructure as code.  Higher downtime when bringing the systems up after a scale down event.Grab’s appetite for consumer obsession and quality drives the engineering teams to innovate and deliver value rapidly. The time that the team spends in fixing build issues or deployment-related tasks has a direct impact on the time they spend on delivering business value.Introduction to ContainerisationUsing the Container architecture helps the team deploy and run multiple applications, isolated from each other, on the same virtual machine or server and with much less overhead.At Grab, both the platform and the core engineering teams wanted to move to the containerisation architecture to achieve the following goals:  Support to build and push container images during the CI process.  Create a standard virtual machine image capable of running container workloads. The AMI is maintained by a central team and comes with Grab infrastructure components such as (DataDog, Filebeat, Vault, etc.).  A deployment experience which allows existing services to migrate to container workload safely by initially running both types of workloads concurrently.The core engineering teams wanted to adopt container workloads to achieve the following benefits:  Provide a containerised version of the service that can be run locally and on different cloud providers without any dependency on Grab’s internal (runtime) tooling.  Allow reuse of common Grab tools in different projects by running the zero dependency version of the tools on demand whenever needed.  Allow a more flexible staging/dev/shadow deployment of new features.Adoption of ContainerisationEngineering teams at Grab use the containerisation model to build and deploy services at scale. Our containerisation efforts help the development teams move faster by:  Providing a consistent environment across development, testing and production  Deploying software efficiently  Reducing infrastructure cost  Abstracting OS dependency  Increasing scalability between cloud vendorsWhen we started using containers we realised that building smaller containers had some benefits over bigger containers. For example, smaller containers:  Include only the needed libraries and therefore are more secure.  Build and deploy faster as they can be pulled to the running container cluster quickly.  Utilise disk space and memory efficiently.During the course of containerising our applications, we noted that some service binaries appeared to be bigger (~110 MB) than they should be. For a statically-linked Golang binary, that’s pretty big! So how do we figure out what’s bloating the size of our binary?Go Binary Size Visualisation ToolIn the course of poking around for tools that would help us analyse the symbols in a Golang binary, we found go-binsize-viz based on this article. We particularly liked this tool, because it utilises the existing Golang toolchain (specifically, Go tool nm) to analyse imports, and provides a straightforward mechanism for traversing through the symbols present via treemap. We will briefly outline the steps that we did to analyse a Golang binary here.      First, build your service using the following command (important for consistency between builds):    $ go build -a -o service_name ./path/to/main.go        Next, copy the binary over to the cloned directory of go-binsize-viz repository.      Run the following script that covers the steps in the go-binsize-viz README.    #!/usr/bin/env bash## This script needs more input parsing, but it serves the needs for now.#mkdir dist# step 1go tool nm -size $1 | c++filt &gt; dist/$1.symtab# step 2python3 tab2pydic.py dist/$1.symtab &gt; dist/$1-map.py# step 3# must be data.jspython3 simplify.py dist/$1-map.py &gt; dist/$1-data.jsrm data.jsln -s dist/$1-data.js data.js        Running this script creates a dist folder where each intermediate step is deposited, and a data.js symlink in the top-level directory which points to the consumable .js file by treemap.html.    # top-level directory$ ll-rw-r--r--   1 stan.halka  staff   1.1K Aug 20 09:57 README.md-rw-r--r--   1 stan.halka  staff   6.7K Aug 20 09:57 app3.js-rw-r--r--   1 stan.halka  staff   1.6K Aug 20 09:57 cockroach_sizes.htmllrwxr-xr-x   1 stan.halka  staff        65B Aug 25 16:49 data.js -&gt; dist/v2.0.709356.segments-paxgroups-macos-master-go1.13-data.jsdrwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 dist...# dist folder$ ll disttotal 71728drwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 .drwxr-xr-x  21 stan.halka  staff   672B Aug 25 16:49 ..-rw-r--r--   1 stan.halka  staff   4.2M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-data.js-rw-r--r--   1 stan.halka  staff   3.4M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-map.py-rw-r--r--   1 stan.halka  staff    11M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13.symtab        As you can probably tell from the file names, these steps were explored on the segments-paxgroups service, which is a microservice used for segment information at Grab. You can ignore the versioning metadata, branch name, and Golang information embedded in the name.        Finally, run a local python3 server to visualise the binary components.    $ python3 -m http.serverServing HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...        So now that we have a methodology to consistently generate a service binary, and a way to explore the symbols present, let’s dive in!        Open your browser and visit http://localhost:8000/treemap_v3.html:    Of the 103MB binary produced, 81MB are recognisable, with 66MB recognised as Golang (UNKNOWN is present, and also during parsing there were a fair number of warnings. Note that we haven’t spent enough time with the tool to understand why we aren’t able to recognise and index all the symbols present).          The next step is to figure out where the symbols are coming from. There’s a bunch of Grab-internal stuff that for the sake of this blog isn’t necessary to go into, and it was reasonably easy to come to the right answer based on the intuitiveness of the go-binsize-viz tool.    This visualisation shows us the source of how 11 MB of symbols are sneaking into the segments-paxgroups binary.          Every message format for any service that reads from, or writes to, streams at Grab is included in every service binary! Not cloud native!  How did This Happen?The short answer is that Golang doesn’t import only the symbols that it requires, but rather all the symbols defined within an imported directory and transitive symbols as well. So, when we think we’re importing just one directory, if our code structure doesn’t follow principles of encapsulation or isolation, we end up importing 11 MB of symbols that we don’t need! In our case, this occurred because a generic Message interface was included in the same directory with all the auto-generated code you see in the pretty picture above.The Streams team did an awesome job of restructuring the code, which when built again, led to this outcome:$$ ll | grep paxgroups-rwxr-xr-x   1 stan.halka  staff   110M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-master-go1.12-rwxr-xr-x   1 stan.halka  staff   103M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-master-go1.13-rwxr-xr-x   1 stan.halka  staff        80M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-tinkered-go1.12-rwxr-xr-x   1 stan.halka  staff        78M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-tinkered-go1.13Not a bad reduction in service binary size!Lessons LearntThe go-binsize-viz utility offers a treemap representation for imported symbols, and is very useful in determining what symbols are contributing to the overall size.Code architecture matters: Keep binaries as small as possible!To reduce your binary size, follow these best practices:  Structure your code so that the interfaces and common classes/utilities are imported from different locations than auto-generated classes.  Avoid huge, flat directory structures.  If it’s a platform offering and has too many interwoven dependencies, try to decouple the actual platform offering from the company specific instantiations. This fosters creating isolated, minimalistic code.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/reducing-your-go-binary-size"
      }
      ,
    
      "customer-support-workforce-routing": {
        "title": "Customer Support Workforce Routing",
        "author": "suman-anandelisa-monacchijasmine-limmatthew-yeowpengcheng-zhao",
        "tags": "[&quot;Workforce Routing&quot;, &quot;Chat&quot;, &quot;Product&quot;, &quot;Routing&quot;, &quot;Queueing&quot;, &quot;Customer Support&quot;]",
        "category": "",
        "content": "IntroductionWith Grab’s wide range of services, we get large volumes of queries a day. Our Customer Support teams address concerns and issues from safety issues to general FAQs. The teams delight our consumers through quick resolutions, resulting from world-class support framework and an efficient workforce routing system.Our routing workforce system ensures that available resources are efficiently assigned to a request based on the right skillset and deciding factors such as department, country, request priority. Scalability to work across support channels (e.g. voice, chat, or digital) is also another factor considered for routing a request to a particular support specialist.   Sample Livechat flow - How it works today  Having an efficient workforce routing system ensures that requests are directed to relevant support specialists who are most suited to handle a certain type of issue, resulting in quicker resolution, happier and satisfied consumers, and reduced cost spent on support.We initially implemented a third-party solution, however there were a few limitations, such as prioritisation, that motivated us to build our very own routing solution that provides better routing configuration controls and cost reduction from licensing costs.This article describes how we built our in-house workforce routing system at Grab and focuses on Livechat, one of the domains of consumer support.ProblemLet’s run through the issues with our previous routing solution in the next sections.Priority ManagementThe third-party solution didn’t allow us to prioritise a group of requests over others. This was particularly important for handling safety issues that were not impacted due to other low-priority requests like enquiries. So our goal for the in-house solution was to ensure that we were able to configure the priority of the request queues.Bespoke Product CustomisationWith the third-party solution being a generic service provider, customisations often required long lead times as not all product requests from Grab were well received by the mass market. Building this in-house meant Grab had full controls over the design and configuration over routing. Here are a few sample use cases that were addressed by customisation:  Bulk configuration changes - Previously, it was challenging to assign the same configuration to multiple agents. So, we introduced another layer of grouping for agents that share the same configuration. For example, which queues the agents receive chats from and what the proficiency and max concurrency should be.  Resource Constraints - To avoid overwhelming resources with unlimited chats and maintaining reasonable wait times for our consumers, we introduced a dynamic queue limit on the number of chat requests enqueued. This limit was based on factors like the number of incoming chats and the agent performance over the last hour.  Remote Work Challenges - With the pandemic situation and more of our agents working remotely, network issues were common. So we released an enhancement on the routing system to reroute chats handled by unavailable agents (due to disconnection for an extended period) to another available agent. The seamless experience helped increase consumer satisfaction.Reporting and AnalyticsSimilar to previous point, having a solution addressing generic use cases didn’t allow us to add further customisations for monitoring. With the custom implementation, we were able to add more granular metrics that are very useful to assess the agent productivity and performance, which helps in planning the resources ahead of time. This is why reporting and analytics were so valuable for workforce planning. Few of the customisations added additionally were:  Agent Time Utilisation - While basic agent tracking was available in the out-of-the-box solution, it limited users to three states (online, away, and invisible). With the custom routing solution, we were able to create customised statuses to reflect the time the agent spent in a particular state due to chat connection issues and failures and reflect this on dashboards for immediate attention.  Chat Transfers - The number of chat transfers could only be tabulated manually. We then automated this process with a custom implementation.SolutionNow that we’ve covered the issues we’re solving, let’s go over the solutions.Prioritising High-priority RequestsDuring routing, the constraint is on the number of resources available. The incoming requests cannot simply be assigned to the first available agent. The issue with this approach is that we would eventually run out of agents to serve the high-priority requests.One of the ways to prevent this is to have a separate group of agents to solely handle high-priority requests. This does not solve issues as the high-priority requests and low-priority requests share the same queue and are de-queued in a First-In, First-out (FIFO) order. As a result, the low-priority requests are directly processed instead of waiting for the queue to fill up before processing high-priority requests. Because of this queuing issue, prioritisation of requests is critical.The Need to PrioritiseHigh-priority requests, such as safety issues, must not be in the queue for a long duration and should be handled as fast as possible even when the system is filled with low-priority requests.There are two different kinds of queues: one to handle requests at priority level and the other to handle individual issues that are on the business queues on which the queue limit constraints apply.To illustrate further, here are two different scenarios of enqueuing/de-queuing:Different Issues with Different PrioritiesIn this scenario, the priority is set to de-queue safety issues, which are in the high-priority queue, before picking up the enquiry issues from the low-priority queue.   Different issues with different priorities  Identical Issues with Different PrioritiesIn this scenario where identical issues have different priorities, the reallocated enquiry issue in the high-priority queue is de-queued first before picking up a low-priority enquiry issue. Reallocations happen when a chat is transferred to another agent or when it was not accepted by the allocated agent. When reallocated, it goes back to the queue with a higher priority.   Identical issues with different priorities  ApproachTo implement different levels of priorities, we decided to use separate queues for each of the priorities and denoted the request queues by groups, which could logically exist in any of the priority queues.For de-queueing, time slices of varied lengths were assigned to each of the queues to make sure the de-queueing worker spends more time on a higher priority queue.The architecture uses multiple de-queueing workers running in parallel, with each worker looping over the queues and waiting for a message in a queue for a certain amount of time, and then allocating it to an agent.for i := startIndex; i &lt; len(consumer.priorityQueue); i++ { queue := consumer.priorityQueue[i] duration := queue.config.ProcessingDurationInMilliseconds for now := time.Now(); time.Since(now) &lt; time.Duration(duration)*time.Millisecond; {   consumer.processMessage(queue.client, queue.config)   // cool down   time.Sleep(time.Millisecond * 100) }}The above code snippet iterates over individual priority queues and waits for a message for a certain duration, it then processes the message upon receipt. There is also a cooldown period of 100ms before it moves on to receive a message from a different priority queue.The caveat with the above approach is that the worker may end up spending more time than expected when it receives a message at the end of the waiting duration. We addressed this by having multiple workers running concurrently.Request StarvationNow when priority queues are used, there is a possibility that some of the low-priority requests remain unprocessed for long periods of time. To ensure that this doesn’t happen, the workers are forced to run out of sync by tweaking the order in which priority queues are processed, such that when worker1 is processing a high-priority queue request, worker2 is waiting for a request in the medium-priority queue instead of the high-priority queue.Customising to Our NeedsWe wanted to make sure that agents with the adequate skills are assigned to the right queues to handle the requests. On top of that, we wanted to ensure that there is a limit on the number of requests that a queue can accept at a time, guaranteeing that the system isn’t flushed with too many requests, which can lead to longer waiting times for request allocation.ApproachThe queues are configured with a dynamic queue limit, which is the upper limit on the number of requests that a queue can accept. Additionally attributes such as country, department, and skills are defined on the queue.The dynamic queue limit takes account of the utilisation factor of the queue and the available agents at the given time, which ensures an appropriate waiting time at the queue level.A simple approach to assign which queues the agents can receive the requests from is to directly assign the queues to the agents. But this leads to another problem to solve, which is to control the number of concurrent chats an agent can handle and define how proficient an agent is at solving a request. Keeping this in mind, it made sense to have another grouping layer between the queue and agent assignment and to define attributes, such as concurrency, to make sure these groups can be reused.   Agent assignment  There are three entities in agent assignment:  Queue  Agent Group  AgentWhen the request is de-queued, the agent list mapped to the queue is found and then some additional business rules (e.g. proficiency check) are applied to calculate the eligibility score of each mapped agent to decide which agent is the best suited to cater to the request.The factors impacting the eligibility score are proficiency (whether the agent is online/offline), current concurrency, max concurrency, and last allocation time.Ensuring the Concurrency is Not BreachedTo make sure that the agent doesn’t receive more chats than their defined concurrency, a locking mechanism is used at per agent level. During agent allocation, the worker acquires a lock on the agent record with an expiry, preventing other workers from allocating a chat to this agent. Only once the allocation process is complete (either failed or successful), the concurrency is updated and the lock is released, allowing other workers to assign more chats to the agent depending on the bandwidth.A similar approach was used to ensure that the queue limit doesn’t exceed the desired limit.Reallocation and TransfersHaving the routing configuration setup, the reallocation of agents is done using the same steps for agent allocation.To transfer a chat to another queue, the request goes back to the queue with a higher priority so that the request is assigned faster.Unaccepted ChatsIf the agent fails to accept the request in a given period of time, then the request is put back into the queue, but this time with a higher priority. This is the reason why there’s a corresponding re-allocation queue with a higher priority than the normal queue to make sure that those unaccepted requests don’t have to wait in the queue again.Informing the Frontend about AllocationWhen an allocation of an agent happens, the routing system needs to inform the frontend by sending messages over websocket to the frontend. This is done with our super reliable messaging system called Hermes, which operates at scale in supporting 12k concurrent connections and establishes real-time communication between agents and consumers.Finding the Online AgentsThe routing system should only send the allocation message to the frontend when the agent is online and accepting requests. Frontend uses the same websocket connection used to receive the allocation message to inform the routing system about the availability of agents. This means that if for some reason, the websocket connection is broken due to internet connection issues, the agent would stop receiving any new chat requests.Enriched Reporting and AnalyticsThe routing system is able to push monitoring metrics, such as number of online agents, number of chat requests assigned to the agent, and so on. Because of the fine-grained control that comes with building this system in-house, it gives us the ability to push more custom metrics.There are two levels of monitoring offered by this system: real-time monitoring and non-real time monitoring. They can be used for analytics for calculating things like the productivity of the agent and the time they spent on each chat.We achieved the discussed solutions with the help of StatsD for real-time monitoring and for analytical purposes. We sent the data used for Tableau visualisations and reporting to Presto tables.Given that the bottleneck for this system is the number of resources (i.e. number of agents), the real time monitoring helps identify which configuration needs to be adjusted when there is a spike in the number of requests. Moreover, the analytical persistent data allows us the ability to predict the traffic and plan the workforce management such that they are efficiently handling the requests.ScalabilityLetting the system behave appropriately when rolled out to multiple regions is a very critical piece that needed to be taken into account. To ensure that there were enough workers to handle requests, horizontal scaling of instances was set when the CPU utilisation increases.Now to understand the system limitations and behaviour before releasing to multiple regions, we ran load tests with 10x more traffic than expected. This gave us the understanding on what monitors and alerts we should add to make sure the system is able to function efficiently and reduce our recovery time if something goes wrong.Next StepsWe have lined up a few enhancements to reduce the consumer wait time and the time spent by the agents on unresponsive consumers. Aside from chats, we plan to implement this solution to handle digital issues (social media and emails) and voice requests (call).Special thanks to Andrea Carlevato and Karen Kue for making sure that the blogpost is interesting and represents the problem we solved accurately.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/customer-support-workforce-routing"
      }
      ,
    
      "mirror-cache-blog": {
        "title": "Serving Driver-partners Data at Scale Using Mirror Cache",
        "author": "indrajit-sarkar",
        "tags": "[&quot;Mirror Cache&quot;, &quot;Data at Scale&quot;]",
        "category": "",
        "content": "Since the early beginnings, driver-partners have been the centrepiece of the wide-range of  services or features provided by the Grab platform. Over time, many backend microservices were developed to support our driver-partners such as earnings, ratings, insurance, etc. All of these different microservices require certain information, such as name, phone number, email, active car types, and so on, to curate the services provided to the driver-partners.We built the Drivers Data service to provide drivers-partners data to other microservices. The service attracts a high QPS and handles 10K requests per second during peak hours. Over the years, we have tried different strategies to serve driver-partners data in a resilient and cost-effective manner, while accounting for low response time. In this blog post, we talk about mirror cache, an in-memory local caching solution built to serve driver-partners data efficiently.What We Started With   Figure 1. Drivers Data service architectureOur Drivers Data service previously used MySQL DB as persistent storage and two caching layers - standalone local cache (RAM of the EC2 instances) as primary cache and Redis as secondary for eventually consistent reads. With this setup, the cache hit ratio was very low.   Figure 2. Request flow chartWe opted for a cache aside strategy. So when a client request comes, the Drivers Data service responds in the following manner:  If data is present in the in-memory cache (local cache), then the service directly sends back the response.  If data is not present in the in-memory cache and found in Redis, then the service sends back the response and updates the local cache asynchronously with data from Redis.  If data is not present either in the in-memory cache or Redis, then the service responds back with the data fetched from the MySQL DB and updates both Redis and local cache asynchronously.   Figure 3. Percentage of response from different sourcesThe measurement of the response source revealed that during peak hours ~25% of the requests were being served via standalone local cache, ~20% by MySQL DB, and ~55% via Redis.The low cache hit rate is caused by the driver-partners data loading patterns: low frequency per driver over time but the high frequency in a short amount of time. When a driver-partner is a candidate for a booking or is involved in an ongoing booking, different services make multiple requests to the Drivers Data service to fetch that specific driver-partner information. The frequency of calls for a specific driver-partner reduces if he/she is not involved in the booking allocation process or is not doing any booking at the moment.While low frequency per driver over time impacts the Redis cache hit rate, high frequency in short amounts of time mostly contributes to in-memory cache hit rate. In our investigations, we found that local caches of different nodes in the Drivers Data service cluster were making redundant calls to Redis and DB for fetching the same data that are already present in a node local cache.Making in-memory cache available on every instance while the data is in active use, we could greatly increase the in-memory cache hit rate, and that’s what we did.Mirror Cache Design GoalsWe set the following design goals:  Support a local least recently used (LRU) cache use-case.  Support active cache invalidation.  Support best effort replication between local cache instances (EC2 instances). If any instance successfully fetches the latest data from the database, then it should try to replicate or mirror this latest data across all the other nodes in the cluster. If replication fails and the item is expired or not found, then the nodes should fetch it from the database.  Support async data replication across nodes to ensure updates for the same key happens only with more recent data. For any older updates, the current data in the cache is ignored. The ordering of cache updates is not guaranteed due to the async replication.  Ability to handle auto-scaling.The Building Blocks   Figure 4. Mirror cacheThe mirror cache library runs alongside the Drivers Data service inside each of the EC2 instances of the cluster. The two main components are in-memory cache and replicator.In-memory CacheThe in-memory cache is used to store multiple key/value pairs in RAM. There is a TTL associated with each key/value pair. We wanted to use a cache that can provide high hit ratio, memory bound, high throughput, and concurrency. After evaluating several options, we went with dgraph’s open-source concurrent caching library Ristretto as our in-memory local cache. We were particularly impressed by its use of the TinyLFU admission policy to ensure a high hit ratio.ReplicatorThe replicator is responsible for mirroring/replicating each key/value entry among all the live instances of the Drivers Data service. The replicator has three main components: Membership Store, Notifier, and gRPC Server.Membership StoreThe Membership Store registers callbacks with our service discovery service to notify mirror cache in case any nodes are added or removed from the Drivers Data service cluster.It maintains two maps - nodes in the same AZ (AWS availability zone) as itself (the current node of the Drivers Data service in which mirror cache is running) and the nodes in the other AZs.NotifierEach service (Drivers Data) node runs a single instance of mirror cache. So effectively, each node has one notifier.  Combine several (key/value) pairs updates to form a batch.  Propagate the batch updates among all the nodes in the same AZ as itself.  Send the batch updates to exactly one notifier (node) in different AZs who, in turn, are responsible for updating all the nodes in their own AZs with the latest batch of data. This communication technique helps to reduce cross AZ data transfer overheads.In the case of auto-scaling, there is a warm-up period during which the notifier doesn’t notify the other nodes in the cluster. This is done to minimise duplicate data propagation. The warm-up period is configurable.gRPC ServerAn exclusive gRPC server runs for mirror cache. The different nodes of the Drivers Data service use this server to receive new cache updates from the other nodes in the cluster.Here’s the structure of each cache update entity:message Entity {    string key = 1; // Key for cache entry.    bytes value = 2; // Value associated with the key.    Metadata metadata = 3; // Metadata related to the entity.    replicationType replicate = 4; // Further actions to be undertaken by the mirror cache after updating its own in-memory cache.    int64 TTL = 5; // TTL associated with the data.    bool  delete = 6; // If delete is set as true, then mirror cache needs to delete the key from it's local cache.}enum replicationType {    Nothing = 0; // Stop propagation of the request.    SameRZ = 1; // Notify the nodes in the same Region and AZ.}message Metadata {    int64 updatedAt = 1; // Same as updatedAt time of DB.}The server first checks if the local cache should update this new value or not. It tries to fetch the existing value for the key. If the value is not found, then the new key/value pair is added. If there is an existing value, then it compares the updatedAt time to ensure that stale data is not updated in the cache.If the replicationType is Nothing, then the mirror cache stops further replication. In case the replicationType is SameRZ then the mirror cache tries to propagate this cache update among all the nodes in the same AZ as itself.Run at Scale   Figure 5. Drivers Data Service new architectureThe behaviour of the service hasn’t changed and the requests are being served in the same manner as before. The only difference here is the replacement of the standalone local cache in each of the nodes with mirror cache. It is the responsibility of mirror cache to replicate any cache updates to the other nodes in the cluster.After mirror cache was fully rolled out to production, we rechecked our metrics related to the response source and saw a huge improvement. The graph showed that during peak hours ~75% of the response was from in-memory local cache. About 15% of the response was served by MySQL DB and a further 10% via Redis.The local cache hit ratio was at 0.75, a jump of 0.5 from before and there was a 5% drop in the number of DB calls too.   Figure 6. New percentage of response from different sourcesLimitations and Future ImprovementsMirror cache is eventually consistent, so it is not a good choice for systems that need strong consistency.Mirror cache stores all the data in volatile memory (RAM) and they are wiped out during deployments, resulting in a temporary load increase to Redis and DB.Also, many new driver-partners are added every day to the Grab system, and we might need to increase the cache size to maintain a high hit ratio. To address these issues we plan to use SSD in the future to store a part of the data and use RAM only to store hot data.ConclusionMirror cache really helped us scale the Drivers Data service better and serve driver-partners data to the different microservices at low latencies. It also helped us achieve our original goal of an increase in the local cache hit ratio.We also extended mirror cache in some other services and found similar promising results.A huge shout out to Haoqiang Zhang and Roman Atachiants for their inputs into the final design. Special thanks to the Driver Backend team at Grab for their contribution.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/mirror-cache-blog"
      }
      ,
    
      "grabmart-product-team-experience": {
        "title": "The GrabMart Journey",
        "author": "clarisse-peraltashweta-padmanabanya-gaoeelin-ngiow",
        "tags": "[&quot;GrabMart&quot;, &quot;Product&quot;]",
        "category": "",
        "content": "Grab is Southeast Asia’s leading superapp, providing everyday services such as ride-hailing, food delivery, payments, and more. In this blog, we’d like to share our journey in discovering the need for GrabMart and coming together as a team to build it.Being There in the Time of NeedBack in March 2020, as the COVID-19 pandemic was getting increasingly widespread in Southeast Asia, people began to feel the pressing threat of the virus in carrying out their everyday activities. As social distancing restrictions tightened across Southeast Asia, consumers’ reliance on online shopping and delivery services also grew.Given the ability of our systems to readily adapt to changes, we were able to introduce a new service that our consumers needed - GrabMart. By leveraging the GrabFood platform and quickly onboarding retail partners, we can now provide consumers with their daily essentials on-demand, within a one hour delivery window.Beginning an ExperimentAs early as November 2019, Grab was already piloting the concept of GrabMart in Malaysia and Singapore in light of the growing online grocery shopping trend. Our Product team decided to first launch GrabMart as a category within GrabFood to quickly gather learnings with minimal engineering effort. Through this pilot, we were able to test the operational flow, identify the value proposition to our consumers, and expand our merchant selection.   GrabMart within the GrabFood flowWe learned that consumers had difficulty finding specific items as there was no search function available and they had to scroll through the full list of merchants on the app. Drivers who received GrabMart orders were not always prepared to accept the booking as the orders - especially larger ones - were not distinguished from GrabFood. Thanks to our agile Engineering teams, we fixed these issues efficiently, ensuring a smoother user experience.Redefining the Mart ExperienceWith the exponential growth of GrabMart regionally at 50% week over week (from around April to September), the team was determined to create a new version of GrabMart that better suited the needs of our users.Our user research validated our hypothesis that shopping for groceries online is completely different from ordering meals online. Replicating the user flow of GrabFood for GrabMart would have led us to completely miss the natural path consumers take at a grocery store on the app. For example, unlike ordering food, grocery shopping begins at an item-level instead of a merchant-level (like with GrabFood). Identifying this distinction led us to highlight item categories on both the GrabMart homepage and search results page. Other important user research highlights include:  Item/Store Categories. For users that already have a store in mind, they often look for the store directly. This behaviour is similar to the offline shopping behaviour. Users, who are unsure of where to find an item, search for it directly or navigate to item categories.  Add to Cart. When purchasing familiar items, users often add the items to cart without clicking to read more about the product. Product details are only viewed when purchasing newer items.  Scheduled Delivery. As far as delivery time goes, every consumer has different needs. Some  prefer paying a higher fee for faster  delivery, while others preferred waiting longer if it meant that the delivery fee was reduced.  Hence we decided to offer on-demand delivery for urgent purchases, and scheduled delivery for non-urgent buys.    The New GrabMart Experience In order to meet our timelines, we divided the deliverables into two main releases and got early feedback from internal users through our Grab Early Access (GEA) programme. Since GEA gives users a sneak-peek into upcoming app features, we can resolve any issues that they encounter before releasing the product to the general public. In addition, we made some large-scale changes required across multiple Grab systems, such as the order management system to account for the new mart order type, the allocation system to allocate the right type of driver for mart orders, and the merchant app and our Partner APIs to enable merchants to prepare mart orders efficiently.Coupled with user research and country insights on grocery shopping behaviour, we ruthlessly prioritised the features to be built. We introduced Item categories to cater to consumers who needed urgent restock of a few items, and Store categories for those shopping for their weekly groceries. We developed add-to-cart to make it easier for consumers to put items in their basket, especially if they have a long list of products to buy. Furthermore, we included a Scheduled Delivery option for our Indonesian consumers who want to receive their orders in person.Designing for Emotional StatesAs we implemented multiple product changes, we realised that we could not risk overwhelming our consumers with the amount of information we wanted to communicate. Thus, we decided to prominently display product images in the item category page and allocated space only for essential product details, such as price. Overall, we strived for an engaging design that balanced showing a mix of products, merchant offers, and our own data-driven recommendations.The Future of E-commerce“COVID-19 has accelerated the adoption of on-demand delivery services across Southeast Asia, and we were able to tap on existing technologies, our extensive delivery network, and operational footprint to quickly scale GrabMart across the region. In a post-COVID19 normal, we anticipate demand for delivery services to remain elevated. We will continue to double down on expanding our GrabMart service to support consumers’ shopping needs,” said Demi Yu, Regional Head of GrabFood and GrabMart.As the world embraces a new normal, we believe that online shopping will become even more essential in the months to come. Along with Grab’s Operations team, we continue to grow our partners on GrabMart so that we can become the most convenient and affordable choice for our consumers regionally. By enabling more businesses to expand online, we can then reach more of our consumers and meet their needs together.To learn more about GrabMart and its supported stores and features, click here.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/grabmart-product-team-experience"
      }
      ,
    
      "trident-real-time-event-processing-at-scale": {
        "title": "Trident - Real-time Event Processing at Scale",
        "author": "jie-zhangabdullah-mamun",
        "tags": "[&quot;A/B Testing&quot;, &quot;Event processing&quot;]",
        "category": "",
        "content": "Ever wondered what goes behind the scenes when you receive advisory messages on a confirmed booking? Or perhaps how you are awarded with rewards or points after completing a GrabPay payment transaction? At Grab, thousands of such campaigns targeting millions of users are operated daily by a backbone service called Trident. In this post, we share how Trident supports Grab’s daily business, the engineering challenges behind it, and how we solved them.   60-minute GrabMart delivery guarantee campaign operated via TridentWhat is Trident?Trident is essentially Grab’s in-house real-time if this, then that (IFTTT) engine, which automates various types of business workflows. The nature of these workflows could either be to create awareness or to incentivise users to use other Grab services.If you are an active Grab user, you might have noticed new rewards or messages that appear in your Grab account. Most likely, these originate from a Trident campaign. Here are a few examples of types of campaigns that Trident could support:  After a user makes a GrabExpress booking, Trident sends the user a message that says something like “Try out GrabMart too”.  After a user makes multiple ride bookings in a week, Trident sends the user a food reward as a GrabFood incentive.  After a user is dropped off at his office in the morning, Trident awards the user a ride reward to use on the way back home on the same evening.  If  a GrabMart order delivery takes over an hour of waiting time, Trident awards the user a free-delivery reward as compensation.  If the driver cancels the booking, then Trident awards points to the user as a compensation.  With the current COVID pandemic, when a user makes a ride booking, Trident sends a message to both the passenger and driver reminding about COVID protocols.Trident processes events based on campaigns, which are basically a logic configuration on what event should trigger what actions under what conditions. To illustrate this better, let’s take a sample campaign as shown in the image below. This mock campaign setup is taken from the Trident Internal Management portal.   Trident process flowThis sample setup basically translates to: for each user, count his/her number of completed GrabMart orders. Once he/she reaches 2 orders, send him/her a message saying “Make one more order to earn a reward”. And if the user reaches 3 orders, award him/her the reward and send a congratulatory message. 😁Other than the basic event, condition, and action, Trident also allows more fine-grained configurations such as supporting the overall budget of a campaign, adding limitations to avoid over awarding, experimenting A/B testing, delaying of actions, and so on.An IFTTT engine is nothing new or fancy, but building a high-throughput real-time IFTTT system poses a challenge due to the scale that Grab operates at. We need to handle billions of events and run thousands of campaigns on an average day. The amount of actions triggered by Trident is also massive.In the month of October 2020, more than 2,000 events were processed every single second during peak hours. Across the entire month, we awarded nearly half a billion rewards, and sent over 2.5 billion communications to our end-users.Now that we covered the importance of Trident to the business, let’s drill down on how we designed the Trident system to handle events at a massive scale and overcame the performance hurdles with optimisation.Architecture DesignWe designed the Trident architecture with the following goals in mind:  Independence: It must run independently of other services, and must not bring performance impacts to other services.  Robustness: All events must be processed exactly once (i.e. no event missed, no event gets double processed).  Scalability: It must be able to scale up processing power when the event volume surges and withstand when popular campaigns run.The following diagram depicts how the overall system architecture looks like.   Trident architectureTrident consumes events from multiple Kafka streams published by various backend services across Grab (e.g. GrabFood orders, Transport rides, GrabPay payment processing, GrabAds events). Given the nature of Kafka streams, Trident is completely decoupled from all other upstream services.Each processed event is given a unique event key and stored in Redis for 24 hours. For any event that triggers an action, its key is persisted in MySQL as well. Before storing records in both Redis and MySQL, we make sure any duplicate event is filtered out. Together with the at-least-once delivery guaranteed by Kafka, we achieve exactly-once event processing.Scalability is a key challenge for Trident. To achieve high performance under massive event volume, we needed to scale on both the server level and data store level. The following mind map shows an outline of our strategies.   Outline of Trident’s scale strategyScale ServersOur source of events are Kafka streams. There are mostly two factors that could affect the load on our system:  Number of events produced in the streams (more rides, food orders, etc. results in more events for us to process).  Number of campaigns running.  Nature of campaigns running. The campaigns that trigger actions for more users cause higher load on our system.There are naturally two types of approaches to scale up server capacity:  Distribute workload among server instances.  Reduce load (i.e. reduce the amount of work required to process each event).Distribute LoadDistributing workload seems trivial with the load balancing and auto-horizontal scaling based on CPU usage that cloud providers offer. However, an additional server sits idle until it can consume from a Kafka partition.Each Kafka partition can only be consumed by one consumer within the same consumer group (our auto-scaling server group in this case). Therefore, any scaling in or out requires matching the Kafka partition configuration with the server auto-scaling configuration.Here’s an example of a bad case of load distribution:   Kafka partitions config mismatches server auto-scaling configAnd here’s an example of a good load distribution where the configurations for the Kafka partitions and the server auto-scaling match:   Kafka partitions config matches server auto-scaling configWithin each server instance, we also tried to increase processing throughput while keeping the resource utilisation rate in check. Each Kafka partition consumer has multiple goroutines processing events, and the number of active goroutines is dynamically adjusted according to the event volume from the partition and time of the day (peak/off-peak).Reduce LoadYou may ask how we reduced the amount of processing work for each event. First, we needed to see where we spent most of the processing time. After performing some profiling, we identified that the rule evaluation logic was the major time consumer.What is Rule Evaluation?Recall that Trident needs to operate thousands of campaigns daily. Each campaign has a set of rules defined. When Trident receives an event, it needs to check through the rules for all the campaigns to see whether there is any match. This checking process is called rule evaluation.More specifically, a rule consists of one or more conditions combined with AND/OR Boolean operators. A condition consists of an operator with a left-hand side (LHS) and a right-hand side (RHS). The left-hand side is the name of a variable, and the right-hand side a value. A sample rule in JSON:Country is Singapore and taxi type is either JustGrab or GrabCar.  {    \"operator\": \"and\",    \"conditions\": [    {      \"operator\": \"eq\",      \"lhs\": \"var.country\",      \"rhs\": \"sg\"      },      {        \"operator\": \"or\",        \"conditions\": [        {          \"operator\": \"eq\",          \"lhs\": \"var.taxi\",          \"rhs\": &lt;taxi-type-id-for-justgrab&gt;          },          {            \"operator\": \"eq\",            \"lhs\": \"var.taxi\",            \"rhs\": &lt;taxi-type-id-for-grabcard&gt;          }        ]      }    ]  }When evaluating the rule, our system loads the values of the LHS variable, evaluates against the RHS value, and returns as result (true/false) whether the rule evaluation passed or not.To reduce the resources spent on rule evaluation, there are two types of strategies:  Avoid unnecessary rule evaluation  Evaluate “cheap” rules firstWe implemented these two strategies with event prefiltering and weighted rule evaluation.Event PrefilteringJust like the DB index helps speed up data look-up, having a pre-built map also helped us narrow down the range of campaigns to evaluate. We loaded active campaigns from the DB every few minutes and organised them into an in-memory hash map, with event type as key, and list of corresponding campaigns as the value. The reason we picked event type as the key is that it is very fast to determine (most of the time just a type assertion), and it can distribute events in a reasonably even way.When processing events, we just looked up the map, and only ran rule evaluation on the campaigns in the matching hash bucket. This saved us at least 90% of the processing time.   Event prefilteringWeighted Rule EvaluationEvaluating different rules comes with different costs. This is because different variables (i.e. LHS) in the rule can have different sources of values:  The value is already available in memory (already consumed from the event stream).  The value is the result of a database query.  The value is the result of a call to an external service.These three sources are ranked by cost:In-memory &lt; database &lt; external serviceWe aimed to maximally avoid evaluating expensive rules (i.e. those that require calling external service, or querying a DB) while ensuring the correctness of evaluation results.First optimisation - Lazy loadingLazy loading is a common performance optimisation technique, which literally means “don’t do it until it’s necessary”.Take the following rule as an example:A &amp; BIf we load the variable values for both A and B before passing to evaluation, then we are unnecessarily loading B if A is false. Since most of the time the rule evaluation fails early (for example, the transaction amount is less than the given minimum amount), there is no point in loading all the data beforehand. So we do lazy loading ie. load data only when evaluating that part of the rule.Second optimisation - Add weightLet’s take the same example as above, but in a different order.B &amp; ASource of data for A is memory and B is external serviceNow even if we are doing lazy loading, in this case, we are loading the external data always even though it potentially may fail at the next condition whose data is in memory.Since most of our campaigns are targeted, a popular condition is to check if a user is in a certain segment, which is usually the first condition that a campaign creator sets. This data resides in another service. So it becomes quite expensive to evaluate this condition first even though the next condition’s data can be already in memory (e.g. if the taxi type is JustGrab).So, we did the next phase of optimisation here, by sorting the conditions based on weight of the source of data (low weight if data is in memory, higher if it’s in our database and highest if it’s in an external system). If AND was the only logical operator we supported, then it would have been quite simple. But the presence of OR made it complex. We came up with an algorithm that sorts the evaluation based on weight keeping in mind the AND/OR. Here’s what the flowchart looks like:   Event flowchartAn example:Conditions: A &amp; ( B | C ) &amp; ( D | E )Actual result: true &amp; ( false | false ) &amp; ( true | true ) --&gt; falseWeight: B &lt; D &lt; E &lt; C &lt; AExpected check order: B, D, CFirstly, we start validating B which is false. Apparently, we cannot skip the sibling conditions here since B and C are connected by |. Next, we check D. D is true and its only sibling E is connected by | so we can mark E “skip”. Then, we check E but since E has been marked “skip”, we just skip it. Still, we cannot get the final result yet, so we need to continue validating C which is false. Now, we know (B | C) is false so the whole condition is also false. We can stop now.Sub-streamsAfter investigation, we learned that we consumed a particular stream that produced terabytes of data per hour. It caused our CPU usage to shoot up by 30%. We found out that we process only a handful of event types from that stream. So we introduced a sub-stream in between, which contains the event types we want to support. This stream is populated from the main stream by another server, thereby reducing the load on Trident.Protect DownstreamWhile we scaled up our servers wildly, we needed to keep in mind that there were many downstream services that received more traffic. For example, we call the GrabRewards service for awarding rewards or the LocaleService for checking the user’s locale. It is crucial for us to have control over our outbound traffic to avoid causing any stability issues in Grab.Therefore, we implemented rate limiting. There is a total rate limit configured for calling each downstream service, and the limit varies in different time ranges (e.g. tighter limit for calling critical service during peak hour).Scale Data StoreWe have two types of storage in Trident: cache storage (Redis) and persistent storage (MySQL and others).Scaling cache storage is straightforward, since Redis Cluster already offers everything we need:  High performance: Known to be fast and efficient.  Scaling capability: New shards can be added at any time to spread out the load.  Fault tolerance: Data replication makes sure that data does not get lost when any single Redis instance fails, and auto election mechanism makes sure the cluster can always auto restore itself in case of any single instance failure.All we needed to make sure is that our cache keys can be hashed evenly into different shards.As for scaling persistent data storage, we tackled it in two ways just like we did for servers:  Distribute load  Reduce load (both overall and per query)Distribute LoadThere are two levels of load distribution for persistent storage: infra level and DB level. On the infra level, we split data with different access patterns into different types of storage. Then on the DB level, we further distributed read/write load onto different DB instances.Infra LevelJust like any typical online service, Trident has two types of data in terms of access pattern:  Online data: Frequent access. Requires quick access. Medium size.  Offline data: Infrequent access. Tolerates slow access. Large size.For online data, we need to use a high-performance database, while for offline data, we can  just use cheap storage. The following table shows Trident’s online/offline data and the corresponding storage.   Trident’s online/offline data and storageWriting offline data is done asynchronously to minimise performance impact, as shown below.   Online/offline data splitFor retrieving data for the users, we have high timeout for such APIs.DB LevelWe further distributed load on the MySQL DB level, mainly by introducing replicas, and redirecting all read queries that can tolerate slightly outdated data to the replicas. This relieved more than 30% of the load from the master instance.Going forward, we plan to segregate the single MySQL database into multiple databases, based on table usage, to further distribute load if necessary.Reduce LoadTo reduce the load on the DB, we reduced the overall number of queries and removed unnecessary queries. We also optimised the schema and query, so that query completes faster.Query ReductionWe needed to track usage of a campaign. The tracking is just incrementing the value against a unique key in the MySQL database. For a popular campaign, it’s possible that multiple increment (a write query) queries are made to the database for the same key. If this happens, it can cause an IOPS burst. So we came up with the following algorithm to reduce the number of queries.  Have a fixed number of threads per instance that can make such a query to the DB.  The increment queries are queued into above threads.  If a thread is idle (not busy in querying the database) then proceed to write to the database then itself.  If the thread is busy, then increment in memory.  When the thread becomes free, increment by the above sum in the database.To prevent accidental over awarding of benefits (rewards, points, etc), we require campaign creators to set the limits. However, there are some campaigns that don’t need a limit, so the campaign creators just specify a large number. Such popular campaigns can cause very high QPS to our database. We had a brilliant trick to address this issue- we just don’t track if the number is high. Do you think people really want to limit usage when they set the per user limit to 100,000? ;)Query OptimisationOne of our requirements was to track the usage of a campaign - overall as well as per user (and more like daily overall, daily per user, etc). We used the following query for this purpose:INSERT INTO … ON DUPLICATE KEY UPDATE value = value + incThe table had a unique key index (combining multiple columns) along with a usual auto-increment integer primary key. We encountered performance issues arising from MySQL gap locks when high write QPS hit this table (i.e. when popular campaigns ran). After testing out a few approaches, we ended up making the following changes to solve the problem:  Removed the auto-increment integer primary key.  Converted the secondary unique key to the primary key.ConclusionTrident is Grab’s in-house real-time IFTTT engine, which processes events and operates business mechanisms on a massive scale. In this article, we discussed the strategies we implemented to achieve large-scale high-performance event processing. The overall ideas of distributing and reducing load may be straightforward, but there were lots of thoughts and learnings shared in detail. If you have any comments or questions about Trident, feel free to leave a comment below.All the examples of campaigns given in the article are for demonstration purpose only, they are not real live campaigns.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/trident-real-time-event-processing-at-scale"
      }
      ,
    
      "pharos-searching-nearby-drivers-on-road-network-at-scale": {
        "title": "Pharos - Searching Nearby Drivers on Road Network at Scale",
        "author": "hao-wuminglei-suthanh-dat-lenuo-xuguanfeng-wangmihai-stroe",
        "tags": "[&quot;Real-Time K Nearest Neighbour Search&quot;, &quot;Spatial Data Store&quot;, &quot;Distributed Systems&quot;]",
        "category": "",
        "content": "Have you ever wondered what happens when you click on the book button when arranging a ride home? Actually, many things happen behind this simple action and it would take days and nights to talk about all of them. Perhaps, we should rephrase this question to be more precise.  So, let’s try again - have you ever thought about how Grab stores and uses driver locations to allocate a driver to you? If so, you will surely find this blog post interesting as we cover how it all works in the backend.What Problems are We Going to Solve?One of the fundamental problems of the ride-hailing and delivery industry is to locate the nearest moving drivers in real-time. There are two challenges from serving this request in real time.Fast-moving VehiclesVehicles are constantly moving and sometimes the drivers go at the speed of over 20 meters per second. As shown in Figure 1a and Figure 1b, the two nearest drivers to the pick-up point (blue dot) change as time passes. To provide a high-quality allocation service, it is important to constantly track the objects and update object locations at high frequency (e.g. per second).   Figure 1: Fast-moving driversRouting Distance CalculationTo satisfy business requirements, K nearest objects need to be calculated based on the routing distance instead of straight-line distance. Due to the complexity of the road network, the driver with the shortest straight-line distance may not be the optimal driver as it could reach the pick-up point with a longer routing distance due to detour.    Figure 2: Straight line vs routing As shown in Figure 2, the driver at the top is deemed as the nearest one to pick-up point by straight line distance. However, the driver at the bottom should be the true nearest driver by routing distance. Moreover, routing distance helps to infer the estimated time of arrival (ETA), which is an important factor for allocation, as shorter ETA reduces passenger waiting time thus reducing order cancellation rate and improving order completion rate.Searching for the K nearest drivers with respect to a given POI is a well studied topic for all ride-hailing companies, which can be treated as a K Nearest Neighbour (KNN) problem. Our predecessor, Sextant, searches nearby drivers with the haversine distance from driver locations to the pick-up point. By partitioning the region into grids and storing them in a distributed manner, Sextant can handle large volumes of requests with low latency. However, nearest drivers found by the haversine distance may incur long driving distance and ETA as illustrated in Figure 2. For more information about Sextant, kindly refer to the paper, Sextant: Grab’s Scalable In-Memory Spatial Data Store for Real-Time K-Nearest Neighbour Search.To better address the challenges mentioned above, we present the next-generation solution, Pharos.   Figure 3: Lighthouse of AlexandriaWhat is Pharos?Pharos means lighthouse in Greek. At Grab, it is a scalable in-memory solution that supports large-volume, real-time K nearest search by driving distance or ETA with high object update frequency.In Pharos, we use OpenStreetMap (OSM) graphs to represent road networks. To support hyper-localised business requirements, the graph is partitioned by cities and verticals (e.g. the road network for a four-wheel vehicle is definitely different compared to a motorbike or a pedestrian). We denote this partition key as map ID.Pharos loads the graph partitions at service start and stores drivers’ spatial data in memory in a distributed manner to alleviate the scalability issue when the graph or the number of drivers grows. These data are distributed into multiple instances (i.e. machines) with replicas for high stability. Pharos exploits Adaptive Radix Trees (ART) to store objects’ locations along with their metadata.To answer the KNN query by routing distance or ETA, Pharos uses Incremental Network Expansion (INE) starting from the road segment of the query point. During the expansion, drivers stored along the road segments are incrementally retrieved as candidates and put into the results. As the expansion actually generates an isochrone map, it can be terminated by reaching a predefined radius of distance or ETA, or even simply a maximum number of candidates.Now that you have an  overview of Pharos, we would like to go into the design details of it, starting with its architecture.Pharos ArchitectureAs a microservice, Pharos receives requests from the upstream, performs corresponding actions and then returns the result back. As shown in Figure 4, the Pharos architecture can be broken down into three layers: Proxy, Node, and Model.  Proxy layer. This layer helps to pass down the request to the right node, especially when the Node is on another machine.  Node layer. This layer stores the index of map IDs to models and distributes the request to the right model for execution.  Model layer. This layer is, where the business logic is implemented, executes the operations and returns the result.As a distributed in-memory driver storage, Pharos is designed to handle load balancing, fault tolerance, and fast recovery.Taking Figure 4 as an example, Pharos consists of three instances. Each individual instance is able to handle any request from the upstream. Whenever there is a request coming from the upstream, it is distributed into one of the three instances, which achieves the purpose of load balancing.   Figure 4: Pharos architectureIn Pharos, each model has two replicas and they are stored on different instances and different availability zones. If one instance is down, the other two instances are still up for service. The fault tolerance module in Pharos automatically detects the reduction of replicas and creates new instances to load graphs and build the models of missing replicas. This proves the reliability of Pharos even under extreme situations.With the architecture of Pharos in mind, let’s take a look at how it stores driver information.Driver StoragePharos acts as a driver storage, and rather than being an external storage, it adopts in-memory storage which is faster and more adequate to handle frequent driver position updates and retrieve driver locations for nearby driver queries. Without loss of generality, drivers are assumed to be located on the vertices, i.e. Edge Based Nodes (EBN) of an edge-based graph.Model is in charge of the driver storage in Pharos. Driver objects are passed down from upper layers to the model layer for storage. Each driver object contains several fields such as driver ID and metadata, containing the driver’s business related information e.g. driver status and particular allocation preferences.There is also a Latitude and Longitude (LatLon) pair contained in the object, which indicates the driver’s current location. Very often, this LatLon pair sent from the driver is off the road (not on any existing road). The computation of routing distance between the query point and drivers is based on the road network. Thus, we need to infer which road segment (EBN) the driver is most probably on.To convert a LatLon pair to an exact location on a road is called Snapping. Model begins with finding EBNs which are close to the driver’s location. After that, as illustrated in Figure 5, the driver’s location is projected to those EBNs, by drawing perpendicular lines from the location to the EBNs. The projected point is denoted as a phantom node. As the name suggests, these nodes do not exist in the graph. They are merely memory representations of the snapped driver.Each phantom node contains information about its projected location such as the ID of EBN it is projected to, projected LatLon and projection ratio, etc. Snapping returns a list of phantom nodes ordered by the haversine distance from the driver’s LatLon to the phantom node in ascending order. The nearest phantom node is bound with the original driver object to provide information about the driver’s snapped location.   Figure 5: Snapping and phantom nodesTo efficiently index drivers from the graph, Pharos uses ART for driver storage. Two ARTs are maintained by each model: Driver ART and EBN ART.Driver ART is used to store the index of driver IDs to corresponding driver objects, while EBN ART is used to store the index of EBN IDs to the root of an ART, which stores the drivers on that EBN.Bi-directional indexing between EBNs and drivers are built because an efficient retrieval from driver to EBN is needed as driver locations are constantly updated. In practice, as index keys, driver IDs, and EBN IDs are both numerical. ART has a better throughput for dense keys (e.g. numerical keys) in contrast to sparse keys such as alphabetical keys, and when compared to other in-memory look-up tables (e.g. hash table). It also incurs less memory than other tree-based methods.Figure 6 gives an example of driver ART assuming that the driver ID only has three digits.   Figure 6: Driver ARTAfter snapping, this new driver object is wrapped into an update task for execution. During execution, the model firstly checks if this driver already exists using its driver ID. If it does not exist, the model directly adds it to driver ART and EBN ART. If the driver already exists, the new driver object replaces the old driver object on driver ART. For EBN ART, the old driver object on the previous EBN needs to be deleted first before adding the new driver object to the current EBN.Every insertion or deletion modifies both ARTs, which might cause changes to roots. The model only stores the roots of ARTs, and in order to prevent race conditions, a lock is used to prevent other read or write operations to access the ARTs while changing the ART roots.Whenever a driver nearby request comes in, it needs to get a snapshot of driver storage, i.e. the roots of two ARTs. A simple example (Figure 7a and 7b) is used to explain how synchronisation is achieved during concurrent driver update and nearby requests.   Figure 7: How ARTs change roots for synchronizationCurrently, there are two drivers A and B stored and these two drivers reside on the same EBN. When there is a nearby request, the current roots of the two ARTs are returned. When processing this nearby request, there could be driver updates coming and modifying the ARTs, e.g. a new root is resulted due to update of driver C. This driver update has no impact on ongoing driver nearby requests as they are using different roots. Subsequent nearby requests will use the new ART roots to find the nearby drivers. Once the current roots are not used by any nearby request, these roots and their child nodes are ready to be garbage collected.Pharos does not delete drivers actively. A deletion of expired drivers is carried out every midnight by populating two new ARTs with the same driver update requests for a duration of driver’s Time To Live (TTL), and then doing a switch of the roots at the end. Drivers with expired TTLs are not referenced and they are ready to be garbage collected. In this way, expired drivers are removed from the driver storage.Driver Update and NearbyPharos mainly has two external endpoints: Driver Update and Driver Nearby. The following describes how the business logic is implemented in these two operations.Driver UpdateFigure 8 demonstrates the life cycle of a driver update request from upstream. Driver update requests from upstream are distributed to each proxy by a load balancer. The chosen proxy firstly constructs a driver object from the request body.RouteTable, a structure in proxy, stores the index between map IDs and replica addresses. Proxy then uses map ID in the request as the key to check its RouteTable and gets the IP addresses of all the instances containing the model of that map ID.Then, proxy forwards the update to other replicas that reside in other instances. Those instances, upon receiving the message, know that the update is forwarded from another proxy. Hence they directly pass down the driver object to the node.After receiving the driver object, Node sends it to the right model by checking the index between map ID and model. The remaining part of the update flow is the same as described in Driver Storage. Sometimes the driver updates to replicas are not successful, e.g. request lost or model does not exist, Pharos will not react to such kinds of scenarios.It can be observed that data storage in Pharos does not guarantee strong consistency. In practice, Pharos favors high throughput over strong consistency of KNN query results as the update frequency is high and slight inconsistency does not affect allocation performance significantly.   Figure 8: Driver update flowDriver NearbySimilar to driver update, after a driver nearby request comes from the upstream, it is distributed to one of the machines by the load balancer. In a nearby request, a set of filter parameters is used to match with driver metadata in order to support KNN queries with various business requirements. Note that driver metadata also carries an update timestamp. During the nearby search, drivers with an expired timestamp are filtered.As illustrated in Figure 9, upon receiving the nearby request, a nearby object is built and passed to the proxy layer. The proxy first checks RouteTable by map ID to see if this request can be served on the current instance. If so, the nearby object is passed to the Node layer. Otherwise, this nearby request needs to be forwarded to the instances that contain this map ID.In this situation, a round-robin fashion is applied to select the right instance for load balancing. After receiving the request, the proxy of the chosen instance directly passes the nearby object to the node. Once the node layer receives the nearby object, it looks for the right model using the map ID as key. Eventually, the nearby object goes to the model layer where K-nearest-driver computation takes place. Model snaps the location of the request to some phantom nodes as described previously - these nodes are used as start nodes for expansion later.   Figure 9: Driver nearby flowK Nearest Driver SearchStarting from the phantom nodes found in the Driver Nearby flow, the K nearest driver search begins. Two priority queues are used during the search: EBNPQ is used to keep track of the nearby EBNs, while driverPQ keeps track of drivers found during expansion by their driving distance to the query point.At first, a snapshot of the current driver storage is taken (using roots of current ARTs) and it shows the driver locations on the road network at the time when the nearby request comes in. From each start node, the parent EBN is found and drivers on these EBNs are appended to driverPQ. After that, KNN search expands to adjacent EBNs and appends these EBNs to EBNPQ. After iterating all start nodes, there will be some initial drivers in driverPQ and adjacent EBNs waiting to be expanded in EBNPQ.Each time the nearest EBN is removed from EBNPQ, drivers located on this EBN are appended to driverPQ. After that, the closest driver is removed from driverPQ. If the driver satisfies all filtering requirements, it is appended to the array of qualified drivers. This step repeats until driverPQ becomes empty. During this process, if the size of qualified drivers reaches the maximum driver limit, the KNN search stops right away and qualified drivers are returned.After driverPQ becomes empty, adjacent EBNs of the current one are to be expanded and those within the predefined range, e.g. three kilometres, are appended to EBNPQ. Then the nearest EBN is removed from EBNPQ and drivers on that EBN are appended to driverPQ again. The whole process continues until EBNPQ becomes empty. The driver array is returned as the result of the nearby query.Figure 10 shows the pseudo code of this KNN algorithm.   Figure 10: KNN search algorithmWhat’s Next?Currently, Pharos is running on the production environment, where it handles requests with P99 latency time of 10ms for driver update and 50ms for driver nearby, respectively. Even though the performance of Pharos is quite satisfying, we still see some potential areas of improvements:  Pharos uses ART for driver storage. Even though ART proves its ability to handle large volumes of driver update and driver nearby requests, the write operations (driver update) are not carried out in parallel. Hence, we plan to explore other data structures that can achieve high concurrency of read and write, eg. concurrent hash table.  Pharos uses OSM Multi-level Dijkstra (MLD) graphs to find K nearest drivers. As the predefined range of nearby driver search is often a few kilometres, Pharos does not make use of MLD partitions or support long distance query. Thus, we are interested in exploiting MLD graph partitions to enable Pharos to support long distance query.  In Pharos, maps are partitioned by cities and we assume that drivers of a city operate within that city. When finding the nearby drivers, Pharos only allocates drivers of that city to the passenger. Hence, in the future, we want to enable Pharos to support cross city allocation.We hope this blog helps you to have a closer look at how we store driver locations and how we use these locations to find nearby drivers around you.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!AcknowledgementsWe would like to thank Chunda Ding, Zerun Dong, and Jiang Liu for their contributions to the distributed layer used in Pharos. Their efforts make Pharos reliable and fault tolerant.Figure 3, Lighthouse of Alexandria is taken from https://www.britannica.com/topic/lighthouse-of-Alexandria#/media/1/455210/187239 authored by Sergey Kamshylin.Figure 5, Snapping and Phantom Nodes, is created by Minbo Qiu. We would like to thank him for the insightful elaboration of the snapping mechanism.Cover Photo by Kevin Huang on Unsplash",
        "url": "/pharos-searching-nearby-drivers-on-road-network-at-scale"
      }
      ,
    
      "reflecting-on-the-five-years-of-bug-bounty-at-grab": {
        "title": "Reflecting on the Five Years of Bug Bounty at Grab",
        "author": "ajay-srivastavaaniket-kulkarniavinash-singhnathaniel-callens",
        "tags": "[&quot;Security&quot;, &quot;HackerOne&quot;, &quot;Bug Bounty&quot;]",
        "category": "",
        "content": "Security has always been a top-priority at Grab; our product security team works round-the-clock to ensure that our consumers’ data remains safe. Five years ago, we launched our private bug bounty programme on HackerOne, which evolved into a public programme in August 2017. The idea was to complement the security efforts our team has been putting through to keep Grab secure. We were a pioneer in Southeast Asia to implement a public bug bounty programme, and now we stand among the Top 20 programmes on HackerOne worldwide.We started as a private bug bounty programme which provided us with fantastic results, thus encouraging us to increase our reach and benefit from the vibrant security community across the globe which have helped us iron-out security issues 24x7 in our products and infrastructure. We then publicly launched our bug bounty programme offering competitive rewards and hackers can even earn additional bonuses if their report is well-written and display an innovative approach to testing.In 2019, we also enrolled ourselves in the Google Play Security Reward Programme (GPSRP), Offered by Google Play, GPSRP allows researchers to re-submit their resolved mobile security issues directly and get additional bounties if the report qualifies under the GPSRP rules. A selected number of Android applications are eligible, including Grab’s Android mobile application. Through the participation in GPSP, we hope to give researchers the recognition they deserve for their efforts.In this blog post, we’re going to share our journey of running a bug bounty programme, challenges involved and share the learnings we had on the way to help other companies in SEA and beyond to establish and build a successful bug bounty programme.Transitioning from Private to a Public ProgrammeAt Grab, before starting the private programme, we defined policy and scope, allowing us to communicate the objectives of our bug bounty programme and list the targets that can be tested for security issues. We did a security sweep of the targets to eliminate low-hanging security issues, assigned people from the security team to take care of incoming reports, and then launched the programme in private mode on HackerOne with a few chosen researchers having demonstrated a history of submitting quality submissions.One of the benefits of running a private bug bounty programme is to have some control over the number of incoming submissions of potential security issues and researchers who can report issues. This ensures the quality of submissions and helps to control the volume of bug reports, thus avoiding overwhelming a possibly small security team with a deluge of issues so that they won’t be overwhelming for the people triaging potential security issues. The invited researchers to the programme are limited, and it is possible to invite researchers with a known track record or with a specific skill set, further working in the programme’s favour.The results and lessons from our private programme were valuable, making our programme and processes mature enough to open the bug bounty programme to security researchers across the world. We still did another security sweep, reworded the policy, redefined the targets by expanding the scope, and allocated enough folks from our security team to take on the initial inflow of reports which was anticipated to be in tune with other public programmes.  Noticeable spike in the number of incoming reports as we went public in July 2017.Lessons Learned from the Public ProgrammeAlthough we were running our bug bounty programme in private for sometime before going public, we still had not worked much on building standard operating procedures and processes for managing our bug bounty programme up until early 2018. Listed below, are our key takeaways from 2018 till July 2020 in terms of improvements, challenges, and other insights.  Response Time: No researcher wants to work with a bug bounty team that doesn’t respect the time that they are putting into reporting bugs to the programme. We initially didn’t have a formal process around response times, because we wanted to encourage all security engineers to pick-up reports. Still, we have been consistently delivering a first response to reports in a matter of hours, which is significantly lower than the top 20 bug bounty programmes running on HackerOne. Know what structured (or unstructured) processes work for your team in this area, because your programme can see significant rewards from fast response times.  Time to Bounty: In most bug bounty programmes the payout for a bug is made in one of the following ways: full payment after the bug has been resolved, full payment after the bug has been triaged, or paying a portion of the bounty after triage and the remaining after resolution. We opt to pay the full bounty after triage. While we’re always working to speed up resolution times, that timeline is in our hands, not the researcher’s. Instead of making them wait, we pay them as soon as impact is determined to incentivise long-term engagement in the programme.  Noise Reduction: With HackerOne Triage and Human-Augmented Signal, we’re able to focus our team’s efforts on resolving unique, valid vulnerabilities. Human-Augmented Signal flags any reports that are likely false-positives, and Triage provides a validation layer between our security team and the report inbox. Collaboration with the HackerOne Triage team has been fantastic and ultimately allows us to be more efficient by focusing our energy on valid, actionable reports. In addition, we take significant steps to block traffic coming from networks running automated scans against our Grab infrastructure and we’re constantly exploring this area to actively prevent automated external scanning.  Team Coverage: We introduced a team scheduling process, in which we assign a security engineer (chosen during sprint planning) on a weekly basis, whose sole responsibility is to review and respond to bug bounty reports. We have integrated our systems with HackerOne’s API and PagerDuty to ensure alerts are for valid reports and verified as much as possible.Looking AheadOne area we haven’t been doing too great is ensuring higher rates of participation in our core mobile applications; some of the pain points researchers have informed us about while testing our applications are:  Researchers’ accounts are getting blocked due to our anti-fraud checks.  Researchers are not able to register driver accounts (which is understandable as our driver-partners have to go through manual verification process)  Researchers who are not residing in the Southeast Asia region are unable to complete end-to-end flows of our applications.We are open to community feedback and how we can improve. We want to hear from you! Please drop us a note at infosec.bugbounty@grab.com for any programme suggestions or feedback.Last but not least, we’d like to thank all researchers who have contributed to the Grab programme so far. Your immense efforts have helped keep Grab’s businesses and users safe. Here’s a shoutout to our programme’s top-earning hackers 🏆:Overall Top 3 Researchers  @reptou  @quanyang  @ngocdhYear 2019/2020 Top 3 Researchers  @reptou  @alexeypetrenko  @chaosboltLastly, here is a special shoutout to @bagipro who has done some great work and testing on our Grab mobile applications!Well done and from everyone on the Grab team, we look forward to seeing you on the programme!Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/reflecting-on-the-five-years-of-bug-bounty-at-grab"
      }
      ,
    
      "how-grab-is-blazing-through-the-super-app-bazel-migration": {
        "title": "How Grab is Blazing Through the Superapp Bazel Migration",
        "author": "sergii-grechukha",
        "tags": "[&quot;Bazel&quot;, &quot;Android&quot;, &quot;iOS&quot;, &quot;Build Time&quot;, &quot;Xcode&quot;, &quot;Gradle&quot;]",
        "category": "",
        "content": "IntroductionAt Grab, we build a seamless user experience that addresses more and more of the daily lifestyle needs of people across Southeast Asia. We’re proud of our Grab rides, payments, and delivery services, and want to provide a unified experience across these offerings.Here are a couple of examples of what Grab does for millions of people across Southeast Asia every day:   Grab Service OfferingsThe Grab Passenger application reached superapp status more than a year ago and continues to provide hundreds of life-changing use cases in dozens of areas for millions of users.With the big product scale, it brings with it even bigger technical challenges. Here are a couple of dimensions that can give you a sense of the scale we’re working with.Engineering and Product StructureTechnical and product teams work in close collaboration to outserve our consumers. These teams are combined into dedicated groups to form Tech Families and focus on similar use cases and areas.Grab consists of many Tech Families who work on food, payments, transport, and other services, which are supported by hundreds of engineers. The diverse landscape makes the development process complicated and requires the industry’s best practices and approaches.Codebase Scale OverviewThe Passenger Applications (Android and iOS) contain more than 2.5 million lines of code each and it keeps growing. We have 1000+ modules in the Android app and 700+ targets in the iOS app. Hundreds of commits are merged by all the mobile engineers on a daily basis.To maintain the health of the codebase and product stability, we run 40K+ unit tests on Android and 30K+ unit tests on iOS, as well as thousands of UI tests and hundreds of end-to-end tests on both platforms.Build Time ChallengesThe described complexity and scale do not come without challenges. A huge codebase propels the build process to the ultimate extreme- challenging the efficiency of build systems and hardware used to compile the superapp, and creating out of the line challenges to be addressed.Local Build TimeLocal build time (the build on engineers’ laptop) is one of the most obvious challenges. More code goes in the application binary, hence the build system requires more time to compile it.ADR Local Build TimeThe Android ecosystem provides a great out-of-the-box tool to build your project called Gradle. It’s flexible and user friendly, and  provides huge capabilities for a reasonable cost. But is this always true? It appears to not be the case due to multiple reasons. Let’s unpack these reasons below.Gradle performs well for medium sized projects with say 1 million line of code. Once the code surpasses that 1 million mark (or so), Gradle starts failing in giving engineers a reasonable build time for the given flexibility. And that’s exactly what we have observed in our Android application.At some point in time, the Android local build became ridiculously long. We even encountered cases  where engineers’ laptops simply failed to build the project due to hardware resources limits. Clean builds took by the hours, and incremental builds easily hit dozens of minutes.iOS Local Build TimeXcode behaved a bit better compared to Gradle. The Xcode build cache was somehow bearable for incremental builds and didn’t exceed a couple of minutes. Clean builds still took dozens of minutes though. When Xcode failed to provide the valid cache, engineers had to rerun everything as a clean build, which killed the experience entirely.CI Pipeline TimeEach time an engineer submits a Merge Request (MR), our CI kicks in running a wide variety of jobs to ensure the commit is valid and doesn’t introduce regression to the master branch. The feedback loop time is critical here as well, and the pipeline time tends to skyrocket alongside the code base growth. We found ourselves on the trend where the feedback loop came in by the hours, which again was just breaking the engineering experience, and prevented  us from delivering the world’s best features to our consumers.As mentioned, we have a large number of unit tests (30K-40K+) and UI tests (700+) that we run on a pre-merge pipeline. This brings us to hours of execution time before we could actually allow MRs to land to the master branch.The number of daily commits, which is by the hundreds, adds another stone to the basket of challenges.All this clearly indicated the area of improvement. We were missing opportunities in terms of engineering productivity.The Extra MileThe biggest question for us to answer was how to put all this scale into a reasonable experience with minimal engineering idle time and fast feedback loop.Build Time Critical Path OptimisationThe most reasonable thing to do was to pay attention to the utilisation of the hardware resources and make the build process optimal.This literally boiled down to the simplest approach:  Decouple building blocks  Make building blocks as small as possibleThis approach is valid for any build system and applies  for both iOS and Android. The first thing we focused on was to understand what our build graph looked like, how dependencies were distributed, and which blocks were bottlenecks.Given the scale of the apps, it’s practically not possible to manage a dependency tree manually, thus we created a tool to help us.Critical Path OverviewWe introduced the Critical Path concept:The critical path is the longest (time) chain of sequential dependencies, which must be built one after the other.   Critical Path buildEven with an infinite number of parallel processors/cores, the total build time cannot be less than the critical path time.We implemented the tool that parsed the dependency trees (for both Android and iOS), aggregated modules/target build time, and calculated the critical path.The concept of the critical path introduced a number of action items, which we prioritised:  The critical path must be as short as possible.  Any huge module/target on the critical path must be split into smaller modules/targets.  Depend on interfaces/bridges rather than implementations to shorten the critical path.  The presence of other teams’ implementation modules/targets in the critical path of the given team is a red flag.   Stack representation of the Critical Path build timeProject’s Scale FactorTo implement the conceptually easy action items, we ran a Grab-wide program. The programme has impacted almost every mobile team at Grab and involved 200+ engineers to some degree. The whole implementation took 6 months to complete.During this period of time, we assigned engineers who were responsible to review the changes, provide support to the engineers across Grab, and monitor the results.ResultsEven though the overall plan seemed to be good on paper, the results were minimal - it just flattened the build time curve of the upcoming trend introduced by the growth of the codebase. The estimated impact was almost the same for both platforms and gave us about a 7%-10% cut in the CI and local build time.Open Source PlanThe critical path tool proved to be effective to illustrate the projects’ bottlenecks in a dependency tree configuration. It is currently widely used by mobile teams at Grab to analyse their dependencies and cut out or limit an unnecessary impact on the respective scope.The tool is currently considered to be open-sourced as we’d like to hear feedback from other external teams and see what can be built on top of it. We’ll provide more details on this in future posts.Remote BuildAnother pillar of the  build process is the hardware where the build runs. The solution is really straightforward - put more muscles on your build to get it stronger and to run faster.Clearly, our engineers’ laptops could not be considered fast enough. To have a fast enough build we were looking at something with 20+ cores, ~200Gb of RAM. None of the desktop or laptop computers can reach those numbers within reasonable pricing. We hit a bottleneck in hardware. Further parallelization of the build process didn’t give any significant improvement as all the build tasks were just queueing and waiting for the resources to be released. And that’s where cloud computing came into the picture where a huge variety of available options is ready to be used.ADR MainframerWe took advantage of the Mainframer tool. When the build must run, the code diff is pushed to the remote executor, gets compiled, and then the generated artifacts are pushed back to the local machine. An engineer might still benefit from indexing, debugging, and other features available in the IDE.To make the infrastructure mature enough, we’ve introduced Kubernetes-based autoscaling based on the load. Currently, we have a stable infrastructure that accommodates 100+ Android engineers scaling up and down (saving costs).This strategy gave us a 40-50% improvement in the local build time. Android builds finished, in the extreme case, x2 faster.iOSGiven the success of the Android remote build infrastructure, we have immediately turned our attention to the iOS builds. It was an obvious move for us - we wanted the same infrastructure for iOS builds. The idea looked good on paper and was proven with Android infrastructure, but the reality was a bit different for our iOS builds.Our very first roadblock was that Xcode is not that flexible and the process of delegating builds to remote is way more complicated as compared to Android. We tackled a series of blockers such as running indexing on a remote machine, sending and consuming build artifacts, and even running the remote build itself.The reality was that the remote build was absolutely possible for iOS. There were minor tradeoffs impacting engineering experience alongside obvious gains from utilising cloud computing resources. But the problem is that legally iOS builds are only allowed to be built on an Apple machine.Even if we get the most powerful hardware - a macPro -  the specs are still not ideal and are unfortunately not optimised for the build process. A 24 core, 194Gb RAM macPro could have given about x2 improvement on the build time, but when it had to run 3 builds simultaneously for different users, the build efficiency immediately dropped to the baseline value.Android remote machines with the above same specs are capable of running up to 8 simultaneous builds. This allowed us to accommodate up to 30-35 engineers per machine, whereas iOS’ infrastructure would require to keep this balance at 5-6 engineers per machine. This solution didn’t seem to be scalable at all, causing us to abandon the idea of the remote builds for iOS at that moment.Test Impact AnalysisThe other battlefront was the CI pipeline time. Our efforts in dependency tree optimisations complemented with comparably powerful hardware played a good part in achieving a reasonable build time on CI.CI validations also include the execution of unit and UI tests and easily take 50%-60% of the pipeline time. The problem was getting worse as the number of tests was constantly growing. We were to face incredibly huge tests’ execution time in the near future. We could mitigate the problem by a muscle approach - throwing more runners and shredding tests - but it won’t make finance executives happy.So the time for smart solutions came again. It’s a known fact that the simpler solution is more likely to be correct. The simplest solution was to stop running ALL tests. The idea was to run only those tests that were impacted by the codebase change introduced in the given MR.Behind this simple idea, we’ve found a huge impact. Once the Test Impact Analysis was applied to the pre-merge pipelines, we’ve managed to cut down the total number of executed tests by up to 90% without any impact on the codebase quality or applications’ stability. As a result, we cut the pipeline for both platforms by more than 30%.Today, the Test Impact Analysis is coupled with our codebase. We are looking to  invest some effort to make it available for open sourcing. We are excited to be  on this path.The End of the Native Build SystemsOne might say that our journey was long and we won the battle for the build time.Today, we hit a limit to the native build systems’ efficiency and hardware for both Android and iOS. And it’s clear to us that in our current setup, we would not be able to scale up while delivering high engineering experience.Let’s Move to BazelTo introduce another big improvement to the build time, we needed to make some ground-level changes. And this time, we focused on the build system itself.Native build systems are designed to work well for small and medium-sized projects, however they have not been as successful in large scale projects such as the Grab Passenger applications.With these assumptions, we considered options and found the Bazel build system to be a good contender. The deep comparison of build systems disclosed that Bazel was promising better results almost in all key areas:  Bazel enables remote builds out of box  Bazel provides sustainable cache capabilities (local and remote). This cache can be reused across all consumers - local builds, CI builds  Bazel was designed with the big codebase as a cornerstone requirement  The majority of the tooling may be reused across multiple platformsWays of AdoptingOn paper, Bazel was awesome and shining. All our playground investigations showed positive results:  Cache worked great  Incremental builds were incredibly fastBut the effort to shift to this new build system was huge. We made sure that we foresee all possible pitfalls and impediments. It took us about 5 months to estimate the impact and put together a sustainable proof of concept, which reflected the majority of our use cases.Migration LimitationsAfter those 5 months of investigation, we got the endless list of incompatible features and major blockers to be addressed. Those blockers touched even such obvious things as indexing and the jump to definition IDE feature, which we used to take for granted.But the biggest challenge was the need to keep the pace of the product release. There was no compromise of stopping the product development even for a day. The way out appeared to be a hybrid build concept. We figured out how to marry native and Bazel build systems to live together in harmony. This move gave us a chance to start migrating target by target, project by project moving from the bottom to top of the dependency graph.This approach was a valid enabler, however we were still faced with a challenge of our app’s  scale. The codebase of over 2.5 million of LOC cannot be migrated overnight. The initial estimation was based on the idea of manually migrating the whole codebase, which would have required us to invest dozens of person-months.Team Capacity LimitationsThis approach was immediately pushed back by multiple teams arguing with the priority and concerns about the impact on their own product roadmap.We were left with not much  choice. On one hand, we had a pressingly long build time. And on the other hand, we were asking for a huge effort from teams. We clearly needed to get buy-ins from all of our stakeholders to push things forward.Getting Buy-inTo get all needed buy-ins, all stakeholders were grouped and addressed separately. We defined key factors for each group.Key FactorsC-level stakeholders:  Impact. The migration impact must be significant - at least a 40% decrease on the build time.  Costs. Migration costs must be paid back in a reasonable time and the positive impact is extended to  the future.  Engineering experience. The user experience must not be compromised. All tools and features engineers used must be available during migration and even after.Engineers:  Engineering experience. Similar to the criteria established at the C-level factor.  Early adopters engagement. A common  core experience must be created across the mobile engineering community to support other engineers in the later stages.  Education. Awareness campaigns must be in place. Planned and conducted a series of tech talks and workshops to raise awareness among engineers and cut the learning curve. We wrote hundreds of pages of documentation and guidelines.Product teams:  No roadmap impact. Migration must not affect the product roadmap.  Minimise the engineering effort. Migration must not increase the efforts from engineering.Migration Automation (Separate Talks)The biggest concern for the majority of the stakeholders appeared to be the estimated migration effort, which impacted the cost, the product roadmap, and the engineering experience. It became evident that we needed to streamline the process and reduce the effort for migration.Fortunately, the actual migration process was routine in nature, so we had opportunities for automation. We investigated ideas on automating the whole migration process.The Tools We’ve CreatedWe found that it’s relatively easy to create a bunch of tools that read the native project structure and create an equivalent Bazel set up. This was a game changer.Things moved pretty smoothly for both Android and iOS projects. We managed to roll out tooling to migrate the codebase in a single click/command (well with some exceptions as of now. Stay tuned for another blog post on this). With this tooling combined with the hybrid build concept, we addressed all the key buy-in factors:  Migration cost dropped by at least 50%.  Less engineers required for the actual migration. There was no need to engage the wide engineering community as a small group of people can manage the whole process.  There is no more impact on the product roadmap.Where We Stand TodayWhen we were in the middle of the actual migration, we decided to take a pragmatic path and migrate our applications in phases to ensure everything was under control and that there were no unforeseen issues.The hybrid build time is racing alongside our migration progress. It has a linear dependency on the amount of the migrated code. The figures look positive and we are confident in achieving our impact goal of decreasing at least 40% of the build time.Plans for Open SourceThe automated migration tooling we’ve created is planned to be open sourced. We are doing a bit better on the Android side decoupling it from our applications’ implementation details and plan to open source it in the near future.The iOS tooling is a bit behind, and we expect it to be available for open-sourcing by the end of Q1’2021.Is it Worth it All?Bazel is not a silver bullet for the build time and your project. There are a lot of edge cases you’ll never know until it punches you straight in your face.It’s far from industry standard and you might find yourself having difficulty hiring engineers with such knowledge. It has a steep learning curve as well. It’s absolutely an overhead for small to medium-sized projects, but it’s undeniably essential once you start playing in a high league of superapps.If you were to ask whether we’d go this path again, the answer would come in a fast and correct way - yes, without any doubts.Authored by Sergii Grechukha on behalf of the passenger app team at Grab. Special thanks to Madushan Gamage, Mikhail Zinov, Nguyen Van Minh, Mihai Costiug, Arunkumar Sampathkumar, Maryna Shaposhnikova, Pavlo Stavytskyi, Michael Goletto, Nico Liu, and Omar Gawish for their contributions.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/how-grab-is-blazing-through-the-super-app-bazel-migration"
      }
      ,
    
      "democratising-fare-storage-at-scale-using-event-sourcing": {
        "title": "Democratising Fare Storage at Scale Using Event Sourcing",
        "author": "sourabh-suman",
        "tags": "[&quot;Pricing&quot;, &quot;Event Sourcing&quot;, &quot;Fare Storage&quot;]",
        "category": "",
        "content": "From humble beginnings, Grab has expanded across different markets in the last couple of years. We’ve added a wide range of features to the Grab platform to continue to delight our consumers and driver-partners. We had to incessantly find ways to improve our existing solutions to better support new features.In this blog, we discuss how we built Fare Storage, Grab’s single source of truth fare data store, and how we overcame the challenges to make it more reliable and scalable to support our expanding features.  To set some context for this blog, let’s define some key terms before proceeding. A Fare is a dollar amount calculated to move someone or something from point A to point B. And, a Fee is a dollar amount added to or subtracted from the original fare amount for any additional service.Now that you’re acquainted with the key concepts, let’s look take a look at the following image. It illustrates that features such as Destination Change Fee, Waiting Fee, Cancellation Fee, Tolls, Promos, Surcharges, and many others store additional fee breakdown along with the original fare. This set of information is crucial for generating receipts and debugging processes. However, our legacy storage system wasn’t designed to host massive quantities of information effectively.  In our legacy architecture, we stored all the booking and fare-related information in a single relational database table. Adding new fare fields and breakdowns required changes in our critical booking system, making iterations prohibitively expensive and hindering innovation.The need to store the fare information and metadata for every additional feature along with other booking information resulted in a bloated booking entity. With millions of bookings created every day at Grab, this posed a scaling and stability threat to our booking service storage. Moreover, the legacy storage only tracked the latest value of fare and lacked a holistic view of all the modifications to the fare. So, debugging the fare was also a massive chore for our Engineering and Tech Operations teams.Drafting a SolutionThe shortcomings of our legacy system led us to explore options for decoupling the fare and its metadata storage from the booking details. We wanted to build a platform that can store and provide access to both fare and its audit trail.High-level functional requirements for the new fare store were:  Provide a platform to store and retrieve fare and associated breakdowns, with no tight coupling between services.  Act as a single source-of-truth for fare and associated fees in the Grab ecosystem.  Enable clients to access the metadata of fare change events in real-time, enabling the Product team to innovate freely.  Provide smooth access to a fare’s audit trail, improving the response time to our consumers’ queries.Non-functional requirements for the fare store were:  High availability for the read and write APIs, with few milliseconds latency.  Handle concurrent updates to the fare gracefully.  Detect duplicate events for a booking for the same transaction.Storing Change Sequence with Event SourcingOur legacy storage solution used a defined schema and only stored the latest state of the fare. We needed an audit trail-based storage system with fast querying capabilities that can store and retrieve changes in chronological order.The Event Sourcing pattern stood out as a flexible architectural pattern as it allowed us to store and query the sequence of changes in the order it occurred. In Martin Fowler’s blog, he described Event Sourcing as:“The fundamental idea of Event Sourcing is to ensure that every change to the state of an application is captured in an event object and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.”With the Event Sourcing pattern, we store all the fare changes as events in the order they occurred for a booking. We iterate through these events to retrieve a complete snapshot of the modifications to the fare.A sample Fare Event looks like this:message Event {  // type of the event, ADD, SUB, SET, resilient  EventType type = 1;  // value which was added, subtracted or modified  double value = 2;  // fare for the booking after applying discount  double fare = 3;  ...  // description bytes generated by SDK  bytes description = 11;  //transactionID for the EventType  string transactionID = 12;}The Event Sourcing pattern also enable us to use the Command Query Responsibility Segregation (CQRS) pattern to decouple the read responsibility for different use cases.  Clients of the fare life cycle read the current fare and create events to change the fare value as per their logic. Clients can also access fare events, when required. This pattern enable clients to modify fares independently, and give them visibility to the sequence for different business needs.The diagram below describes the overall fare life cycle from creation, modification to display using the event store.  Architecture Overview  Clients interact with the Fare LifeCycle service through an SDK. The SDK offers various features such as metadata serialisation, deserialisation, retries, and timeouts configurations, some of which are discussed later.The Fare LifeCycle Store service uses DynamoDB as Event Store to persist and read the fare change events backed by a cache for eventually consistent reads. For further processing, such as archiving and generation of receipts, the successfully updated events are streamed out to a message queue system.Ensuring the Integrity of the Fare SequenceDemocratising the responsibility of fare modification means that multiple services might try to update the fare in parallel without prior synchronisation. Concurrent fare updates for the same booking might result in a race condition. Concurrency and consistency problems are always highlights of distributed storage systems.Let’s understand why the ordering of fare updates are important. Business rules for different cities and countries regulate the pricing features based on local market conditions and prevailing laws. For example, in some scenarios, Tolls and Waiting Fees may not be eligible for discounts or promotions. The service applying discounts needs to consider this information while applying a discount. Therefore, updates to the fare are not independent of the previous fare events.  We needed a mechanism to detect race conditions and handle them appropriately to ensure the integrity of the fare. To handle race conditions based on our use case, we explored Pessimistic and Optimistic locking mechanisms.All the expected fare change events happen based on certain conditions being true or false. For example, less than 1% of the bookings have a payment change request initiated by passengers during a ride. And, the probability of multiple similar changes happening on the same booking is rather low. Optimistic Locking offers both efficiency and correctness for our requirements where the chances of race conditions are low, and the records are independent of each other.The logic to calculate the fare/surcharge is coupled with the business logic of the system that calculates the fare component or fees. So, handling data race conditions on the data store layer was not an acceptable option either. It made more sense to let the clients handle it and keep the storage system decoupled from the business logic to compute the fare.  To achieve Optimistic Locking, we store a fare version and increment it on every successful update. The client must pass the version they read to modify the fare. Should there be a version mismatch between the update query and the current fare, the update is rejected. On version mismatches, the clients read the updated checksum(version) and retry with the recalculated fare.Idempotency of Event UpdatesThe next challenge we came across was how to handle client retries - ensuring that we do not duplicate the same event for the booking. Clients might encounter sporadic errors as a result of network-related issues, although the update was successful. Under such circumstances, clients retry to update the same event, resulting in duplicate events. Duplicate events not only result in an extra space requirement, but it also impairs the clients’ understanding on whether we’ve taken an action multiple times on the fare.As discussed in the previous section, retrying with the same version would fail due to the version mismatch. If the previous attempt successfully modified the fare, it would also update the version.However, clients might not know if their update modified the version or if any other clients updated the data. Relying on clients to check for event duplication makes the client-side complex and leaves a chance of error if the clients do not handle it correctly.  To handle the duplicate events, we associate each event with a unique UUID (transactionID) generated from the client-side using a UUID library from the Fare LifeCycle service SDK. We check whether the transactionID is already part of successful transaction IDs before updating the fare. If we identify a non-unique transactionID, we return duplicate event errors to the client.For unique transactionIDs, we append it to the list of transactionIDs and save it to the Event Store along with the event.Schema-less MetadataMetadata are the breakdowns associated with the fare. We require the metadata for specific fee/fare calculation for the generation of receipts and debugging purposes. Thus, for the storage system and multiple clients, they need not know the metadata definition of all events.One goal for our data store was to give our clients the flexibility to add new fields to existing metadata or to define new metadata without changing the API. We adopted an SDK-based approach for metadata, where clients interact with the Fare LifeCycle service via SDK. The SDK has the following responsibilities for metadata:  Serialise the metadata into bytes before making an API call to the Fare LifeCycle service.  Deserialise the bytes metadata returned from the Fare LifeCycle service into a Go struct for client access.  Serialising and deserialising the metadata on the client-side decoupled it from the Fare LifeCycle Store API. This helped teams update the metadata without deploying the storage service each time.For reading the breakdown, the clients pass the metadata bytes to the SDK along with the Event Type, and then it converts them back into the corresponding proto schema. With this approach, clients can update the metadata without changing the Data Store Service.ConclusionThe Fare LifeCycle service enabled us to revolutionise the fare storage at scale for Grab’s ecosystem of services. Further benefits realised with the system are:  The feature agnostic platform helped us to reduce the time-to-market for our hyper-local features so that we can further outserve our consumers and driver-partners.  Decoupling the fare information from the booking information also helped us to achieve a better separation of concerns between services.  Improve the overall reliability and scalability of the Grab platform by decoupling fare and booking information, allowing them to scale independently of each other.  Reduce unnecessary coupling between services to fetch fare related information and update fare.  The audit-trail of fare changes in the chronological order reduced the time to debug fare and improved our response to consumers for fare-related queries.We hope this post helped you to have a closer look at how we used the Event Source pattern for building a data store and how we handled a few caveats and challenges in the process.Authored by Sourabh Suman on behalf of the Pricing team at Grab. Special thanks to Karthik Gandhi, Kurni Famili, ChandanKumar Agarwal, Adarsh Koyya, Niteesh Mehra, Sebastian Wong, Matthew Saw, Muhammad Muneer, and Vishal Sharma for their contributions.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/democratising-fare-storage-at-scale-using-event-sourcing"
      }
      ,
    
      "keeping-170-libraries-up-to-date-on-a-large-scale-android-app": {
        "title": "Keeping 170 Libraries Up to Date on a Large Scale Android App",
        "author": "lucas-nelaupe",
        "tags": "[&quot;Mobile&quot;, &quot;Android&quot;, &quot;Engineering&quot;]",
        "category": "",
        "content": "To scale up to the needs of our consumers, we’ve adopted ways to efficiently deliver our services through our everyday superapp - whether it’s through continuous process improvements or coding best practices. For one, libraries have made it possible for us to increase our development velocity. In the passenger app Android team, we’ve a mix of libraries - from libraries that we’ve built in-house to open source ones.Every week, we release a new version of our passenger app. Each update contains on average between five to ten library updates. In this article, we will explain how we keep all libraries used by our app up to date, and the different actions we take to avoid defect leaks into production.How Many Libraries are We Using?Before we add a new library to a project, it goes through a rigorous assessment process covering many parts, such as security issue detection and usability tests measuring the impact on the app size and app startup time. This process ensures that only libraries up to our standards are added.In total, there are more than 170 libraries powering the superapp, including 55 AndroidX artifacts and 22 libraries used for the sole purpose of writing automation testing (Unit Testing or UI Testing).Who is Responsible for Updating?While we do have an internal process on how to update the libraries, it doesn’t mention who and how often it should be done. In fact, it’s everyone’s responsibility to make sure our libraries are up to date. Each team should be aware of the libraries they’re using and whenever a new version is released.However, this isn’t really the case. We’ve a few developers taking ownership of the libraries as a whole and trying to maintain it. With more than 170 external libraries, we surveyed the Android developer community on how they manage libraries in the company. The result can be summarised as follow:    Survey ResultsWhile most developers are aware of updates, they don’t update a library because the risk of defects leaking into production is too high.Risk ManagementThe risk is to have a defect leaking into production. It can cause regressions on existing features or introduce new crashes in the app. In a worst case scenario, if this isn’t caught before publishing, it can force us to make a hotfix and a certain number of users will be impacted.Before updating (bump) a library, we evaluate two metrics:  the usage of this library in the codebase.  the number of changes introduced in the library between the current version and the targeted version.The risk needs to be assessed between the number of usages of a certain library and the size of the changes. The following chart illustrate this point.    Risk Assessment RadarThis arbitrary scale helps us in deciding if we will require additional sign-off from the QA team. If the estimation places the item on the bottom-left corner, the update will be less risky while if it’s on the top-right corner, it means we should follow extra verification to reduce the risk.A good practice to reduce the risks of updating a library is to update it frequently, decreasing the diffs hence reducing the scope of impact.Reducing the RiskThe first thing we’re doing to reduce the risk is to update our libraries on a weekly basis. As described above, small changes are always less risky than large changes even if the usage of this partial library is wide. By following incremental updates, we avoid accumulating potential issues over a longer period of time.For example, the Android Jetpack and Firebase libraries follow a two-week release train. So every two weeks, we check for new updates, read the changelogs, and proceed with the update.In case of a defect detected, we can easily revert the change until we figure out a proper solution or raise the issue to the library owner.AutomationTo reduce risk on any merge request (not limited to library update), we’ve spent a tremendous amount of effort on automating tests. For each new feature we’ve a set of test cases written in Gherkin syntax.Automation is implemented as UI tests that run on continuous integration (CI) for every merge request. If those tests fail, we won’t be able to merge any changes.To further elaborate, let’s take this example: Team A developed a lot of features and now has a total of 1,000 test cases. During regression testing before each release, only a subset of those are executed manually based on the impacted area. With automation in place, team A now has 60% of those tests executed as part of CI. So, when all the tests successfully pass, we’re already 60% confident that no defect is detected. This tremendously increases our confidence level while reducing manual testing.QA Sign-offWhen the update is in the risk threshold area and the automation tests are insufficient, the developer works with QA engineers on analysing impacted areas. They would then execute test cases related to the impacted area.For example, if we’re updating Facebook library, the impacted area would be the “Login with Facebook” functionality. QA engineers would then run test cases related to social login.A single or multiple team can be involved. In some cases, QA sign-off can be required by all the teams if they’re all affected by the update.This process requires a lot of effort from different teams and can affect the current roadmap. To avoid falling into this category, we refine the impacted area analysis to be as specific as possible.Update Before it Becomes MandatoryGoogle updates the Google Play requirements regularly to ensure that published apps are fully compatible with the latest Android version.For example, starting 1st November 2020 all apps must target API 29. This change causes behaviour changes for some API. New behaviour has to be supported and verified for our code, but also for all the libraries we use. Libraries bundled inside our app are also affected if they’re using Android API. However, the support for newer API is done by each library maintainer. By keeping our libraries up to date, we ensure compatibility with the latest Android API.Key Takeaways      Keep updating your libraries. If they’re following a release plan, try to match it so it won’t accumulate too many changes. For every new release at Grab, we ship a new version each week, which includes between 5 to 10 libraries bump.        For each update, identify the potential risks on your app and find the correct balance between risk and effort required to mitigate this. Don’t overestimate the risk, especially if the changes are minimal and only include some minor bug fixing. Some library updates don’t even change any single line of code and are only documentation updates.        Invest in robust automation testing to create a high confidence level when making changes, including potentially large changes like a huge library bump.  Authored by Lucas Nelaupe on behalf of the Grab Android Development team. Special thanks to Tridip Thrizu and Karen Kue for the design and copyediting contributions.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/keeping-170-libraries-up-to-date-on-a-large-scale-android-app"
      }
      ,
    
      "optimally-scaling-kafka-consumer-applications": {
        "title": "Optimally Scaling Kafka Consumer Applications",
        "author": "shubham-badkur",
        "tags": "[&quot;Event Sourcing&quot;, &quot;Stream Processing&quot;, &quot;Kubernetes&quot;, &quot;Back End&quot;, &quot;Platform&quot;, &quot;Go&quot;]",
        "category": "",
        "content": "Earlier this year, we took you on a journey on how we built and deployed our event sourcing and stream processing framework at Grab. We’re happy to share that we’re able to reliably maintain our uptime and continue to service close to 400 billion events a week. We haven’t stopped there though. To ensure that we can scale our framework as the Grab business continuously grows, we have spent efforts optimising our infrastructure.In this article, we will dive deeper into our Kubernetes infrastructure setup for our stream processing framework. We will cover why and how we focus on optimal scalability and availability of our infrastructure.Quick Architecture Recap          The Coban platform provides lightweight Golang plugin architecture-based data processing pipelines running in Kubernetes. These are essentially Kafka consumer pods that consume data, process it, and then materialise the results into various sinks (RDMS, other Kafka topics).Anatomy of a Processing Pod          Each stream processing pod (the smallest unit of a pipeline’s deployment) has three top level components:  Trigger: An interface that connects directly to the source of the data and converts it into an event channel.  Runtime: This is the app’s entry point and the orchestrator of the pod. It manages the worker pools, triggers, event channels, and lifecycle events.  Pipeline plugin: This is provided by the user, and conforms to a contract that the platform team publishes. It contains the domain logic for the pipeline and houses the pipeline orchestration defined by a user based on our Stream Processing Framework.Optimal ScalingWe initially architected our Kubernetes setup around horizontal pod autoscaling (HPA), which scales the number of pods per deployment based on CPU and memory usage. HPA keeps CPU and memory per pod specified in the deployment manifest and scales horizontally as the load changes.These were the areas of application wastage we observed on our platform:  As Grab’s traffic is uneven, we’d always have to provision for peak traffic. As users would not (or could not) always account for ramps, they would be fairly liberal with setting limit values (CPU and memory), leading to resource wastage.  Pods often had uneven traffic distribution despite fairly even partition load distribution in Kafka. The Stream Processing Framework(SPF) is essentially Kafka consumers consuming from Kafka topics, hence the number of pods scaling in and out resulted in unequal partition load per pod.Vertically Scaling with Fixed Number of PodsWe initially kept the number of pods for a pipeline equal to the number of partitions in the topic the pipeline consumes from. This ensured even distribution of partitions to each pod providing balanced consumption. In order to abstract this from the end user, we automated the application deployment process to directly call the Kafka API to fetch the number of partitions during runtime.After achieving a fixed number of pods for the pipeline, we wanted to move away from HPA. We wanted our pods to scale up and down as the load increases or decreases without any manual intervention. Vertical pod autoscaling (VPA) solves this problem as it relieves us from any manual operation for setting up resources for our deployment.We just deploy the application and let VPA handle the resources required for its operation. It’s known to not be very susceptible to quick load changes as it trains its model to monitor the deployment’s load trend over a period of time before recommending an optimal resource. This process ensures the optimal resource allocation for our pipelines considering the historic trends on throughput.We saw a ~45% reduction in our total resource usage vs resource requested after moving to VPA with a fixed number of pods from HPA.          Managing AvailabilityWe broadly classify our workloads as latency sensitive (critical) and latency tolerant (non-critical). As a result, we could optimise scheduling and cost efficiency using priority classes and overprovisioning on heterogeneous node types on AWS.Kubernetes Priority ClassesThe main cost of running EKS in AWS is attributed to the EC2 machines that form the worker nodes for the Kubernetes cluster. Running On-Demand brings all the guarantees of instance availability but it is definitely very expensive. Hence, our first action to drive cost optimisation was to include Spot instances in our worker node group.With the uncertainty of losing a spot instance, we started assigning priority to our various applications. We then let the user choose the priority of their pipeline depending on their use case. Different priorities would result in different node affinity to different kinds of instance groups (On-Demand/Spot). For example, Critical pipelines (latency sensitive) run on On-Demand worker node groups and Non-critical pipelines (latency tolerant) on Spot instance worker node groups.We use priority class as a method of preemption, as well as a node affinity that chooses a certain priority pipeline for the node group to deploy to.OverprovisioningWith spot instances running we realised a need to make our cluster quickly respond to failures. We wanted to achieve quick rescheduling of evicted pods, hence we added overprovisioning to our cluster. This means we keep some noop pods occupying free space running in our worker node groups for the quick scheduling of evicted or deploying pods.The overprovisioned pods are the lowest priority pods, thus can be preempted by any pod waiting in the queue for scheduling. We used cluster proportional autoscaler to decide the right number of these overprovisioned pods, which scales up and down proportionally to cluster size (i.e number of nodes and CPU in worker node group). This relieves us from tuning the number of these noop pods as the cluster scales up or down over the period keeping the free space proportional to current cluster capacity.Lastly, overprovisioning also helped improve the deployment time because there is no  dependency on the time required for Auto Scaling Groups (ASG) to add a new node to the cluster every time we want to deploy a new application.Future ImprovementsEvolution is an ongoing process. In the next few months, we plan to work on custom resources for combining VPA and fixed deployment size. Our current architecture setup works fine for now, but we would like to create a more tuneable in-house CRD(Custom Resource Definition) for VPA that incorporates rightsizing our Kubernetes deployment horizontally.Authored By Shubham Badkur on behalf of the Coban team at Grab - Ryan Ooi, Karan Kamath, Hui Yang, Yuguang Xiao, Jump Char, Jason Cusick, Shrinand Thakkar, Dean Barlan, Shivam Dixit, Andy Nguyen, and Ravi Tandon.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/optimally-scaling-kafka-consumer-applications"
      }
      ,
    
      "our-journey-to-continuous-delivery-at-grab": {
        "title": "Our Journey to Continuous Delivery at Grab (Part 1)",
        "author": "sylvain-bougerel",
        "tags": "[&quot;Deployment&quot;, &quot;CI&quot;, &quot;Continuous Integration&quot;, &quot;Continuous Deployment&quot;, &quot;Deployment Process&quot;, &quot;Cloud Agnostic&quot;, &quot;Spinnaker&quot;, &quot;Continuous Delivery&quot;, &quot;Multi Cloud&quot;]",
        "category": "",
        "content": "This blog post is a two-part presentation of the effort that went into improving the continuous delivery processes for backend services at Grab in the past two years. In the first part, we take stock of where we started two years ago and describe the software and tools we created while introducing some of the integrations we’ve done to automate our software delivery in our staging environment.  Continuous Delivery is the ability to get changes of all types—including new features, configuration changes, bug fixes and experiments—into production, or into the hands of users, safely and quickly in a sustainable way.    — continuousdelivery.comAs a backend engineer at Grab, nothing matters more than the ability to innovate quickly and safely. Around the end of 2018, Grab’s transportation and deliveries backend architecture consisted of roughly 270 services (the majority being microservices). The deployment process was lengthy, required careful inputs and clear communication. The care needed to push changes in production and the risk associated with manual operations led to the introduction of a Slack bot to coordinate deployments. The bot ensures that deployments occur only during off-peak and within work hours:          Overview of the Grab Delivery Process  Once the build was completed, engineers who desired to deploy their software to the Staging environment would copy release versions from the build logs, and paste them in a Jenkins job’s parameter. Tests needed to be manually triggered from another dedicated Jenkins job.Prior to production deployments, engineers would generate their release notes via a script and update them manually in a wiki document. Deployments would be scheduled through interactions with a Slack bot that controls release notes and deployment windows. Production deployments were made once again by pasting the correct parameters into two dedicated Jenkins jobs, one for the canary (a.k.a. one-box) deployment and the other for the full deployment, spread one hour apart. During the monitoring phase, engineers would continuously observe metrics reported on our dashboards.In spite of the fragmented process and risky manual operations impacting our velocity and stability, around 614 builds were running each business day and changes were deployed on our staging environment at an average rate of 300 new code releases per business day, while production changes averaged a rate of 28 new code releases per business day.          Our Deployment Funnel, Towards the End of 2018  These figures meant that, on average, it took 10 business days between each service update in production, and only 10% of the staging deployments were eventually promoted to production.Automating Continuous Deployments at GrabWith an increased focus on Engineering efficiency, in 2018 we started an internal initiative to address frictions in deployments that became known as Conveyor. To build Conveyor with a small team of engineers, we had to rely on an already mature platform which exhibited properties that are desirable to us to achieve our mission.Hands-off DeploymentsDeployments should be an afterthought. Engineers should be as removed from the process as possible, and whenever possible, decisions should be taken early, during the code review process. The machine will do the heavy lifting, and only when it can’t decide for itself, should the engineer be involved. Notifications can be leveraged to ensure that engineers are only informed when something goes wrong and a human decision is required.          Hands-off Deployment Principle  Confidence in DeploymentsGrab’s focus on gathering internal Engineering NPS feedback helped us collect valuable metrics. One of the metrics we cared about was our engineers’ confidence in their production deployments. A team’s entire deployment process to production could last for more than a day and may extend up to a week for teams with large infrastructures running critical services. The possibility of losing progress in deployments when individual steps may last for hours is detrimental to the improvement of Engineering efficiency in the organisation. The deployment automation platform is the bedrock of that confidence. If the platform itself fails regularly or does provide a path of upgrade that is transparent to end-users, any features built on top of it would suffer from these downtimes and ultimately erode confidence in deployments.Tailored to Most But Extensible for the FewOur backend engineering teams are working on diverse stacks, and so are their deployment processes. Right from the start, we wanted our product to benefit the largest population of engineers that had adopted the same process, so as to maximise returns on our investments. To ease adoption, we decided to tailor a deployment pipeline such that:  It would model the exact sequence of manual processes followed by this population of engineers.  Switching to use that pipeline should require as little work as possible by service teams.However, in cases where this model would not fit a team’s specific process, our deployment platform should be open and extensible and support new customisations even when they are not originally supported by the product’s ecosystem.Cloud-agnosticityWhile we were going to target a specific process and team, to ensure that our solution would stand the test of time, we needed to ensure that our solution would support the variety of environments currently used in production. This variety was also likely to increase, and we wanted a platform that would mature together with the rest of our ecosystem.Overview Of ConveyorSetting Sail with SpinnakerConveyor is based on Spinnaker, an open-source, multi-cloud continuous delivery platform. We’ve chosen Spinnaker over other platforms because it is a mature deployment platform with no single point of failure, supports complex workflows (referred to as pipelines in Spinnaker), and already supports a large array of cloud providers. Since Spinnaker is open-source and extensible, it allowed us to add the features we needed for the specificity of our ecosystem.To further ease adoption within our organization, we built a tailored user interface and created our own domain-specific language (DSL) to manage its pipelines as code.          Outline of Conveyor's Architecture  Onboarding to a Simpler InterfaceSpinnaker comes with its own interface, it has all the features an engineer would want from an advanced continuous delivery system. However, Spinnaker interface is vastly different from Jenkins and makes for a steep learning curve.To reduce our barrier to adoption, we decided early on to create a simple interface for our users. In this interface, deployment pipelines take the center stage of our application. Pipelines are objects managed by Spinnaker, they model the different steps in the workflow of each deployment. Each pipeline is made up of stages that can be assembled like lego-bricks to form the final pipeline. An instance of a pipeline is called an execution.          Conveyor Dashboard  With this interface, each engineer can focus on what matters to them immediately: the pipelines they have started, or those started by other teammates working on the same services as they are. Conveyor also provides a search bar (on the top) and filters (on the left) that work in concert to explore all pipelines executed at Grab.We adopted a consistent set of colours to model all information in our interface:  blue: represent stages that are currently running;  red: stages that have failed or important information;  yellow: stages that require human interaction;  and finally, in green: stages that were successfully completed.Conveyor also provides a task and notifications area, where all stages requiring human intervention are listed in one location. Manual interactions are often no more than just YES or NO questions:          Conveyor Tasks  Finally, in addition to supporting automated deployments, we greatly simplified the start of manual deployments. Instead of being required to copy/paste information, each parameter can be selected on the interface from a set of predefined items, sorted chronologically, and presented with contextual information to help engineers in their decision.Several parameters are required for our deployments and their values are selected from the UI to ensure correctness.          Simplified Manual Deployments  Ease of Adoption with Our Pipeline-as-code DSLEase of adoption for the team is not simply about the learning curve of the new tools. We needed to make it easy for teams to configure their services to deploy with Conveyor. Since we focused on automating tasks that were already performed manually, we needed only to configure the layer that would enable the integration.We set on creating a pipeline-as-code implementation when none were widely being developed in the Spinnaker community. It’s interesting to see that two years on, this idea has grown in parallel in the community, with the birth of other pipeline-as-code implementations. Our pipeline-as-code is referred to as the Pipeline DSL, and its configuration is located inside each team’s repository. Artificer is the name of our Pipeline DSL interpreter and it runs with every change inside our monorepository:          Artificer: Our Pipeline DSL  Pipelines are being updated at every commit if necessary.Creating a conveyor.jsonnet file within the service’s directory of our monorepository with the few lines below is all that’s required for Artificer to do its work and get the benefits of automation provided by Conveyor’s pipeline:local default = import 'default.libsonnet';[  {     name: \"service-name\",     group: [       \"group-name\",     ]  }]Sample minimal conveyor.jsonnet configuration to onboard services.In this file, engineers simply specify the name of their service and the group that a user should belong to, to have deployment rights for the service.Once the build is completed, teams can log in to Conveyor and start manual deployments of their services with our pipelines. Three pipelines are provided by default: the integration pipeline used for tests and developments, the staging pipeline used for pre-production tests, and the production pipeline for production deployment.Thanks to the simplicity of this minimal configuration file, we were able to generate these configuration files for all existing services of our monorepository. This resulted in the automatic onboarding of a large number of teams and was a major contributing factor to the adoption of Conveyor throughout our organisation.Our Journey to Engineering Efficiency (for Backend Services)The sections below relate some of the improvements in engineering efficiency we’ve delivered since Conveyor’s inception. They were not made precisely in this order but for readability, they have been mapped to each step of the software development lifecycle.Automate Deployments at Build Time          Continuous Integration Job  Continuous delivery begins with a pushed code commit in our trunk-based development flow. Whenever a developer pushes changes onto their development branch or onto the trunk, a continuous integration job is triggered on Jenkins. The products of this job (binaries, docker images, etc) are all uploaded into our artefact repositories. We’ve made two additions to our continuous integration process.The first modification happens at the step “Upload &amp; Register artefacts”. At this step, each artefact created is now registered in Conveyor with its associated metadata. When and if an engineer needs to trigger a deployment manually, Conveyor can display the list of versions to choose from, eliminating the need for error-prone manual inputs:          Staging  Each selectable version shows contextual information: title, author, version and link to the code change where it originated. During registration, the commit time is also recorded and used to order entries chronologically in the interface. To ensure this integration is not a single point of failure for deployments, manual input is still available optionally.The second modification implements one of the essential feature continuous delivery: your deployments should happen often, automatically. Engineers are now given the possibility to start automatic deployments once continuous integration has successfully completed, by simply modifying their project’s continuous integration settings:{  \"AfterBuild\": [    {      \"AutoDeploy\": {        \"OnDiff\": false,        \"OnLand\": true      },      \"TYPE\": \"conveyor\"    }  ],  // other settings...}Sample settings needed to trigger auto-deployments. Diff refers to code review submissions, and Land refers to merged code changes.Staging PipelineBefore deploying a new artefact to a service in production, changes are validated on the staging environment. During the staging deployment, we verify that canary (one-box) deployments and full deployments with automated smoke and functional tests suites.          Staging Pipeline  We start by acquiring a deployment lock for this service and this environment. This prevents another deployment of the same service on the same environment to happen concurrently, other deployments will be waiting in a FIFO queue until the lock is released.The stage “Compute Changeset” ensures that the deployment is not a rollback. It verifies that the new version deployed does not correspond to a rollback by comparing the ancestry of the commits provided during the artefact registration at build time: since we automate deployments after the build process has completed, cases of rollback may occur when two changes are created in quick succession and the latest build completes earlier than the older one.After the stage “Deploy Canary” has completed, smoke test run. There are three kinds of tests executed at different stages of the pipeline: smoke, functional and security tests. Smoke tests directly reach the canary instance’s endpoint, by-passing load-balancers. If the smoke tests fail, the canary is immediately rolled back and this deployment is terminated.All tests are generated from the same builds as the artefact being tested and their versions must match during testing. To ensure that the right version of the test run and distinguish between the different kind of tests to perform, we provide additional metadata that will be passed by Conveyor to the tests system, known internally as Gandalf:local default = import 'default.libsonnet';[  {    name: \"service-name\",    group: [      \"group-name\",    ],    gandalf_smoke_tests: [      {        path: \"repo.internal/path/to/my/smoke/tests\"      }    ],    gandalf_functional_tests: [      {        path: \"repo.internal/path/to/my/functional/tests\"      }    ],    gandalf_security_tests: [      {        path: \"repo.internal/path/to/my/security/tests\"      }    ]  }]Sample conveyor.jsonnet configuration with integration tests added.Additionally, in parallel to the execution of the smoke tests, the canary is also being monitored from the moment its deployment has completed and for a predetermined duration. We leverage our integration with Datadog to allow engineers to select the alerts to monitor. If an alert is triggered during the monitoring period, and while the tests are executed, the canary is again rolled back, and the pipeline is terminated. Engineers can specify the alerts by adding them to the conveyor.jsonnet configuration file together with the monitoring duration:local default = import 'default.libsonnet';[  {    name: \"service-name\",    group: [      \"group-name\",    ],    gandalf_smoke_tests: [      {        path: \"repo.internal/path/to/my/smoke/tests\"      }    ],    gandalf_functional_tests: [      {        path: \"repo.internal/path/to/my/functional/tests\"      }    ],    gandalf_security_tests: [      {        path: \"repo.internal/path/to/my/security/tests\"      }    ],    monitor: {      stg: {        duration_seconds: 300,        alarms: [          {            type: \"datadog\",            alert_id: 12345678          },          {            type: \"datadog\",            alert_id: 23456789          }        ]      }    }  }]Sample conveyor.jsonnet configuration with alerts in staging added.When the smoke tests and monitor pass and the deployment of new artefacts is completed, the pipeline execution triggers functional and security tests. Unlike smoke tests, functional &amp; security tests run only after that step, as they communicate with the cluster through load-balancers, impersonating other services.Before releasing the lock, release notes are generated to inform engineers of the delta of changes between the version they just released and the one currently running in production. Once the lock is released, the stage “Check Policies” verifies that the parameters and variable of the deployment obeys a specific set of criteria, for example: if its service metadata is up-to-date in our service inventory, or if the base image used during deployment is sufficiently recent.Here’s how the policy stage, the engine, and the providers interact with each other:          Check Policy Stage  In Spinnaker, each event of a pipeline’s execution updates the pipeline’s state in the database. The current state of the pipeline can be fetched by its API as a single JSON document, describing all information related to its execution: including its parameters, the contextual information related to each stage or even the response from the various interfacing components. The role of our “Policy Check” stage is to query this JSON representation of the pipeline, to extract and transform the variables which are forwarded to our policy engine for validation. Our policy engine gathers judgements passed by different policy providers. If the validation by the policy engine fails, the deployment is not rolled back this time; however, promotion to production is not possible and the pipeline is immediately terminated.The journey through staging deployment finally ends with the stage “Register Deployment”. This stage registers that a successful deployment was made in our staging environment as an artefact. Similarly to the policy check above, certain parameters of the deployment are picked up and consolidated into this document. We use this kind of artefact as proof for upcoming production deployment.Continuing Our Journey to Engineering EfficiencyWith the advancements made in continuous integration and deployment to staging, Conveyor has reduced the efforts needed by our engineers to just three clicks in its interface, when automated deployment is used. Even when the deployment is triggered manually, Conveyor gives the assurance that the parameters selected are valid and it does away with copy/pasting and human interactions across heterogeneous tools.In the sequel to this blog post, we’ll dive into the improvements that we’ve made to our production deployments and introduce a crucial concept that led to the creation of our proof for successful staging deployment. Finally, we’ll cover the impact that Conveyor had on the continuous delivery of our backend services, by comparing our deployment velocity when we started two years ago versus where we are today.All these improvements in efficiency for our engineers would never have been possible without the hard work of all team members involved in the project, past and present: Evan Sebastian, Tanun Chalermsinsuwan, Aufar Gilbran, Deepak Ramakrishnaiah, Repon Kumar Roy (Kowshik), Su Han, Voislav Dimitrijevikj, Qijia Wang, Oscar Ng, Jacob Sunny, Subhodip Mandal, and many others who have contributed and collaborated with them.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/our-journey-to-continuous-delivery-at-grab"
      }
      ,
    
      "uncovering-the-truth-behind-lua-and-redis-data-consistency": {
        "title": "Uncovering the Truth Behind Lua and Redis Data Consistency",
        "author": "allen-wang",
        "tags": "[&quot;Redis&quot;, &quot;Lua Scripts&quot;, &quot;High CPU Usage&quot;, &quot;Data Consistency&quot;]",
        "category": "",
        "content": "Our team at Grab uses Redis as one of our message queues. The Redis server is deployed in a master/replica setup. Quite recently, we have been noticing a spike in the CPU usage of the Redis replicas every time we deploy our service, even when the replicas are not in use and when there’s no read traffic to it. However, the issue is resolved once we reboot the replica.Because a reboot of the replica fixes the issue every time, we thought that it might be due to some Elasticache replication issues and didn’t pursue further. However, a recent Redis failover brought this to our attention again. After the failover, the problematic replica becomes the new master and its CPU immediately goes to 100% with the read traffic, which essentially means the cluster is not functional after the failover. And this time we investigated the issue with new vigour. What we found in our investigation led us to deep dive into the details of Redis replication and its implementation of Hash.Did you know that Redis master/replica can become inconsistent in certain scenarios?Did you know the encoding of Hash objects on the master and the replica are different even if the writing operations are exactly the same and in the same order? Read on to find out why.The ProblemThe following graph shows the CPU utilisation of the master vs. the replica immediately after our service is deployed.    CPU UtilizationFrom the graph, you can see the following CPU usage trends. Replica’s CPU usage:  Increases immediately after our service is deployed.  Spikes higher than the master after a certain time.  Get’s back to normal after a reboot.Cursory InvestigationBecause the spike occurs only when we deploy our service, we scrutinised all the scripts that were triggered immediately after the deployment. Lua monitor script was identified as a possible suspect. The script redistributes inactive service instances’ messages in the queue to active service instances so that messages can be processed by other healthy instances.We ran a few experiments related to the Lua monitor script using the Redis monitor command to compare the script’s behaviour on master and the replica. A side note, because this command causes performance degradation, use it with discretion. Coming back to the script, we were surprised to note that the monitor script behaves differently between the master and the replica:  Redis executes the script separately on the master and the replica. We expected the script to execute only on master and the resulting changes to be replicated to the secondary.  The Redis command HGETALL used in the script returns the hash keys in a different order on master compared to the replica.Due to the above reasons, the script causes data inconsistencies between the master and its replica. From that point on, the data between the master and the replica keeps diverging till they become completely distinct. Due to the inconsistency, the data on the secondary does not get deleted correctly thereby growing into an extremely large dataset. Any further operations on the large dataset requires a higher CPU usage, which explains why the replica’s CPU usage is higher than the master.During replica reboots, the data gets synced and consistent again, which is why the CPU usage gets to normal values after rebooting.Diving Deeper on HGETALLWe knew that the keys of a hash are not ordered and we should not rely on the order. But it still puzzled us that the order is different even when the writing sequence is the same between the master and the replica. Plus the fact that the orders are always the same in our local environment with a similar setup made us even more curious.So to better understand the underlying magic of Redis and to avoid similar bugs in the future, we decided to hammer on and read the Redis source code to get more details.HGETALL Command Handling CodeThe HGETALL command is handled by the function genericHgetallCommand and it further calls hashTypeNext to iterate through the Hash object. A snippet of the code is shown as follows:/* Move to the next entry in the hash. Return C_OK when the next entry * could be found and C_ERR when the iterator reaches the end. */int hashTypeNext(hashTypeIterator *hi) {    if (hi-&gt;encoding == OBJ_ENCODING_ZIPLIST) {        // call zipListNext    } else if (hi-&gt;encoding == OBJ_ENCODING_HT) {        // call dictNext    } else {        serverPanic(\"Unknown hash encoding\");    }    return C_OK;}From the code snippet, you can see that the Redis Hash object actually has two underlying representations:  ZIPLIST  HASHTABLE (dict)A bit of research online helped us understand that, to save memory, Redis chooses between the two hash representations based on the following limits:  By default, Redis stores the Hash object as a zipped list when the hash has less than 512 entries and when each element’s size is smaller than 64 bytes.  If either limit is exceeded, Redis converts the list to a hashtable, and this is irreversible. That is, Redis won’t convert the hashtable back to a list again, even if the entries/size falls below the limit.Eureka MomentBased on this understanding, we checked the encoding of the problematic hash in staging.stg-bookings-qu-002.pcxebj.0001.apse1.cache.amazonaws.com:6379&gt; object encoding queue_stats\"hashtable\"stg-bookings-qu-001.pcxebj.0001.apse1.cache.amazonaws.com:6379&gt; object encoding queue_stats\"ziplist\"To our surprise, the encodings of the Hash object on the master and its replica were different. Which means if we add or delete elements in the hash, the sequence of the keys won’t be the same due to hashtable operation vs. list operation!Now that we have identified the root cause, we were still curious about the difference in encoding between the master and the replica.How Could the Underlying Representations be Different?We reasoned, “If the master and its replica’s writing operations are exactly the same and in the same order, why are the underlying representations still different?”To answer this, we further looked through the Redis source to find all the possible places that a Hash object’s representation could be changed and soon found the following code snippet:/* Load a Redis object of the specified type from the specified file. * On success a newly allocated object is returned, otherwise NULL. */robj *rdbLoadObject(int rdbtype, rio *rdb) {  //...  if (rdbtype == RDB_TYPE_HASH) {    //...    o = createHashObject();  // ziplist    /* Too many entries? Use a hash table. */    if (len &gt; server.hash_max_ziplist_entries)        hashTypeConvert(o, OBJ_ENCODING_HT);    //...  }}Reading through the code we understand the following behaviour:  When restoring from an RDB file, Redis creates a ziplist first for Hash objects.  Only when the size of the Hash object is greater than the hash_max_ziplist_entries, the ziplist is converted to a hashtable.So, if you have a Redis Hash object encoded as a hashtable with its length less than hash_max_ziplist_entries (512) in the master, when you set up a replica, it is encoded as a ziplist.We were able to verify this behaviour in our local setup as well.How did We Fix it?We could use the following two approaches to address this issue:  Enable script effect replication mode. This tells Redis to replicate the commands generated by the script instead of running the whole script on the replica. One disadvantage to using this approach is that it adds network traffic between the master and the replica.  Ensure the behaviour of the Lua monitor script is deterministic. In our case, we can do this by sorting the outputs of HKEYS/HGETALL.We chose the latter approach because:  The Hash object is pretty small ( &lt; 30 elements) so the sorting overhead is low, less than 1ms for 100 elements based on our tests.  Replicating our script effect would end up replicating thousands of Redis writing commands on the secondary causing a much higher overhead compared to replicating just the script.After the fix, the CPU usage of the replica remained in range after each deployment. This also prevented the Redis cluster from being destroyed in the event of a master failover.Key TakeawaysIn addition to writing clear and maintainable code, it’s equally important to understand the underlying storage layer that you are dealing with to produce efficient and bug-free code.The following are some of the key learnings on Redis:  Redis does not guarantee the consistency between master and its replica nodes when Lua scripts are used. You have to ensure that the behaviour of the scripts are deterministic to avoid data inconsistency.  Redis replicates the whole Lua script instead of the resulting commands to the replica. However, this is the default behaviour and you can disable it.  To save memory, Redis uses different representations for Hash. Your Hash object could be stored as a list in memory or a hashtable. This is not guaranteed to be the same across the master and its replicas.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/uncovering-the-truth-behind-lua-and-redis-data-consistency"
      }
      ,
    
      "data-gateway": {
        "title": "Securing and Managing Multi-cloud Presto Clusters with Grab’s DataGateway",
        "author": "vinnson-lee",
        "tags": "[&quot;Engineering&quot;, &quot;Presto&quot;, &quot;Data&quot;, &quot;Data Pipeline&quot;, &quot;Access Control&quot;, &quot;Workload Distribution&quot;, &quot;Cluster&quot;]",
        "category": "",
        "content": "IntroductionData is the lifeblood of Grab and the insights we gain from it drive all the most critical business decisions made by Grabbers and our leaders every day.Grab’s Data Engineering (DE) team is responsible for maintaining the data platform, which consists of data pipelines, job schedulers, and the query/computation engines that are the key components for generating insights from data. SQL is the core language for analytics at Grab and as of early 2020, our Presto platform serves about 200 user groups that add up to 500 users who run 350,000 queries every day. These queries span across 10,000 tables that process up to 1PB of data daily.In 2016, we started the DataGateway project to enable us to manage data access for the hundreds of Grabbers who needed access to Presto for their work. Since then, DataGateway has grown to become much more than just an access control mechanism for Presto. In this blog, we want to share what we’ve achieved since the initial launch of the project.The Problems We Wanted to SolveAs we were reviewing the key challenges around data access in Grab and assessing possible solutions, we came up with this prioritised list of user requirements we wanted to work on:  Use a single endpoint to serve everyone.  Manage user access to clusters, schemas, tables, and fields.  Provide seamless user experience when presto clusters are scaled up/down, in/out, or provisioned/decommissioned.  Capture audit trail of user activities.To provide Grabbers with the critical need of interactive querying, as well as performing extract, transform, load (ETL) jobs, we evaluated several technologies. Presto was among the ones we evaluated, and was what we eventually chose although it didn’t meet all of our requirements out of the box. In order to address these gaps, we came up with the idea of a security gateway for the Presto compute engine that could also act as a load balancer/proxy, this is how we ended up creating the DataGateway.DataGateway is a service that sits between clients and Presto clusters. It is essentially a smart HTTP proxy server that is an abstraction layer on top of the Presto clusters that handles the following actions:  Parse incoming SQL statements to get requested schemas, tables, and fields.  Manage user Access Control List (ACL) to limit users’ data access by checking against the SQL parsing results.  Manage users’ cluster access.  Redirect users’ traffic to the authorised clusters.  Show meaningful error messages to users whenever the query is rejected or exceptions from clusters are encountered.Anatomy of DataGatewayThe DataGateway’s key components are as follows:  API Service  SQL Parser  Auth framework  Administration UIWe leveraged Kubernetes to run all these components as microservices.          Figure 1. DataGateway Key Components  API ServiceThis is the component that manages all users and cluster-facing processes. We integrated this service with the Presto API, which means it appears to be the same as a Presto cluster to a client. It accepts query requests from clients, gets the parsing result and runs authorisation from the SQL Parser and the Auth Framework.If everything is good to go, the API Service forwards queries to the assigned clusters and continues the entire query process.Auth FrameworkThis handles both authentication and authorisation requests. It stores the ACL of users and communicates with the API Service and the SQL Parser to run the entire authentication process. But why is it a microservice instead of a module in API Service, you ask? It’s because we keep evolving the security checks at Grab to ensure that everything is compliant with our security requirements, especially when dealing with data.We wanted to make it flexible to fulfil ad-hoc requests from the security team without affecting the API Service. Furthermore, there are different authentication methods out there that we might need to deal with (OAuth2, SSO, you name it). The API Service supports multiple authentication frameworks that enable different authentication methods for different users.SQL ParserThis is a SQL parsing engine to get schema, tables, and fields by reading SQL statements. Since Presto SQL parsing works differently in each version, we would compile multiple SQL Parsers that are identical to the Presto clusters we run. The SQL Parser becomes the single source of truth.Admin UIThis is a UI for Presto administrators to manage clusters and user access, as well as to select an authentication framework, making it easier for the administrators to deal with the entire ecosystem.How We Deployed DataGateway Using KubernetesIn the past couple of years, we’ve had significant growth in workloads from analysts and data scientists. As we were very enthusiastic about Kubernetes, DataGateway was chosen as one of the earliest services for deployment in Kubernetes. DataGateway in Kubernetes is known to be highly available and fully scalable to handle traffic from users and systems.We also tested the HPA feature of Kubernetes, which is a dynamic scaling feature to scale in or out the number of pods based on actual traffic and resource consumption.          Figure 2. DataGateway deployment using Kubernetes  Functionality of DataGatewayThis section highlights some of the ways we use DataGateway to manage our Presto ecosystem efficiently.Restrict Users Based on Schema/Table Level AccessIn a setup where a Presto cluster is deployed on AWS Amazon Elastic MapReduce (EMR) or Elastic Kubernetes Service (EKS), we configure an IAM role and attach it to the EMR or EKS nodes. The IAM role is set to limit the access to S3 storage. However, the IAM only provides bucket-level and file-level control; it doesn’t meet our requirements to have schema, table, and column-level ACLs. That’s how DataGateway is found useful in such scenarios.One of the DataGateway services is an SQL Parser. As previously covered, this is a service that parses and digs out schemas and tables involved in a query. The API service receives the parsing result and checks against the ACL of users, and decides whether to allow or reject the query. This is a remarkable improvement in our security control since we now have another layer to restrict access, on top of the S3 storage. We’ve implemented an SQL-based access control down to table level.As shown in the Figure 3, user A is trying run a SQL statement select * from locations.cities. The SQL Parser reads the statement and tells the API service that user A is trying to read data from the table cities in the schema locations. Then, the API service checks against the ACL of user A. The service finds that user A has only read access to table countries in schema locations. Eventually, the API service denies this attempt because user A doesn’t have read access to table cities in the schema locations.          Figure 3. An example of how to check user access to run SQL statements  The above flow shows an access denied result because the user doesn’t have the appropriate permissions.Seamless User Experience During the EMR MigrationWe use AWS EMR to deploy Presto as an SQL query engine since deployment is really easy. However, without DataGateway, any EMR operations such as terminations, new cluster deployment, config changes, and version upgrades, would require quite a bit of user involvement. We would sometimes need users to make changes on their side. For example, request users to change the endpoints to connect to suitable clusters.With DataGateway, ACLs exist for each of the user accounts. The ACL includes the list of EMR clusters that users are allowed to access. As a Presto access management platform, here the DataGateway redirects user traffics to an appropriate cluster based on the ACL, like a proxy. Users always connect to the same endpoint we offer, which is the DataGateway. To switch over from one cluster to another, we just need to edit the cluster ACL and everything is handled seamlessly.          Figure 4. Cluster switching using DataGateway  Figure 4 highlights the case when we’re switching EMR from one cluster to another. No changes are required from users.We executed the migration of our entire Presto platform from an AWS EMR instance to another AWS EMR instance using the same methodology. The migrations were executed with little to no disruption for our users. We were able to move 40 clusters with hundreds of users. They were able to issue millions of queries daily in a few phases over a couple of months.In most cases, users didn’t have to make any changes on their end, they just continued using Presto as usual while we made the changes in the background.Multi-cloud Data Lake/Presto Cluster MaintenanceRecently, we started to build and maintain data lakes not just in one cloud, but two - in AWS and Azure. Since most end-users are AWS-based, and each team has their own AWS sub-account to run their services and workloads, it would be a nightmare to bridge all the connections and access routes between these two clouds from end-to-end, sub-account by sub-account.Here, the DataGateway plays the role of the multi-cloud gateway. Since all end-users’ AWS sub-accounts have peered to DataGateway’s network, everything becomes much easier to handle.For end-users, they retain the same Presto connection profile. The DE team then handles the connection setup from DataGateway to Azure, and also the deployment of Presto clusters in Azure.When all is set, end-users use the same endpoint to DataGateway. We offer a feature called Cluster Switch that allows users to switch between AWS Presto cluster and Azure Presto Cluster on the fly by filling in parameters on the connection string. This feature allows users to switch to their target Presto cluster without any endpoint changes. The switch works instantly whenever they do the change. That means users can run different queries in different clusters based on their requirements.This feature has helped the DE team to maintain Presto Cluster easily. We can spin up different Presto clusters for different teams, so that each team has their own query engine to run their queries with dedicated resources.          Figure 5. Sub-account connections and Queries  Figure 5 shows an example of how sub-accounts connect to DataGateway and run queries on resources in different clouds and clusters.          Figure 6. Sample scenario without DataGateway  Figure 6 shows a scenario of what would happen if DataGatway doesn’t exist. Each of the accounts would have to maintain its own connections, Virtual Private Cloud (VPC) peering, and express link to connect to our Presto resources.SummaryDataGateway is playing a key role in Grab’s entire Presto ecosystem. It helps us manage user access and cluster selections on a single endpoint, ensuring that everyone is running their Presto queries on the same place. It also helps distribute workload to different types and versions of Presto clusters.When we started to deploy the DataGateway on Kubernetes, our vision for the Presto ecosystem underwent an epic change as it further motivated us to continuously improve. Since then, we’ve had new ideas on deployment method/pipeline, microservice implementations, scaling strategy, resource control, we even made use of Kubernetes and designed an on-demand, container-based Presto cluster provisioning engine. We’ll share this in another engineering blog, so do stay tuned!.We also made crucial enhancements on data access control as we extended Presto’s access controls down to the schema/table-level.In day-to-day operations, especially when we started to implement data lake in multiple clouds, DataGateway solved a lot of implementation issues. DataGateway made it simpler to switch a user’s Presto cluster from one cloud to another or allow a user to use a different Presto cluster using parameters. DataGateway allowed us to provide a seamless experience to our users.Looking forward, we’ve more and more ideas for our Presto ecosystem, such Spark DataGateway or AWS Athena integrations, to keep our data safe at any time and to provide our users with a smoother experience when dealing with data used for analysis or research.Authored by Vinnson Lee on behalf of the Presto Development Team at Grab - Edwin Law, Qui Hieu Nguyen, Rahul Penti, Wenli Wan, Wang Hui and the Data Engineering Team.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/data-gateway"
      }
      ,
    
      "go-module-a-guide-for-monorepos-part-2": {
        "title": "Go Modules- A Guide for monorepos (Part 2)",
        "author": "michael-cartmell",
        "tags": "[&quot;Go&quot;, &quot;Monorepo&quot;, &quot;Vendoring&quot;, &quot;Vendors&quot;, &quot;Libraries&quot;]",
        "category": "",
        "content": "This is the second post on the Go module series, which highlights Grab’s experience working with Go modules in a multi-module monorepo. In this article, we’ll focus on suggested solutions for catching unexpected changes to the go.mod file and addressing dependency issues. We’ll also cover automatic upgrades and other learnings uncovered from the initial obstacles in using Go modules.Vendoring Process IssuesOur previous vendoring process fell solely on the developer who wanted to add or update a dependency. However, it was often the case that the developer came across many unexpected changes due to previous vendoring attempts, accidental imports and changes to dependencies.The developer would then have to resolve these issues before being able to make a change, costing time and causing frustration with the process. It became clear that it wasn’t practical to expect the developer to catch all of the potential issues while vendoring, especially since Go modules itself was new and still in development.Avoiding Unexpected ChangesReluctantly, we added a check to our CI process which ran on every merge request. This helped ensure that there are no unexpected changes required to go mod. This added time to every build and often flagged a failure, but it saved a lot of post-merge hassle. We then realised that we should have done this from the beginning.Since we hadn’t enabled Go modules for builds yet, we couldn’t rely on the \\mod=readonly flag. We implemented the check by running go mod vendor and then checking the resulting difference.If there were any changes to go.mod or the vendor directory, the merge request would get rejected. This worked well in ensuring the integrity of our go.mod.Roadblocks and LearningsHowever, as this was the first time we were using Go modules on our CI system, it uncovered some more problems.Private Repository AccessThere was the problem of accessing private repositories. We had to ensure that the CI system was able to clone all of our private repositories as well as the main monorepo, by adding the relevant SSH deploy keys to the repository.False PositivesThe check sometimes fired false positives - detecting a go mod failure when there were no changes. This was often due to network issues, especially when the modules are hosted by less reliable third-party servers. This is somewhat solved in Go 1.13 onwards with the introduction of proxy servers, but our workaround was simply to retry the command several times.We also avoided adding dependencies hosted by a domain that we haven’t seen before, unless absolutely necessary.Inconsistent Go VersionsWe found several inconsistencies between Go versions - running go mod vendor on one Go version gave different results to another. One example was a change to the checksums. These inconsistencies are less common now, but still remain between Go 1.12 and later versions. The only solution is to stick to a single version when running the vendoring process.Automated UpgradesThere are benefits to using Go modules for vendoring. It’s faster than previous solutions, better supported by the community and part of the language, so it doesn’t require any extra tools or wrappers to use it.One of the most useful benefits from using Go modules is that it enables automated upgrades of dependencies in the go.mod file - and it becomes more useful as more third-party modules adopt Go modules and semantic versioning.    Automated updates workflowWe call our solution for automating updates at Grab the AutoVend Bot. It is built around a single Go command, go list -m -u all, which finds and lists available updates to the dependencies listed in go.mod (add \\json for JSON output). We integrated the bot with our development workflow and change-request system to take the output from this command and create merge requests automatically, one per update.Once the merge request is approved (by a human, after verifying the test results), the bot would push the change. We have hundreds of dependencies in our main monorepo module, so we’ve scheduled it to run a small number each day so we’re not overwhelmed.By reducing the manual effort required to update dependencies to almost nothing, we have been able to apply hundreds of updates to our dependencies, and ensure our most critical dependencies are on the latest version. This not only helps keep our dependencies free from bugs and security flaws, but it makes future updates far easier and less impactful by reducing the set of changes needed.In SummaryUsing Go modules for vendoring has given us valuable and low-risk exposure to the feature. We have been able to detect and solve issues early, without affecting our regular builds, and develop tooling that’ll help us in future.Although Go modules is part of the standard Go toolchain, it shouldn’t be viewed as a complete off the shelf solution that can be dropped into a codebase, especially a monorepo.Like many other Go tools, the Modules feature comprises many small, focused tools that work best when combined together with other code. By embracing this concept and leveraging things like go list, go mod graph and go mod vendor, Go modules can be made to integrate into existing workflows, and deliver the benefits of structured versioning and reproducible builds.I hope you have enjoyed this article on using Go modules and vendoring within a monorepo.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!CreditsThe cute Go gopher logo for this blog’s cover image was inspired by Renee French’s original work.",
        "url": "/go-module-a-guide-for-monorepos-part-2"
      }
      ,
    
      "the-journey-of-deploying-apache-airflow-at-grab": {
        "title": "The Journey of Deploying Apache Airflow at Grab",
        "author": "chandulal-kavar",
        "tags": "[&quot;Engineering&quot;, &quot;Data Pipeline&quot;, &quot;Scheduling&quot;, &quot;Airflow&quot;, &quot;Kubernetes&quot;, &quot;Platform&quot;]",
        "category": "",
        "content": "At Grab, we use Apache Airflow to schedule and orchestrate the ingestion and transformation of data,  train machine learning models, and the copy data between clouds. There are many engineering teams at Grab that use Airflow, each of which originally had their own Airflow instance.The proliferation of independently managed Airflow instances resulted in inefficient use of resources, where each team ended up solving the same problems of logging, scaling, monitoring, and more. From this morass came the idea of having a single dedicated team to manage all the Airflow instances for anyone in Grab that wants to use Airflow as a scheduling tool.We designed and implemented an Apache Airflow-based scheduling and orchestration platform that currently runs close to 20 Airflow instances for different teams at Grab. Sounds interesting? What follows is a brief history.Early DaysCirca 2018, we were running a few hundred Directed Acyclic Graphs (DAGs) on one Airflow instance in the Data Engineering team. There was no dedicated team to maintain it, and no Airflow expert in our team. We were struggling to maintain our Airflow instance, which was causing many jobs to fail every day. We were facing issues with library management, scaling, managing and syncing artefacts across all Airflow components, upgrading Airflow versions, deployment, rollbacks, etc.After a few postmortem reports, we realised that we needed a dedicated team to maintain our Airflow. This was how our Airflow team was born.In the initial months, we dedicated ourselves to stabilising our Airflow environment. During this process, we realised that Airflow has a steep learning curve and requires time and effort to understand and maintain properly. Also, we found that tweaking of Airflow configurations required a thorough understanding of Airflow internals.We felt that for the benefit of everyone at Grab, we should leverage what we learned about Airflow to help other teams at Grab; there was no need for anyone else to go through the same struggles we did. That’s when we started thinking about managing Airflow for other teams.We talked to the Data Science and Engineering teams who were also running Airflow to schedule their jobs. Almost all the teams were struggling to maintain their Airflow instance. A few teams didn’t have enough technical expertise to maintain their instance. The Data Scientists and Analysts that we spoke to were more than happy to outsource the overhead of Airflow maintenance and wanted to focus more on their Data Science use cases instead.We started working with one of the Data Science teams and initiated the discussion to create a dockerised Airflow instance and run it on our Kubernetes cluster.We created the Airflow instance and maintained it for them. Later, we were approached by two more teams to help with their Airflow instances. This was the trigger for us to design and create a platform on which we can efficiently manage Airflow instances for different teams.Current StateAs mentioned, we are currently serving close to 20 Airflow instances for various teams on this platform and leverage Apache Airflow to schedule thousands of daily jobs.  Each Airflow instance is currently scheduling 1k to 60k daily jobs. Also, new teams can quickly try out Airflow without worrying about infrastructure and maintenance overhead. Let’s go through the important aspects of this platform such as design considerations, architecture, deployment, scalability, dependency management, monitoring and alerting, and more.Design ConsiderationsThe initial step we took towards building our scheduling platform was to define a set of expectations and guidelines around ownership, infrastructure, authentication, common artifacts and CI/CD, to name a few.These were the considerations we had in mind:  Deploy containerised Airflow instances on Kubernetes cluster to isolate Airflow instances at the team level. It should scale up and scale out according to usage.  Each team can have different sets of jobs that require specific dependencies on the Airflow server.  Provide common CI/CD templates to build, test, and deploy Airflow instances. These CI/CD templates should be flexible enough to be extended by users and modified according to their use case.  Common plugins, operators, hooks, sensors will be shipped to all Airflow instances. Moreover, each team can have its own plugins, operators, hooks, and sensors.  Support LDAP based authentication as it is natively supported by Apache Airflow. Each team can authenticate Airflow UI by their LDAP credentials.  Use the Hashicorp Vault to store Airflow specific secrets. Inject these secrets via sidecar in Airflow servers.  Use ELK stack to access all application logs and infrastructure logs.  Datadog and PagerDuty will be used for monitoring and alerting.  Ingest job statistics such as total number of jobs scheduled, no of failed jobs, no of successful jobs, active DAGs, etc. into the data lake and will be accessible via Presto.    Architecture diagramInfrastructure ManagementInitially, we started deploying Airflow instances on Kubernetes clusters managed via Kubernetes Operations (KOPS). Later, we migrated to Amazon EKS to reduce the overhead of managing the Kubernetes control plane. Each Kubernetes namespace deploys one Airflow instance.We chose Terraform to manage infrastructure as code. We deployed each Airflow instance using Terraform modules, which include a helm_release Terraform resource on top of our customised Airflow Helm Chart.Each Airflow instance connects to its own Redis and RDS. RDS is responsible for storing Airflow metadata and Redis is acting as a celery broker between Airflow scheduler and Airflow workers.The Hashicorp Vault is used to store secrets required by Airflow instances and injected via sidecar by each Airflow component. The ELK stack stores all logs related to Airflow instances and is used for troubleshooting any instance. Datadog, Slack, and PagerDuty are used to send alerts.Presto is used to access job statistics, such as numbers on scheduled jobs, failed jobs, successful jobs, and active DAGs, to help each team to analyse their usage and stability of their jobs.Doing Things at ScaleThere are two kinds of scaling we need to talk about:  Scaling of Airflow instances on a resource level handling different loads  Scaling in terms of teams served on the platformTo scale Airflow instances, we set the request and the limit of each Airflow component allowing any of the components to scale up easily. To scale out Airflow workers, we decided to enable the horizontal pod autoscaler (HPA) using Memory and CPU parameters. The cluster autoscaler on EKS helps in scaling the platform to accommodate more teams.Moreover, we categorised all our Airflow instances in three sizes (small, medium, and large) to efficiently use the resources. This was based on how many hourly/daily jobs it scheduled. Each Airflow instance type has a specific RDS instance type and storage, Redis instance type and CPU and memory, request/limit for scheduler, worker, web server, and flower. There are different Airflow configurations for each instance type to optimise the given resources to the Airflow instance.Airflow Image and Version ManagementThe Airflow team builds and releases one common base Docker image for each Airflow version. The base image has Airflow installed with specific versions, as well as common Python packages, plugins, helpers, tests, patches, and so on.Each team has their customised Docker image on top of the base image. In their customised Docker image, they can update the Python packages and can download other artifacts that they require. Each Airflow instance will be deployed using the team’s customised image.There are common CI/CD templates provided by the Airflow team to build the customised image, run unit tests, and deploy Airflow instances from their GitLab pipeline.To upgrade the Airflow version, the Airflow team reviews and studies the changelog of the released Airflow version, note down the important features and its impacts, open issues, bugs, and workable solutions. Later, we build and release the base Docker image using the new Airflow version.We support only one Airflow version for all Airflow instances to have less maintenance overhead. In the case of minor or major versions, we support one old and new versions until the retirement period.How We DeployThere is a deployment ownership guideline that explains the schedule of deployments and the corresponding PICs. All teams have agreed on this guideline and share the responsibility with the Airflow Team.There are two kinds of deployment:  DAG deployment: This is part of the common GitLab CI/CD template. The Airflow team doesn’t trigger the DAG deployment, it’s fully owned by the teams.  Airflow instance deployment: The Airflow instance deployment is required in these scenarios:          Update in base Docker image      Add/update in Python packages by any team      Customisation in the base image by any team      Change in Airflow configurations      Change in the resource of scheduler, worker, web server or flower      Base Docker Image UpdateThe Airflow team maintains the base Docker image on the AWS Elastic Container Registry. The GitLab CI/CD builds the updated base image whenever the Airflow team changes the base image. The base image is validated by automated deployment on the test environment and automated smoke test. The Airflow instance owner of each team needs to trigger their build and deployment pipeline to apply the base image changes on their Airflow instance.Python Package Additions or UpdatesEach team can add or update their Python dependencies. The GitLab CI/CD pipeline builds a new image with updated changes. The Airflow instance owner manually triggers the deployment from their CI/CD pipeline. There is a flag to make it automated deployment as well.Based Image CustomisationEach team can add any customisations on the base image. Similar to the above scenario, the GitLab CI/CD pipeline builds a new image with updated changes. The Airflow instance owner manually triggers the deployment from their CI/CD pipeline. To automate the deployment, a flag is made available.Configuration Airflow and Airflow Component Resource ShangesTo optimise the Airflow instances, the Airflow Team makes changes to the Airflow configurations and resources of any of the Airflow components. The Airflow configurations and resources are also part of the Terraform code. Atlantis (https://www.runatlantis.io/) deploys the Airflow instances with Terraform changes.There is no downtime in any form of deployment and doesn’t impact the running tasks and the Airflow UI.TestingDuring the process of making our first Airflow stable, we started exploring testing in Airflow. We wanted to validate the correctness of DAGs, duplicate DAG IDs, checking typos and cyclicity in DAGs, etc. We then later wrote the tests by ourselves and published a detailed blog in several channels: usejournal (part1) and medium (part2).These tests are available in the base image and run in the GitLab pipeline from the user’s repository to validate their DAGs. The unit tests run using the common GitLab CI/CD template provided by the Airflow team.Monitoring &amp; AlertingOur scheduling platform runs the Airflow instance for many critical jobs scheduled by each team. It’s important for us to monitor all Airflow instances and alert respective stakeholders in case of any failure.We use Datadog for monitoring and alerting. To create a common Datadog dashboard, it is required to pass tags with metrics from Airflow and till Airflow 1.10.x, it doesn’t support tagging to Datadog metrics.We have contributed to the community to enable Datadog support and it will be released in Airflow 2.0.0 (https://github.com/apache/Airflow/pull/7376). We internally patched this pull request and created the common Datadog dashboard.There are three categories of metrics that we are interested in:  EKS cluster metrics: It includes total In-Service Nodes, allocated CPU cores, allocated Memory, Node status, CPU/Memory request vs limit, Node disk and Memory pressure, Rx-Tx packets dropped/errors, etc.  Host Metrics: These metrics are for each host participating in the EKS cluster. It includes Host CPU/Memory utilisation, Host free memory, System disk, and EBS IOPS, etc.  Airflow instance metrics: These metrics are for each Airflow instance. It includes scheduler heartbeats, DagBag size, DAG processing import errors, DAG processing time, open/used slots in a pool, each pod’s Memory/CPU usage, CPU and Memory utilisation of metadata DB, database connections as well as the number of workers, active/paused DAGs, successful/failed/queued/running tasks, etc.    Sample Datadog dashboardWe alert respective stakeholders and oncalls using Slack and PagerDuty.BenefitsThese are the benefits of having our own Scheduling Platform:  Scaling: HPA on Airflow workers running on EKS with autoscaler helps Airflow workers to scale automatically to theoretically infinite scale. This enables teams to run thousands of DAGs.  Logging: Centralised logging using Kibana.  Better Isolation: Separate Docker images for each team provide better isolation.  Better Customisation: All teams are provided with a mechanism to customise their Airflow worker environment according to their requirements.  Zero Downtime: Rolling upgrade and termination period on Airflow workers helps in zero downtime during the deployment.  Efficient usage of infrastructure: Each team doesn’t need to allocate infrastructure for Airflow instances. All Airflow instances are deployed on one shared EKS cluster.  Less maintenance overhead for users:  Users can focus on their core work and don’t need to spend time maintaining Airflow instances and it’s resources.  Common plugins and helpers: All common plugins and helpers available to use on Airflow instances. Each team doesn’t need to add.ConclusionDesigning and implementing our own scheduling platform started with many challenges and unknowns. We were not sure about the scale we were aiming for, the heterogeneous workload from each team, or the level of triviality or complexity we were going to be faced. After two years, we have successfully built and productionised a scalable scheduling platform that helps teams at Grab to schedule their workload.We have many failure stories, odd things we ran into, hacks and workarounds we patched. But, we went through it and provided a cost-effective and scalable scheduling platform with low maintenance overhead to all teams at Grab.What’s NextMoving ahead, we will be exploring to add the following capabilities:  REST APIs to enable teams to access their Airflow instance programmatically and have better integration with other tools and frameworks.  Support of dynamic DAGs at scale to help in decreasing the DAG maintenance overhead.  Template-based engine to act as a middle layer between the scheduling platform and external systems. It will have a set of templates to generate DAGs which helps in better integration with the external system.We suggest anyone who is running multiple Airflow instances within different teams to look at this approach and build the centralised scheduling platform. Before you begin,  review the feasibility of building the centralised platform as it requires a vision, a lot of effort, and cross-communication with many teams.Authored by Chandulal Kavar on behalf of the Airflow team at Grab - Charles Martinot, Vinnson Lee, Akash Sihag, Piyush Gupta, Pramiti Goel, Dewin Goh, QuiHieu Nguyen, James Anh-Tu Nguyen, and the Data Engineering Team.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/the-journey-of-deploying-apache-airflow-at-Grab"
      }
      ,
    
      "how-we-built-our-in-house-chat-platform-for-the-web": {
        "title": "How We Built Our In-house Chat Platform for the Web",
        "author": "vasudevan-k",
        "tags": "[&quot;Chat&quot;, &quot;Web&quot;, &quot;Customer Support&quot;, &quot;Engineering&quot;]",
        "category": "",
        "content": "At Grab, we’ve built an in-house chat platform to help connect our passengers with drivers during a booking, as well as with their friends and family for social sharing purposes.    P2P chat for the Angbow campaign and GrabHitch chatWe wanted to focus on our consumer support chat experience, and so we replaced the third-party live chat tool that we’ve used for years with our newly developed chat platform. As a part of this initiative, we extended this platform for the web to integrate with our internal Customer Support portal.    Sample chat between a driver and a customer support agentThis is the first time we introduced chat on the web, and we faced a few challenges while building it. In this article, we’ll go over some of these challenges and how we solved them.Current ArchitectureA vast majority of the communication from our Grab passenger and driver apps happens via TCP. Our TCP gateway takes care of processing all the incoming messages, authenticating, and routing them to the respective services. Our TCP connections are unicast, which means there is only one active connection possible per user at any point in time. This served us well, as we only allow our users to log in from one device at a time.    A TL;DR version of our current systemHowever, this model breaks on the web since our users can have multiple tabs open at the same time, and each would establish a new socket connection. Due to the unicast nature of our TCP connections, the older tabs would get disconnected and wouldn’t receive any messages from our servers. Our Customer Support agents love their tabs and have a gazillion open at any time. This behaviour would be too disruptive for them.The obvious answer was to change our TCP connection strategy to multicast. We took a look at this and quickly realised that it was going to be a huge undertaking and could introduce a lot of unknowns for us to deal with.We had to consider a different approach for the web and zeroed in on a hybrid approach with a little known Javascript APIs called SharedWorker and BroadcastChannel.Understanding the BasicsBefore we jump in, let’s take a quick detour to review some of the terminologies that we’ll be using in this post.If you’re familiar with how WebWorker works, feel free to skip ahead to the next section. For the uninitiated, JavaScript on the browser runs in a single-threaded environment. Workers are a mechanism to introduce background, OS-level threads in the browser. Creating a worker in JavaScript is simple. Let’s look at it with an example://instantiate a workerconst worker = new WebWorker(\"./worker.js\");worker.postMessage({ message: \"Ping\" });worker.onMessage((e) =&gt; {  console.log(\"Message from the worker\");});// and in  worker.jsonMessage = (e) =&gt; {  console.log(e.message);  this.postMessage({ message: \"pong\" });};The worker API comes with a handy postMessage method which can be used to pass messages between the main thread and worker thread. Workers are a great way to add concurrency in a JavaScript application and help in speeding up an expensive process in the background.Note: While the method looks similar, worker.postMessage is not the same as window.postMessage.What is a SharedWorker?SharedWorker is similar to a WebWorker and spawns an OS thread, but as the name indicates, it’s shared across browser contexts. In other words, there is only one instance of that worker running for that domain across tabs/windows. The API is similar to WebWorker but has a few subtle differences.SharedWorkers internally use MessagePort to pass messages between the worker thread and the main thread. There are two ports- one for sending a message to the main thread and the other to receive. Let’s explore it with an example:const mySharedWorker = new SharedWorker(\"./worker.js\");mySharedWorker.port.start();mySharedWorker.port.postMessage(message);onconnect = (e) =&gt; {  const port = e.ports[0];  // Handle messages from the main thread  port.onmessage = handleEventFromMainThread.bind(port);};// Message from the main threadconst handleEventFromMainThread = (params) =&gt; {  console.log(\"I received\", params, \"from the main thread\");};const sendEventToMainThread = (params) =&gt; {  connections.forEach((c) =&gt; c.postMessage(params));};There is a lot to unpack here. Once a SharedWorker is created, we’ve to manually start the port using mySharedWorker.port.start() to establish a connection between the script running on the main thread and the worker thread. Post that, messages can be passed via the worker’s postMessage method. On the worker side, there is an onconnect callback which helps in setting up listeners for connections from each browser context.Under the hood, SharedWorker spawns a single OS thread per worker script per domain. For instance, if the script name is worker.js running in the domain https://ce.grab.com. The logic inside worker.js runs exactly once in this domain. The advantage of this approach is that we can run multiple worker scripts in the same-origin each managing a different part of the functionality. This was one of the key reasons why we picked SharedWorker over other solutions.What are Broadcast ChannelsIn a multi-tab environment, our users may send messages from any of the tabs and switch to another for the next message. For a seamless experience, we need to ensure that the state is in sync across all the browser contexts.    Message passing across tabsThe BroadcastChannel API creates a message bus that allows us to pass messages between multiple browser contexts within the same origin. This helps us sync the message that’s being sent on the client to all the open tabs.Let’s explore the API with a code example:const channel = new BroadcastChannel(\"chat_messages\");// Sets up an event listener to receive messages from other browser contextschannel.onmessage = ({ data }) =&gt; {  console.log(\"Received \", data);};const sendMessage = (message) =&gt; {  const event = { message, type: \"new_message\" };  send(event);  // Publish event to all browser contexts listening on the chat\\_messages channel  channel.postMessage(event);};const off = () =&gt; {  // clear event listeners  channel.close();};One thing to note here is that communication is restricted to listeners from the same origin.How Our Chat Rooms are PoweredNow that we have a basic understanding of how SharedWorker and Broadcast channels work, let’s take a peek into how Grab is using it.Our Chat SDK abstracts the calls to the worker and the underlying transport mechanism. On the surface, the interface just exposes two methods: one for sending a message and another for listening to incoming events from the server.export interface IChatSDK {  sendMessage: (message: ChatMessage) =&gt; string;  sendReadReceipt: (receiptAck: MessageReceiptACK) =&gt; void;  on: (callback: ICallBack) =&gt; void;  off: (topic?: SDKTopics) =&gt; void;  close: () =&gt; void;}The SDK does all the heavy lifting to manage the connection with our TCP service, and keeping the information in-sync across tabs.    SDK flowIn our worker, we additionally maintain all the connections from browser contexts. When an incoming event arrives from the socket, we publish it to the first active connection. Our SDK listens to this event, processes it, sends out an acknowledgment to the server, and publishes it in the BroadcastChannel. Let’s look at how we’ve achieved this via a code example.Managing connections in the worker:let socket;let instances = 0;let connections = [];let URI: string;// Called when a  new worker is connected.// Worker is created atonconnect = e =&gt; { const port = e.ports[0]; port.start(); port.onmessage = handleEventFromMainThread.bind(port); connections.push(port); instances ++;};// Publish ONLY to the first connection.// Let the caller decide on how to sync this with other tabsconst callback= (topic, payload) =&gt; {    connections[0].postMessage({      topic,      payload,    }); } const handleEventFromMainThread = e =&gt; {   switch (e.data.topic) {     case SocketTopics.CONNECT: {       const config = e.data.payload;       if (!socket) {         // Establishes a WebSocket connection with the server          socket = new SocketManager({...})        } else {          callback(SocketTopics.CONNECTED, '');        }        break;      }      case SocketTopics.CLOSE: {        const index = connections.indexOf(this);        if (index != -1 &amp;&amp; instances &gt; 0) {            connections.splice(index, 1);            instances--;        }        break;      }        // Forward everything else to the server      default: {        const payload = e.data;        socket.sendMessage(payload);        break;      }    }  }And in the ChatSDK:// Implements IChatSDK// Rough outline of our GrabChat implementationclass GrabChatSDK {  constructor(config) {    this.channel = new BroadcastChannel('incoming_events');    this.channel.onmessage = ({data}) =&gt; {        switch(data.type) {            // Handle events from other tabs            // .....        }    }    this.worker = new SharedWorker('./worker', {        type: 'module',        name: `${config.appID}-${config.appEnv}`,        credentials: 'include',      });      this.worker.port.start();      // Publish a connected event, so the worker manager can register this connection      this.worker.port.postMessage({        topic: SocketTopics.CONNECT,        payload,      });      // Incoming event from the shared worker      this.worker.port.onmessage = this._handleIncomingMessage;      // Disconnect this port before tab closes      addEventListener('beforeunload', this._disconnect);    }    sendMessage(message) {      // Attempt a delivery of the message      worker.postMessage({        topic: SocketTopics.NEW_MESSAGE,        getPayload(message),      });      // Send the message to all tabs to keep things in sync      this.channel.postMessage(getPayload(message));    }    // Hit if this connection is the leader of the SharedWorker connection    _handleIncomingMessage(event) {      // Send an ACK to our servers confirming receipt of the message      worker.postMessage({        topic: SocketTopics.ACK,        payload,      });      if (shouldBroadcast(event.type)) {        this.channel.postMessage(event);      }      this.callback(event);    }    _disconnect() {      this.worker.port.postMessage(data);      removeEventListener('beforeunload', this._disconnect);    }}This ensures that there is only one connection between our application and the TCP service irrespective of the number of tabs the page is open in.Some CaveatsWhile SharedWorker is a great way to enforce singleton objects across browser contexts, the developer experience of SharedWorker leaves a lot to be desired. There aren’t many resources on the web, and it could be quite confusing if this is the first time you’re using this feature.We faced some trouble integrating SharedWorker with bundling the worker code along. This plugin from GoogleChromeLabs did a great job of alleviating some pain. Debugging an issue with SharedWorker was not obvious. Chrome has a dedicated page for inspecting SharedWorkers (chrome://inspect/#workers), and it took some getting used to.The browser support for SharedWorker is far from universal. While it works great in Chrome, Firefox, and Opera, Safari and most mobile browsers lack support. This was an acceptable trade-off in our use case, as we built this for an internal portal and all our users are on Chrome.    Shared raceSharedWorker enforces uniqueness using a combination of origin and the script name. This could potentially introduce an unintentional race condition during deploy times if we’re not careful. Let’s say the user has a tab open before the latest deployment, and another one after deployment, it’s possible to end up with two different versions of the same script. We built a wrapper over the SharedWorker which cedes control to the latest connection, ensuring that there is only one version of the worker active.Wrapping UpWe’re happy to have shared our learnings from building our in-house chat platform for the web, and we hope you found this post helpful. We’ve built the web solution as a reusable SDK for our internal portals and public-facing websites for quick and easy integration, providing a powerful user experience.We hope this post also helped you get a deeper sense of how SharedWorker and BroadcastChannels work in a production application.Authored By Vasu on behalf of the Real-Time Communications team at Grab. Special thanks to the working team for their contributions- Sanket Thanvi, Dinh Duong, Kevin Lee, and Matthew Yeow.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/how-we-built-our-in-house-chat-platform-for-the-web"
      }
      ,
    
      "go-module-a-guide-for-monorepos-part-1": {
        "title": "Go Modules- A Guide for monorepos (Part 1)",
        "author": "michael-cartmell",
        "tags": "[&quot;Go&quot;, &quot;Monorepo&quot;, &quot;Vendoring&quot;, &quot;Vendors&quot;, &quot;Libraries&quot;]",
        "category": "",
        "content": "Go modules are a new feature in Go for versioning packages and managing dependencies. It has been almost 2 years in the making, and it’s finally production-ready in the Go 1.14 release early this year. Go recommends using single-module repositories by default, and warns that multi-module repositories require great care.At Grab, we have a large monorepo and changing from our existing monorepo structure has been an interesting and humbling adventure. We faced serious obstacles to fully adopting Go modules. This series of articles describes Grab’s experience working with Go modules in a multi-module monorepo, the challenges we faced along the way, and the solutions we came up with.To fully appreciate Grab’s journey in using Go Modules, it’s important to learn about the beginning of our vendoring process.Native Support for Vendoring Using the Vendor FolderWith Go 1.5 came the concept of the vendor folder, a new package discovery method, providing native support for vendoring in Go for the first time.With the vendor folder, projects influenced the lookup path simply by copying packages into a vendor folder nested at the project root. Go uses these packages before traversing the GOPATH root, which allows a monorepo structure to vendor packages within the same repo as if they were 3rd-party libraries. This enabled go build to work consistently without any need for extra scripts or env var modifications.Initial ObstaclesThere was no official command for managing the vendor folder, and even copying the files in the vendor folder manually was common.At Grab, different teams took different approaches. This meant that we had multiple version manifests and lock files for our monorepo’s vendor folder. It worked fine as long as there were no conflicts. At this time very few 3rd-party libraries were using proper tagging and semantic versioning, so it was worse because the lock files were largely a jumble of commit hashes and timestamps.    Jumbled commit hashes and timestampsAs a result of the multiple versions and lock files, the vendor directory was not reproducible, and we couldn’t be sure what versions we had in there.Temporary ReliefWe eventually settled on using Glide, and standardised our vendoring process. Glide gave us a reproducible, verifiable vendor folder for our dependencies, which worked up until we switched to Go modules.Vendoring Using Go ModulesI first heard about Go modules from Russ Cox’s talk at GopherCon Singapore in 2018, and soon after started working on adopting modules at Grab, which was to manage our existing vendor folder.This allowed us to align with the official Go toolchain and familiarise ourselves with Go modules while the feature matured.Switching to Go ModulesGo modules introduced a go mod vendor command for exporting all dependencies from go.mod into vendor. We didn’t plan to enable Go modules for builds at this point, so our builds continued to run exactly as before, indifferent to the fact that the vendor directory was created using go mod.The initial task to switch to go mod vendor was relatively straightforward as listed here:  Generated a go.mod file from our glide.yaml dependencies. This was scripted so it could be kept up to date without manual effort.  Replaced the vendor directory.  Committed the changes.  Used go mod instead of glide to manage the vendor folder.The change was extremely large (due to differences in how glide and go mod handled the pruning of unused code), but equivalent in terms of Go code. However, there were some additional changes needed besides porting the version file.Addressing Incompatible DependenciesSome of our dependencies were not yet compatible with Go modules, so we had to use Go module’s replace directive to substitute them with a working version.A more complex issue was that parts of our codebase relied on nested vendor directories, and had dependencies that were incompatible with the top level. The go mod vendor command attempts to include all code nested under the root path, whether or not they have used a sub-vendor directory, so this led to conflicts.Problematic PathsRather than resolving all the incompatibilities, which would’ve been a major undertaking in the monorepo, we decided to exclude these paths from Go modules instead. This was accomplished by placing an empty go.mod file in the problematic paths.Nested ModulesThe empty go.mod file worked. This brought us to an important rule of Go modules, which is central to understanding many of the issues we encountered: A module cannot contain other modulesThis means that although the modules are within the same repository, Go modules treat them as though they are completely independent. When running go mod commands in the root of the monorepo, Go doesn’t even ‘see’ the other modules nested within.Tackling Maintenance IssuesAfter completing the initial migration of our vendor directory to go mod vendor however, it opened up a different set of problems related to maintenance.With Glide, we could guarantee that the Glide files and vendor directory would not change unless we deliberately changed them. This was not the case after switching to Go modules; we found that the go.mod file frequently required unexpected changes to keep our vendor directory reproducible.There are two frequent cases that cause the go.mod file to need updates: dependency inheritance and implicit updates.Dependency InheritanceDependency inheritance is a consequence of Go modules version selection. If one of the monorepo’s dependencies uses Go modules, then the monorepo inherits those version requirements as well.When starting a new module, the default is to use the latest version of dependencies. This was an issue for us as some of our monorepo dependencies had not been updated for some time. As engineers wanted to import their module from the monorepo, it caused go mod vendor to pull in a huge amount of updates.To solve this issue, we wrote a quick script to copy the dependency versions from one module to another.One key learning here is to have other modules use the monorepo’s versions, and if any updates are needed then the monorepo should be updated first.Implicit UpdatesImplicit updates are a more subtle problem. The typical Go modules workflow is to use standard Go commands: go build, go test, and so on, and they will automatically update the go.mod file as needed. However, this was sometimes surprising, and it wasn’t always clear why the go.mod file was being updated. Some of the reasons we found were:  A new import was added by mistake, causing the dependency to be added to the go.mod file  There is a local replace for some module B, and B changes its own go.mod. When there’s a local replace, it bypasses versioning, so the changes to B’s go.mod are immediately inherited.  The build imports a package from a dependency that can’t be satisfied with the current version, so Go attempts to update it.This means that simply creating a tag in an external repository is sometimes enough to affect the go.mod file, if you already have a broken import in the codebase.Resolving Unexpected Dependencies Using GraphsTo investigate the unexpected dependencies, the command go mod graph proved the most useful.Running graph with good old grep was good enough, but its output is also compatible with the digraph tool for more sophisticated queries. For example, we could use the following command to trace the source of a dependency on cloud.google.com/go:$ go mod graph | digraph somepath grab.com/example cloud.google.com/go@v0.26.0github.com/hashicorp/vault/api@v1.0.4 github.com/hashicorp/vault/sdk@v0.1.13github.com/hashicorp/vault/sdk@v0.1.13 google.golang.org/genproto@v0.0.0-20190404172233-64821d5d2107google.golang.org/genproto@v0.0.0-20190404172233-64821d5d2107 google.golang.org/grpc@v1.19.0google.golang.org/grpc@v1.19.0 cloud.google.com/go@v0.26.0    Diagram generated using modgraphvizStay Tuned for MoreI hope you have enjoyed this article. In our next post, we’ll cover the other solutions we have for catching unexpected changes to the go.mod file and addressing dependency issues.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!CreditsThe cute Go gopher logo for this blog’s cover image was inspired by Renee French’s original work.",
        "url": "/go-module-a-guide-for-monorepos-part-1"
      }
      ,
    
      "does-southeast-asia-run-on-coffee": {
        "title": "Does Southeast Asia Run on Coffee?",
        "author": "siu-sing-lailara-pureum-yim",
        "tags": "[&quot;Data&quot;, &quot;Data Analytics&quot;, &quot;Data Visualisation&quot;]",
        "category": "",
        "content": "This article was originally published in the Grab Medium account on December 4, 2019. Reposting it here for your reading pleasure.There is no surprise as to why coffee is a go-to drink in the region. For one, almost a third of coffee is produced in Asia, giving us easy access to beans. Coupled with the plethora of local cafes and stores at every corner in Southeast Asia, coffee has become an accessible and affordable drink — and one that enjoys a huge following.For many, a morning cuppa is fuel to kick start their day. For some, it’s the secret weapon to a food coma; for others, it’s the fuel to keep them going throughout the day.To get a glimpse of how our fellow Southeast Asians refuel with coffee on a daily basis, we took a look (along with our ‘kopi’) at GrabFood data, and here is what we found.Did You Know: Coffee Orders have Grown 1,400% on GrabFood?How much do we actually love our coffee? It seems like we do, a lot.Coffee orders on GrabFood has been growing pervasively throughout the major cities, and a time-lapse visualisation based on data from GrabFood orders show us the growth of orders across major cities over a 9-month period:  Time for Coffee?But how reliant are we on caffeine? We analysed the coffee consumption behaviour of GrabFood users from major SEA countries across a typical week.Coffee Orders by Day of the Week: Singapore Coffee Orders Peak on the Weekends  Turns out most coffee orders are placed on Wednesdays — clearly a much needed shot to overcome the dreaded hump day. And as we head into the weekend, orders begin to decline as Southeast Asians wind down from the work week.However, the complete opposite happens for our friends in Singapore and the Philippines! Coffee orders actually spike on the weekends, and especially so on Sundays. It can only mean that Singaporeans and Filipinos surely enjoy their coffee catch-ups with friends and family.AM- Coffee… PM- Still CoffeeThe question begets — when exactly do SEA coffee drinkers summon that life saving cup from our delivery heroes in green?Check out this trippy visualisation that resembles jumping coffee beans:Coffee Orders by Hour of Day — Orders Peak at 10am for Thailand and 2pm in Indonesia  While other cities generally reach for the Grab app at noon for that extra boost to fight that food coma through the rest of the day, our friends in Thailand gets their caffeine fix early, with most orders coming in at 10.00am, just before the lunch hour.Interestingly, coffee orders for Singapore peak at about 4pm in the afternoon… are they working hard, or are they hardly working?GrabFood’s Love is in the Air, and It Smells Like CoffeeCurious as to what coffee flavours our SEA neighbours prefer? We spill the (coffee) beans!Top 3 Coffee Flavours in Each Country  What is a Non-coffee Drinker to Do?  Also known as Matcha Latte, Green Tea Latte seems to be the next big beverage fad in the region , serving as a perfect coffee alternative for non-coffee drinkers.Matcha latte, made with concentrated shots of green tea and topped with frothy, steamed milk, is gaining popularity. While it offers the same quantity of caffeine as a cup of brewed coffee, the drink is perceived to be as more energising , because of the slower release of caffeine.It has consistently been one of the top 10 beverage items ordered on GrabFood, and we’ve delivered over 25 million cups of these green, frothy and creamy ‘heaven in a cup’ over the last nine months!Southeast Asian’s love of tea-based latte (other than green tea) is apparent in Grab’s data! Some of the unique flavours that are being ordered on GrabFood include the following flavours:  GrabFood Coffee is a Hug in a MugIs your blood type coffee? Whether you feel like caramelly and chocolatey Macchiato, or fruity and floral aroma of freshly brewed Americano, or intense and bitter double-shot Long Black — GrabFood has got you covered! May your coffee get delivered (and kick in) before reality does!Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/does-southeast-asia-run-on-coffee"
      }
      ,
    
      "grabchat-much-talk-data-to-me": {
        "title": "GrabChat Much? Talk Data to Me!",
        "author": "jason-leelara-pureum-yim",
        "tags": "[&quot;Data&quot;, &quot;Data Analytics&quot;, &quot;Data Visualisation&quot;]",
        "category": "",
        "content": "This article was originally published in the Grab Medium account on November 20, 2019. Reposting it here for your reading pleasure.In September 2016 GrabChat was born, a platform designed to allow seamless communication between passenger and driver-partner. Since then, Grab has continuously improved the GrabChat experience by introducing features such as instant translation, images, and audio chats, and as a result — reduced cancellation rates by up to 50%! We’ve even experimented with various features to deliver hyper-localised experiences in each country! So with all these features, how have our users responded? Let’s take a deeper look into this to uncover some interesting insights from our data in Singapore, Malaysia and Indonesia.The Chattiest Country    Number of Chats by CountryIn a previous blog post several years ago, we revealed that Indonesia was the chattiest nation in South-east Asia. Our latest data is no different. Indonesia is still the chattiest country out of the three, having an average of 5.5 chats per bookings, while Singapore is the least chatty! Furthermore, passengers in Singapore tend to be chattier than driver-partners, while the reverse relationship is true for the other two countries.But what do people talk about?      Common words in Indonesia      Common words in Singapore      Common words in MalaysiaAs expected, most of the chats revolve around pick-up points. There are many similarities between the three countries, such as typing courtesies such as ‘Hi’ and ‘Thank you’, and that the driver-partner/passenger is coming. However, there are slight differences between the countries. Can you spot them all?In Indonesia, chats are usually in Bahasa Indonesia, and tend to be mostly driver-partners thanking passengers for using Grab.Chats in Singapore on the other hand, tend to be in English, and contain mostly pick-up locations, such as a car park. There are quite a few unique words in the Singapore context, such as ‘rubbish chute’ and ‘block’ that reflect features of the ubiquitous HDB’s (public housing) found everywhere in Singapore that serve as popular residential pickup points.Malaysia seems to be a blend of the other two countries, with chats in a mix of English and Bahasa Malaysia. Many of the chats highlight pickup locations, such as a guard house, as well as the phrase all Malaysians know: being stuck in traffic.Time Trends    Time TrendAnalysis in chat trends across the three countries revealed an unexpected insight: a trend of talking more from midnight until around 4am. Perplexed but intrigued, we dug further to discover what prompted our users to talk more in such odd hours.From midnight to 4am shops and malls are usually closed during these hours, and pickup locations become more obscure as people wander around town late at night. Driver-partners and passengers thus tend to have more conversations to determine the pickup point. This also explains why the proportion of pick-up location based messages out of all messages is highest between 12 and 6am. On the other hand, these messages are less common in the mornings (6am-12pm) as people tend to be picked up from standard residential locations.Image Trends    GrabChat’s Image-function uptake in Jakarta, Singapore, and Kuala Lumpur (Nov 2018 — March 2019) - Image 1    GrabChat’s Image-function uptake in Jakarta, Singapore, and Kuala Lumpur (Nov 2018 — March 2019) - Image 2    GrabChat’s Image-function uptake in Jakarta, Singapore, and Kuala Lumpur (Nov 2018 — March 2019) - Image 3The ability to send images on GrabChat was introduced in September 2018, with the aim of helping driver-partners identify the exact pickup location of passengers. Within the first few weeks of release, 22,000 images were sent in Singapore alone. The increase in uptake of the image feature for the cities of Jakarta, Singapore and Kuala Lumpur can be seen in the images above.From analysis, we found that areas that were more remote such as Tengah in Singapore tended to have the highest percentage of images sent, indicating that images are useful for users in unfamiliar places.Safety FirstAside from images, Grab also introduced two other features: templates and audio chats, to avoid driver-partners from texting while driving.    Templates and audio features used by driver-partners, and a reduced number of typed texts by driver-partners per booking“Templates” (pre-populated phrases) allowed driver-partners to send templated messages with just a quick tap. In our recent data analysis, we discovered that almost 50% of driver-partner texts comprised of templates.“Audio chat” alongside “images chat” were introduced in September 2018, and the use of this feature has been steadily increasing, with audio comprising an increasing percentage of driver-partner texts.With both features being picked up by driver-partners across all three countries, Grab has successfully seen a decrease in the overall number of driver-partner texts (non-templates) per booking within a 3 month period.A Brief Pick-up GuideNo one likes a cancelled ride, right? Well, after analysing millions of data points, we’ve unearthed some neat tips and tricks to help you complete your ride, and we’re sharing them with you!    Completed RidesThis first tip might be a no-brainer, but replying your driver-partner would result in a higher completion rate. No one likes to be blue-ticked do they?Next, we discovered various things you could say that would result in higher completion rates, explained below in the graphic.    Tips for a Better Pickup ExperienceInforming the driver-partner that you’re coming, giving them directions, and telling them how to identify you results in almost double the chances of completing the ride!Last but not least, let’s not forget our manners. Grab’s data analysis revealed that saying ‘thank you’ correlated with an increase in completion rates! Also, be at the pickup point on time — remember, time is money for our driver-partners!ConclusionJust like in Shakespeare’s Much Ado about Nothing, ample information can be gathered from the mere whim of a message. Grab is constantly aspiring to achieve the best experience for both passengers and driver-partners, and data plays a huge role in helping us achieve this.This is just the first page of the book. The amount of information lurking between every page is endless. So stay tuned for more interesting insights about our GrabChat platform!Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/grabchat-much-talk-data-to-me"
      }
      ,
    
      "seven-facts-about-grab-driver-partners-in-sg": {
        "title": "7 Fun Facts about Grab’s Driver-Partners in Singapore",
        "author": "lara-pureum-yimyou-zhen-chongsze-han-ongmichael-chiricokelly-kuokenny-chan",
        "tags": "[&quot;Data&quot;, &quot;Data Analytics&quot;]",
        "category": "",
        "content": "This article was originally published in the Grab Medium account on June 17, 2019. Reposting it here for your reading pleasure.Grab’s Big Data StoryGrab is on an incredible mission to empower our driver-partners in 336 cities in 8 countries.Curious about what Grab’s data tells us about driver-partners on the platform?Let us share with you the most interesting data points we found among our driver-partners in Singapore!1. It’s a Small World  Lim Chu Kang may feel like a world away from Katong, but Singapore is a small world for our driver-partners — a driver-partner has a 1 in 400 chance of having a repeat passenger amongst the 5.4 million population!2. Saturday Night Fever  The annual average number of rides that a Grab driver-partner complete on Saturday nights is 110. But there was one special driver-partner who did 1,131 Saturday night trips in the year of 2018! Weekend parties just wouldn’t be the same without you. Rock on!3. Share the Love  Did you know that having more passengers in a car can yield more 5-star ratings? Our GrabShare passengers share more than just rides — they share their appreciation too! GrabShare rides have an average trip rating of 4.8!4. The Road More Travelled  Which neighbourhoods are painting the town green? Our driver-partners picked up the most passengers from Tampines, while Orchard &amp; Marina Bay areas were the most popular destination in 2018!5. Tricks of the Trade  Ever wondered if seasoned driver-partners who have been with us for more than 2 years, have different driving preferences and habits? They tend to start their day 1 hour earlier around 6–7am, and are on auto-accept most of the time. Did you know that drivers on auto-accept spend less time idle waiting for new bookings?6. Busy Bee  Did you know? Drivers are twice as likely to get back-to-back allocations during evening peak hours! Drivers with frequent back-to-back bookings earn about 50% more per hour.7. Road Runner  One of our most active driver-partners covered 57,000km ferrying passengers in 2018 — that’s like driving every road, street, jalan, lorong and tanjong in Singapore for more than 57 times!How our driver-partners utilise the Grab platform to make a living (and break a few records along the way) never ceases to amaze us.Interested to know more about the winning strategies among our driver-partners? Look out for the next data story!Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/seven-facts-about-grab-driver-partners-in-sg"
      }
      ,
    
      "tackling-ui-test-execution-time-imbalance-for-xcode-parallel-testing": {
        "title": "Tackling UI Test Execution Time Imbalance for Xcode Parallel Testing",
        "author": "ngoc-thuyen-trinh",
        "tags": "[&quot;Xcode&quot;, &quot;Testing&quot;, &quot;Mobile&quot;, &quot;Parallelism&quot;, &quot;UI tests&quot;, &quot;CI&quot;, &quot;iOS&quot;]",
        "category": "",
        "content": "IntroductionTesting is a common practice to ensure that code logic is not easily broken during development and refactoring. Having tests running as part of Continuous Integration (CI) infrastructure is essential, especially with a large codebase contributed by many engineers. However, the more tests we add, the longer it takes to execute. In the context of iOS development, the execution time of the whole test suite might be significantly affected by the increasing number of tests written. Running CI pre-merge pipelines against a change, would cost us more time. Therefore, reducing test execution time is a long term epic we have to tackle in order to build a good CI infrastructure.Apart from splitting tests into subsets and running each of them in a CI job, we can also make use of the Xcode parallel testing feature to achieve parallelism within one single CI job. However, due to platform-specific implementations, there are some constraints that prevent parallel testing from working efficiently. One constraint we found is that tests of the same Swift class run on the same simulator. In this post, we will discuss this constraint in detail and introduce a tip to overcome it.BackgroundXcode Parallel TestingThe parallel testing feature was shipped as part of the Xcode 10 release. This support enables us to easily configure test setup:  There is no need to care about how to split a given test suite.  The number of workers (i.e. parallel runners/instances) is configurable. We can pass this value in the xcodebuild CLI via the -parallel-testing-worker-count option.  Xcode takes care of cloning and starts simulators accordingly.However, the distribution logic under the hood is a black-box. We do not really know how tests are assigned to each worker or simulator, and in which order.    Three simulators running tests in parallelIt is worth mentioning that even without the Xcode parallel testing support, we can still achieve similar improvements by running subsets of tests in different child processes. But it takes more effort to dispatch tests to each child process in an efficient way, and to handle the output from each test process appropriately.Test Time ImbalanceGenerally, a parallel execution system is at its best efficiency if each parallel task executes in roughly the same duration and ends at roughly the same time.If the time spent on each parallel task is significantly different, it will take more time than expected to execute all tasks. For example, in the following image, it takes the system on the left 13 mins to finish 3 tasks. Whereas, the one on the right takes only 10.5 mins to finish those 3 tasks.    Bad parallelism vs. good parallelismAssume there are N workers. The ith worker executes its tasks in ti seconds/minutes. In the left plot, t1 = 10 mins, t2 = 7 mins, t3 = 13 mins.We define the test time imbalance metric as the difference between the min and max end time:max(ti) - min(ti)For the example above, the test time imbalance is 13 mins - 7 mins = 6 mins.Contributing Factors in Test Time ImbalanceThere are several factors causing test time imbalance. The top two prominent factors are:  Tests vary in execution time.  Tests of the same class run on the same simulator.An example of the first factor is that in our project, around 50% of tests execute in a range of 20-40 secs. Some tests take under 15 secs to run while several take up to 2 minutes. Sometimes tests taking longer execution time is inevitable since those tests usually touch many flows, which cannot be split. If such tests run last, the test time imbalance may increase.However, this issue, in general, does not matter that much because long-time-execution tests do not always run last.Regarding the second factor, there is no official Apple documentation that explicitly states this constraint. When Apple first introduced parallel testing support in Xcode 10, they only mentioned that test classes are distributed across runner processes:  “Test parallelisation occurs by distributing the test classes in a target across multiple runner processes. Use the test log to see how your test classes were parallelised. You will see an entry in the log for each runner process that was launched, and below each runner you will see the list of classes that it executed.”For example, we have a test class JobFlowTests that includes five tests and another test class TutorialTests that has only one single test.final class JobFlowTests: BaseXCTestCase {func testHappyFlow() { ... }  func testRecoverFlow() { ... }  func testJobIgnoreByDax() { ... }  func testJobIgnoreByTimer() { ... }  func testForceClearBooking() { ... }}...final class TutorialTests: BaseXCTestCase {  func testOnboardingFlow() { ... }}When executing the two tests with two simulators running in parallel, the actual run is like the one shown on the left side of the following image, but ideally it should work like the one on the right side.    Tests of the same class are supposed to run on the same simulator but they should be able to run on different simulators.Diving Deep into Xcode Parallel TestingDemystifying Xcode Scheduling LogAs mentioned above, Xcode distributes tests to simulators/workers in a black-box manner. However, by looking at the scheduling log generated when running tests, we can understand how Xcode parallel testing works.When running UI tests via the xcodebuild command:$ xcodebuild -workspace Driver/Driver.xcworkspace \\    -scheme Driver \\    -configuration Debug \\    -sdk 'iphonesimulator' \\    -destination 'platform=iOS Simulator,id=EEE06943-7D7B-4E76-A3E0-B9A5C1470DBE' \\    -derivedDataPath './DerivedData' \\    -parallel-testing-enabled YES \\    -parallel-testing-worker-count 2 \\    -only-testing:DriverUITests/JobFlowTests \\    # 👈👈👈👈👈    -only-testing:DriverUITests/TutorialTests \\    test-without-buildingThe log can be found inside the *.xcresult folder under DerivedData/Logs/Test. For example: DerivedData/Logs/Test/Test-Driver-2019.11.04\\_23-31-34-+0800.xcresult/1\\_Test/Diagnostics/DriverUITests-144D9549-FD53-437B-BE97-8A288855E259/scheduling.log    Scheduling log under xcresult folder2019-11-05 03:55:00 +0000: Received worker from worker provider: 0x7fe6a684c4e0 [0: Clone 1 of DaxIOS-XC10-1-iP7-1 (3D082B53-3159-4004-A798-EA5553C873C4)]2019-11-05 03:55:13 +0000: Worker 0x7fe6a684c4e0 [4985: Clone 1 of DaxIOS-XC10-1-iP7-1 (3D082B53-3159-4004-A798-EA5553C873C4)] finished bootstrapping2019-11-05 03:55:13 +0000: Parallelization enabled; test execution driven by the IDE2019-11-05 03:55:13 +0000: Skipping test class discovery2019-11-05 03:55:13 +0000: Executing tests {(\t# 👈👈👈👈👈    DriverUITests/JobFlowTests,    DriverUITests/TutorialTests)}; skipping tests {()}2019-11-05 03:55:13 +0000: Load balancer requested an additional worker2019-11-05 03:55:13 +0000: Dispatching tests {(  # 👈👈👈👈👈    DriverUITests/JobFlowTests)} to worker: 0x7fe6a684c4e0 [4985: Clone 1 of DaxIOS-XC10-1-iP7-1 (3D082B53-3159-4004-A798-EA5553C873C4)]2019-11-05 03:55:13 +0000: Received worker from worker provider: 0x7fe6a1582e40 [0: Clone 2 of DaxIOS-XC10-1-iP7-1 (F640C2F1-59A7-4448-B700-7381949B5D00)]2019-11-05 03:55:39 +0000: Dispatching tests {(  # 👈👈👈👈👈    DriverUITests/TutorialTests)} to worker: 0x7fe6a684c4e0 [4985: Clone 1 of DaxIOS-XC10-1-iP7-1 (3D082B53-3159-4004-A798-EA5553C873C4)]...Looking at the log below, we know that once a test class is dispatched or distributed to a worker/simulator, all tests of that class will be executed in that simulator.2019-11-05 03:55:39 +0000: Dispatching tests {(    DriverUITests/TutorialTests)} to worker: 0x7fe6a684c4e0 [4985: Clone 1 of DaxIOS-XC10-1-iP7-1 (3D082B53-3159-4004-A798-EA5553C873C4)]Even when we customise a test suite (by swizzling some XCTestSuite class methods or variables), to split a test suite into multiple suites, it does not work because the made-up test suite is only initialised after tests are dispatched to a given worker.Therefore, any hook to bypass this constraint must be done early on.Passing the -only-testing Argument to xcodebuild CommandNow, we pass tests (instead of test classes) to the -only-testing argument.$ xcodebuild -workspace Driver/Driver.xcworkspace \\    # ...    -only-testing:DriverUITests/JobFlowTests/testJobIgnoreByTimer \\    -only-testing:DriverUITests/JobFlowTests/testRecoverFlow \\    -only-testing:DriverUITests/JobFlowTests/testJobIgnoreByDax \\    -only-testing:DriverUITests/JobFlowTests/testHappyFlow \\    -only-testing:DriverUITests/JobFlowTests/testForceClearBooking \\    -only-testing:DriverUITests/TutorialTests/testOnboardingFlow \\    test-without-buildingBut still, the scheduling log shows that tests are grouped by test class before being dispatched to workers (see the following log for reference). This grouping is automatically done by Xcode (which it should not).2019-11-05 04:21:42 +0000: Executing tests {(\t# 👈    DriverUITests/JobFlowTests/testJobIgnoreByTimer,    DriverUITests/JobFlowTests/testRecoverFlow,    DriverUITests/JobFlowTests/testJobIgnoreByDax,    DriverUITests/TutorialTests/testOnboardingFlow,    DriverUITests/JobFlowTests/testHappyFlow,    DriverUITests/JobFlowTests/testForceClearBooking)}; skipping tests {()}2019-11-05 04:21:42 +0000: Load balancer requested an additional worker2019-11-05 04:21:42 +0000: Dispatching tests {(  # 👈 ❌    DriverUITests/JobFlowTests/testJobIgnoreByTimer,    DriverUITests/JobFlowTests/testForceClearBooking,    DriverUITests/JobFlowTests/testJobIgnoreByDax,    DriverUITests/JobFlowTests/testHappyFlow,    DriverUITests/JobFlowTests/testRecoverFlow)} to worker: 0x7fd781261940 [6300: Clone 1 of DaxIOS-XC10-1-iP7-1 (93F0FCB6-C83F-4419-9A75-C11765F4B1CA)]......Overcoming Grouping Logic in Xcode Parallel TestingTweaking the -only-testing Argument ValuesBased on our observation, we can imagine how Xcode runs tests in parallel. See the example below.Step 1.   tests = detect_tests_to_run() # parse -only-testing argumentsStep 2.   groups_of_tests = group_tests_by_test_class(tests)Step 3.   while groups_of_tests is not empty:Step 3.1. \tworker = find_free_worker()Step 3.2.     if worker is not None:                  dispatch_tests_to_workers(groups_of_tests.pop())In the pseudo-code above, we do not have much control to change step 2 since that grouping logic is implemented by Xcode. But we have a good guess that Xcode groups tests, by the first two components (class name) only (For example,  DriverUITests/JobFlowTests). In other words, tests having the same class name run together on one simulator.The trick to break this constraint is simple. We can tweak the input (test names) so that each group contains only one test. By inserting a random token in the class name, all class names in the tests that are passed via -only-testing argument are different.For example, instead of passing:-only-testing:DriverUITests/JobFlowTests/testJobIgnoreByTimer \\-only-testing:DriverUITests/JobFlowTests/testRecoverFlow \\We rather use:-only-testing:DriverUITests/JobFlowTests_AxY132z8/testJobIgnoreByTimer \\-only-testing:DriverUITests/JobFlowTests_By8MTk7l/testRecoverFlow \\Or we can use the test name itself as the token:-only-testing:DriverUITests/JobFlowTests_testJobIgnoreByTimer/testJobIgnoreByTimer \\-only-testing:DriverUITests/JobFlowTests_testRecoverFlow/testRecoverFlow \\After that, looking at the scheduling log, we will see that the trick can bypass the grouping logic. Now, only one test is dispatched to a worker once ready.2019-11-05 06:06:56 +0000: Dispatching tests {(\t# 👈 ✅    DriverUITests/JobFlowTests_testJobIgnoreByDax/testJobIgnoreByDax)} to worker: 0x7fef7952d0e0 [13857: Clone 2 of DaxIOS-XC10-1-iP7-1 (9BA030CD-C90F-4B7A-B9A7-D12F368A5A64)]2019-11-05 06:06:58 +0000: Dispatching tests {(\t# 👈 ✅    DriverUITests/TutorialTests_testOnboardingFlow/testOnboardingFlow)} to worker: 0x7fef7e85fd70 [13719: Clone 1 of DaxIOS-XC10-1-iP7-1 (584F99FE-49C2-4536-B6AC-90B8A10F361B)]2019-11-05 06:07:07 +0000: Dispatching tests {(\t# 👈 ✅    DriverUITests/JobFlowTests_testRecoverFlow/testRecoverFlow)} to worker: 0x7fef7952d0e0 [13857: Clone 2 of DaxIOS-XC10-1-iP7-1 (9BA030CD-C90F-4B7A-B9A7-D12F368A5A64)]Handling Tweaked Test NamesWhen a worker/simulator receives a request to run a test, the app (could be the runner app or the hosting app) initialises an XCTestSuite corresponding to the test name. In order for the test suite to be properly made up, we need to remove the inserted token.This could be done easily by swizzling the XCTestSuite.init(forTestCaseWithName:). Inside that swizzled function, we remove the token and then call the original init function.extension XCTestSuite {  /// For 'Selected tests' suite  @objc dynamic class func swizzled_init(forTestCaseWithName maskedName: String) -&gt; XCTestSuite {    /// Recover the original test name    /// - masked: UITestCaseA_testA1/testA1      \t--&gt; recovered: UITestCaseA/testA1    /// - masked: Driver/UITestCaseA_testA1/testA1   --&gt; recovered: Driver/UITestCaseA/testA1    guard let testBaseName = maskedName.split(separator: \"/\").last else {      return swizzled_init(forTestCaseWithName: maskedName)    }    let recoveredName = maskedName.replacingOccurrences(of: \"_\\(testBaseName)/\", with: \"/\") # 👈 remove the token    return swizzled_init(forTestCaseWithName: recoveredName) # 👈 call the original init  }}    Swizzle function to run tests properlyTest Class DiscoveryIn order to adopt this tip, we need to know which test classes we need to run in advance. Although Apple does not provide an API to obtain the list before running tests, this can be done in several ways. One approach we can use is to generate test classes using Sourcery. Another alternative is to parse the binaries inside .xctest bundles (in build products) to look for symbols related to tests.ConclusionIn this article, we identified some factors causing test execution time imbalance in Xcode parallel testing (particularly for UI tests).We also looked into how Xcode distributes tests in parallel testing. We also try to mitigate a constraint in which tests within the same class run on the same simulator. The trick not only reduces the imbalance but also gives us more confidence in adding more tests to a class without caring about whether it affects our CI infrastructure.Below is the metric about test time imbalance recorded when running UI tests. After adopting the trick, we saw a decrease in the metric (which is a good sign). As of now, the metric stabilises at around 0.4 mins.    Tracking data of UI test time imbalance (in minutes) in our project, collected by multiple runsJoin usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/tackling-ui-test-execution-time-imbalance-for-xcode-parallel-testing"
      }
      ,
    
      "returning-storage-space-back-to-our-users": {
        "title": "Returning 575 Terabytes of Storage Space to Our Users",
        "author": "lucas-nelaupe",
        "tags": "[&quot;Mobile&quot;, &quot;Android&quot;, &quot;Performance&quot;]",
        "category": "",
        "content": "Have you ever run out of storage on your phone? Mobile phones come with limited storage and with the multiplication of apps and large video files, many of you are running out of space.In this article, we explain how we measure and reduce the storage footprint of the Grab app on a user’s device to help you overcome this issue.The Wakeup CallAndroid vitals (information provided by Google play Console about our app performance) gives us two main pieces of information about storage footprint.15.7% of users have less than 1GB of free storage and they tend to uninstall more than other users (1.2x).The proportion of 30 day active devices which reported less than 1GB free storage. Calculated as a 30 days rolling average.    Active devices with &lt;1GB free spaceThis is the ratio of uninstalls on active devices with less than 1GB free storage to uninstalls on all active devices. Calculated as a 30 days rolling average.    Ratio of uninstalls on active devices with less than 1GBInstrumentation to Know Where We StandFirst things first, we needed to know how much space the Grab app occupies on user device. So we started using our personal devices. We can find this information by opening the phone settings and selecting Grab app.    App SettingsFor this device (screenshot), the application itself (Installed binary) was 186 MB and the total footprint was 322 MB. Since this information varies a lot based on the usage of the app, we needed this information directly from our users in production.Disclaimer: We are only measuring files that are inside the internal Grab app folder (Cache/Database). We do NOT measure any file that is not inside the private Grab folder.We decided to leverage on our current implementation using StorageManager API to gather the following information during each session launch:  Application Size (Installed binary size)  Cache folder size  Total footprint    Sample code to retrieve storage information on AndroidData AnalysisWe began analysing this data one month after our users’ updated their app and found that the cache size was anomaly huge (&gt; 1GB) for a lot of users. Intrigued, we dug deeper.We added code to log the top largest files inside the cache folder, and we found that most of the files were inside a sub cache folder that was no longer in use. This was due to a usage of a 3rd party library that was removed from our app. We added a specific metric to track the size of this folder.In the end, a lot of users still had this old cache data and for some users the amount of data can be up to 1GB.Root Cause AnalysisThe Grab app relies a lot on 3rd party libraries. For example, Picasso was a library we used in the past for image display which is now replaced by Glide. Picasso uses a cache to store images and avoid making network calls again and again. After removing Picasso from the app, we didn’t delete this cache folder on the user device. We knew there would likely be more third-party libraries that had been discontinued so we expanded our analysis to look at how other 3rd party libraries cached their data.Freeing Up Space on Users’ PhonesHere comes the fun part. We implemented a cleanup mechanism to remove old cache folders. When users update the Grab app, any old cache folders which were there before would automatically be removed. By doing this, we released up to 1GB of data in a second back to our users. In total, we removed 575 terabytes of old cache data across more than 13 million devices (approximately 40MB per user on average).Data SummaryThe following graph shows the total size of junk data (in Terabytes) that we can potentially remove each day, calculated by summing up the maximum size of cache when a user opens the Grab app each day.The first half of the graph reflects the amount of junk data in relation to the latest app version before auto-clean up was activated. The second half of the graph shows a dramatic dip in junk data after auto-clean up was activated. We were deleting up to 33 Terabytes of data per day on the user’s device when we first started!    Sum of all junk data on user’s device reported per day in TerabytesNext StepThis is the first phase of our journey in reducing the storage footprint of our app on Android devices. We specifically focused on making improvements at scale i.e. deliver huge storage gains to the most number of users in the shortest time. In the next phase, we will look at more targeted improvements for specific groups of users that still have a high storage footprint. In addition, we are also reviewing iOS data to see if a round of clean up is necessary.Concurrently, we are also reducing the maximum size of cache created by some libraries. For example, Glide by default creates a cache of 250MB but this can be configured and optimised.We hope you found this piece insightful and please remember to update your app regularly to benefit from the improvements we’re making every day. If you find that your app is still taking a lot of space on your phone, be assured that we’re looking into it.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/returning-storage-space-back-to-our-users"
      }
      ,
    
      "grab-posisi": {
        "title": "Grab-Posisi - Southeast Asia’s First Comprehensive GPS Trajectory Dataset",
        "author": "zhengmin-xupoornima-badrinathxiaocheng-huangabeesh-thomas",
        "tags": "[&quot;GPS&quot;, &quot;Datasets&quot;, &quot;Maps&quot;]",
        "category": "",
        "content": "Introduction        At Grab, thousands of bookings happen daily via the Grab app. The driver phones and GPS devices enable us to collect large-scale GPS trajectories.Apart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.Currently, it’s a herculean task to create and maintain the GPS datasets since it’s costly and laborious. As a result, most of the GPS datasets available today in the market have poor coverage or contain outdated information. They cover only a small area of a city, have low sampling rates and contain less contextual information of the GPS pings, such as no accuracy level, bearing, and speed. Despite over a dozen mapping communities engaged in collecting GPS trajectory datasets, a significant amount of effort would be required for data cleaning and data pre-processing in order to utilise them.To overcome the shortfalls in the existing datasets, we built Grab-Posisi, the first GPS trajectory dataset of Southeast Asia. The term Posisi refers to a position in Bahasa. The data was collected from Grab drivers’ phones while in transit. By tackling the addition of major arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped, Posisi substantially improves mapping productivity.What’s Inside the DatasetThe whole Grab-Posisi dataset contains in total 84K trajectories that consist of more than 80 million GPS pings and cover over 1 million km. The average trajectory length is 11.94 km and the average duration per trip is 21.50 minutes.The data were collected very recently in April 2019 with a 1 second sampling rate, which is the highest amongst all the publicly available datasets. It also has richer contextual information, including the accuracy level, bearing and speed. The accuracy level is important because GPS measurements are noisy and the true location can be anywhere inside a circle centred at the reported location with a radius equal to the accuracy level. The bearing is the horizontal direction of travel, measured in degrees relative to true north. Finally, the speed is reported in meters/second over ground.As the GPS trajectories were collected from Grab drivers’ phones while in transit, we labelled each trajectory by phone device type being either Android or iOS. This is the first dataset which differentiates such device information. Furthermore, we also label the trajectories by driving mode (Car or Motorcycle).All drivers’ personal information is encrypted and the real start/end locations are removed within the dataset.Data FormatEach trajectory is serialised in a file in Apache Parquet format. The whole dataset size is around 2 GB. Each GPS ping is associated with values for a trajectory ID, latitude, longitude, timestamp (UTC), accuracy level, bearing and speed. The GPS sampling rate is 1 second, which is the highest among all the existing open source datasets. Table 1 shows a sample of the dataset.    Table 1: Sample datasetCoverageFigure 1a shows the spatial coverage of the dataset in Singapore. Compared with the GPS datasets available in the market that only cover a specific area of a city, the Grab-Posisi dataset encompasses almost the whole island of Singapore. Figure 1b depicts the GPS density in Singapore. Red represents high density while green represents low density. Expressways in Singapore are clearly visible because of their dense GPS pings.    Figure 1a. Spatial coverage (Singapore)    Figure 1b. GPS density (highways have more GPS)Figure 2a illustrates that the Grab-Posisi dataset encloses not only central Jakarta but also extends to external highways. Figure 2b depicts the GPS density of cars in Jakarta. Compared with Singapore, trips in Jakarta are spread out in all different areas, not just concentrated on highways.    Figure 2a. Spatial coverage (Jakarta)    Figure 2b. GPS density (Car)Applications of Grab-PosisiThe following are some of the applications of Grab-Posisi dataset.On Map InferenceThe traditional method used in updating road networks in maps is time-consuming and labour-intensive. That’s why maps might have important roads missing and real-time traffic conditions might be unavailable. To address this problem, we can use GPS trajectories in reconstructing road networks automatically.A bunch of map generation algorithms can be applied to infer both map topology and road attributes. Figure 3b shows a snippet of the inferred map from our GPS trajectories (Figure 3a) using one of the algorithms. As you can see from the blue dots, the skeleton of the underlining map inferred is correct, although some section of the inferred road is disconnected, and at the roundabout in the bottom right corner it’s not a smooth curve.          Figure 3a. Raw GPS trajectories              Figure 3b. Inferred Map  On Map Matching                                         The map matching refers to the task of automatically determining the correct route where the driver has travelled on a digital map, given a sequence of raw and noisy GPS points. The correction of the raw GPS data has been important for many location-based applications such as navigation, tracking, and road attribute detection as aforementioned. The accuracy levels provided in the Grab-Posisi dataset can be of great use to address this issue.On Traffic Detection and Forecast                         In addition to the inference of a static digital map, the Grab-Posisi GPS dataset can also be used to perform real-time traffic forecasting, which is very important for congestion detection, flow control, route planning, and navigation. Some examples of the fundamental indicators that are mostly used to monitor the current status of traffic conditions include the average speed, volume, and density in each road segment. These variables can be computed based on drivers’ GPS trajectories and can be used to predict the future traffic conditions.On Mode Detection                         Transportation mode detection refers to the task of identifying the travel mode of a user (some examples of transportation mode include walk, bike, car, bus, etc.). The GPS trajectories in our dataset are associated with rich attributes including GPS accuracy, bearing, and speed in addition to the latitude and longitude of geo-coordinates, which can be used to develop mode detection models. Our dataset also provides labels for each trajectory to be collected from a car or motorcycle, which can be used to verify performance of those models.Economics Perspective                                         The real-world GPS trajectories of people reveal realistic travel patterns and demands, which can be of great help for city planning. As there are some realistic constraints faced by governments such as budget limitations and construction inconvenience, it is important to incorporate both the planning authorities’ requirements and the realistic travel demands mined from trajectories for intelligent city planning. For example, the trajectories of cars can provide suggestions on how to schedule highway constructions. The trajectories of motorcycles can help the government to choose the optimal locations to construct motorcycle lanes for safety concerns.Want to Access Our Dataset?Grab-Posisi dataset offers a great value and is a significant resource to the community for benchmarking and revisiting existing technologies.         If you want to access our dataset for research purposes, email grab.posisi@grabtaxi.com with the following details:  Your Name and contact details  Your institution  Your potential usage of the datasetWhen using Grab-Posisi dataset, please cite the following paper:Huang, X., Yin, Y., Lim, S., Wang, G., Hu, B., Varadarajan, J., … &amp; Zimmermann, R. (2019, November). Grab-Posisi: An Extensive Real-Life GPS Trajectory Dataset in Southeast Asia. In Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Prediction of Human Mobility (pp. 1-10). DOI: https://doi.org/10.1145/3356995.3364536Click here to download the published paper.Click here to download the BibTex file.Note: You cannot use Grab-Posisi dataset for commercial purposes.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/grab-posisi"
      }
      ,
    
      "preventing-app-performance-degradation-due-to-sudden-ride-demand-spikes": {
        "title": "How We Prevented App Performance Degradation from Sudden Ride Demand Spikes",
        "author": "corey-scott",
        "tags": "[&quot;Resiliency&quot;, &quot;Circuit Breakers&quot;]",
        "category": "",
        "content": "In Southeast Asia, when it rains, it pours. It’s a major mood dampener especially if you are stuck outside when the rain starts, you are about to have an awful day.In the early days of Grab, if the rains came at the wrong time, like during morning rush hour, then we engineers were also in for a terrible day.In those days, demand for Grab’s ride services grew much faster than our ability to scale our tech system and this often meant clocking late nights just to ensure our system could handle the ever-growing demand. When there’s a massive, sudden spike in ride bookings, our system often struggled to manage the load.There were also other contributors to demand spikes, for example when public transport services broke down or when a major event such as an international concert ends and event-goers all need a ride at the same time.Upon reflection, we realised there were two integral aspects to these incidents.Firstly, they were localised events. The increase in demand came from a particular geographical location; in some cases a very small area. These localised events had the potential to cause so much load on our system that it impacted the experience of other users outside the geolocation.Secondly, the underlying problem was a lack of drivers (supply) in that particular geographical area.At Grab, our goal has always been to get everyone a ride when and where they needed it, but in this situation, it was just not possible. We needed to find a way to ensure this localised demand spike did not affect our ability to meet the needs of other users.Enter the Spampede FilterThe Spampede (a play of the words spam and stampede) filter was inspired by another concept you may have read on this blog, circuit breakers.In software, as in electronics, circuit breakers are designed to protect a system by short-circuiting in the face of adverse conditions.Let’s break this down.There are two key concepts here: short-circuiting and adverse conditions.Firstly, short-circuiting, in this context means performing minimal processing on a particular booking, and by doing so, reducing the overall load on the system. Secondly, adverse conditions, in this, we refer to a large number of unfulfilled requests for a particular service, from a small geographical area, within a short time window. With these two concepts in mind, we devised the following process.Spampede DesignFirst, we needed to track unallocated requests in a location-aware manner. To do this, we convert the requested pickup location of an unallocated request using the Geohash Integer algorithm.  After the conversion, the resulting value is an exact location. We can convert this location into a “bucket” or area by reducing the precision.This method is by no means smart or aware of the local geography, but it is incredibly CPU efficient and requires no external resources like network API calls.Now that we can track unallocated requests, we needed a way for the tracking to be time-aware. After all, traffic conditions, driver locations, and passenger demand are continually changing. We could have implemented something precise like a sliding window sum, but that would have introduced a lot of complexity and a significantly higher CPU and memory cost.By using the Unix timestamp, we converted the current time to a “bucket” of time by using the straightforward formula:            where bs is the size of the time buckets in secondsWith location and time buckets calculated, we can track the unallocated bookings using Redis. We could have used any data store, but Redis was familiar and battle-proven to us.To do this, we first constructed the Redis key by combining the service type, the geographic location, and the time bucket. With this key, we call the INCR command, which increments the value stored in that location and returns the new value.If the value returned is 1, this indicates that this is the first value stored for this bucket combination, and we would then make a second call, this time to EXPIRE. With this second call, we would set a time to live (TTL) on the Redis item, allowing the data to be self-cleaning.You will notice that we are blindly calling increment and only making a second call if needed. This pattern is more efficient and resource-friendly than using a more traditional, load-check-store pattern.The next step was the configuration. Specifically, setting how many unallocated bookings could happen in a particular location and time bucket before the circuit opened. For this, we decided on Redis again. Again, we could have used anything, but we were already using Redis and, as mentioned previously, quite familiar with it.Finally, the last piece. We introduced code at the beginning of our booking processing, most importantly, before any calls to any other services and before any significant processing was done. This code compared the location, time, and requested service to the currently configured Spampede setting, along with the previously unallocated bookings. If the maximum had already been reached, then we immediately stopped processing the booking.This might sound harsh- to immediately refuse a booking request without even trying to fulfil it. But the goal of the Spampede filter is to prevent excessive, localised demand from impacting all of the users of the system.ConclusionReading about this as a programmer, it probably feels strange, intentionally dropping bookings and impacting the business this way.After all, we want nothing more than to help people get to where they need to be. This process is a system safety mechanism to ensure that the system stays alive and able to do just that.I would be remiss if I didn’t highlight the critical software-engineering takeaway here is a combination of the Observer effect and the underlying goals of the CAP theorem. Observing a system will influence the system due to the cost of instrumentation and monitoring.Generally, the higher the accuracy or consistency of the monitoring and limits, the higher the resource cost.In this case, we have intentionally chosen the most resource-efficient options and traded accuracy for more throughput.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!",
        "url": "/preventing-app-performance-degradation-due-to-sudden-ride-demand-spikes"
      }
      ,
    
      "plumbing-at-scale": {
        "title": "Plumbing At Scale",
        "author": "karan-kamath",
        "tags": "[&quot;Event Sourcing&quot;, &quot;Stream Processing&quot;, &quot;Kubernetes&quot;, &quot;Back End&quot;, &quot;Platform&quot;, &quot;Go&quot;]",
        "category": "",
        "content": "When you open the Grab app and hit book, a series of events are generated that define your personalised experience with us: booking state machines kick into motion, driver-partners are notified, reward points are computed, your feed is generated, etc. While it is important for you to know that a request has been received, a lot happens asynchronously in our back-end services.As custodians and builders of the streaming platform at Grab operating at massive scale (think terabytes of data ingress each hour), the Coban team’s mission is to provide a NoOps, managed platform for seamless, secure access to event streams in real-time, for every team at Grab.          Coban Sewu Waterfall In Indonesia. (Streams, get it?)  Streaming systems are often at the heart of event-driven architectures, and what starts as a need for a simple message bus for asynchronous processing of events quickly evolves into one that requires a more sophisticated stream processing paradigms.Earlier this year, we saw common patterns of event processing emerge across our Go backend ecosystem, including:  Filtering and mapping stream events of one type to another  Aggregating events into time windows and materialising them back to the event log or to various types of transactional and analytics databasesGenerally, a class of problems surfaced which could be elegantly solved through an event sourcing1 platform with a stream processing framework built over it, similar to the Keystone platform at Netflix2.This article details our journey building and deploying an event sourcing platform in Go, building a stream processing framework over it, and then scaling it (reliably and efficiently) to service over 300 billion events a week.Event SourcingEvent sourcing is an architectural pattern where changes to an application state are stored as a sequence of events, which can be replayed, recomputed, and queried for state at any time. An implementation of the event sourcing pattern typically has three parts to it:  An event log  Processor selection logic: The logic that selects which chunk of domain logic to run based on an incoming event  Processor domain logic: The domain logic that mutates an application’s state          Event Sourcing  Event sourcing is a building block on which architectural patterns such as Command Query Responsibility Segregation3, serverless systems, and stream processing pipelines are built.The Case For Stream ProcessingHere are some use cases serviced by stream processing, built on event sourcing.Asynchronous State ManagementA pub-sub system allows for change events from one service to be fanned out to multiple interested subscribers without letting any one subscriber block the progress of others. Abstracting the event log and centralising it democratises access to this log to all back-end services. It enables the back-end services to apply changes from this centralised log to their own state, independent of downstream services, and/or publish their state changes to it.Time Windowed AggregationsTime-windowed aggregates are a common requirement for machine learning models (as features) as well as analytics. For example, personalising the Grab app landing page requires counting your interaction with various widget elements in recent history, not any one event in particular. Similarly, an analyst may not be interested in the details of a singular booking in real-time, but in building demand heatmaps segmented by geohashes. For latency-sensitive lookups, especially for the personalisation example, pre-aggregations are preferred instead of post-aggregations.Stream Joins, Filtering, MappingEvent logs are typically sharded by some notion of topics to logically divide events of interest around a theme (booking events, profile updates, etc.). Building bigger topics out of smaller ones, as well as smaller ones from bigger ones are common ways to compose “substreams”  of the log of interest directed towards specific services. For example, a promo service may only be interested in listening to booking events for promotional bookings.Realtime Business IntelligenceOutputs of stream processing workloads are also plugged into realtime Business Intelligence (BI) and stream analytics solutions upstream, as raw data for visualizations on operations dashboards.ArchivalFor offline analytics, as well as reconciliation and disaster recovery, having an archive in a cold store helps for certain mission critical streams.Platform RequirementsAny processing platform for event sourcing and stream processing has certain expectations around its functionality.Scaling and ElasticityStream/Event Processing pipelines need to be elastic and responsive to changes in traffic patterns, especially considering that user activity (rides, food, deliveries, payments) varies dramatically during the course of a day or week. A spike in food orders on rainy days shouldn’t cause indefinite order processing latencies.NoOpsFor a platform team, it’s important that users can easily onboard and manage their pipeline lifecycles, at their preferred cadence. To scale effectively, the process of scaffolding, configuring, and deploying pipelines needs to be standardised, and infrastructure managed. Both the platform and users are able to leverage common standards of telemetry, configuration, and deployment strategies, and users benefit from a lack of infrastructure management overhead.Multi-tenancyOur platform has quickly scaled to support hundreds of pipelines. Workload isolation, independent processing uptime guarantees, and resource allocation and cost audit are important requirements necessitating multi-tenancy, which help amortise platform overhead costs.ResiliencyWhether latency sensitive or latency tolerant, all workloads have certain expectations on processing uptime. From a user’s perspective, there must be guarantees on pipeline uptimes and data completeness, upper bounds on processing delays, instrumentation for alerting, and self-healing properties of the platform for remediation.Tunable TradeoffsSome pipelines are latency sensitive, and rely on processing completeness seconds after event ingress. Other pipelines are latency tolerant, and can tolerate disruption to processing lasting in tens of minutes. A one size fits all solution is likely to be either cost inefficient or unreliable. Having a way for users to make these tradeoffs consciously becomes important for ensuring efficient processing guarantees at a reasonable cost. Similarly, in the case of upstream failures or unavailability, being able to tune failure modes (like wait, continue, or retry) comes in handy.Stream Processing FrameworkWhile basic event sourcing covers simple use cases like archival, more complicated ones benefit from a common framework that shifts the mental model for processing from per event processing to stream pipeline orchestration.Given that Go is a “paved road” for back-end development at Grab, and we have service code and bindings for streaming data in a mono-repository, we built a Go framework with a subset of capabilities provided by other streaming frameworks like Flink4.          Logic Blocks In A Stream Processing Pipeline  CapabilitiesSome capabilities built into the framework include:  Deduplication:  Enables pipelines to idempotently reprocess data in case of rewinds/replays, and provides some processing guarantees within a time window for certain use cases including sinking to datastores.  Filtering and Mapping: An ability to filter a source stream data and map them onto target streams.  Aggregation: An ability to generate and execute aggregation logic such as sum, avg, max, and min in a window.  Windowing: An ability to window processing into tumbling, sliding, and session windows.  Join: An ability to combine two streams together with certain join keys in a window.  Processor Chaining: Various functionalities can be chained to build more complicated pipelines from simpler ones. For example: filter a large stream into a smaller one, aggregate it over a time window, and then map it to a new stream.  Rewind: The ability to rewind the processing logic by a few hours through configuration.  Replay: The ability to replay archived data into the same or a separate pipeline via configuration.  Sinks: A number of connectors to standard Grab stores are provided, with concerns of auth, telemetry, etc. managed in the runtime.  Error Handling: Providing an easy way to indicate whether to wait, skip, and/or retry in case of upstream failures is an important tuning parameter that users need for making sensible tradeoffs in dimensions of back pressure, latency, correctness, etc.Architecture          Coban Platform  Our event log is primarily a bunch of critical Kafka clusters, which are being polled by various pipelines deployed by service teams on the platform for incoming events. Each pipeline is an isolated deployment, has an identity, and the ability to connect to various upstream sinks to materialise results into, including the event log itself.There is also a metastore available as an intermediate store for processing pipelines, so the pipelines themselves are stateless with their lifecycle completely beholden to the whims of their owners.Anatomy of a Processing Pipeline          Anatomy Of A Stream Processing Pod  Anatomy of a Stream Processing PodEach stream processing pod (the smallest unit of a pipeline’s deployment) has three top level components:  Triggers: An interface that connects directly to the source of the data and converts it into an event channel.  Runtime: This is the app’s entry point and the orchestrator of the pod. It manages the worker pools, triggers, event channels, and lifecycle events.  The Pipeline plugin: The plugin is provided by the user, and conforms to a contract that the platform team publishes. It contains the domain logic for the pipeline and houses the pipeline orchestration defined by a user based on our Stream Processing Framework.Deployment InfrastructureOur deployment infrastructure heavily leverages Kubernetes on AWS. After a (pretty high) initial cost for infrastructure set up, we’ve found scaling to hundreds of pipelines a breeze with the Kubernetes provided controls. We package our stateless pipeline workloads into Kubernetes deployments, with each pod containing a unit of a stream pipeline, with sidecars that integrate them with our monitoring systems. Other cluster wide tooling deployed (usually as DaemonSets) deal with metric collection, log ingestion, and autoscaling. We currently use the Horizontal Pod Autoscaler5 to manage traffic elasticity, and the Cluster Autoscaler6 to manage worker node scaling.          A Typical Kubernetes Set Up On AWS  MetastoreSome pipelines require storage for use cases ranging from deduplication to stores for materialised results of time windowed aggregations. All our pipelines have access to clusters of ScyllaDB instances (which we use as our internal store), made available to pipeline authors via interfaces in the Stream Processing Framework. Results of these aggregations are then made available to backend services via our GrabStats service, which is a thin query layer over the latest pipeline results.Compute IsolationA nice property of packaging pipelines as Kubernetes deployments is a good degree of compute workload isolation for pipelines. While node resources of pipeline pods are still shared (and there are potential noisy neighbour issues on matters like logging throughput), the pipeline pods of various pods can be scheduled and rescheduled across a wide range of nodes safely and swiftly, with minimal impact to pods of other pipelines.RedundancyStateless processing pods mean we can set up backup or redundant Kubernetes clusters in hot-hot, hot-warm, or hot-cold modes. We use this to ensure high processing availability despite limited control plane guarantees from any single cluster. (Since EKS SLAs for the Kubernetes control plane guarantee only 99.9% uptime today7.) Transparent to our users, we make the deployment systems aware of multiple available targets for scheduling.Availability vs CostAs alluded to in the “Platform Requirements” section, having a way of trading off availability for cost becomes important where the requirements and criticality of each processing pipeline are very different. Given that AWS spot instances are a lot cheaper8 than on-demand ones, we use user annotated Kubernetes priority classes to determine deployment targets for pipelines. For latency tolerant pipelines, we schedule them on Spot instances which are routinely between 40-90% cheaper than on demand instances on which latency sensitive pipelines run. The caveat is that Spot instances occasionally disappear, and these workloads are disrupted until a replacement node for their scheduling can be found.What’s Next?  Expand the ecosystem of triggers to support custom sources of data i.e. the “event log”, as well as push based (RPC driven) versus just pull based triggers  Build a control plane for API integration with pipeline lifecycle management  Move some workloads to use the Vertical Pod Autoscaler9 in Kubernetes instead of horizontal scaling, as most of our workloads have a limit on parallelism (which is their partition count in Kafka topics)  Move from Go plugins for pipelines to plugins over RPC, like what HashiCorp does10, to enable processing logic in non-Go languages.  Use either pod gossip or a service mesh with a control plane to set up quotas for shared infrastructure usage per pipeline. This is to protect upstream dependencies and the metastore from surges in event backlogs.  Improve availability guarantees for pipeline pods by occasionally redistributing/rescheduling pods across nodes in our Kubernetes cluster to prevent entire workloads being served out of a few large nodes.Authored By Karan Kamath on behalf of the Coban team at Grab-Zezhou Yu, Ryan Ooi, Hui Yang, Yuguang Xiao, Ling Zhang, Roy Kim, Matt Hino, Jump Char, Lincoln Lee, Jason Cusick, Shrinand Thakkar, Dean Barlan, Shivam Dixit, Shubham Badkur, Fahad Pervaiz, Andy Nguyen, Ravi Tandon, Ken Fishkin, and Jim Caputo.Join usGrab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, join our team today!FootnotesCoban Sewu Waterfall Photo by Dwinanda Nurhanif Mujito on UnsplashCover Photo by tian kuan on Unsplash            https://martinfowler.com/eaaDev/EventSourcing.html &#8617;              https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a &#8617;              https://martinfowler.com/bliki/CQRS.html &#8617;              https://flink.apache.org &#8617;              https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ &#8617;              https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler &#8617;              https://aws.amazon.com/eks/sla/ &#8617;              https://aws.amazon.com/ec2/pricing/ &#8617;              https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler &#8617;              https://github.com/hashicorp/go-plugin &#8617;      ",
        "url": "/plumbing-at-scale"
      }
      ,
    
      "journey-to-a-faster-everyday-super-app": {
        "title": "Journey to a Faster Everyday Superapp Where Every Millisecond Counts",
        "author": "renu-yadav",
        "tags": "[&quot;Superapp&quot;, &quot;Mobile&quot;, &quot;Performance&quot;]",
        "category": "",
        "content": "IntroductionAt Grab, we are moving faster than ever. In 2019 alone, we released dozens of new features in the Grab passenger app. With our goal to delight users in Southeast Asia with a powerful everyday superapp, the app’s performance became one of the most critical components in delivering that experience to our users.This post narrates the journey of our performance improvement efforts on the Grab passenger app. It highlights how we were able to reduce the time spent starting the app by more than 60%, while preventing regressions introduced by new features. We use the p95 scale when referring to these improvements.Here’s a quick look at the improvements and timeline:  Improving App PerformanceWhile app performance consists of different aspects - such as battery consumption rate, network performance, app responsiveness, etc. - the first thing users notice is the time it takes for an app to start. Apps that take too long to load frustrate users, leading to bad reviews and uninstalls.We focused our efforts on the app’s time to interactive(TTI), which consists of two main operations:  Starting the app  Displaying interactive service tiles (these are the icons for the services offered on the app such as Transport, Food, Delivery, and so on)There are many other operations that occur in the background, which we won’t cover in this article.We prioritised on optimising the app’s ability to load the service tiles (highlighted in the image below) and render them as interactive upon startup (cold start). This allowed users to use the app as soon as they launch it.  Instrumentation and BenchmarkingBefore we could start improving the app’s performance, we needed to know where we stood and set measurable goals.We couldn’t get a baseline from local performance testing as it did not simulate the real environment condition, where network variability and device performance are contributing factors. Thus, we needed to use real production data to get an accurate reflection of our current performance at a scale. In production, we measured the performance of ~8-9 millions users per day - a small subset of our overall active user base.As a start, we measured the different components contributing to TTI, such as binary loading, library initialisations, and tiles loading. For example, if we had to measure the time taken by function A, this is how it looked like in the code:functionA (){// start the timer...........//Stop the timer, calculate the time difference and send it as an analytic event}With all the numbers from the contributing components, we took the sum to calculate the full TTI (as shown in the following image).  When the numbers started rolling in from production, we needed specific measurements to interpret those numbers, so we started looking at TTI’s  50th, 90th, and 95th percentile. A 90th percentile (p90) of x seconds means that 90% of the users have an interactive screen in at most x seconds.We chose to only focus on p50 and p95 as these cover the majority of our users who deal with performance issues. Improving performance for &lt;p50 (who already have high-end devices) would not bring too much of a value, and improving for &gt;p95 would be very difficult as the app performance improvements will be limited by device performance.By the end of January, we got the p50, p90, and p95 numbers for the contributing components that summed up to TTI numbers for tiles, which allowed us to start identifying areas with potential improvements.Caching and Animation RemovalWhile reviewing the TTI numbers, we were drawn to contributors with high time consumption rates such as tile loading and app start animation. Other evident improvement we worked on was caching data between app launches instead of waiting for a network response for loading tiles at every single app launch.Tile CachingBased on the gathered data, the service tiles only change when a user travels between cities. This is because the available services vary in each city. Since users do not frequently change cities, the service tiles do not change very frequently either, and so caching the tiles made sense. However, we also needed to sync the fresh tiles, in case of any change. So, we updated the logic based on these findings. as illustrated in the following image:  Caching tiles brought us a huge improvement of ~3s on each platform.Animation RemovalWe came across a beautifully created animation at appstart that didn’t provide any additional value in terms of information or practicality.With detailed discussions and trade-offs with designers, we removed the animation and improved our TTI further by 1s.In conclusion, with the caching and animation removal alone, we improved the TTI by 4s.Welcome Static Linking and CoroutinesAt this point, our users gained 4 seconds of their time back, but we didn’t want to stop with that number. So, we dug through the data to see what further enhancements we could do. When we could not find anything else that was similar to caching and animation removal, we shifted to architecture fundamentals.We knew that this was not an easy route to take and that it would come with a cost; if we decided to choose a component related to architecture fundamentals, all the other teams working on the Grab app would be impacted. We had to evaluate our options and make decisions with trade-offs for overall improvements. And this eventually led to static linking on iOS and coroutines on Android.Binary LoadingBinary loading is one of the first steps in both mobile platforms when an app is launched. It primarily contributes to pre-main and dex-loading, on iOS and Android respectively.The pre-main time on iOS was about 7.9s. It is known in the iOS development world that each framework (binary) can either be dynamically or statically linked. While static helps in a faster app start, it brings complexity in building frameworks that are elaborate or contain resources bundles.Building a lot of libraries statically also impact build times negatively.With proper evaluations, we decided to take the route to enable more static linking due to the trade-offs.Apple recommends a maximum of half a dozen dynamic frameworks for an optimal performance. Guess what? Our passenger app had 107 dynamically linked frameworks, a lot of them were internal.The task looked daunting at first, since it affected all parts of the app, but we were ready to tackle the challenge head on. Deciding to take this on was the easy part, the actual work entailed lots of tricky coordination and collaboration with multiple teams.We created an RFC (Request For Comments) doc to propose the static linking of frameworks, wherever applicable, and co-ordinated with teams with the agreed timelines to execute this change.While collaborating with teams, we learned that we could remove 12 frameworks entirely that were no longer required. This exercise highlighted the importance of regular cleanup and deprecation in our codebase, and was added into our standard process.And so, we were left with 95 frameworks; 75 of which were statically linked successfully, resulting in our p90 pre-main dropping by 41%.As Grabbers, it’s in our DNA to push ourselves a little more. With the remaining 20 frameworks, our pre-main was still considerably high. Out of the 20 frameworks, 10 could not be statically linked without issues. As a workaround, we merged multiple dynamic frameworks into one. One of our outstanding engineers even created a plug-in for this, which is called the CocoaPods Merge. With this plug-in, we were able to merge 10 dynamically linked frameworks into 2. We’ve made this plug-in open source: https://github.com/grab/cocoapods-pod-merge.With all of the above steps, we were finally left with 12 dynamic frameworks - a huge 88% reduction.The following image illustrates the complex numbers mentioned above:  Using CocoaPods merge further helped us with ~0.8s of improvement.CoroutinesWhile we were executing the static linking initiative on iOS, we also started refactoring the application initialisation for a modular and clean code on Android. This resulted in creating an ApplicationInitialiser class, which handles the entire application initialisation process with maximum parallelism using coroutines.Now all the libraries are being initialised in parallel via coroutines and thus enabling better utilisations of computing resources and a faster TTI.This refactoring and background initialisation for libraries on Android helped gain ~0.4s of improvements.Changing the Basics - Visualisation SetupBy the end of H1 2019, we observed a 50% improvement in TTI, and now it was time to set new goals for H2 2019. Until this point, we would query our database for all metric numbers, copy the numbers into a spreadsheet, and compare them against weeks and app versions.Despite the high loads of manual work and other challenges, this method still worked at the beginning due to the improvements we had to focus on.However, in H2 2019 it became apparent that we had to reassess our methodology of reading numbers. So, we started thinking about other ways to present and visualise these numbers better. With help from our Product Analyst, we took advantage of metabase’s advanced capabilities and presented our goals and metrics in a clear and easy to understand format.For example, here is a graph that shows the top contributing metrics for Android:  Looking at it, we could clearly tell which metric needed to be prioritised for improvements.We did this not only for our metrics, but also for our main goals, which allowed us to easily see our progress and track our improvements on a daily basis.  The colour bars in the above image depicts the status of our numbers against our goals and also shows the actual numbers at p50, p90, and p95.As our tracking progressed, we started including more granular and precise measurements, to help guide the team and achieve more impactful improvements of around ~0.3-0.4s.Fortunately, we were deprecating a third-party library for analytics and experimentation, which happened to be one of the highest contributing metrics for both platforms due to a high number of operations on the main thread. We started using our own in-house experimentation platform where we had better control over performance. We removed this third-party dependency, and it helped us with huge improvements of ~2.5s on Android and ~0.5-0.7s on iOS.You might be wondering as to why there is such a big difference on the iOS and Android improvement numbers for this dependency. This was due to the setting user attributes operations that ran only in the Android codebase, which was performed on the main thread and took a huge amount of time. These were the times that made us realise that we should focus more on the consistency for both platforms, as well as to identify the third-party library APIs that are used, and to assess whether they are absolutely necessary.*Tip*: So, it is time for you as well to eliminate such inconsistencies, if there are any.Ok, there goes our third quarter with ~3s of improvement on Android and ~1.3s on iOS.Performance Regression DetectionEntering into Q4 brought us many challenges as we were running out of improvements to make. Even finding an improvement worth ~0.05s was really difficult! We were also strongly challenged by regressions (increase in TTI numbers) because of continuous feature releases and code additions to the app start process.So, maintaining the TTI numbers became our primary task for this period. We started looking into setting up processes to block regressions from being merged to the master, or at least get notified before they hit production.To begin with, we identified the main sources of regressions: static linking breakage on iOS and library initialisation in the app startup process on Android.We took the following measures to cover these cases:LintersWe built linters on the Continuous Integration (CI) pipeline to detect potential changes in static linking on iOS and the ApplicationInitialiser class on Android. The linters block the change list and enforce a special review process for such changes.Library Integration ProcessThe team also focused on setting up a process for library integrations, where each library (internal or third party) will first be evaluated for performance impact before it is integrated into the codebase.While regression guarding was in process, we were simultaneously trying to bring in more improvements for TTI. We enabled the Link Time Optimisations (LTO) flag on iOS to improve the overall app performance. We also experimented on order files on iOS and anko layout on Android, but  were ruled out due to known issues.On Android, we hit the bottom hard as there were minimal improvements. Fortunately, it was a different story for iOS. We managed to get improvements worth ~0.6s by opting for lazy loading, optimising I/O operations, and deferring more operations to post app start (if applicable).Next StepsWe will be looking at the different aspects of performance such as network, battery, and storage, while maintaining our current numbers for TTI.  Network performance - Track the turnaround time for network requests then move on to optimisations.  Battery performance - Focus on profiling the app for CPU and energy intensive operations, which drains the battery, and then move to optimisations.  Storage performance - Review our caching and storage mechanisms, and then look for ways to optimise them.In addition to these, we are also focusing on bringing performance initiatives for all the teams at Grab. We believe that performance is a collaborative approach, and we would like to improve the app performance in all aspects.We defined different metrics to track performance e.g. Time to Interactive, Time to feedback (the time taken to get the feedback for a user action), UI smoothness indicators, storage, and network metrics.We are enabling all teams to benchmark their performance numbers based on defined metrics and move on to a path of improvement.ConclusionOverall, we improved by 60%, and this calls for a big celebration! Woohoo! The bigger celebration came from knowing that we’ve improved our consumers’ experience in using our app.This graph represents our performance improvement journey for the entire 2019, in terms of TTI.  Based on the graph, looking at our p95 improvements and converting them to number of hours saved per day gives us ~21,388 hours on iOS and ~38,194 hours saved per day on Android.Hey, did you know that it takes approximately 80-85 hours to watch all the episodes of Friends? Just saying. :)We will continue to serve our consumers for a better and faster experience in the upcoming years.",
        "url": "/journey-to-a-faster-everyday-super-app"
      }
      ,
    
      "marionette-enabling-e2e-user-scenario-simulation": {
        "title": "Marionette - Enabling E2E User-scenario Simulation",
        "author": "anish-jhabiju-jacobphuc-lam-nguyenvineet-nairyiwei-yeo",
        "tags": "[&quot;Back End&quot;, &quot;Testing&quot;, &quot;Microservice&quot;]",
        "category": "",
        "content": "IntroductionA plethora of interconnected microservices is what powers the Grab’s app. The microservices work behind the scenes to delight millions of our consumers in Southeast Asia. It is a no-brainer that we emphasise on strong testing tools, so our app performs flawlessly to continuously meet our consumers’ needs.BackgroundWe have a microservices-based architecture, in which microservices are interconnected to numerous other microservices. Each passing day sees teams within Grab updating their microservices, which in turn enhances the overall app. If any of the microservices fail after changes are rolled out, it may lead to the whole app getting into an unstable state or worse. This is a major risk and that’s why we stress on conducting “end-to-end (E2E) testing” as an integral part of our software test life-cycle.E2E tests are done for all crucial workflows in the app, but not for every detail. For that we have conventional tests such as unit tests, component tests, functional tests, etc. Consider E2E testing as the final approval in the quality assurance of the app.Writing E2E tests in the microservices world is not a trivial task. We are not testing just a single monolithic application. To test a workflow on the app from a user’s perspective, we need to traverse multiple microservices, which communicate through different protocols such as HTTP/HTTPS and TCP. E2E testing gets even more challenging with the continuous addition of microservices. Over the years, we have grown tremendously with hundreds of microservices working in the background to power our app.Some major challenges in writing E2E tests for the microservices-based apps are:      Availability    Getting all microservices together for E2E testing is tough. Each development team works independently and is responsible only for its microservices. Teams use different programming languages, data stores, etc for each microservice. It’s hard to construct all pieces in a common test environment as a complete app for E2E testing each time.        Data or resource set up    E2E testing requires comprehensive data set up. Otherwise, testing results are affected because of data constraints, and not due to any recent changes to underlying microservices. For example, we need to create real-life equivalent driver accounts, passenger accounts, etc and to have those, there are a few dependencies on other internal systems which manage user accounts. Further, data and booking generation should be robust enough to replicate real-world scenarios as far as possible.        Access and authentication    Usually, the test cases require sequential execution in E2E testing. In a microservices architecture, it is difficult to test a workflow which requires access and permissions to several resources or data that should remain available throughout the test execution.        Resource and time intensive    It is expensive and time consuming to run E2E tests; significant time is involved in deploying new changes, configuring all the necessary test data, etc.  Though there are several challenges, we had to find a way to overcome them and test workflows from the beginning to the end in our app.Our Approach to Overcome ChallengesWe knew what our challenges were and what we wanted to achieve from E2E testing, so we started thinking about how to develop a platform for E2E tests. To begin with, we determined that the scope of E2E testing that we’re going to primarily focus on is Grab’s transport domain — the microservices powering the driver and passenger apps.One approach is to “simulate” user scenarios through a single platform before any new versions of these microservices are released. Ideally, the platform should also have the capabilities to set up the data required for these simulations. For example, ride booking requires data set up such as driver accounts, passenger accounts, location coordinates, geofencing, etc.We wanted to create a single platform that multiple teams could use to set up their test data and run E2E user-scenario simulations easily. We put ourselves to work on that idea, which resulted in the creation of an internal platform called “Marionette”. It simulates actions performed by Grab’s passenger and driver apps as they are expected to behave in the real world. The objective is to ensure that all standard user workflows are tested before deploying new app versions.Introducing MarionetteMarionette enables Grabbers (developers and QAs) to run E2E user-scenario simulations without depending on the actual passenger and driver apps. Grabbers can set up data as well as configure data such as drivers, passengers, taxi types, etc to mimic the real-world behaviour.Let’s look at the overall architecture to understand Marionette better:  Grabbers can interact with Marionette through three channels: UI, SDK, and through RESTful API endpoints in their test scripts. All requests are routed through a load balancer to the Marionette platform. The Marionette platform in turn talks to the required microservices to create test data and to run the simulations.The BenefitsWith Marionette, Grabbers now have the ability to:  Simulate the whole booking flow including consumer and driver behaviour as well as transition through the booking life cycle including pick-up, drop-off, cancellation, etc. For example, developers can make passenger booking from the UI and configure pick-up points, drop-off points, taxi types, and other parameters easily. They can define passenger behaviour such as “make bookings after a specified time interval”, “cancel each booking”, etc. They can also set driver locations, define driver behaviour such as “always accept booking manually”, “decline received bookings”, etc.  Simulate bookings in all cities where Grab operates. Further, developers can run simulations for multiple Grab taxi types such as JustGrab, GrabShare, etc.  Visualise passengers, drivers, and ride transitions on the UI, which lets them easily test their workflows.  Save efforts and time spent on installing third-party android or iOS emulators, troubleshooting or debugging .apk installation files, etc before testing workflows.  Conduct E2E testing without real mobile devices and installed apps.  Run automatic simulations, in which a particular set of scenarios are run continuously, thus helping developers with exploratory testing.How We Isolated Simulations Among UsersIt is important to have independent simulations for each user. Otherwise, simulations don’t yield correct results. This was one of the challenges we faced when we first started running simulations on Marionette.To resolve this issue, we came up with the idea of “cohorts”. A cohort is a logical group of passengers and drivers who are located in a particular city. Each simulation on Marionette is run using a “cohort” containing the number of drivers and passengers required for that simulation. When a passenger/driver needs to interact with other passengers/drivers (such as for ride bookings), Marionette ensures that the interaction is constrained to resources within the cohort. This ensures that drivers and passengers are not shared in different test cases/simulations, resulting in more consistent test runs.How to Interact with MarionetteLet’s take a look at how to interact with Marionette starting with its user interface first.User InterfaceThe Marionette UI is designed to provide the same level of granularity as available on the real passenger and driver apps.Generally, the UI is used in the following scenarios:  To test common user scenarios/workflows after deploying a change on staging.  To test the end-to-end booking flow right from the point where a passenger makes a booking till drop-off at the destination.  To simulate functionality of other teams within Grab - the passenger app developers can simulate the driver app for their testing and vice versa. Usually, teams work independently and the ability to simulate the dependent app for testing allows developers to work independently.  To perform E2E testing (such as by QA teams) without writing any test scripts.The Marionette UI also allows Grabbers to create and set up data. All that needs to be done is to specify the necessary resources such as number of drivers, number of passengers, city to run the simulation, etc. Running E2E simulations involves just the click of a button after data set up. Reports generated at the end of running simulations provide a graphical visualisation of the results. Visual reports save developers’ time, which otherwise is spent on browsing through logs to ascertain errors.SDKMarionette also provides an SDK, written in the Go programming language.It lets developers:  Create resources such as passengers, drivers, and cohorts for simulating booking flows.  Create booking simulations in both staging and production.  Set bookings to specific states as needed for simulation through customisable driver and passenger behaviour.  Make HTTP requests and receive responses that matter in tests.  Run load tests by scaling up booking requests to match the required workload (QPS).Let’s look at a high-level booking test case example to understand the simulation workflow.Assume we want to run an E2E booking test with this driver behaviour type — “accepts passenger bookings and transits between booking states according to defined behaviour parameters”. This is just one of the driver behaviour types in Marionette; other behaviour types are also supported. Similarly, passengers also have behaviour types.To write the E2E test for this example case, we first define the driver behaviour in a function like this:  Then, we handle the booking request for the driver like this:  The SDK client makes the handling of passengers, drivers, and bookings very easy as developers don’t need to worry about hitting multiple services and multiple APIs to set up their required driver and passenger actions. Instead, teams can focus on testing their use cases.To ensure that passengers and drivers are isolated in our test, we need to group them together in a cohort before running the E2E test.  In summary, we have defined the driver’s behaviour, created the booking request, created the SDK client and associated the driver and passenger to a cohort. Now, we just have to trigger the E2E test from our IDE. It’s just that simple and easy!Previously, developers had to write boilerplate code to make HTTP requests and parse returned HTTP responses. With the Marionette SDK in place, developers don’t have to write any boilerplate code saving significant time and effort in E2E testing.RESTful APIs in Test ScriptsMarionette provides several RESTful API endpoints that cover different simulation areas such as resource or data creation APIs, driver APIs, passenger APIs, etc. APIs are particularly suitable for scripted testing. Developers can directly call these APIs in their test scripts to facilitate their own tests such as load tests, integration tests, E2E tests, etc.Developers use these APIs with their preferred programming languages to run simulations. They don’t need to worry about any underlying complexities when using the APIs. For example, developers in Grab have created custom libraries using Marionette APIs in Python, Java, and Bash to run simulations.What’s NextCurrently, we cover E2E tests for our transport domain (microservices for the passenger and driver apps) through Marionette. The next phase is to expand into a full-fledged platform that can test microservices in other Grab domains such as Food, Payments, and so on. Going forward, we are also looking to further simplify the writing of E2E tests and running them as a part of the CD pipeline for seamless testing before deployment.In ConclusionWe had an idea of creating a simulation platform that can run and facilitate E2E testing of microservices. With Marionette, we have achieved this objective. Marionette has helped us understand how end users use our apps, allowing us to make improvements to our services. Further, Marionette ensures there are no breaking changes and provides additional visibility into potential bugs that might be introduced as a result of any changes to microservices.If you have any comments or questions about Marionette, please leave a comment below.",
        "url": "/marionette-enabling-e2e-user-scenario-simulation"
      }
      ,
    
      "domain-driven-development-in-golang": {
        "title": "How We Implemented Domain-Driven Development in Golang",
        "author": "kapil-chaurasiapreeti-karkera",
        "tags": "[&quot;Back End&quot;, &quot;Go&quot;]",
        "category": "",
        "content": "Partnerships have always been core to Grab’s superapp strategy. We believe in collaborating with partners who are the best in what they do - combining their expertise with what we’re good at so that we can bring high-quality new services to our consumers, at the same time create new opportunities for the merchant and driver-partners in our ecosystem.That’s why we launched GrabPlatform last year. To make it easier for partners to either integrate Grab into their services, or integrate their services into Grab.In view of that, part of the GrabPlatform’s team mission is to make it easy for partners to integrate with Grab services. These partners are external companies that would like to offer Grab’s services such as ride-booking through their own websites or applications. To do that, we decided to build a website that will serve as a one-stop-shop that would allow them to self-service these integrations.The Challenges We Faced with the Conventional ApproachIn the process of building this website, our team noticed that the majority of the functions and responsibilities were added to files without proper segregation. A single file would contain more than 500 lines of code. Each of these files were  imported from different collections of source codes, resulting in an unstructured codebase. Any changes to the existing functions risked breaking existing functionality; we realised then that we needed to proactively plan for the future. Hence, we decided to use the principles of Domain-Driven Design (DDD) and idiomatic Go. This blog aims to demonstrate the process of how we leveraged those concepts to design a modern application.How We Implemented DDD in Our CodebaseHere’s how we went about solving our unstructured codebase using DDD principles.Step 1: Gather Domain (Business) KnowledgeWe collaborated closely with our domain experts (in our case, this was our product team) to identify functionality and flow. From them, we discovered the following key points:  After creating a project, developers are added to the project.  The domain experts wanted an ability to add other products (e.g. Pricing service, ETA service, GrabPay service) to their projects.  They wanted the ability to create multiple authentication clients to access the above products.Step 2: Break Down Domain Knowledge into Bounded ContextNow that we had gathered the required domain knowledge (i.e. what our code needed to reflect to our partners), it was time to use the DDD strategic tool Bounded Context to break down problems into subcontexts. Here is a graphical representation of how we converted the problem into smaller units.  We identified several dependencies on each of the units involved in the project. Take some of these examples:  The project domain overlapped with the product and developer domains.  Our RideBooking project can only exist if it has some products like Ridebooking APIs and not the other way around.What this means is a product can exist independent of the project, but a project will have no significance without any product. In the same way, a project is dependent on the developers, but developers can exist whether or not they belong to a project.Step 3: Identify Value Objects or Entities (Lowest Layer)Looking at the above bounded contexts, we figured out the building blocks (i.e. value objects or entity) to break down the above functionality and flow.// ProjectDAO ...type ProjectDAO struct {  ID            int64  UUID          string  Status        ProjectStatus  CreatedAt     time.Time}// DeveloperDAO ...type DeveloperDAO struct {  ID            int64  UUID          string  PhoneHash     *string  Status        Status  CreatedAt     time.Time}// ProductDAO ...type ProductDAO struct {  ID            int64  UUID          string  Name          string  Description   *string  Status        ProductStatus  CreatedAt     time.Time}// DeveloperProjectDAO to map developer's to a projecttype DeveloperProjectDAO struct {  ID            int64  DeveloperID   int64  ProjectID     int64  Status        DeveloperProjectStatus}// ProductProjectDAO to map product's to a projecttype ProductProjectDAO struct {  ID            int64  ProjectID     int64  ProductID     int64  Status        ProjectProductStatus}All the objects shown above have ID as a field and can be identifiable, hence they are identified as entities and not as value objects. But if we apply domain knowledge, DeveloperProjectDAO and ProductProjectDAO are actually not independent entities. Project object is the aggregate root since it must exist before the child fields, DevProjectDAO and ProdcutProjectDAO, can exist.Step 4: Create the RepositoriesAs stated above, we created an interface to abstract the working logic of a particular domain (i.e. Repository). Here is an example of how we designed the repositories:// ProductRepositoryImpl responsible for product functionalitytype ProductRepositoryImpl struct {  productDao storage.IProductDao // private field}type ProductRepository interface {  GetProductsByIDs(ctx context.Context, ids []int64) ([]IProduct, error)}// DeveloperRepositoryImpltype DeveloperRepositoryImpl struct {  developerDAO storage.IDeveloperDao // private field}type DeveloperRepository interface {  FindActiveAllowedByDeveloperIDs(ctx context.Context, developerIDs []interface{}) ([]*Developer, error)  GetDeveloperDetailByProfile(ctx context.Context, developerProfile *appdto.DeveloperProfile) (IDeveloper, error)}Here is a look at how we designed our repository for aggregate root project:// Unexported Structtype productProjectRepositoryImpl struct {  productProjectDAO storage.IProjectProductDao // private field}type ProductProjectRepository interface {  GetAllProjectProductByProjectID(ctx context.Context, projectID int64) ([]*ProjectProduct, error)}// Unexported Structtype developerProjectRepositoryImpl struct {  developerProjectDAO storage.IDeveloperProjectDao // private field}type DeveloperProjectRepository interface {  GetDevelopersByProjectIDs(ctx context.Context, projectIDs []interface{}) ([]*DeveloperProject, error)  UpdateMappingWithRole(ctx context.Context, developer IDeveloper, project IProject, role string) (*DeveloperProject, error)}// Unexported Structtype projectRepositoryImpl struct {  projectDao storage.IProjectDao // private field}type ProjectRepository interface {  GetProjectsByIDs(ctx context.Context, projectIDs []interface{}) ([]*Project, error)  GetActiveProjectByUUID(ctx context.Context, uuid string) (IProject, error)  GetProjectByUUID(ctx context.Context, uuid string) (*Project, error)}type ProjectAggregatorImpl struct {  projectRepositoryImpl           // private field  developerProjectRepositoryImpl  // private field  productProjectRepositoryImpl    // private field}type ProjectAggregator interface {  GetProjects(ctx context.Context) ([]*dto.Project, error)  AddDeveloper(ctx context.Context, request *appdto.AddDeveloperRequest) (*appdto.AddDeveloperResponse, error)  GetProjectWithProducts(ctx context.Context, uuid string) (IProject, error)}Step 5: Identify Domain EventsThe functions described in Step 4 only returns the ID of the developer and product, which conveys no information to the users. In order to provide developer and product information, we use the domain-event technique to return the actual product and developer attributes.A domain event is something that happened in a bounded context that you want another context of a domain to be aware of. For example, if there are new updates to the developer domain, it’s important to convey these updates to the project domain. This propagation technique is termed as domain event. Domain events enable independence between different classes.One way to implement it is seen here:// file: project\\_aggregator.gofunc (p *ProjectAggregatorImpl) GetProjects(ctx context.Context) ([]*dto.Project, error) {  ....  ....  developers := p.EventHandler.Handle(DomainEvent.FindDeveloperByDeveloperIDs{DeveloperIDs})  ....}// file: event\\_type.gotype FindDeveloperByDeveloperIDs struct{ developerID []interface{} }// file: event\\_handler.gofunc (e *EventHandler) Handle(event interface{}) interface{} {  switch op := event.(type) {      case FindDeveloperByDeveloperIDs:            developers, _ := e.developerRepository.FindDeveloperByDeveloperIDs(op.developerIDs)            return developers      case ....      ....    }}  Some Common Mistakes to Avoid When Implementing DDD in Your Codebase  Not engaging with domain experts. Not interacting with domain experts is a common mistake when using DDD. Talking to domain experts to get an understanding of the problem domain from their perspective is at the core of DDD. Starting with schemas or data modelling instead of talking to domain experts may create code based on a relational model instead of it built around a domain model.  Ignoring the language of the domain experts. Creating a ubiquitous language shared with domain experts is also a core DDD practice. This common language must be used in all discussions as well as in the code, e.g. in class and method names.  Not identifying bounded contexts. A common approach to solving a complex problem is breaking it down into smaller parts. Creating bounded contexts is breaking down a large domain into smaller ones, each handling one cohesive part of the domain.  Using an anaemic domain model. This is a common sign that a team is not doing DDD and often a symptom of a failure in the modelling process. At first, an anaemic domain model often looks like a real domain model with correct names, but the classes lack functionalities. They contain only the Get and Set methods.How the DDD Model Improved Our Software DevelopmentThanks to this brand new clean up, we achieved the following:  Core functionalities are evenly distributed to the overall codebase and not limited to just a few files.  The developers are aware of what each folder is responsible for by simply looking at the file naming and folder structure.  The risk of breaking major functionalities by merely making small changes is greatly reduced. Changing a feature is now more efficient.The team now finds the code well structured and we require less hand-holding for onboarders, thanks to the simplicity of the structure.Finally, the most important thing, we now have a system oriented towards our business necessities. Everyone ends up using the same language and terms. Developers communicate better with the business team. The work is more efficient when it comes to establishing solutions for the models that reflect how the business operates, instead of how the software operates.Lessons Learnt  Use DDD to collaborate among all project disciplines (product, business, partner, and so on) and clearly understand the business requirements.  Establish a ubiquitous language to discuss domain-related concepts.  Use bounded contexts to break down complex domains into manageable parts.  Implement a layered architecture (i.e. DDD building blocks) to focus on particular aspects of the application.  To simplify your dependency, use domain event to communicate with sub-bounded context.",
        "url": "/domain-driven-development-in-golang"
      }
      ,
    
      "driving-sea-forward-through-people-focused-design": {
        "title": "Driving Southeast Asia Forward Through People-Focused Design",
        "author": "philip-madeley",
        "tags": "[&quot;Design&quot;, &quot;User Research&quot;]",
        "category": "",
        "content": "Southeast Asia is home to around 650 million people from diverse and comparatively different economic, political and social backgrounds. Many people in the region today rely on superapps like Grab to earn a daily living or get from A to B more efficiently and safely. This means that decisions made have real impact on people’s lives – so how do you know when your decisions are right or wrong?In this post, I’ll share key consumer insights that have guided my decisions and informed my design thinking over the last year whilst working as a product designer for Grab in Singapore. I’ve broken my learnings down into 3 transient areas for thinking about product development and how each one addressed our consumers’ needs.  Relevance – does the design solve the consumer problem? For example, loss of connectivity which is common in Southeast Asia should not completely prevent a consumer from accessing the content on our app.  Inclusivity – does the design consider the full range of consumer diversity? For example, a driver waiting in the hot sun for his passenger can still use the product. Inclusive design covers people with a range of perspectives, disabilities and environments.  Engagement – does the design invoke a feeling of satisfaction? For example, building a compelling narrative around your product that solicits a higher engagement.Under each of these areas, I’ll elaborate on how we’ve built empathy from consumer insights and applied these to our design thinking.But before jumping in, think about the lens which frames any consumer experience – the mobile device. In Southeast Asia, the commonly used devices are inexpensive low-end devices made by OPPO, Xiaomi, and Samsung. Knowing which devices consumers use helps us understand potential performance constraints, different screen resolutions, and custom Android UIs.Designing for Relevance      Shopping mall in Medan, IndonesiaConnectivityIn Southeast Asia, it’s not too hard to find public WiFi. However, the main challenge is finding a reliable network. Take this shopping mall in Medan, Indonesia. The WiFi connectivity didn’t live up to the modern infrastructure of the building. The locals knew this and used mobile data over spotty and congested connectivity. Mobile data is the norm for most people and 4G reach is high, but the power of the connections is relatively low.Building EmpathyTo genuinely design for consumers’ needs, designers at Grab regularly get out the office to understand what people are doing in the real world. But how do we integrate empathy and compassion into the design process? Throughout this article, I’ll explain how the insights we gathered from around Southeast Asia can inform your decision making process.  For simulating a loss of connectivity, switch to airplane mode to observe current UI states and limitations. If you have the resources, create a 2G network to compare how bandwidth constraints page loading speeds. Network Link Conditioner for Mac and iOS or Lighthouse by Chrome DevTools can replicate a slower network.Design ImplicationsThis diagram is from Scott Hurff’s book, Designing Products People Love. The book is amazing, but if you don’t have the time to read it, this article offers a quick overview.        Scott Hurff’s UI StackAn ideal state (the fully loaded experience) is primarily what a lot of designers think about when problem-solving. However, when connectivity is a common consumer pain-point, designers at Grab have to design for the less desirable: Blank, Loading, Partial, and Error states in tandem with all the happy paths. Latency can make or break the user experience, so buffer wait times with visual progress to cushion each millisecond. Loading skeletons when you open Grab, act as momentary placeholders for content and reduce the perceived latency to load the full experience.A loss of connectivity shouldn’t mean the end of your product’s experience. Prepare connectivity issues by keeping screens alive through intuitive visual cues, messaging, and cached content.Device Type and ConditionIn Southeast Asia, people tend to opt for low-end or hand-me-down devices that can sometimes have cracked screens or depleting batteries. These devices are usually in circulation much longer than in developed markets, and the device’s OS might not be the latest version because of the perceived effort or risk to update.          A driver’s device taken during research in IndonesiaBuilding EmpathyAt Grab, we often use a range of popular, in-market devices to understand compatibility during the design process. Installing mainstream apps to a device with a small screen size, 512MB internal memory, low resolution and depleting battery life will provide insights into performance.  If these apps have lite versions or Progressive Web Apps (PWA), try to understand the trade-offs in user experience compared to the parent app.        Grab’s passenger app on the left versus the driver appDesign ImplicationsDesign for small screens first to reduce the chances of design debt later in the development lifecycle. For interactive elements, it’s important to think about all types of consumers that will use the product and in what circumstances. For Grab’s driver-partners who may have their devices mounted to the dashboard, tap targets need to be larger and more explicit.  Similarly, colour contrast will vary depending on screen resolution and time of the day. Practical tests involve dimming the screen and standing near a window in bright sunshine (our HQ is in Singapore which helps!). To further improve accessibility, use a tool like Sketch’s Stark plugin to understand if contrast ratios are accessible to visually impaired consumers. A general rule is to aim for higher contrast between essential UI components, text and interactive affordances.Fancy transitions can look great on high-end devices but can appear choppy or delayed on older and less performant phones. Aim for simple animations to offer a more seamless experience.        Passenger verification to improve safetyDay-to-day BudgetingMany people in Southeast Asia earn a daily income, so it’s no surprise that prepaid mobile is more common over a monthly contract. This mindset to ration on a day-to-day basis also extends itself to other essentials like washing powder and nappies. Data can be an expensive necessity, and consumers are selective over the types of content that will consume a daily or weekly budget. Some consumers might turn off data after getting a ride, and not turn it back on until another Grab service is required.Building EmpathyRationing data consumption daily can be achieved through not connecting to WiFi, or a more granular way is to turn off WiFi and use an app like Google’s Datally on Android to cap data usage. Starting low, around 50MB per day will increase your understanding around the data trade-offs you make and highlight the apps that require more data to perform certain actions.Design ImplicationsWhere possible, avoid using video when SVG animations can be just as effective, scalable and lightweight. For Grab’s passenger verification flow, we decided to move away from a video tutorial and keep data consumption to a minimum through utilising SVG animations. When a video experience is required, like Grab’s feed on the home screen, disabling autoplay and clearly distinguishing the media as video allowed consumers to decide on committing data.Design for Inclusivity  Mobile-onlyThe expression “mobile-first” has been bounced around for the last decade, but in Southeast Asia, “mobile-only” is probably more accurate. Most consumers have never owned a tablet or laptop, and mobile numbers are more synonymous with a method of registration over an email address. In the region, people rely more on social media and chat apps to understand broadcast or published news reports, events and recommendations. Consumers who sign up for a new Grab account, prefer phone numbers and OTP (one-time-password) registration over providing an email address and password. And anecdotally from interviews conducted at Grab, consumers didn’t feel the need for email when communication can take place via SMS, WhatsApp, or other messaging apps.Building EmpathyAt Grab, we apply design thinking from a mobile-only perspective for our passenger, merchant,  and driver-partner experiences by understanding our consumers’ journeys online and off.  These journeys are synthesised back in the office and sometimes recreated with video and physical artifacts to simulate the consumer experience. It’s always helpful to remove smartwatches, put away laptops and use an in-market device that offers a similar experience to your consumers.Design ImplicationsWhen onboarding new consumers, offer a relevant sign-in method for a mobile-only consumer, like phone number and social account registration. Grab’s passenger sign-up experience addresses these priorities with phone number first, social accounts second.          Grab’s sign-in screenPC-era icons are also widely misunderstood by mobile-only consumers, so avoid floppy disks to imply Save, or a folder to Change Directory as these offer little symbolic meaning. When icons are paired with text, this can often reinforce meaning and quicken recognition.  For example, a pencil icon alone can be confusing, so adding the word “Edit” will provide more clarity.          Nightfall in Yogyakarta, IndonesiaDiversity and SafetyThis photo was taken in Yogyakarta, Indonesia. In the evening, women often formed groups to improve personal safety. In an online environment, women often face discrimination, harassment, blackmail, cyberstalking, and more.  Minorities in emerging markets are further marginalised due to employment, literacy, and financial issues.  Building EmpathySoutheast Asia has a very diverse population, and it’s important to understand gender, ethnic, and class demographics before you plan any research. Research recruitment at Grab involves working with local vendors to recruit diverse groups of consumers for interviews and focus groups. When spending time with consumers, we try to understand how diversity and safety factors contribute to the experience of the product.If you don’t have the time and resources to arrange face-to-face interviews, I’d recommend this article for creating a survey: Respectful Collection of Demographic DataDesign for InclusivityAllow people to control how they represent their identities through pseudonym names and avatars. But does this undermine trust on the platform? No, not really. Credit card registration or more recently, Grab’s passenger and driver selfie verification feature has closed the loop on suspect accounts whilst maintaining everyone’s privacy and safety.  On the visual design side, our illustration and content guide incorporates diverse representations of ethnic backgrounds, clothing, physical ability, and social class. You can see examples in the app or through our Dribbble page. For user-generated content, allow people to report and flag abusive material. While data and algorithms can do so much, facts and ethics cannot be policed by machine learning.LanguageIn Southeast Asia and other emerging markets, consumers may set their phone to a language which they aspire to learn but may not fully comprehend. Swipe, tap, drag, pinch, and other specific terms relating to interactions might not easily translate into the local language, and English might be the preferred language regardless of comprehension. It’s surprisingly common to attend an interview with a translator but the device’s UI is set to English.          A Grab pick-up point taken in Medan, IndonesiaBuilding EmpathyIf your app supports multiple languages, try setting your phone to a different language but know how to change it back again! At Grab, we test design robustness by incorporating translated text strings into our mocks. Look for visual cues to infer meaning since some consumers might be illiterate or not fully comprehend English.        Grab’s Safety Centre in different languagesDesign for Different Languages, Formats and Visual CuesTo reduce design debt later on, it’s a good idea to start with the smallest screen size and test the most vulnerable parts of the UI with translated text strings. Keep in mind, dates, times, addresses, and phone numbers may have different formats and require special attention. You can apply multiple visual cues to represent important UI states, such as a change in colour, shape and imagery.Design for EngagementSharingFrom our research studies, word-of-mouth communication and consuming viral content via Instagram or Facebook was more popular than trawling through search page results. The social aspect is extended to the physical environment where devices can sometimes be shared with more than one person, or in some cases, one mobile is used concurrently with more than one user at a time. In numerous interviews, consumers talk about not using biometric authentication so that family members can access their devices.Building EmpathyTo understand the layers of personalisation, privacy and security on a device, it’s worth loaning a device from your research team or just borrow a friend’s phone (if they let you!).  How far do you get before you require biometric authentication or a PIN to proceed further? If you decide to wipe a personal device, what steps can you miss out from the setup, and how does that affect your experience post setup?        Offline to Online: GrabNow connecting with driverDesign for SharingIf necessary, facilitate device sharing through easy switching of accounts, and enable people to remove or hide private content after use. Allow content to be easily shared for both online and offline in-person situations. Using this approach, GrabNow allows passengers to find and connect with a nearby driver without having to pre-book and wait for a driver to arrive. This offline to online interaction also saves data and battery for the consumer.Support and TutoringIn Southeast Asia, people find troubleshooting issues from inside a help page troublesome and generally prefer human assistance, like speaking to someone through a call centre. The opportunity for face-to-face tutoring on how something works is often highly desired and is much more effective than standard onboarding flows that many apps use. From the many physical phone stores, it’s not uncommon for people to go and ask for help or get apps manually installed onto their device.Building EmpathyApart from speaking with your consumers regularly, always look through the Play and App Store reviews for common issues. Understand your consumers’ problems and the jargon they use to describe what happened. If you have a consumer support team, the tickets created will be a key indicator of where your consumers need the most support.Help and Feedback Design ImplicationsMake support accessible through a variety of methods: online forms, email, and if possible, allow consumers to call in. With in-app or online forms, try to use drop-downs or pre-populated quick responses to reduce typing, triage the type of support, and decrease misunderstanding when a request comes in.  When a consumer makes a Grab transport booking for the first time, we assist the consumer through step-by-step contextual call-outs.Local AestheticsThis photo was taken in Medan, Indonesia, on the day of an important wedding. It was impressive to see hundreds of handcrafted, colourful placards lining the streets for miles, but maybe more admirable that such an occasion was shared with the community and passers-by, and not only for the wedding guests.          A wedding celebration flower board in Medan, IndonesiaThese types of public displays are not exclusive to weddings in Southeast Asia, vibrant colours and decorative patterns are woven into the fabric of everyday life, indicative of a jovial spirit that many people in the region possess.Building EmpathyWhat are some of the immediate patterns and surfaces that exist in your workspace? Looking around your immediate environment can provide an immediate assessment of visual stimuli that can influence your decisions on a day-to-day basis.Wall space can be incredibly valuable when you can display photos from your research trip, or find online inspiration to recreate some of the visual imagery from your target markets.  When speaking with your consumers, ask to see mobile wallpapers, and think about how fashion could also play a role in determining an aesthetic choice. Lastly, take time out when on a research trip to explore the streets, museums, and absorb some of the local cultures.Design to Delight and Surprise ConsumersCapture local inspiration on research trips to incorporate into visual collections that can be a source of inspiration for colour, imagery, and textures. Find opportunities in your product to delight and engage consumers through appropriate images and visuals. Grab’s marketing consent experience leverages illustrative visuals to help consumers understand the different categories that require their consent.For all our markets, we work with local teams around culturally sensitive visuals and imagery to ensure our content is not offensive or portrays the wrong connotations.My Top 5 for Guerrilla Field ResearchIf you don’t have enough time, stakeholder buy-in or budget to do research, getting out of the office to do your own is sometimes the only answer. Here are my top 5 things to keep in mind.  Don’t jump in. Always start with observation to capture consumers’ natural behaviour.  Sanity check scripts. Your time and consumers’ time is valuable; streamline your script and prepare for u-turns and potential Facebook and Instagram friend requests at the end!    Ask the right people. It’s difficult to know who wants to or has time for your 10-minute intercept. Look for individuals sitting around and not groups if possible (group feedback can be influenced by the most vocal person).  Focus on the user. Never multitask when speaking to the user. Jotting notes on an answer sheet is less distracting than using your mobile or laptop (and less dangerous in some places!). Ask permission to record audio if you want to avoid note-taking all together but this does create more work later on.  Use insights to enrich understanding. Insights are not trends and should be used in conjunction with quantitative data to validate decision making.Feel inspired by this article and want to learn more? Grab is hiring across Southeast Asia and Seattle. Connect with me on LinkedIn or Twitter @PhilipMadeley to learn more about design at Grab.",
        "url": "/driving-sea-forward-through-people-focused-design"
      }
      ,
    
      "griffin": {
        "title": "Griffin, an Anti-fraud Risk Rule Engine Making Billions of Predictions Daily",
        "author": "muqi-ligregory-allanvarun-kansal",
        "tags": "[&quot;Engineering&quot;, &quot;Anti-Fraud&quot;, &quot;Security&quot;, &quot;Fraud Detection&quot;, &quot;Data&quot;]",
        "category": "",
        "content": "IntroductionAt Grab, the scale and fast-moving nature of our business means we need to be vigilant about potential risks to our consumers and to our business. Some of the things we watch for include promotion abuse, or passenger safety on late-night ride allocations. To overcome these issues, the TIS (Trust/Identity/Safety) task force was formed with a group of AI developers dedicated to fraud detection and prevention.The team’s mission is:  to keep fraudulent users away from our app or services  ensure our consumers’ safety, and  Manage user identities to securely login to the Grab app.The TIS team’s scope covers not just transport, but also our food, deliver and other Grab verticals.How We Prevented Fraudulent Transactions in the Earlier DaysIn our early days when Grab was smaller, we used a rules-based approach to block potentially fraudulent transactions. Rules are like boolean conditions that determines if the result will be true or false. These rules were very effective in mitigating fraud risk, and we used to create them manually in the code.We started with very simple rules. For example:Rule 1: IF a credit card has been declined today THEN this card cannot be used for bookingTo quickly incorporate rules in our app or service, we integrated them in our backend service code and deployed our service frequently to use the latest rules.It worked really well in the beginning. Our logic was relatively simple, and only one developer managed the changes regularly. It was very lightweight to trigger the rule deployment and enforce the rules.However, as the business rapidly expanded, we had to exponentially increase the rule complexity. For example, consider these two new rules:Rule 2:IF a credit card has been declined today but this passenger has good booking historyTHEN we would still allow this booking to go through, but precharge X amountRule 3:IF a credit card has been declined(but paid off) more than twice in the last 3-monthsTHEN we would still not allow this bookingThe system scans through the rules, one by one, and if it determines that any rule is tripped it will check the other rules. In the example above, if a credit card has been declined more than twice in the last 3-months, the passenger will not be allowed to book even though he has a good booking history.Though all rules follow a similar pattern, there are subtle differences in the logic and they enable different decisions. Maintaining these complex rules was getting harder and harder.Now imagine we added more rules as shown in the example below. We first check if the device used by the passenger is a high-risk one. e.g using an emulator for booking. If not, we then check the payment method to evaluate the risk (e.g. any declined booking from the credit card), and then make a decision on whether this booking should be precharged or not. If passenger is using a low-risk  device but is in some risky location where we traditionally see a lot of fraud bookings, we would then run some further checks about the passenger booking history to decide if a pre-charge is also needed.Now consider that instead of a single passenger, we have thousands of passengers. Each of these passengers can have a large number of rules for review. While not impossible to do, it can be difficult and time-consuming, and it gets exponentially more difficult the more rules you have to take into consideration. Time has to be spent carefully curating these rules.  The more rules you add to increase accuracy, the more difficult it becomes to take them all into consideration.Our rules were getting 10X more complicated than the example shown above. Consequently, developers had to spend long hours understanding the logic of our rules, and also be very careful to avoid any interference with new rules.In the beginning, we implemented rules through a three-step process:  Data Scientists and Analysts dived deep into our transaction data, and discovered patterns.  They abstracted these patterns and wrote rules in English (e.g. promotion based booking should be limited to 5 bookings and total finished bookings should be greater than 6, otherwise unallocate current ride)  Developers implemented these rules and deployed the changes to productionSometimes, the use of English between steps 2 and 3 caused inaccurate rule implementation (e.g. for “X should be limited to 5”, should the implementation be X &lt; 5 or  X &lt;= 5?)Once a new rule is deployed, we monitored the performance of the rule. For example,  How often does the rule fire (after minutes, hours, or daily)?  Is it over-firing?  Does it conflict with other rules?Based on implementation, each rule had dependency with other rules. For example, if Rule 1 is fired, we should not continue with Rule 2 and Rule 3.As a result, we couldn’t  keep each rule evaluation independent.  We had no way to observe the performance of a rule with other rules interfering. Consider an example where we change Rule 1:From IF a credit card has been declined todayTo   IF a credit card has been declined this weekAs Rules 2 and 3 depend on Rule 1, their trigger-rate would drop significantly. It means we would have unstable performance metrics for Rule 2 and Rule 3 even though the logic of Rule 2 and Rule 3 does not change. It is very hard for a rule owner to monitor the performance of Rules 2 and Rule 3.When it comes to the of A/B testing of a new rule, Data Scientists need to put a lot of effort into cleaning up noise from other rules, but most of the time, it is mission-impossible.After several misfiring events (wrong implementation of rules) and ever longer rule development time (weekly), we realised “No one can handle this manually.“Birth of Griffin Rule EngineWe decided to take a step back, sit down and closely review our daily patterns. We realised that our daily patterns fall into two categories:  Fetching new data:  e.g. “what is the credit card risk score”, or “how many food bookings has this user ordered in last 7 days”, and transform this data for easier consumption.  Updating/creating rules: e.g. if a credit card risk score is high, decline a booking.These two categories are essentially divided into two independent components:  Data orchestration - collecting/transforming the data from different data sources.  Rule-based predictionBased on these findings, we got started with our Data Orchestrator (open sourced at https://github.com/grab/symphony) and Griffin projects.The intent of Griffin is to provide data scientists and analysts with a way to add new rules to monitor, prevent, and detect fraud across Grab.Griffin allows technical novices to apply their fraud expertise to add very complex rules that can automate the review of rules without manual intervention.Griffin now predicts billions of events every day with 100K+ Queries per second(QPS) at peak time (on only 6 regular EC2s).Data scientists and analysts can self-service rule changes on the web portal directly, deploy rules with just a few clicks, experiment and monitor performance in real time.Why We Came up with Griffin Instead of Using Third-party Tools in the MarketBefore we decided to create our in-built tool, we did some research for common business rule engines available in the market such as Drools and checked if we should use them. In that process, we found:  Drools has its own Java-based DSL with a non-trivial learning curve (whereas our major users are from Python background).  Limited [expressive power](https://en.wikipedia.org/wiki/Expressive_power_(computer_science),  Limited support for some common math functions (e.g. factorial/ Greatest Common Divisor).  Our nature of business needed dynamic dataset for predictions (for example, a rule may need only passenger booking history on Day 1, but it may use passenger booking history, passenger credit balance, and passenger favourite places on Day 2). On the other hand, Drools usually works well with a static list of dataset instead of dynamic dataset.Given the above constraints, we decided to build our own rule engine which can better fit our needs.Griffin ArchitectureThe diagram depicts the high-level flow of making a prediction through Griffin.  Components  Data Orchestration: a service that collects all data needed for predictions  Rule Engine: a service that makes prediction based on rules  Rule Editor: the portal through which users can create/update rulesWorkflow  Users create/update rules in the Rule Editor web portal, and save the rules in the database.  Griffin Rule Engine reloads rules immediately as long as it detects any rule changes.  Data Orchestrator sends all dataset (features) needed for a prediction (e.g. whether to block a ride based on passenger past ride pattern, credit card risk) to the Rule Engine  Griffin Rule Engine makes a prediction.How You can Create Rules Using GriffinIn an abstract view, a rule inside Griffin is defined as:Rule:Input:JSON =&gt; Result:BooleanWe allow users (analysts, data scientists) to write Python-based rules on WebUI to accommodate some very complicated rules like:len(list(filter(lambdax: x \\&gt;7, (map(lambdax: math.factorial(x), \\[1,2,3,4,5,6\\]))))) \\&gt;2This significantly optimises the expressive power of rules.To match and evaluate a rule more efficiently, we also have other key components associated:Scenarios  Here are some examples: PreBooking, PostBookingCompletion, PostFoodDeliveryActions  Actions such as NotAllowBooking, AuthCapture, SendNotification  If a rule result is True, it returns a list of treatments as selected by users, e.g. AuthCapture and SendNotification (the example below is treatments for one Safety-related rule).The one below is for a checkpoint to detect credit-card risk.    Each checkpoint has a default treatment. If no rule inside this checkpoint is hit, the rule engine would return the default one (in most cases, it is just “do nothing”).  A treatment can only belong to one checkpoint, but one checkpoint can have multiple treatments.For example, the graph below demonstrates a checkpoint PaxPreRide associated with three treatments: Pass, Decline, Hold  Segments  The scope/dimension of a rule. Based on the sample segments below, a rule can be applied only to countries=\\[MY,PH\\] and verticals=\\[GrabBus, GrabCar\\]  It can be changed at any time on WebUI as well.  Values of a ruleWhen a rule is hit, more than just treatments, users also want some dynamic values returned. E.g. a max distance of the ride allowed if we believe this booking is medium risk.Does Python Make Griffin Run Slow?We picked Python to enjoy its great expressive power and neatness of syntax, but some people ask: Python is slow, would this cause a latency bottleneck?Our answer is No.The following graph shows the Latency P99 of Prediction Request from load balancer side (actually the real latency for each prediction is &lt; 6ms, the metrics are peaked at 30ms because some batch requests contain 50 predictions in a single call).  What We Did to Achieve This  The key idea is to make all computations in CPU and memory only (in other words, no extra I/O).  We do not fetch the rules from database for each prediction. Instead, we keep a record called dirty_key, which keeps the latest rule update timestamp. The rule engine would actively check this timestamp and trigger a rule reload only when the dirty_key timestamp in the DB is newer than the latest rule reload time.  Rule engine would not fetch any additional new data, instead, all data should be from Data Orchestrator.  So the whole prediction flow is only between CPU &amp; memory (and if the data size is small, it could be on CPU cache only).  Python GIL essentially enforces a process to have up to one active thread running at a time, no matter how many cores a CPU has. We have Gunicorn to wrap our service, so on the Production machine, we have (2x$num_cores) + 1 processes (see Gunicorn Design - How Many Workers?). The formula is based on the assumption that for a given core, one worker will be reading or writing from the socket while the other worker is processing a request.The following screenshot is the process snapshot on C5.large machine with 2 vCPU. Note only green processes are active.  A lot of trial and error performance tuning:  We used to have python-jsonpath-rw for JSONPath query, but the performance was not strong enough. We switched to jmespath and observed about 10ms latency reduction.  We use sqlalchemy for DB Query and ORM. We enabled cache for some use cases, but turned out it was over-optimised with stale data. We ended up turning off some caching points to ensure the data consistency.  For new dict/list creation, we prefer native call (e.g. {}/[]) instead of function call (see the comparison below).    Use built-in functions https://docs.python.org/3/library/functions.html. It is written in C, no one can beat it.  Add randomness to rule reload so that not all machines run at the same time causing latency spikes.  Caching atomic feature units as they are used so that we don’t have to requery for them each time a checkpoint uses it.How Griffin Makes On-call Engineers RelaxOne of the most popular aspects of Griffin is the WebUI. It opens a door for non-developers to make production changes in real time which significantly boosts organisation productivity. In the past a rule change needed 1 week for code change/test/deployment, now it is just 1 minute.But this also introduces extra risks. Anyone can turn the whole checkpoint down, whether unintentionally or maliciously.Hence we implemented Shadow Mode and Percentage-based rollout for each rule. Users can put a rule into Shadow Mode to verify the performance without any production impact, and if needed, rollout of a rule can be from 1% all the way to 100%.We implemented version control for every rule change, and in case anything unexpected happened, we could rollback to the previous version quickly.    We also built RBAC-based permission system, along with Change Approval flow to make sure any prod change needs at least two people(and approver role has higher permission)Closing ThoughtsGriffin evolved from a fraud-based rule engine to generic rule engine. It can apply to any rule at Grab. For example, Grab just launched Appeal automation several days ago to reduce 50% of the  human effort it typically takes to review straightforward appeals from our passengers and drivers. It was an unplanned use case, but we are so excited about this.This could happen because from the very beginning we designed Griffin with minimised business context, so that it can be generic enough.After the launch of this, we observed an amazing adoption rate for various fraud/safety/identity use cases. More interestingly, people now treat Griffin as an automation point for various integration points.Speak to usGrabDefence is a proprietary fraud prevention platform built by Grab, Southeast Asia’s leading superapp. Since 2019, the GrabDefence team has shared its fraud management capabilities and platform with enterprises and startups to leverage Grab’s advanced AI/ML models, hyper local insights and patented device intelligence technologies.To learn more about GrabDefence or to speak to our fraud management experts, contact us at gd.contact@grabtaxi.com.",
        "url": "/griffin"
      }
      ,
    
      "using-grabs-trust-counter-service-to-detect-fraud-successfully": {
        "title": "Using Grab’s Trust Counter Service to Detect Fraud Successfully",
        "author": "chao-wangmuqi-ligregory-allanvarun-kansal",
        "tags": "[&quot;Engineering&quot;, &quot;Anti-Fraud&quot;, &quot;Security&quot;, &quot;Fraud Detection&quot;, &quot;Data&quot;]",
        "category": "",
        "content": "BackgroundFraud is not a new phenomenon, but with the rise of the digital economy it has taken different and aggressive forms. Over the last decade, novel ways to exploit technology have appeared, and as a result, millions of people have been impacted and millions of dollars in revenue have been lost. According to ACFE survey, companies lost USD6.3 billion due to fraud. Organisations lose 5% of its revenue annually due to fraud.In this blog, we take a closer look at how we developed an anti-fraud solution using the Counter service, which can be an indispensable tool in the highly complex world of fraud detection.Anti-fraud Solution Using CountersAt Grab, we detect fraud by deploying data science, analytics, and engineering tools to search for anomalous and suspicious transactions, or to identify high-risk individuals who are likely to commit fraud. Grab’s Trust Platform team provides a common anti-fraud solution across a variety of business verticals, such as transportation, payment, food, and safety. The team builds tools for managing data feeds, creates SDK for engineering integration, and builds rules engines and consoles for fraud detection.One example of fraudulent behaviour could be that of an individual who masquerades as both driver and passenger, and makes cashless payments to get promotions, for example, earn a one dollar rebate in the next transaction.In our system, we analyse real time booking and payment signals, compare it with the historical data of the driver and passenger pair, and create rules using the rule engine. We count the number of driver and passenger pairs at a given time frame. This counter is provided as an input to the rule.If the counter value exceeds a predefined threshold value, the rule evaluates it as a fraud transaction. We send this verdict back to the booking service.The Conventional MethodFraud detection is a job that requires cross-functional teams like data scientists, data analysts, data engineers, and backend engineers to work together. Usually data scientists or data analysts come up with an offline idea and apply it to real-time traffic. For example, a rule gets invented after brainstorming sessions by data scientists and data analysts. In the conventional method, the rule needs to be communicated to engineers.Automated Solution Using the Counter ServiceTo overcome the challenges in the conventional method, the Trust platform team decided to come out with the Counter service, a self-service platform, which provides management tools for users, and a computing engine for integrating with the backend services. This service provides an interface, such as a UI based rule editor and data feed, so that analysts can experiment and create rules without interacting with engineers. The platform team also decided to provide different data contracts, APIs, and SDKs to engineers so that the business verticals can use it quickly and easily.The Major Engineering Challenges Faced in Designing the Counter ServiceThere are millions of transactions happening at Grab every day, which implies we needed to perform billions of fraud and safety detections. As seen from the example shared earlier, most predictions require a group of counters. In the above use case, we need to know how many counts of the cashless payment happened for a driver and passenger pair. Due to the scale of Grab’s business, the potential combinations of drivers and passengers could be exponential. However, this is only one use case. So imagine that there could be hundreds of counters for different use cases. Hence, it’s important that we provide a platform for stakeholders to manage counters.Read on to learn about some of the common challenges we faced.ScalabilityAs mentioned above, we could potentially have an exponential number of passengers and drivers in a single counter. So it’s a great challenge to store the counters in the database, read, and query them in real-time. When there are billions of counter keys across a long period of time, the Trust team had to find a scalable way to write and fetch keys effectively and meet the client’s SLA.Self-servingA counter is usually invented by data scientists or analysts and used by engineers. For example, every time a new type of counter is needed from data scientists, developers need to manually make code changes, such as adding a new stream, capturing related data sets for the counter, and storing it on the fraud service, then doing a deployment to make the counters ready. It usually takes two or more weeks for the whole iteration, and if there are any changes from the data analysts’ side, which happens often, the situation loops again. The team had to come up with a solution to prevent the long loop of manual tasks by coming out with a self-serving interface.Manageable and ExtendableDue to a lack of connection between real-time and offline data, data analysts and data scientists did not have a clear picture of what is written in the counters. That’s because the conventional counter data were stored in Redis database to satisfy the query SLA. They could not track the correctness of counter value, or its history. With the new solution, the stakeholders can get a real-time picture of what is stored in the counters using the data engineering tools.The Machine Learning Challenges Solved by the Counter ServiceThe Counter service plays an important role in our Machine Learning (ML) workflow.Data Consistency Challenge/IssueMost of the machine learning workflows need dedicated input data. However, when there is an anti-fraud model that is trained using offline data from the data lake, it is difficult to use the same model in real-time. This is because the model lacks the data contract and the consistency with the data source. In this case, the Counter service becomes a type of data source by providing the value of counters to file system.ML FeaturingCounters are important features for the ML models. Imagine there is a new invention of counters, which data scientists need to evaluate. We need to provide a historical data set for counters to work. The Counter service provides a counter replay feature, which allows data scientists to simulate the counters via historical payload.In general, the Counter service is a bridge between online and offline datasets, data scientists, and engineers. There was technical debt with regards to data consistency and automation on the ML pipeline, and the Counter service closed this loop.How We Designed the Counter ServiceWe followed the principle of asynchronised data ingestion, and synchronised transaction for designing the Counter service.The diagram shows how the counters are generated and saved to database.  Counter creation workflow  User opens the Counter Creation UI and creates a new key “fraud:counter:counter_name”.  Configures required fields.  The Counter service monitors the new counter-creation, puts a new counter into load script storage, and starts processing new counter events (see Counter Write below).Counter write workflow  The Counter service monitors multiple streams, assembles extra data from online data services (i.e. Common Data Service (CDS), passenger service, hydra service, etc), so that rich dataset would also be available for editors on each stream resource.  The Counter Processor evaluates the user-configured expression and writes the evaluated values to the dedicated Grab-Stats stream using the GrabPlugin tool.Counter read workflow  We use Grab-Stats as our storage service. Basically Grab-Stats runs above ScyllaDB, which is a distributed NoSQL data store. We use ScyllaDB because of its good performance on aggregation in memory to deal with the time series dataset. In comparison with in-memory storage like AWS elasticCache, it is 10 times cheaper and as reliable as AWS in terms of stability. The p99 of reading from ScyllaDB is less than 150ms which satisfies our SLA.How We Improved the Counter Service PerformanceWe used the multi-buckets strategy to improve the Counter service performance.BackgroundThere are different time windows when you perform a query. Some counters are time sensitive so that it needs to know what happened in the last 30 or 60 minutes. Some other counters focus on the long term and need to know the events in the last 30 or 90 days.From a transactional database perspective, it’s not possible to serve small range as well as long term events at the same time. This is because the more the need for the accuracy of the data and the longer the time range, the more aggregations need to happen on database. Which means we would not be able to satisfy the SLA. Otherwise we will need to block other process which leads to the service downgrade.Solution for Improving the QueryWe resolved this problem by using different granularities of the tables. We pre-aggregated the signals into different time buckets, such as 15min, 1 hour, and 1 day.When a request comes in, the time-range of the request will be divided by the buckets, and the results are conquered. For example, if there is a request for 9/10 23:15:20 to 9/12 17:20:18, the handler will query 15min buckets within the hour.  It will query for hourly buckets for the same day. And it will query the daily buckets for the rest of 2 days. This way, we avoid doing heavy aggregations, but still keep the accuracy in 15 minutes level in a scalable response time.Counter Service UIWe allowed data analysts and data scientists to onboard counters by themselves, from a dedicated web portal. After the counter is submitted, the Counter service takes care of the integration and parsing the logic at runtime.  Backend IntegrationWe provide SDK for quicker and better integration. The engineers only need to provide the counter identifier ID (which is shown in the UI) and the time duration in the query. Under the hood we provide a GRPC protocol to communicate across services. We divide the query time window to smaller granularities, fetching from different time series tables and then conquering the result. We are also providing a short TTL cache layer to take the uncommon traffic from client such as network retry or traffic throttle. Our QPS are designed to target 100K.Monitoring the Counter ServiceThe Counter service dashboard helps to track the human errors while editing the counters in real-time. The Counter service sends alerts to slack channel to notify users if there is any error.  We setup Datadog for monitoring multiple system metrics. The figure below shows a portion of stream processing and counter writing. In the example below, the total stream QPS would reach 5k at peak hour, and the total counter saved to storage tier is about 4k. It will keep climbing without an upper limit, when more counters are onboarded.  The Counter service UI portal also helps users to fetch real-time counter results for verification purposes.  Future PlansHere’s what we plan to do in the near future to improve the Counter service.Close the ML Workflow LoopAs mentioned above, we plan to send the resource payload of the Counter service to the offline data lake, in order to complete the counter replay function for data scientists. We are working on the project called “time traveler”. As the name indicates, it is used not only for serving the online transactional data, but also supports historical data analytics, and provides more flexibility on counter inventions and experiments.There are more automation steps we plan to do, such as adding a replay button on the web portal, and hooking up with the offline big data engine to trigger the analytics jobs. The performance metrics will be collected and displayed on the web portal. A single platform would be able to manage both the online and offline data.Integration with GriffinGriffin is our rule engine. Counters are sometimes an input to a particular rule, and one rule usually needs many counters to work together. We need to provide a better integration with Griffin on backend. We plan to minimise the current engineering effort when using counters on Griffin. A counter then becomes an automated input variable on Griffin, which can be configured on the web portal by any users.Speak to usGrabDefence is a proprietary fraud prevention platform built by Grab, Southeast Asia’s leading superapp. Since 2019, the GrabDefence team has shared its fraud management capabilities and platform with enterprises and startups to leverage Grab’s advanced AI/ML models, hyper local insights and patented device intelligence technologies.To learn more about GrabDefence or to speak to our fraud management experts, contact us at gd.contact@grabtaxi.com.",
        "url": "/using-grabs-trust-counter-service-to-detect-fraud-successfully"
      }
      ,
    
      "about-being-a-principal-engineer-at-grab": {
        "title": "Being a Principal Engineer at Grab",
        "author": "roman-atachiants",
        "tags": "[&quot;Career&quot;, &quot;Engineering&quot;, &quot;Microservice&quot;]",
        "category": "",
        "content": "Over the past few years Grab has grown from a small startup to one of the largest technology companies in South-East Asia. Along with the company’s growth, the number of microservices, features and teams also grew substantially. At the time of writing this blog, we have around 350 microservices powering our superapp.A great engineering team is a critical component of our success. As an engineer you have two career paths in front of you: an individual contributor role, or a management role. While a management role is generally better understood, this article clarifies what it means to be a principal engineer at Grab, which is one of the highest levels of our engineering career ladder.  Improving the Quality“You set the standard for engineering excellence in your technical family. Your architectures are exemplary in terms of efficiency, stability, extensibility, testability and the ability to evolve over time. Your software is robust in the presence of failures, scalable, and cost-effective. Your coding practices are exemplary in terms of code organization, clarity, simplicity, error handling, and documentation. You tackle intrinsically hard problems, acquiring expertise as needed. You decompose complex problems into straightforward solutions.” - Grab’s Engineering Career Ladder&nbsp;So, what does a principal engineer do? As your career progresses from junior to senior to lead engineer we have more and more responsibilities; you manage larger and larger systems. For example, junior engineer might manage a specific component of a micro-service. A senior engineer would be tasked with designing and operating an entire micro-service or product. While a lead engineer would typically be concerned with the architecture at a team level.Principal engineer level is akin to a senior manager where instead of indirectly managing people (manager of managers) you take care of the architecture of an entire sub-organisation, known as Tech Family/Platform. These Tech Families usually have more than 50 engineers spread across multiple teams and function as a tiny company with their own business owners, designers, product managers, etc.Challenging Projects“You take engineering ownership of projects that may require the work of several teams to implement; you divide responsibilities so that each team can work independently and have the system come together into an integrated whole. Your projects often cross team, tech family, platform, and even R&amp;D center boundaries. You solicit differing views and keep an open mind. You are adept at building consensus.” - Grab’s Engineering Career Ladder&nbsp;As a principal engineer, your job is to solve larger problems and translate somewhat vague problems into a set of actionable items. You might be faced with a large problem such as “improve efficiency and interoperability of Grab’s transportation system.” You will need to understand the problem, the business impact and see how can it be improved. It might require you to design new systems, change existing systems, understand the costs involved and get the right people together to make it happen.Solving such a problem all by yourself is pretty much impossible. You have to work with other managers and other engineers together as a team to make it happen. Help your lead/senior engineers to design the right system by giving them a clear objective but let them take care of the system-level architecture.  You will also need to work with managers, advise them to get things done, and get the right things prioritised by the team. While you don’t need to be well-versed in project management and agile methodologies, you do need to be able to plan ahead with your teams and have an understanding of how much time a project or migration will take.A Tech Family can easily have 20 or more micro-services. You need to have a good understanding of their functional requirements and interactions. This is challenging as learning new things is always “uncomfortable” and takes time. You must reach out to engineers, product managers, and data scientists, ideally face-to-face to build empathy. Keep asking questions and try to understand how things work. You will also need to read the existing documentation and their code.Technical Ownership“You are the origin of significant technical contributions to our architecture and infrastructure. You take technical ownership of the design and quality of the security, performance, availability, and operational aspects of the software built by one or more teams. You identify where your time is needed, transitioning between coding, design, and architecture based on project and team needs. You deliver software in ways that empower teams to self-service, providing clear adoption/migration paths.” - Grab’s Engineering Career Ladder&nbsp;As a principal engineer you work together with the Head of Engineering and managers within the Tech Family and improve the quality of systems across the board. Typically, no-one tells you what needs to be done. You need to identify gaps, raise them and keep improving the systems.You also need to learn how to manage your own time better so you can prioritise effectively. This boils down to knowing your strengths, your weaknesses. For example, if you are really good in building distributed systems but have no clue about the latest-and-greatest design in information security, get the right InfoSec engineers in this meeting and consider skipping it yourself. Avoid trying to do everything at once and be in every single meeting you get invited - you still have to review code, design and focus, so plan accordingly.You will also need to understand the business impact of your decisions. For example, if you contribute to product features, know how impactful this feature is going to be to the organisation. If you don’t know it - ask the Product Manager responsible for it. If you work on a platform feature, for example improving the build system, know how it will help: saving 30 minutes of build time for every engineer each day is a huge achievement.More often than not, you will have to drive migrations, this is akin to code refactoring but on a system-level and will involve a lot of collaboration with the people. Understand what a technical debt is and how it can be mitigated - a good architecture minimises technical debt and in turn accelerates time-to-market and helps business flourish.Technical Leadership“You amplify your impact by leading design reviews for complex software and/or critical features. You probe assumptions, illuminate pitfalls, and foster shared understanding. You align teams toward coherent architectural strategies.” - Grab’s Engineering Career Ladder&nbsp;In Grab we have a process known as RFC (Request For Comments) which allows engineers to submit designs and ideas for a larger audience to debate. This is especially important given that our organisation is spread across several continents with research and development offices in Southeast Asia, the US, India and China. While any engineer is welcome to comment on these RFCs, it is a duty of lead and principal engineers’ to review them on a regular basis. This will help you to expand your knowledge of existing systems and help others with improving their designs.Communication is a key skill that you need to keep improving and it is often the Achilles’ heel of many engineers who would rather be doing work in their corner without talking to anyone else. This is perfectly fine for a junior (or even some senior engineers) but it is critical for a principal engineer to communicate. Let’s break this down to a set of specific skills that you’d need to sharpen.You need to be able to write effectively in order to convey your ideas to others. This includes knowing your audience and wording it in such a way that readers can understand. A technical design document whose audience are engineers is not written the same way as a design proposal whose audience are product and business managers.You need to be able to publicly present and talk about various projects that you are working on. This includes creation of slide decks with good visuals and distilling down months of work to just a couple of slides. The best way of learning this is to get out there and keep presenting your work - you will get better over time.You also need to be able to drive meetings and discussions without wasting anyone’s time. As a technical leader, one of your key responsibilities is to get people moving in the same direction and driving consensus during meetings.Teaching and Learning“You educate other engineers, both at an individual level and at scale: keeping the engineering community up to date on advanced technical issues, technologies, and trends. Examples include onboarding bootcamps for new hires, interns, specific skill-gap training development, and sharing specialised knowledge to raise the technical bar for other engineers/teams/dev centers.”&nbsp;A principal engineer is a technical leader and as a leader you have the responsibility to mentor, coach fellow engineers, regardless of their level. In addition to code-reviews, you can organise office hours in your team and knowledge sharing sessions where everyone could present something. You could also help with bootcamps and help new hires in getting up-to-speed.Most importantly, you will also need to keep learning whichever way works for you - reading journals and papers, blog posts, watching video-recorded talks, attending conferences and browsing through a variety of open-source projects. You will also learn from other Grabbers as even a junior engineer can teach you something, we all have our strengths and weaknesses. Keep improving and working on yourself!",
        "url": "/about-being-a-principal-engineer-at-grab"
      }
      ,
    
      "data-first-sla-always": {
        "title": "Data First, SLA Always",
        "author": "johan-kokpramiti-goelfeng-chengirfan-hanifdeepak-barr",
        "tags": "[&quot;Data Pipeline&quot;]",
        "category": "",
        "content": "Introducing Trailblazer, the Data Engineering team’s solution to implementing change data capture of all upstream databases. In this article, we introduce the reason why we needed to move away from periodic batch ingestion towards a real time solution and show how we achieved this through an end to end streaming pipeline.ContextOur mission as Grab’s Data Engineering team is to fulfil 100% of SLAs for data availability to our downstream users. Our 40 person team is responsible for providing accurate and reliable data to data analysts and data scientists so that they can produce actionable reports that will help Grab’s leadership team make data-driven decisions. We maintain data for a variety of business intelligence tools such as Tableau, Presto and Holistics as well as predictive algorithms for all of Grab.We ingest data from multiple upstream sources, such as relational databases, Kafka or third party applications such as Salesforce or Zendesk. The majority of these source data exists in MySQL and we run ETL pipelines to mirror any updates into our data lake. These pipelines are triggered on an hourly or daily basis and are powered by an in-house Loader application which performs Spark batch ingestion and loading of data from source to sink.Problems with the Loader application started to surface when Grab’s data exceeded the petabyte threshold. As such for larger tables, the most practical method to ingest data was to perform ETL only on rows that were updated within a specified timeframe. This is akin to issuing the querySELECT * FROM table WHERE updated &gt;= [start_time] AND updated &lt; [end_time]Now imagine two situations. One, firing this query to a huge table without an updated field. Two, firing the same query to the huge table, this time without indexes on the updated field. In the first scenario, the query will never work and we can never perform incremental ingestion on the table based on a timed window. The second scenario carries the dangers of creating high CPU load to replicate the database that we are querying from. Neither has an ideal outcome.One other problem that we identified was the unpredictability of growth in data volume. Tables smaller than one gigabyte were ingested by fully scanning the table and overwriting the data in the data lake. This worked out well for us until the table size increased exponentially, at which point our Spark jobs failed due to JDBC timeouts. If we were only dealing with a handful of tables, this issue could have been addressed by switching our data ingestion strategy from full scan to a timed window.When assessing the issue, we discovered that there were hundreds of tables running under the full scan strategy, all of them potentially crashing our data system, all time bombs silently waiting to explode.The team urgently needed a new approach to ETL. Our Loader application was highly coupled to upstream table characteristics. We needed to find solutions that were truly scalable, which meant decoupling our pipelines from the upstream.Change Data Capture (CDC)Much like event sourcing, any log change to the database is captured and streamed out for downstream applications to consume. This process is lightweight since any row level update to the table is instantly captured by a real time processor, avoiding the need for large chunked queries on the table. In addition, CDC works regardless of upstream table definition, so we do not need to worry about missing updated columns impacting our data migration process.Binary Logs (binlogs) are the CDC agents of MySQL. All updates, insertions or deletions performed on the table are captured as a series of logged events containing the past state of the row and it’s newly modified state. Check out the binlogs reference to find out more.In order to persist all binlogs generated upstream, our team created a Spark Structured Streaming application called Trailblazer. Trailblazer streams all MySQL binlogs to our data lake. These binlogs serve as a foundation for us to build Presto tables for data auditing and help to remove the direct dependency of our batch ETL jobs to the source MySQL.Trailblazer is an amalgamation of various data streaming stacks. Binlogs are captured by Debezium which runs on Kafka connect clusters. All binlogs are sent to our Kafka cluster, which is managed by the Data Engineering Infrastructure team and are streamed out to a real time bucket via a Spark structured streaming application. Hourly or daily ETL compaction jobs ingests the change logs from the real time bucket to materialise tables for downstream users to consume.    CDC in action where binlogs are streamed to Kafka via Debezium before being consumed by Trailblazer streaming &amp; compaction services&nbsp;Some StatisticsTo date, we are streaming hundreds of tables across 60 Spark streaming jobs and with the constant increase in Grab’s database instances, the numbers are expected to keep growing.Designing Trailblazer StreamsWe built our streaming application using Spark structured streaming 2.3. Structured streaming was designed to remove the technical aspects of provisioning streams. Developers can focus on perfecting business logic without worrying about fundamentals such as checkpoint management or reading and writing to data sources.    Key architecture for Trailblazer streaming&nbsp;In the design phase, we made sure to follow several key principles that helped in managing our streams.Checkpoints Have to be Externally ManagedStructured streaming manages checkpoints both in a local directory and in a ‘_metadata’ directory on S3 buckets, such that the state of the stream can be restored in the event of failure and restart.This is all well and good, with two exceptions. First, changing the starting point of data ingestion meant ssh-ing into the machine and manipulating metadata, which could be extremely dangerous. Second, we could not assume cluster prevalence since clusters can die and be recreated with data erased from its local disk or the distributed file system.Our solution was to do a work around at the application level. All checkpoints will be stored in temporary directories with the existing timestamp appended as path (eg /tmp/checkpoint/job_A/1560697200/… ). A linearly progressive timestamp guarantees that the same directory will never be reused by new instances of the stream. This explains why we never restore its state from local disk but instead, store all checkpoints in a highly available Redis cluster, with key as the Kafka topic and value as a JSON of partition : offset.Keydebz-schema-A.schema_A.table_BValue{\"11\":19183566,\"12\":19295602,\"13\":18992606[[a]](#cmnt1)[[b]](#cmnt2)[[c]](#cmnt3)[[d]](#cmnt4)[[e]](#cmnt5)[[f]](#cmnt6),\"14\":19269499,\"15\":19197199,\"16\":19060873,\"17\":19237853,\"18\":19107959,\"19\":19188181,\"0\":19193976,\"1\":19072585,\"2\":19205764,\"3\":19122454,\"4\":19231068,\"5\":19301523,\"6\":19287447,\"7\":19418871,\"8\":19152003,\"9\":19112431,\"10\":19151479}  Example of how offsets are stored in Redis as Key : Value pairs&nbsp;Fortunately, structured streaming provides the StreamQueryListener class which we can use to register checkpoints after the completion of each microbatch.Streams Must Handle 0, 1 or 1 Million DataScalability is at the heart of all well-designed applications. Spark streaming jobs are built for scalability in the face of varying data volumes.    In general, the rate of messages input to Kafka is cyclical across 24 hrs. Streaming jobs should be robust enough to handle data loads during peak hours of the day without breaching microbatch timing&nbsp;There are a few settings that we can configure to influence the degree of scalability for a streaming app:  spark.dynamicAllocation.enabled=true gives spark autonomy to provision / revoke executors to suit the workload  spark.dynamicAllocation.maxExecutors controls the maximum job parallelism  maxOffsetsPerTrigger controls the maximum number of messages ingested from Kafka per microbatch  trigger controls the duration between microbatchs and is a property of the DataStreamWriter classData as Key Health IndicatorScaling the number of streaming jobs without prior collection of performance metrics is a bad idea. There is a high chance that you will discover a dead stream when checking your stream hours after initialisation. I’ll cite Murphy’s law as proof.Thus we vigilantly monitored our data streams. We used tools such as Datadog for metric monitoring, Slack for oncall issue reporting, PagerDuty for urgent cases and our inhouse data auditor as a service (DASH) for counts discrepancy reporting between streamed and source data. More details on monitoring will be discussed in the later part.Streams are EphemeralStreams may die due to a hundred and one reasons so don’t blame yourself or your programming insecurities. Issues with upstream dependencies, such as a node within your Kafka cluster running out of disk space, could lead to partition unavailability which would crash the application. On one occasion, our streaming application was unable to resolve DNS when writing to AWS S3 storage. This amounted to multiple failures within our Spark job that eventually culminated in the termination of the stream.In this case, allow the stream to  shutdown gracefully, send out your alerts and have a mechanism in place to retry the failed stream. We run all streaming jobs on Airflow and any failure to the stream will automatically be retried through a new task issued by the scheduler.If you have had experience with large scale management of streams, please leave a comment so we can continue this discussion!Monitoring Data StreamsHere are some key features that were set up to monitor our streams.Running : Active Jobs RatioThe number of streaming jobs could increase in the future, thus becoming a challenge for the oncall team to track all jobs that are supposed to be up and running.One proposal  is  to track the number of jobs in production against the number of jobs that are actually running. By querying MySQL tables, we can filter out all the jobs that are meant to be active. Since Trailblazer streams are spark-submit jobs managed by YARN, we can query YARN’s resource manager REST API to retrieve  all the jobs that are running. We then construct a ratio of running : active jobs and report them to Datadog. If the ratio is not 1 for an extended duration, an alert will be issued for the oncall to take action.    If the ratio of running : active jobs falls below 1 for a period of time, we will immediately trigger an alert&nbsp;Microbatch RuntimeWe define a 30 second window for each microbatch and track the actual runtime using metrics reported by the query listener. A runtime that exceeds the designated window is a potential indicator that the streaming job is deprived of resources and needs to be scaled up.Job LivelinessEach job reports its health by emitting a count of 1 heartbeat. This heartbeat is created at the end of every microbatch via a query listener. This process is useful in detecting stale jobs (jobs that are registered as RUNNING in YARN but are actually hung).Kafka Offset DivergenceIn order to ensure that the message output rate to the consumer exceeds the message input rate from the producer, we sum up all presently ingested topic-partition offsets and compare that value to the sum of all topic-partition end offsets in Kafka. We then add an alerting logic on top of these metrics to inform the oncall team if the difference between the two values grows too big.It is important to track the offset divergence parameter as streams can be lagging. Should the rate of consumption fall below the rate of message production, we would run the risk of falling short of Kafka’s retention window, leading to data losses.Hourly Data ChecksDASH runs hourly and serves as our first line of defence to detect any data quality issues within the streams. We issue queries to the source database and our streaming layer to confirm that the ID counts of data created within the last hour match.DASH helps in the early detection of upstream issues. We have noticed cases where our Debezium connectors failed and our checker reported fewer data than expected since there were no incoming messages to Kafka.      DASH matches and mismatches reported to Slack&nbsp;Materialising Tables Through CompactionHaving CDC data in our data lake does not conclude our responsibilities. Batched compaction allows us to apply all captured CDC, to be available as Presto tables for downstream consumption. The job is set to trigger hourly and process all changes to the database within the past hour.  For example, changes to a record are visible in real-time, but the latest state of the record will not be reflected until the next time a batch job runs. We addressed several issues with streaming during this phase.Deduplication of DataTrailblazer was not built to deliver exactly once guarantees. We ensure that the issues regarding duplicated CDCs are addressed during compaction.Availability of All Data Until a Certain HourWe want to make sure that downstream pipelines use output data of the hourly batch job only when the pipeline has all records for that hour. In case there is an event that is processed late by streaming, the current pipeline will wait until the data is completed. In this case, we are consciously choosing consistency over availability for our downstream users. For example, missing a few insert booking records in peak hours due to consumer processing delay can generate the wrong downstream results leading to miscalculation in revenue. We want to start  downstream processes only when the data for the hour or day is complete.Need for the Latest State of Each EventOur compaction job performs upserts on the data to ensure that our downstream users can consume records in their latest state.  Future ApplicationsTrailblazer is a milestone for the Data Engineering team as it represents our commitment to achieve large scale data streams to reduce latencies for our end users. Moving ahead, our team will be exploring how we can further optimise streaming jobs by analysing data trends over time and to build applications such as snapshot tables on top of the CDCs being streamed in our data lake.",
        "url": "/data-first-sla-always"
      }
      ,
    
      "save-your-place-with-grab": {
        "title": "Save Your Place with Grab!",
        "author": "summit-sauravneeraj-mishrashirley-yang",
        "tags": "[&quot;Maps&quot;, &quot;Data&quot;]",
        "category": "",
        "content": "Do you find it tedious to type and search for your destination or have a hard time remembering that address of the friend you are going to meet? It can be really confusing when it comes to keeping track of so many addresses that you frequent on a regular basis. To solve this pain point, Grab rolled out a new feature called Saved Places in January’19 across SouthEast Asia.With Saved Places, you can save an address and also add a label like “Home”, “Work”, “Gym”, etc which makes finding and selecting an address for booking a ride or ordering your favourite food a breeze!Never Forget Your Addresses Again!To use the feature, fire up your Grab app, head to the “Saved Places” section on the app navigation bar and start adding all your favourite destinations such as your home, your office, your favourite mall or the airport and you are done with the hassle of typing them again.  &nbsp;Hola! your saved addresses are just a click away to order a ride or your favourite meal.Inspiration Behind the WorkWe at Grab continuously engage with our consumers to understand how we can outserve them better. Difficulty in choosing the correct address was one of the key feedback shared by our consumers. Our drivers shared funny stories about places that have similar names but outrightly different locations e.g. Sime Road is in Bukit Timah but Simei Road is in Simei almost 20 km away, Nicoll Highway is in Kallang but Nicoll Drive is in Changi almost 20 km away. In this case, even though the users use the address frequently, there remains scope for misselection.Data-driven DecisionsOur vast repository of data and insights has helped us identify and solve some challenging problems. Our analysis of millions of transport bookings and food orders revealed that consumers usually visit five to seven unique locations and order food at one or two addresses.One intriguing yet intuitive insight that came out was a set pattern in user’s booking behaviour during weekdays. A set of passengers mostly commute between two addresses, probably going to the office in the morning and coming back home in the evening. These identifiable clusters of pick-up and drop-off locations during peak hours signified our hypothesis of users using a small set of locations for their Grab bookings. The pictures below show such clusters in Singapore and Jakarta where passengers generally commute to and fro in morning and in evening respectively.  &nbsp;This insight also motivated us to test out the concept of user created labels which allows the users to mark their saved places with their own labels like “Home”, “Changi Airport”, “Sis’s House” etc. Initial experiment results were extremely encouraging and we got significantly higher usage and repeat rates from users.A group of cross functional teams - Analytics, Product, Design, Engineering etc came together, worked backwards from the consumer, brainstormed multiple ideas, and finalised a product approach. We then went on to conduct in depth user research and usability testing to ensure that the final product met user expectations and was easy to understand and use.And Users Love it!Since the launch, we have seen significant user adoption for the feature. More than 14 million users have saved close to 45 Million saved places. That’s ~3 places per user!Consumers from Singapore and Myanmar tend to save around 3 addresses each whereas consumers from Indonesia, Malaysia, Philippines, Thailand, Vietnam and Cambodia save 2 addresses each. A consumer from Indonesia has saved a whopping 1,191 addresses!Users across Southeast Asia have adopted the feature and as of today, a significant portion of our bookings are made using a saved place for either pickup or drop off. If you were curious, here are the most frequently used labels for saving addresses in Singapore (left) and Indonesia (right):  &nbsp;Apart from saving home and office addresses our consumers are also saving their child’s school address and places of worship. Some of them are also saving their favourite shopping destinations.Another observation, as someone may have guessed, is regarding cluster of home addresses. Home addresses in Singapore are evenly scattered across the island (map on upper left) but the same are concentrated in specific pockets of the city in Jakarta (map on lower left). However office addresses are concentrated in specific areas in both cities - CBD and Changi area in Singapore (map on upper right) and along central Jakarta in Jakarta (map on lower right).  &nbsp;This is Only the BeginningWe’re constantly striving to improve the user experience with Grab and make it as seamless as possible. We have only taken the first steps with Saved Places and the path forward involves deeper understanding of user behaviour with the help of saved places data to create a more personalised experience. This is just the beginning and we’re planning to launch some very innovative features in the coming months.",
        "url": "/save-your-place-with-grab"
      }
      ,
    
      "automated-erp-charges": {
        "title": "No More Forgetting to Input ERP Charges - Hello Automated ERP!",
        "author": "garvee-gargkudali-robinson-immanuellara-pureum-yim",
        "tags": "[&quot;Data&quot;, &quot;Maps&quot;, &quot;Tech&quot;]",
        "category": "",
        "content": "ERP, standing for Electronic Road Pricing, is a system used to manage road congestion in Singapore. Drivers are charged when they pass through ERP gantries during peak hours. ERP rates vary for different roads and time periods based on the traffic conditions at the time. This encourages people to change their mode of transport, travel route or time of travel during peak hours. ERP is seen as an effective measure in addressing traffic conditions and ensuring drivers continue to have a smooth journey.Did you know that Singapore has a total of 79 active ERP gantries? Did you also know that every ERP gantry changes its fare 10 times a day on average? For example, total ERP charges for a journey from Ang Mo Kio to Marina will cost $10 if you leave at 8:50am, but $4 if you leave at 9:00am on a working day!Imagine how troublesome it would have been for Grab’s driver-partners who, on top of having to drive and check navigation, would also have had to remember each and every gantry they passed, calculating their total fare and then manually entering the charges to the total ride cost at the end of the ride.In fact, based on our driver-partners’ feedback, missing out on ERP charges was listed as one of their top-most pain points. Not only did the drivers find the entire process troublesome, this also led to earnings loss as they would have had to bear the cost of the  ERP fares.We’re glad to share that, as of 15th March 2019, we’ve successfully resolved this pain point for our driver-partners by introducing automated ERP fare calculation!So, how did we achieve automating the ERP fare calculation for our drivers-partners? How did we manage to reduce the number of trips where drivers would forget to enter ERP fare to almost zero? Read on!How We Approached the ProblemThe question we wanted to solve was - how do we create an impactful feature to make sure that driver -partners have one less thing to handle when they drive?We started by looking at the problem at hand. ERP fares in Singapore are very dynamic; it changes on the basis of day and time.    Caption: Example of ERP fare changes on a normal weekday in Singapore&nbsp;We wanted to create a system which can identify the dynamic ERP fares at any given time and location, while simultaneously identifying when a driver-partner has passed through any of these gantries.However, that wasn’t enough. We wanted this feature to be scalable to every country where Grab is in - like Indonesia, Thailand, Malaysia, Philippines, Vietnam. We started studying the ERP (or tolls - as it is known locally) system in other countries. We realised that every country has its own style of calculating toll. While in Singapore ERP charges for cars and taxis are the same, Malaysia applies different charges for cars and taxis. Similarly, Vietnam has different tolls for 4-seaters and 7-seaters. Indonesia and Thailand have couple gantries where you pay only at one of the gantries.Suppose A and B are couple gantries, if you passed through A, you won’t need to pay at B and vice versa. This is where our Ops team came to the rescue!Boots on the Ground!Collecting all the ERP or toll data for every country is no small feat, recalls Robinson Kudali, programme manager for the project. “We had around 15 people travelling across the region for 2-3 weeks, working on collecting data from every possible source in every country.”Getting the right geographical coordinates for every gantry is very important. We track driver GPS pings frequently, identify the nearest road to that GPS ping and check the presence of a gantry using its coordinates. The entire process requires you to be very accurate; incorrect gantry location can easily lead to us miscalculating the fare.Bayu Yanuaragi, our regional mapops lead, explains - “To do this, the first step was to identify all toll gates for all expressways &amp; highways in the country. The team used various mapping software to locate and plot all entry &amp; exit gates using map sources, open data and more importantly government data as references. Each gate was manually plotted using satellite imagery and aligned with our road layers in order to extract the coordinates with a unique gantry ID.”Location precision is vital in creating the dataset as it dictates whether a toll gate will be detected by the Grab app or not. Next step was to identify the toll charge from one gate to another. Accuracy of toll charge per segment directly reflects on the fare that the passenger pays after the trip.    Caption: ERP gantries visualisation on our map - The purple bars are the gantries that we drew on our map&nbsp;Once the data compilation is done, team would then conduct fieldwork to verify its integrity. If data gaps are identified, modifications would be made accordingly.Upon submission of the output, stack engineers would perform higher level quality check of the content in staging.Lastly, we worked with a local team of driver-partners who volunteered to make sure the new system is fully operational and the prices are correct. Inconsistencies observed were reported by these driver-partners, and then corrected in our system.Closing the LoopCreating a strong dataset did help us in predicting correct fares, but we needed something which allows us to handle the dynamic behaviour of the changing toll status too. For example, Singapore government revises ERP fare every quarter, while there could also be ad-hoc changes like activating or deactivating of gantries on an on-going basis.Garvee Garg, Product Manager for this feature explains: “Creating a system that solves the current problem isn’t sufficient. Your product should be robust enough to handle all future edge case scenarios too. Hence we thought of building a feedback mechanism with drivers.”In case our ERP fare estimate isn’t correct or there are changes in ERPs on-ground, our driver-partners can provide feedback to us. These feedback directly flow to Customer Experience team, who does the initial investigation, and from there to our Ops team. A dedicated person from Ops team checks the validity of the feedback, and recommends updates. It only takes 1 day on average to update the data from when we receive the feedback from the driver-partner.However, validating the driver feedback was a time consuming process. We needed a tool which can ease the life of Ops team by helping them in de-bugging each and every case.Hence the ERP Workflow tool came into the picture.99% of the time, feedback from our driver-partners are about error cases. When feedback comes in, this tool would allow the Ops team to check the entire ride history of the driver and map driver’s ride trajectory with all the underlying ERP gantries at that particular point of time. The Ops team  would then be able to identify if ERP fare calculated by our system or as said by driver is right or wrong.This is Only the BeginningBy creating a system that can automatically calculate and key in ERP fares for each trip, Grab is proud to say that our driver-partners can now drive with less hassle and focus more on the road which will bring the ride experience and safety for both the driver and the passengers to a new level!The Automated ERP feature is currently live in Singapore and we are now testing it with our driver-partners in Indonesia and Thailand. Next up, we plan to pilot in the Philippines and Malaysia and soon to every country where Grab is in - so stay tuned for even more innovative ideas to enhance your experience on our superapp!To know more about what Grab has been doing to improve the convenience and experience for both our driver-partners and passengers, check out other stories on this blog!",
        "url": "/automated-erp-charges"
      }
      ,
    
      "how-built-logging-stack": {
        "title": "How We Built a Logging Stack at Grab",
        "author": "daniel-kasen",
        "tags": "[&quot;Logging&quot;]",
        "category": "",
        "content": "ProblemLet me take you back a year ago at Grab. When we lacked any visualisations or metrics for our service logs. When performing a query for a string from the last three days was something only run before you went for a beverage.When a service stops responding, Grab’s core problems were and are:  We need to know it happened before the consumer does.  We need to know why it happened.  We need to solve our consumers’ problems fast.We had a hodgepodge of log-based solutions for developers when they needed to figure out the above, or why a driver never showed up, or a consumer wasn’t receiving our promised promotions. These included logs in a cloud based storage service (which could take hours to retrieve). Or a SAS provider constantly timing out on our queries. Or even asking our SREs to fetch logs from the potential machines for the service engineer, a rather laborious process.Here’s what we did with our logs to solve these problems.IssuesOur current size and growth rate ruled out several available logging systems. By size, we mean a LOT of data and a LOT of users who search through hundreds of billions of logs to generate reports. Or who track down that one user who managed to find that pesky corner case in our code.When we started this project, we generated 25TB of logging data a day. Our first thought was “Do we really need all of these logs?”. To this day our feeling is “probably not”.However, we can’t always define what another developer can and cannot do. Besides, this gave us an amazing opportunity to build something to allow for all that data!Some of our SREs had used the ELK Stack (Elasticsearch / Logstash / Kibana). They thought it could handle our data and access loads, so it was our starting point.How We Built a Multi-Petabyte ClusterInformation GatheringIt started with gathering numbers. How much data did we produce each day? How many days were retained? What’s a reasonable response time to wait for?Before starting a project, understand your parameters. This helps you spec out your cluster, get buy-in from higher ups, and increase your success rate when rolling out a product used by the entire engineering organization. Remember, if it’s not better than what they have now, why will they switch?A good starting point was opening the floor to our users. What features did they want? If we offered a visualisation suite so they can see ERROR event spikes, would they use it? How about alerting them about SEGFAULTs? Hands down the most requested feature was speed; “I want an easy webUI that shows me the user ID when I search for it, and get all the results in &lt;5 seconds!”Getting Our Feet WetNew concerns always pop up during a project. We’re sure someone has correlated the time spent in R&amp;D to the number of problems. We had an always moving target, since as our proof of concept began, our daily logger volume kept increasing.Thankfully, using Elasticsearch as our data store meant we could fully utilise horizontal scaling. This let us start with a simple 5 node cluster as we built out our proof-of-concept (POC). Once we were ready to onboard more services, we could move into a larger footprint.The specs at the time called for about 80 nodes to handle all our data. But if we designed our system correctly, we’d only need to increase the number of Elasticsearch nodes as we enrolled more consumers. Our key operating metrics were CPU utilisation, heap memory needed for the JVM, and total disk space.Initial DesignFirst, we set up tooling to use Ansible both to launch a machine and to install and configure Elasticsearch. Then we were ready to scale.Our initial goal was to keep the design as simple as possible. Opting to allow each node in our cluster to perform all responsibilities. In this setup each node would behave as all of the four available types:  Ingest: Used for transforming and enriching documents before sending them to data nodes for indexing.  Coordinator: Proxy node for directing search and indexing requests.  Master: Used to control cluster operations and determine a quorum on indexed documents.  Data: Nodes that hold the indexed data.These were all design decisions made to move our proof of concept along, but in hindsight they might have created more headaches down the road with troubleshooting, indexing speed, and general stability. Remember to do your homework when spec’ing out your cluster.It’s challenging to figure out why you are losing master nodes because someone filled up the field data cache performing a search. Separating your nodes can be a huge help in tracking down your problem.We also decided to further reduce complexity by going with ingest nodes over Logstash. But at the time, the documentation wasn’t great so we had a lot of trial and error in figuring out how they work. Particularly as compared to something more battle tested like Logstash.If you’re unfamiliar with ingest node design, they are lightweight proxies to your data nodes that accept a bulk payload, perform post-processing on documents, and then send the documents to be indexed by your data nodes. In theory, this helps keep your entire pipeline simple. And in Elasticsearch’s defense, ingest nodes have made massive improvements since we began.But adding more ingest nodes means ADDING MORE NODES! This can create a lot of chatter in your cluster and cause more complexity when  troubleshooting problems. We’ve seen when an ingest node failing in an odd way caused larger cluster concerns than just a failed bulk send request.MonitoringThis isn’t anything new, but we can’t overstate the usefulness of monitoring. Thankfully, we already had a robust tool called Datadog with an additional integration for Elasticsearch. Seeing your heap utilisation over time, then breaking it into smaller graphs to display the field data cache or segment memory, has been a lifesaver. There’s nothing worse than a node falling over due to an OOM with no explanation and just hoping it doesn’t happen again.At this point, we’ve built out several dashboards which visualise a wide range of metrics from query rates to index latency. They tell us if we sharply drop on log ingestion or if circuit breakers are tripping. And yes, Kibana has some nice monitoring pages for some cluster stats. But to know each node’s JVM memory utilization on a 400+ node cluster, you need a robust metric system.PitfallsCommon ProblemsThere are many blogs about the common problems encountered when creating an Elasticsearch cluster and Elastic does a good job of keeping blog posts up to date. We strongly encourage you to read them. Of course, we ran into classic problems like ensuring our Java objects were compressed (Hints: Don’t exceed 31GB of heap for your JVM and always confirm you’ve enabled compression).But we also ran into some interesting problems that were less common. Let’s look at some major concerns you have to deal with at this scale.Grab’s ProblemsField Data CacheSo, things are going well, all your logs are indexing smoothly, and suddenly you’re getting Out Of Memory (OOMs) events on your data nodes. You rush to find out what’s happening, as more nodes crash.A visual representation of your JVM heap’s memory usage is very helpful here. You can always hit the Elasticsearch API, but after adding more then 5 nodes to your cluster this kind of breaks down. Also, you don’t want to know what’s going on while a node is down, but what happened before it died.Using our graphs, we determined the field data cache went from virtually zero memory used in the heap to 20GB! This forced us to read up on how this value is set, and, as of this writing, the default value is still 100% of the parent heap memory. Basically, this breaks down to allowing 70% of your total heap being allocated to a single search in the form of field data.Now, this should be a rare case and it’s very helpful to keep the field names and values in memory for quick lookup. But, if, like us, you have several trillion documents, you might want to watch out.From our logs, we tracked down a user who was sorting by the id field. We believe this is a design decision in how Kibana interacts with Elasticsearch. A good counter argument would be a user wants a quick memory lookup if they search for a document using the id. But for us, this meant a user could load into memory every ID in the indices over a 14 day period.The consequences? 20+GB of data loaded into the heap before the circuit breaker tripped. It then only took 2 queries at a time to knock a node over.You can’t disable indexing that field, and you probably don’t want to. But you can prevent users from stumbling into this and disable the id field in the Kibana advanced settings. And make sure you re-evaluate your circuit breakers. We drastically lowered the available field cache and removed any further issues.Translog CompressionAt first glance, compression seems an obvious choice for shipping shards between nodes. Especially if you have the free clock cycles, why not minimize the bandwidth between nodes?However, we found compression between nodes can drastically slow down shard transfers. By disabling compression, shipping time for a 50GB shard went from 1h to 20m. This was because Lucene segments are already compressed, a new issue we ran into full force and are actively working with the community to fix. But it’s also a configuration to watch out for in your setup, especially if you want a fast recovery of a shard.Segment MemoryMost of our issues involved the heap memory being exhausted. We can’t stress enough the importance of having visualisations around how the JVM is used. We learned this lesson the hard way around segment memory.This is a prime example of why you need to understand your data when building a cluster. We were hitting a lot of OOMs and couldn’t figure out why. We had fixed the field cache issue, but what was using all our RAM?There is a reason why having a 16TB data node might be a poorly spec’d machine. Digging into it, we realised we simply allocated too many shards to our nodes. Looking up the total segment memory used per index should give a good idea of how many shards you can put on a node before you start running out of heap space. We calculated on average our 2TB indices used about 5GB of segment memory spread over 30 nodes.The numbers have since changed and our layout was tweaked, but we came up with calculations showing we could allocate about 8TB of shards to a node with 32GB heap memory before we running into issues. That’s if you really want to push it, but it’s also a metric used to keep your segment memory per node around 50%. This allows enough memory to run queries without knocking out your data nodes. Naturally this led us to ask “What is using all this segment memory per node?”Index Mapping and Field TypesCould we lower how much segment memory our indices used to cut our cluster operation costs? Using the segments data found in the ES cluster and some simple Python loops, we tracked down the total memory used per field in our index.We used a lot of segment memory for the id field (but can’t do much about that). It also gave us a good breakdown of our other fields. And we realised we indexed fields in completely unnecessary ways. A few fields should have been integers but were keyword fields. We had fields no one would ever search against and which could be dropped from index memory.Most importantly, this began our learning process of how tokens and analysers work in Elasticsearch/Lucene.Picking the Wrong AnalyserBy default, we use Elasticsearch’s Standard Analyser on all analysed fields. It’s great, offering a very close approximation to how users search and it doesn’t explode your index memory like an N-gram tokeniser would.But it does a few things we thought unnecessary, so we thought we could save a significant amount of heap memory. For starters, it keeps the original tokens: the Standard Analyser would break IDXVB56KLM into tokens IDXVB, 56,  and KLM. This usually works well, but it really hurts you if you have a lot of alphanumeric strings.We never have a user search for a user ID as a partial value. It would be more useful to only return the entire match of an alphanumeric string. This has the added benefit of only storing the single token in our index memory. This modification alone stripped a whole 1GB off our index memory, or at our scale meant we could eliminate 8 nodes.We can’t stress enough how cautious you need to be when changing analysers on a production system. Throughout this process, end users were confused why search results were no longer returning or returning weird results. There is a nice kibana plugin that gives you a representation of how your tokens look with a different analyser, or use the build in ES tools to get the same understanding.Be Careful with Cloud MaintainersWe realised that running a cluster at this scale is expensive. The hardware alone sets you back a lot, but our hidden bigger cost was cross traffic between availability zones.Most cloud providers offer different “zones” for your machines to entice you to achieve a High-Availability environment. That’s a very useful thing to have, but you need to do a cost/risk analysis. If you migrate shards from HOT to WARM to COLD nodes constantly, you can really rack up a bill. This alone was about 30% of our total cluster cost, which wasn’t cheap at our scale.We re-worked how our indices sat in the cluster. This let us create a different index for each zone and pin logging data so it never left the zone it was generated in. One small tweak to how we stored data cut our costs dramatically. Plus, it was a smaller scope for troubleshooting. We’d know a zone was misbehaving and could focus there vs. looking at everything.ConclusionRunning our own logging stack started as a challenge. We roughly knew the scale we were aiming for; it wasn’t going to be trivial or easy. A year later, we’ve gone from pipe-dream to production and immensely grown the team’s ELK stack knowledge.We could probably fill 30 more pages with odd things we ran into, hacks we implemented, or times we wanted to pull our hair out. But we made it through and provide a superior logging platform to our engineers at a significant price reduction while maintaining a stable platform.There are many different ways we could have started knowing what we do now. For example, using Logstash over Ingest nodes, changing default circuit breakers, and properly using heap space to prevent node failures. But hindsight is 20/20 and it’s rare for projects to not change.We suggest anyone wanting to revamp their centralised logging system look at the ELK solutions. There is a learning curve, but the scalability is outstanding and having subsecond lookup time for assisting a consumer is phenomenal. But, before you begin, do your homework to save yourself weeks of troubleshooting down the road. In the end though, we’ve received nothing but praise from Grab engineers about their experiences with our new logging system.",
        "url": "/how-built-logging-stack"
      }
      ,
    
      "grab-everyday-super-app": {
        "title": "Making Grab’s Everyday App Super",
        "author": "justin-boliliaromain-bassevillekaren-kue",
        "tags": "[&quot;Superapp&quot;, &quot;Feed&quot;, &quot;Recommendations&quot;, &quot;Data Science&quot;, &quot;Machine Learning&quot;]",
        "category": "",
        "content": "Grab is Southeast Asia’s leading superapp, providing highly-used daily services such as ride-hailing, food delivery, payments, and more. Our goal is to give people better access to the services that matter to them, with more value and convenience, so we’ve been expanding our ecosystem to include bill payments, hotel bookings, trip planners, and videos - with more to come. We want to outserve our consumers - not just by packing the Grab app with useful features and services, but by making the whole experience a unique and personalised one for each of them.To realise our superapp ambitions, we work with partners who, like us, want to help drive Southeast Asia forward.A lot of the collaborative work we do with our partners can be seen in the Grab Feed. This is where we broadcast various types of content about Grab and our partners in an aggregated manner, adding value to the overall user experience. Here’s what the feed looks like:    Waiting for the next promo? Check the Feed.Looking for news and entertainment? Check the Feed.Want to know if it's a good time to book a car? CHECK. THE. FEED.&nbsp;As we continue to add more cards, services, and chunks of content into Grab Feed, there’s a risk that our users will find it harder to find the information relevant to them. So we work to ensure that our platform is able to distinguish and show information based on what’s most suited for the user’s profile. This goes back to what has always been our central focus - the consumer - and is why we put so much importance in personalising the Grab experience for each of them.To excel in a heavily diversified market like Southeast Asia, we leverage on the depth of our data to understand what sorts of information users want to see and when they should see them. In this article we will discuss Grab Feed’s recommendation logic and strategies, as well as its future roadmap.Start Your Engines  The problem we’re trying to solve here is known as the recommendations problem. In a nutshell, this problem is about inferring the preference of consumers to recommend content and services to them. In Grab Feed, we have different types of content that we want to show to different types of consumers and our challenge is to ensure that everyone gets quality content served to them.  To solve this, we have built a recommendation engine, which is a system that suggests the type of content a user should consider consuming. In order to make a recommendation, we need to understand three factors:  Users. There’s a lot we can infer about our users based on how they’ve used the Grab app, such as the number of rides they’ve taken, the type of food they like to order, the movie voucher deals they’ve purchased, the games they’ve played, and so on. This information gives us the opportunity to understand our users’ preferences better, enabling us to match their profiles with relevant and suitable content.  Items. These are the characteristics of the content. We consider the type of the content (e.g. video, games, rewards) and consumability (e.g. purchase, view, redeem). We also consider other metadata such as store hours for merchants, points to burn for rewards, and GPS coordinates for points of interest.  Context. This pertains to the setting in which a user is consuming our content. It could be the time of day, the user’s location, or the current feed category.Using signals from all these factors, we build a model that returns a ranked set of cards to the user. More on this in the next few sections.Understanding Our User  Interpreting user preference from the signals mentioned above is a whole challenge in itself. It’s important here to note that we are in a constant state of experimentation. Slowly but surely, we are continuing to fine tune how to measure content preferences. That being said, we look at two areas:      Action. We firmly believe that not all interactions are made equal. Does liking a card actually mean you like it? Do you like things at the same rate as your friends? What about transactions, are those more preferred? The feed introduces a lot of ways for the users to give feedback to the platform. These events include likes, clicks, swipes, views, transactions, and call-to-actions. Depending on the model, we can take slightly different approaches. We can learn the importance of each event and aggregate them to have an expected rating, or we can predict the probability of each event and rank accordingly.        Recency. Old interactions are probably not as useful as new ones. The feed is a product that is constantly evolving, and so are the preferences of our users. Failing to decay the weight of older interactions will give us recommendations that are no longer meaningful to our users.  Optimising the Experience  Building a viable recommendation engine requires several phases. Working iteratively, we are able to create a few core recommendation strategies to produce the final model in determining the content’s relevance to the user. We’ll discuss each strategy in this section.  Popularity. This strategy is better known as trending recommendations. We capture online clickstream events over a rolling time window and aggregate the events to show the user what’s popular to everyone at that point in time. Listening to the crowds is generally an effective strategy, but this particular strategy also helps us address the cold start problem by providing recommendations for new feed users.  User Favourites. We understand that our users have different tastes and that users will have content that they engage with more than other users would.  In this strategy, we capture that personal engagement and the user’s evolving preferences.  Collaborative Filtering. A key goal in building our everyday superapp is to let users experience different services. To allow discoverability, we study similar users to uncover a s et of similar preferences they may have, which we can then use to guide what we show other users.  Habitual Behaviour. There will be times where users only want to do a specific thing, and we wouldn’t want them to scroll all the way down just to do it. We’ve built in habitual recommendations to address this. So if users always use the feed to scroll through food choices at lunch or to take a peek at ride peaks (pun intended) on Sunday morning, we’ve still got them covered.  Deep Recommendations. We’ve shown you how we use Feed data to drive usage across the platform. But what about using the platform data to drive the user feed behaviour? By embedding users’ activities from across our multiple businesses, we’re also able to leverage this data along with clickstream to determine the content preferences for each user.We apply all these strategies to find out the best recommendations to serve the users either by selection or by aggregation. These decisions are determined through regular experiments and studies of our users.Always LearningWe’re constantly learning and relearning about our users. There are a lot of ways to understand behaviour and a lot of different ways to incorporate different strategies, so we’re always iterating on these to deliver the most personal experience on the app.To identify a user’s preferences and optimal strategy exposure, we capitalise on our Experimentation Platform to expose different configurations of our Recommendation Engine to different users. To monitor the quality of our recommendations, we measure the impact with online metrics such as interaction, clickthrough, and engagement rates and offline metrics like Recall@K and Normalised Discounted Cumulative Gain (NDCG).Future WorkThrough our experience building out this recommendations platform, we realised that the space was large enough and that there’s a lot of pieces that can continuously be built. To keep improving, we’re already working on the following items:  Multi-objective optimisation for business and technical metrics  Building out automation pipelines for hyperparameter optimisation  Incorporating online learning for real-time model updates  Multi-armed bandits for user personalised recommendation strategies  Recsplanation system to allow stakeholders to better understand the systemConclusionGrab is one of Southeast Asia’s fastest growing companies. As its business, partnerships, and offerings continue to grow, the superapp real estate problem will only keep on getting bigger. In this post, we discuss how we are addressing that problem by building out a recommendation system that understands our users and personalises the experience for each of them. This system (us included) continues to learn and iterate from our users feedback to deliver the best version for them.If you’ve got any feedback, suggestions, or other great ideas, feel free to reach me at justin.bolilia@grab.com. Interested in working on these technologies yourself? Check out our career page.",
        "url": "/grab-everyday-super-app"
      }
      ,
    
      "catwalk-serving-machine-learning-models-at-scale": {
        "title": "Catwalk: Serving Machine Learning Models at Scale",
        "author": "nutdanai-phansooksaijuho-leepratyush-moreromain-basseville",
        "tags": "[&quot;Machine Learning&quot;, &quot;Models&quot;, &quot;Data Science&quot;, &quot;TensorFlow&quot;]",
        "category": "",
        "content": "IntroductionGrab’s unwavering ambition is to be the best superapp in Southeast Asia that adds value to the every day for our consumers. In order to achieve that, the consumer experience must be flawless for each and every Grab service. Let’s take our frequently used ride-hailing service as an example. We want fair pricing for both drivers and passengers, accurate estimation of ETAs, effective detection of fraudulent activities, and ensured ride safety for our consumers. The key to perfecting these consumer journeys is artificial intelligence (AI).Grab has a tremendous amount of data that we can leverage to solve complex problems such as fraudulent user activity, and to provide our consumers personalised experiences on our products. One of the tools we are using to make sense of this data is machine learning (ML).As Grab made giant strides towards increasingly using machine learning across the organization, more and more teams were organically building model serving solutions for their own use cases. Unfortunately, these model serving solutions required data scientists to understand the infrastructure underlying them. Moreover, there was a lot of overlap in the effort it took to build these model serving solutions.That’s why we came up with Catwalk: an easy-to-use, self-serve, machine learning model serving platform for everyone at Grab.GoalsTo determine what we wanted Catwalk to do, we first looked at the typical workflow of our target audience - data scientists at Grab:  Build a trained model to solve a problem.  Deploy the model to their project’s particular serving solution. If this involves writing to a database, then the data scientists need to programmatically obtain the outputs, and write them to the database. If this involves running the model on a server, the data scientists require a deep understanding of how the server scales and works internally to ensure that the model behaves as expected.  Use the deployed model to serve users, and obtain feedback such as user interaction data. Retrain the model using this data to make it more accurate.  Deploy the retrained model as a new version.  Use monitoring and logging to check the performance of the new version. If the new version is misbehaving, revert back to the old version so that production traffic is not affected. Otherwise run an AB test between the new version and the previous one.We discovered an obvious pain point - the process of deploying models requires additional effort and attention, which results in data scientists being distracted from their problem at hand. Apart from that, having many data scientists build and maintain their own serving solutions meant there was a lot of duplicated effort. With Grab increasingly adopting machine learning, this was a state of affairs that could not be allowed to continue.To address the problems, we came up with Catwalk with goals to:  Abstract away the complexities and expose a minimal interface for data scientists  Prevent duplication of effort by creating an ML model serving platform for everyone in Grab  Create a highly performant, highly available, model versioning supported ML model serving platform and integrate it with existing monitoring systems at Grab  Shorten time to market by making model deployment self-serviceWhat is Catwalk?In a nutshell, Catwalk is a platform where we run Tensorflow Serving containers on a Kubernetes cluster integrated with the observability stack used at Grab.In the next sections, we are going to explain the two main components in Catwalk - Tensorflow Serving and Kubernetes, and how they help us obtain our outlined goals.What is Tensorflow Serving?Tensorflow Serving is an open-source ML model serving project by Google. In Google’s own words, “Tensorflow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. It makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. Tensorflow Serving provides out-of-the-box integration with Tensorflow models, but can be easily extended to serve other types of models and data.”Why Tensorflow Serving?There are a number of ML model serving platforms in the market right now. We chose Tensorflow Serving because of these three reasons, ordered by priority:  Highly performant. It has proven performance handling tens of millions of inferences per second at Google according to their website.  Highly available. It has a model versioning system to make sure there is always a healthy version being served while loading a new version into its memory  Actively maintained by the developer community and backed by GoogleEven though, by default, Tensorflow Serving only supports models built with Tensorflow, this is not a constraint though, because Grab is actively moving toward using Tensorflow.How are we using Tensorflow Serving?In this section, we will explain how we are using Tensorflow Serving and how it helps abstract away complexities for data scientists.Here are the steps showing how we are using Tensorflow Serving to serve a trained model:  Data scientists export the model using tf.saved_model API and drop it to an S3 models bucket. The exported model is a folder containing model files that can be loaded to Tensorflow Serving.  Data scientists are granted permission to manage their folder.  We run Tensorflow Serving and point it to load the model files directly from the S3 models bucket. Tensorflow Serving supports loading models directly from S3 out of the box. The model is served!  Data scientists come up with a retrained model. They export and upload it to their model folder.  As Tensorflow Serving keeps watching the S3 models bucket for new models, it automatically loads the retrained model and serves. Depending on the model configuration, it can either gracefully replace the running model version with a newer version or serve multiple versions at the same time.  The only interface to data scientists is a path to their model folder in the S3 models bucket. To update their model, they upload exported models to their folder and the models will automatically be served. The complexities are gone. We’ve achieved one of the goals!Well, not really…Imagine you are going to run Tensorflow Serving to serve one model in a cloud provider, which means you  need a compute resource from a cloud provider to run it. Running it on one box doesn’t provide high availability, so you need another box running the same model. Auto scaling is also needed in order to scale out based on the traffic. On top of these many boxes lies a load balancer. The load balancer evenly spreads incoming traffic to all the boxes, thus ensuring that there is a single point of entry for any clients, which can be abstracted away from the horizontal scaling. The load balancer also exposes an HTTP endpoint to external users. As a result, we form a Tensorflow Serving cluster that is ready to serve.Next, imagine you have more models to deploy. You have three options  Load the models into the existing cluster - having one cluster serve all models.  Spin up a new cluster to serve each model - having multiple clusters, one cluster serves one model.  Combination of 1 and 2 - having multiple clusters, one cluster serves a few models.The first option would not scale, because it’s just not possible to load all models into one cluster as the cluster has limited resources.The second option will definitely work but it doesn’t sound like an effective process, as you need to create a set of resources every time you have a new model to deploy. Additionally, how do you optimise the usage of resources, e.g., there might be unutilised resources in your clusters that could potentially be shared by the rest.The third option looks promising, you can manually choose the cluster to deploy each of your new models into so that all the clusters’ resource utilisation is optimal. The problem is you have to manually manage it. Managing 100 models using 25 clusters can be a challenging task. Furthermore, running multiple models in a cluster can also cause a problem as different models usually have different resource utilisation patterns and can interfere with each other. For example, one model might use up all the CPU and the other model won’t be able to serve anymore.Wouldn’t it be better if we had a system that automatically orchestrates model deployments based on resource utilisation patterns and prevents them from interfering with each other? Fortunately, that  is exactly what Kubernetes is meant to do!So What is Kubernetes?Kubernetes abstracts a cluster of physical/virtual hosts (such as EC2) into a cluster of logical hosts (pods in Kubernetes terms). It provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads.Let’s look at some of the definitions of Kubernetes resources:    Cluster - a cluster of nodes running Kubernetes.  Node - a node inside a cluster.  Deployment - a configuration to instruct Kubernetes the desired state of an application. It also takes care of rolling out an update (canary, percentage rollout, etc), rolling back and horizontal scaling.  Pod - a single processing unit. In our case, Tensorflow Serving will be running as a container in a pod. Pod can have CPU/memory limits defined.  Service - an abstraction layer that abstracts out a group of pods and exposes the application to clients.  Ingress - a collection of routing rules that govern how external users access services running in a cluster.  Ingress Controller - a controller responsible for reading the ingress information and processing that data accordingly such as creating a cloud-provider load balancer or spinning up a new pod as a load balancer using the rules defined in the ingress resource.Essentially, we deploy resources to instruct Kubernetes the desired state of our application and Kubernetes will make sure that it is always the case.How are we using Kubernetes?In this section, we will walk you through how we deploy Tensorflow Serving in Kubernetes cluster and how it makes managing model deployments very convenient.We used a managed Kubernetes service, to create a Kubernetes cluster and manually provisioned compute resources as nodes. As a result, we have a Kubernetes cluster with nodes that are ready to run applications.An application to serve one model consists of:  Two or more Tensorflow Serving pods that serves a model with an autoscaler to scale pods based on resource consumption  A load balancer to evenly spread incoming traffic to pods  An exposed HTTP endpoint to external usersIn order to deploy the application, we need to:  Deploy a deployment resource specifying  Number of pods of Tensorflow Serving  An S3 url for Tensorflow Serving to load model files  Deploy a service resource to expose it  Deploy an ingress resource to define an HTTP endpoint urlKubernetes then allocates Tensorflow Serving pods to the cluster with the number of pods according to the value defined in deployment resource. Pods can be allocated to any node inside the cluster, Kubernetes makes sure that the node it allocates a pod into has sufficient resources that the pod needs. In case there is no node that has sufficient resources, we can easily scale out the cluster by adding new nodes into it.In order for the rules defined inthe ingressresource to work, the cluster must have an ingress controller running, which is what guided our choice of the load balancer. What an ingress controller does is simple: it keeps checking the ingress resource, creates a load balancer and defines rules based on rules in the ingress resource. Once the load balancer is configured, it will be able to redirect incoming requests to the Tensorflow Serving pods.That’s it! We have a scalable Tensorflow Serving application that serves a model through a load balancer! In order to serve another model, all we need to do is to deploy the same set of resources but with the model’s S3 url and HTTP endpoint.To illustrate what is running inside the cluster, let’s see how it looks like when we deploy two applications: one for serving pricing model another one for serving fraud-check model. Each application is configured to have two Tensorflow Serving pods and exposed at /v1/models/model  There are two Tensorflow Serving pods that serve fraud-check model and exposed through a load balancer. Same for the pricing model, the only differences are the model it is serving and the exposed HTTP endpoint url. The load balancer rules for pricing and fraud-check model look like this            If      Then forward to                  Path is /v1/models/pricing      pricing pod ip-1              pricing pod ip-2              Path is /v1/models/fraud-check      fraud-check pod ip-1              fraud-check pod ip-2      Stats and LogsThe last piece is how stats and logs work. Before getting to that, we need to introduce DaemonSet. According to the document, DaemonSet ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected. Deleting a DaemonSet will clean up the pods it created.We deployed datadog-agent and filebeat as a DaemonSet. As a result, we always have one datadog-agent pod and one filebeat pod in all nodes and they are accessible from Tensorflow Serving pods in the same node. Tensorflow Serving pods emit a stats event for every request to datadog-agent pod in the node it is running in.Here is a sample of DataDog stats:  And logs that we put in place:  Benefits Gained from CatwalkCatwalk has become the go-to, centralised system to serve machine learning models. Data scientists are not required to take care of the serving infrastructure hence they can focus on what matters the most: come up with models to solve consumer problems. They are only required to provide exported model files and estimation of expected traffic in order to prepare sufficient resources to run their model. In return, they are presented with an endpoint to make inference calls to their model, along with all necessary tools for monitoring and debugging. Updating the model version is self-service, and the model improvement cycle is much shorter than before. We used to count in days, we now count in minutes.Future PlansImprovement on AutomationCurrently, the first deployment of any model will still need some manual task from the platform team. We aim to automate this process entirely. We’ll work with our awesome CI/CD team who is making the best use of Spinnaker.Model Serving on Mobile DevicesAs a platform, we are looking at setting standards for model serving across Grab. This includes model serving on mobile devices as well. Tensorflow Serving also provides a Lite version to be used on mobile devices. It is a whole new paradigm with vastly different tradeoffs for machine learning practitioners. We are quite excited to set some best practices in this area.gRPC SupportCatwalk currently supports HTTP/1.1. We’ll hook Grab’s service discovery mechanism to open gRPC traffic, which TFS already supports.If you are interested in building pipelines for machine learning related topics, and you share our vision of driving Southeast Asia forward, come join us!",
        "url": "/catwalk-serving-machine-learning-models-at-scale"
      }
      ,
    
      "react-native-in-grabpay": {
        "title": "React Native in GrabPay",
        "author": "sushant-tiwarivinod-prajapati",
        "tags": "[&quot;Grab&quot;, &quot;Mobile&quot;, &quot;GrabPay&quot;, &quot;React&quot;]",
        "category": "",
        "content": "OverviewIt wasn’t too long ago that Grab formed a new team, GrabPay, to improve the cashless experience in Southeast Asia and to venture into the promising mobile payments arena. To support the work, Grab also decided to open a new R&amp;D center in Bangalore.It was an exciting journey for our team from the very beginning, as it gave us the opportunity to experiment with new cutting edge technologies. Our first release was the GrabPay Merchant App, the first all React Native Grab app. Its success gave us the confidence to use React Native to optimise the Grab Passenger app.React Native is an open source mobile application framework. It lets developers use React (a JavaScript library for building user interfaces) with native platform capabilities. Its two big advantages are:  Ability to create cross-platform mobile apps and components completely in JavaScript.  Its hot reloading feature that significantly reduces development time.This post describes our work on developing React Native components for Grab apps (specifically the Grab Passenger app), the challenges faced during implementation, our learnings from other internal React Native projects, and our future roadmap.Before embarking on our work with React Native, these were the goals we set out. We wanted to:  Have a reusable code between Android and iOS as well as across various Grab apps (driver app, merchant app, etc.).  Have a single codebase to minimise the effort needed to modify and maintain our code long term.  Match the performance and standards of existing Grab apps.  Use as few Engineering resources as possible.ChallengesMany Grab teams located across Southeast Asia and in the United States support the app platform. It was hard to convince all of them to add React Native as a project dependency and write new feature code with React Native. In particular, having React Native dependency significantly increases a project’s binary’s size, but the initial cost was worth it. We now have only a few modules, all written in React Native:  Express  Transaction History  Postpaid BillPayWe have a single codebase for both iOS and Android apps, which means that the modules take half the maintenance resources. Debugging is faster with React Native’s hot reloading. And it’s much easier and faster to implement one of our modules in another app, such as the Grab driver app.Another challenge was creating a universally acceptable format for a bridging library to communicate between existing code and React Native modules. We had to define fixed guidelines to create new bridges and define communication protocols between React Native modules and existing code.Invoking a module written in React Native from a native module (written in a standard computer language such as Swift or Kotlin) should follow certain guidelines. Once all Grab’s tech families reached a consensus on solutions to these problems, we started making our bridges and doing the groundwork to use React Native.FoundationOn the native side, we used the Grablet architecture to add our React Native modules. Grablet gave us a wonderful opportunity to scale our Grab platform so it could be used by any tech family to plug and play their module. And the module could be in any of  Native, React Native, Flutter, or Web.We also created a framework encapsulating all the project’s React Native Binaries. This simplified the React Native Upgrade process. Dependencies for the framework are react, react-native, and react-native-event-bridge.We had some internal proof of concept projects for determining React Native’s performance on different devices, as discussed here. Many teams helped us make an extensive set of JS bridges for React Native in Android and iOS. Oleksandr Prokofiev wrote this bridge creation example:publicfinalclassDeviceKitModule: NSObject, RCTBridgeModule { privateletdeviceKit: DeviceKitService publicinit(deviceKit: DeviceKitService) {   self.deviceKit = deviceKit   super.init() } publicstaticfuncmoduleName() -&gt; String {   return\"DeviceKitModule\" } publicfuncmethodsToExport() -&gt; [RCTBridgeMethod] {   let methods: [RCTBridgeMethod?] = [     buildGetDeviceID()     ]   return methods.compactMap { $0 } } privatefuncbuildGetDeviceID() -&gt; BridgeMethodWrapper? {   returnBridgeMethodWrapper(\"getDeviceID\", { [weakself] (_: [Any], _, resolve) in     letvalue = self?.deviceKit.getDeviceID()     resolve(value)   }) }}GrabPay Components and React NativeThe GrabPay merchant app gave us a good foundation for React Native in terms of:  Component libraries  Networking layer and API middleware  Real world data for internal assessment of performance and stabilityWe used this knowledge to build the Transaction History and GrabPay Digital Marketplace components inside the Grab passenger app with React Native.Component LibraryWe selected particularly useful components from the Merchant app codebase such as GPText, GPTextInput, GPErrorView, and GPActivityIndicator. We expanded that selection to a common (internal) component library of approximately 20 stateless and stateful components.API CallsWe used to make API calls using axios (now deprecated). We now make calls from the Native side using bridges that return a promise and make API calls using an existing framework. This helped us remove the dependency for getting an access token from Native-Android or Native-iOS to make the calls. Also it helped us optimise the API requests, as suggested by Parashuram from Facebook’s React Native team.LocaleWe use React Localize Redux for all our translations and moment for our date time conversion as per the device’s current Locale. We currently support translation in five languages: English, Chinese Simplified, Bahasa Indonesia, Malay, and Vietnamese. This Swift code shows how we get the device’s current Locale from the native-react Native Bridge.public func methodsToExport() -&gt; [RCTBridgeMethod] {   let methods: [RCTBridgeMethod?] =  [     BridgeMethodWrapper(\"getLocaleIdentifier\", { (_, _, resolver) in     letlocaleIdentifier = self.locale.getLocaleIdentifier()     resolver(localeIdentifier)   })]   return methods.compactMap { $0 } }ReduxRedux is an extremely lightweight predictable state container that behaves consistently in every environment. We use Redux with React Native to manage its state.NavigationFor in-app navigation, we use react-navigation. It is very flexible in adapting to both the Android and iOS navigation and gesture recognition styles.End ProductAfter setting up our foundation bridges and porting the skeleton boilerplate code from the GrabPay Merchant app, we wrote two payments modules using GrabPay Digital Marketplace (also known as BillPay), React Native, and Transaction History.This is the Android version of the app.  And this is the iOS version:  The UIs for the iOS and Android versions are identical, the code are identical too. A single codebase lets us debug faster, deliver quicker, and maintain smaller.We launched BillPay first in Indonesia, then in Vietnam and Malaysia. So far, it’s been a very stable product with little to no downtime.Transaction History started in Singapore and is now rolling out in other countries.Flow For BillPay  The above shows BillPay’s flow.  We start with the first screen, called Biller List. It shows all the postpaid billers available for the current region. For now, we show Billers based on which country the user is in. The user selects a biller.  We then asks for your customerID (or prefills that value if you have paid your bill before). The amount is either fetched from the backend or filled in by the user, depending on the region and biller type.  Next, the user confirms all the entered details before they pay the dues.  Finally, the user sees their bill payment receipt. It comes directly from the biller, and so it’s a valid proof of payment.Our React Native version has kept the same experience as our Native developed app and helps users pay their bills seamlessly and hassle free.FutureWe are moving our code to Typescript to reduce compile-time bugs and clean up our code. In addition to reducing native dependencies, we will refactor modules as needed. We will also have 100% unit test code coverage. But most importantly, we plan to open source our component library as soon as we meet our milestones around improved stability.",
        "url": "/react-native-in-grabpay"
      }
      ,
    
      "connecting-the-invisibles-to-design-seamless-experiences": {
        "title": "Connecting the Invisibles to Design Seamless Experiences",
        "author": "stephanie-lukitojia-liang-wong",
        "tags": "[&quot;Design&quot;, &quot;Service Design&quot;]",
        "category": "",
        "content": "    Leonardo Da Vinci's Vitruvian Man (Source: Public Doman @Wikicommons)Before We Begin, What is Service Design Anyway?In the world of design jargon, meet “service design”. Unlike other objectives in design to simplify and clarify, service design is not about building singular touchpoints. Rather, it is about bringing ease and harmony into large and often complex ecosystems.Think of the human body. There are organ systems such as the cardiovascular, respiratory, musculoskeletal, and nervous systems. These systems perform key functions that we see and feel every day, like breathing, moving, and feeling.Service design serves as the connective tissue that brings the amazing systems together to work in harmony. Much of the work done by the service design team at Grab revolves around connecting online experiences to the offline world, connecting challenges across a complex ecosystem, and enabling effective collaboration across cross-functional teams.Connecting Online Experiences to the Offline WorldWe explore holistic experiences by visualising the connections across features, both through the online-offline as well as internal-external interactions. At Grab, we have a collection of (very cool!) features that many teams have worked hard to build. However, equally important is how a person arrives from feature to feature seamlessly, from the app to their physical experiences, as well as how our internal teams at Grab support and execute behind-the-scenes throughout our various systems.For example, placing an order on GrabFood requires much more work than sending information to the merchant through the Grab app. How might Grab  allocate drivers effectively,  support unhappy paths with our consumer support team,  resolve discrepancies in our operations teams, and  store this data in a system that can continue to expand for future uses to come?  Connecting Challenges Across a Complex EcosystemSometimes, as designers, we might get too caught up in solving problems through a singular lens, and overlook how it affects the rest of the system. Meanwhile, many problems are part of a connected network. Changing one part of the problem can potentially affect other parts of the network.Considering those connections, or the “stuff in between”, makes service design a holistic practice - crossing boundaries between teams in search of a root cause, and considering how treating one problem might affect other parts of the network.  If this happens, then what?  Which point in the system is easiest to fix and has the greatest impact?For example, if we want to introduce a feature for drivers to report restaurant closings, how might Grab  Ensure the report is accurate?  Deal with accidental closings or fraud?  Use that data for our operations team to make decisions?  Let drivers know when their report has led to a successful action?  Last but not least, is this the easiest point in the system to fix restaurant opening inaccuracies, or should this be tackled through an operational fix?  Facilitating Effective Collaborations in Cross-functional TeamsFinally, we believe in the power of a participatory design process to unlock meaningful, consumer-centric solutions. Working on the “stuff in between” often puts the service design team in the thick of alignment of priorities, creation of a common vision, and coherent action plans. Achieving this requires solid facilitation and processes for cross-team collaboration.  Who are the right stakeholders and how do we engage?  How does an initiative affect stakeholders, and how can they contribute?  How can we create visual processes that allow diverse stakeholders to have a shared understanding and co-create solutions?  What’s the Ultimate Goal? A Harmonious Backstage for a Delightful Consumer ExperienceBy facilitating cross-functional collaborations and espousing a whole-of-Grab approach, the service design team at Grab helps to connect the dots in an interconnected superapp service ecosystem. By empathising with our users, and having a deep understanding of how different parts of the Grab ecosystem affect one another, we hope to unleash the full power of Grab to deliver maximum value and delight to serve our users.",
        "url": "/connecting-the-invisibles-to-design-seamless-experiences"
      }
      ,
    
      "tourist-chat-data-story": {
        "title": "Tourists on GrabChat!",
        "author": "lara-pureum-yimdustin-chung",
        "tags": "[&quot;Data&quot;, &quot;Analytics&quot;, &quot;Data Analytics&quot;]",
        "category": "",
        "content": "Just over two years ago we introduced GrabChat, Southeast Asia’s first of its kind in-app messaging platform. Since then we’ve added all sorts of useful features to it. Auto-translated messages, the ability to send photos, and even voice messages! It’s been a great tool to facilitate smoother communications between our driver-partners and our passengers, and one group in particular has found it incredibly useful: tourists!Now, we’ve analysed tourist data before, but we were curious about how GrabChat in particular has served this demographic. So we looked for interesting insights using sampled tourist chat data from Singapore, Malaysia, and Indonesia for the period of December 2018 to March 2019. That’s more than 3.7 million individual GrabChat messages sent by tourists! Here’s what we found.  Looking at the volume of the chats being transmitted per booking, we can see that the “chattiest” tourists are from East Timor, Nigeria, and Ukraine with averages of 6.0, 5.6, and 5.1 chats per booking respectively.Then we wondered: if tourists from all over the world are talking this much to our driver-partners, how are they actually communicating if their mother-tongue is not the local language?Need a Translator?When we go to another country, we eat all the heavenly good food, fall in love with the culture, and admire the scenery. Language and communication barriers shouldn’t get in the way of all of that. That’s why Grab’s Chat feature has got it covered!With Grab’s in-house translation solutions, any Grab passenger can send messages in their preferred language without fear of being misunderstood - or not understood at all! Their messages will be automatically translated into Bahasa Indonesia, Bahasa Melayu, Simplified Chinese, Thai, or Vietnamese depending on where they are. This applies not only apply to Grab’s transport services- GrabChat can be used when ordering GrabFood too!    Indonesia saw the highest usage of translations on a by-booking basis!&nbsp;Let’s look deeper into the tourist translation statistics for each country with the donut charts below. We can see that the most popular translation route for tourists in Indonesia was from English to Indonesian. The story is different for Singapore and Malaysia: we can see that there are translations to and from a more diverse set of languages, reflecting a more multicultural demographic.    The most popular translation routes for tourist bookings in Indonesia, Malaysia, and Singapore.&nbsp;Tap for Templates!GrabChat also provides a chat template feature. Templates are prewritten messages that you can send with just one tap! Did we mention that they are translated automatically too? Passengers and drivers can have a fast, simple, and translated conversation with each other without typing a single word- and sometimes, templates are really all you need.    Examples of chat templates, as they appear in GrabChat!&nbsp;As if all this wasn’t convenient enough, you can also make your own custom templates! Use them for those repetitive, identical messages you always seem to be sending out like telling your drivers where the hotel lobby is, or how to navigate right to your doorstep, or even to send a quick description of what you look like to make it easier for a driver to find you!  Taking a look at individual country data, tourists in Indonesia used templates the most with almost 60% of all of them using a template in their conversations at least once. Malaysia and Singapore saw lower but still sizeable utilisation rates of this feature, at 53% and 33% respectively.    Indonesia saw the highest usage of templates on a by-booking basis.&nbsp;In our analysis, we found an interesting insight! There was a positive correlation between template usage and the success rate of rides. Overall, bookings that used templates in their conversations saw 10% more completions over bookings that didn’t.  Picture This: a Hassle-free ExperienceA picture says a thousand words, and for tourists using GrabChat’s image feature, those thousand words don’t even need to be translated. Instead of typing out a description of where they are standing for pickup, they can just click, snap, and send an image!Our data revealed that GrabChat’s image functionality is most frequently used in areas where the tourist traffic is the highest. In fact, image function in GrabChat saw the most use in pickup areas such as airports, large shopping malls, public transport stations, and hotels, because it was harder for drivers to find their passengers in these crowded areas. Even with our super convenient Entrances feature, every little bit of information goes a long way to help your driver find you!  If we take it a step further and look at the actual areas within the cities where images were sent the most, we see that our initial hypothesis still holds fast.    The top 5 pickup areas per country in which images were the most prevalent in GrabChat (for tourists).&nbsp;In Singapore, we see the most images being sent out at the Downtown Core area - this area contains the majestic Marina Bay Sands, the Merlion statue, and the Esplanade, amongst other iconic attractions.In Malaysia, the highest image usage occurs at none other than the Kuala Lumpur City Centre (KLCC) itself. This area includes the Twin Towers, a plethora of malls and hotels, Bukit Bintang (a bustling and lively night-life zone), and even an aquarium.Indonesia’s top location for image chats is Kuta. A beach village in Bali, Kuta is a tourist hotspot with surfing, water parks, bars, budget-friendly yet delicious food, and numerous cultural attractions.Speak Up!Allowing for two-way communication via GrabChat empowers both passengers and drivers to improve their journeys by divulging useful information, and asking clarifying questions: how many bags do you have? Does your car accommodate my pet dog? I’m standing by the lobby with my two kids- these are the sorts of things that are talked about in GrabChat messages.During the analysis of our multitudes of wide-ranging GrabChat conversations, we picked up some pro-tips for you to get a Grab ride with even more convenience and ease, whether you’re a tourist or not:Tip #1: Did Some Shopping on Your Trip? Swamped with Bags? Send a Message to Your Driver to Let Them Know How Many Pieces of Luggage You Have with You.As one might expect, chats that have keywords such as “luggage” or “baggage” (or any other related term) occur the most when riders are going to, or leaving, an airport. Most of the tourists on GrabChat asked the drivers if there was space for all of their things in the car. Interestingly, some of them also told the drivers how to recognise them for pickup based off of the descriptions of their bags!Tip #2: Your Children Make Good Landmarks! If You’re in a Crowded Spot and You’re Worried Your Driver Can’t Find You, Drop Them a Message to Let Them Know You’re that Family with a Baby and a Little Girl in Pigtails.When it comes to children, we found that passengers mainly use them to help identify themselves to the driver. Messages like “I’m with my two kids” or “We are a family with a baby” came up numerous times, and served as descriptions to facilitate fast pickup. These sorts of chats were the most prevalent in crowded areas like airports and shopping centres.Tip #3: Don’t Get Caught Off Guard - Be Sure Your Furry Friends Have a Seat!Taking a look at pet related chats, we learned that our tourists have used GrabChat to ask clarifying questions to the driver. Passengers have likely considered that not every driver or vehicle is accommodating towards animals. The most common type of message was about whether pets are allowed in the vehicle. For example: “Is it okay if I bring a puppy?” or “I have a dog with me in a carrier, is that alright?”. Better safe than sorry! Alternatively, if you’re travelling with a pet, why not see if GrabPet is available in your country?From the chat content analysis we have learned that tourists do indeed use GrabChat to talk to their drivers about specific details of their trip. We see that the chat feature is an invaluable tool that anyone can use to clear up any ambiguities and make their journeys more pleasant.",
        "url": "/tourist-chat-data-story"
      }
      ,
    
      "bubble-tea-craze-on-grabfood": {
        "title": "Bubble Tea Craze on GrabFood!",
        "author": "lara-pureum-yimming-xuan-lee",
        "tags": "[&quot;Data&quot;, &quot;Analytics&quot;, &quot;Data Analytics&quot;]",
        "category": "",
        "content": "Bigger and More Bubble Tea!Bubble tea orders on GrabFood has been constantly and dramatically increasing with an impressive regional average growth rate of 3,000% in the year of 2018! Just look at the percentage increase over the year of 2018, across all countries!            Countries      Bubble tea growth by percentage in 2018*                  Indonesia      &gt;8500% growth from Jan 2018 to Dec 2018              Philippines      &gt;3,500% growth from June 2018 to Dec 2018              Thailand      &gt;3,000% growth from Jan 21018 to Dec 2018              Vietnam      &gt;1,500% growth from May 2018 to Dec 2018              Singapore      &gt;700% growth from May 2018 to Dec 2018              Malaysia      &gt;250% growth from May 2018 to Dec 2018      *Time period: January 2018 to December 2018, or from the time GrabFood was launched.What’s driving this growth is not just die-hard bubble tea fans who can’t go a week without drinking this sweet treat, but a growing bubble tea fan club in Southeast Asia. The number of bubble tea lovers on GrabFood grew over 12,000% in 2018 - and there’s no sign of stopping!With increasing consumer demand, how is Southeast Asia’s bubble tea supply catching up?  As of December 2018, GrabFood has close to 4,000 bubble tea outlets from a network of over 1,500 brands - a 200% growth in bubble tea outlets in Southeast Asia!  If this stat doesn’t stick, here is a map to show you how much bubble tea orders in different Southeast Asian cities have grown!  And here is a little shoutout to our star merchants including Chatime, Coco Fresh Tea &amp; Juice, Macao Imperial Tea, Ochaya, Koi Tea, Cafe Amazon, The Alley, iTEA, Gong Cha, and Serenitea.Just How Much do You Drink?On average, Southeast Asians drink 4 cups of bubble tea per person per month on GrabFood. Thai consumers top the regional average by 2 cups, consuming about six cups of bubble tea per person per month. This is closely followed by Filipino consumers who drink an average of 5 cups per person per month.  Favourite Flavours!Have a look at the dazzling array of Bubble Tea flavours available on GrabFood today and you’ll find some uniquely Southeast Asian flavours like Chendol, Durian, and Gula Melaka, as well as rare flavours like salted cream and cheese! Can you spot your favourite flavours here?  Let’s break it down by the country that GrabFood serves, and see who likes which flavours of Bubble Tea more!  Top the Toppings!Pearl seems to be the unbeatable best topping of most of the countries, except Vietnam whose No. 1 topping turned out to be Cheese Pudding! Top 3 toppings that topped your favourite bubble tea are:  Best Time for Bubble Tea!Don’t we all need a cup of sweet Bubble Tea in the afternoon to get us through the day?  Across Southeast Asia, GrabFood’s data reveals that most people order bubble tea to accompany their meals at lunch, or as a  perfect midday energiser!  ConclusionSo hazelnut or chocolate, pearl or (and) pudding (who says we can’t have the best of both worlds!)? The options are abundant and the choice is yours to enjoy!If you have a sweet tooth, or simply want to reward yourself with Southeast Asia’s most popular drink, go ahead - you are only a couple of taps away from savouring this cup full of delight",
        "url": "/bubble-tea-craze-on-grabfood"
      }
      ,
    
      "why-you-should-organise-an-immersion-trip-for-your-next-project": {
        "title": "Why You Should Organise an Immersion Trip for Your Next Project",
        "author": "sherizan-sheikh",
        "tags": "[&quot;Hyperlocal&quot;, &quot;Immersion&quot;]",
        "category": "",
        "content": "Sherizan Sheikh is a Design Lead at Grab Ventures, an incubation arm that looks at experiences beyond ride-hailing, for example, groceries, healthcare and autonomous deliveries.Grab Ventures is where exciting initiatives are birthed in Grab. From strategic partnerships like GrabFresh, Grab’s first on-demand grocery delivery service, to exploratory concepts such as on-demand e-scooter rentals, there has never been a more exciting time to be in this unique space.  &nbsp;In my role as Design Lead for Grab Ventures, I juggle between both sides of the coin and whether it’s a partnership or exploratory concept, I ask myself:“How do I Know Who My Consumers are, and What are Their Pain Points?”So I like to answer that question by starting with traditional research methods like desktop research and surveys, just to name a few. At Grab, it’s usually not enough to answer those questions.That said, I find that some of the best insights are formed from immersion trips.In one sentence, an immersion trip is getting deeply involved in a user’s life by understanding him or her through observation and conversation.    Our CEO, Anthony Tan, picking items for a consumer, on an immersion trip.&nbsp;For designers and researchers in Singapore, it plucks you out of your everyday reality and drops you into someone else’s, somewhere else, where 99.9% of the time, everything you expect and anticipate gets thrown out in a matter of minutes. I’ve trained myself to switch my mindset, go back to basics, and learn (or relearn) everything I need to know about the country I’d be visiting even if I’ve been there countless times.  Fun fact: In 2018, I spent about 100 days in Indonesia. That means roughly 30% of 2018 was spent on the ground, observing, shadowing, interviewing people (and getting stuck in traffic) and loving it.Why Immersions?Understanding one’s country, culture and her people is something that gets me excited as I continuously build empathy visit upon a visit, interview after interview.I remembered one time during an immersion trip, we interviewed locals at different supermarkets to learn and understand their motivations: why they prefer to visit the supermarket vs purchasing them online. One of our hypotheses was that the key motivator for Indonesians to buy groceries online must be down to convenience. We were wrong.It boiled down to 2 key factors.1) Freshness: We found out that many of the locals still felt the need to touch and feel the products before they buy. There were many instances where they felt the need to touch the fresh produce on the shelves, cutting off a piece of fruit or even poking the eyes of the fish to check its freshness.  &nbsp;2) Price: The de-facto for most locals as they are price-sensitive. Every decision was made with the price tag in mind. They are willing to travel far, spend the time to go through the traffic just to get to the wet or supermarket that offers the lowest prices and value for money. Through observations, while shadowing at a local wet market, we also found something interesting. Most of the wet market vendors are getting WhatsApp messages from their regular consumers seeking fresh produce and making orders. The transactions were mostly via e-wallets or bank transfers. The vendors then packed them and get bike drivers to help with the delivery. I couldn’t have gotten this valuable information if I was just sitting at my desk.An immersion trip is an excellent opportunity to learn about our consumers and the meaning behind their behaviours. There is only so much we can learn from white papers and reports. As soon as you are in the same environment as your users, seeing your users do everyday errands or acts, like grocery shopping or hopping on a bike, feeling their frustrations and experiencing them yourself, you’ll get so much more fruitful and valuable insights to help shape your next product. (Or even, improve an existing one!)    My colleagues trying to blend in.&nbsp;Now that I’ve sold you on this idea, here are some tips on how to plan and execute effective immersion trips, share your findings and turn them into actionable insights for your team and stakeholders.Pro Tip #1 - Generate a HypothesisGenerating a hypothesis is a valuable exercise. It enables you to focus on the “wants vs. needs” and to validate your assumptions beyond desktop research. Be sure to get your core team members together, including Business, Ops and Tech, to generate a hypothesis. I’ll give an example below.Pro Tip #2 - Have Short Immersion Days with a Debrief at the End for EveryoneScheduling really depends on your project. I have planned for trips that are from a few hours to up to fourteen days long. Be sure not to have too many locations in a single day and spread them out evenly in case there are unexpected roadblocks such as traffic jams that might contribute to rushed research.Do include Brief and Debrief sessions into your schedule. I’d recommend shorter immersion days so that you have enough energy left for the critical Debrief session at the end of the day. The structure should be kept very simple with focus of collating ALL observations from the contextual inquiries you did into writing. It’s actually up to you how you structure your document.    Be prepared for the unexpected.&nbsp;Pro Tip #3 - Recce Locations BeforehandOnce you’ve nailed down the locations, it is essential for you to get a local resident to recce the places first. In Southeast Asia, more often than so would you realise that information found online is unreliable and misleading, so doing a physical recce will save you a lot of time.I had experienced a few time-wasting incidents when we did not expect specific locations to be what was intended. For example, while on our grocery-run, we wanted to visit a local wet market that opens only very early in the morning. We got up at 5 am, drove about 1.5 hours and only to realise the wet market is not open to the public and we eventually got chased out by the security guards.Pro Tip #4 - Never Assume a Consumer’s Journey(Even though you’ve experienced it before as a consumer)One of the most important processes throughout a product life cycle is to understand a consumer’s journey. It’s particularly important to understand the journey if we are not familiar with the actual environment. Take our GrabFresh service as an example. It’s a complex journey that happens behind the scenes. Desktop research might not be enough to fully validate the journey hence, an immersion trip that allows you to be on the field will ensure you go through the lifecycle of the entire process to observe and note all the phases that happen in the real environment.  &nbsp;Pro Tip #5 - Be 100% Sure of Your Open-ended, Non-leading Questions that will Validate Your HypothesisThis part is an essential piece to the quality of your immersion outcome. Not spending enough time crafting or vetting the questions thoroughly might end up with skewed insights and could jeopardise your entire immersion. Please be sure your questions links up with your hypothesis and provide backup questions to support your assumptions.For example, don’t suggest answers in questions.Bad: “Why do you like this supermarket? Cheap? Convenient?”Good: “Tell me why you chose this particular supermarket?”Pro Tip #6 - Break into Smaller Groups of 2 to 3. Dress Comfortably and Like a Local. Keep Your Expensive Belongings Out of Sight.During my recent trip, I visited a lot of places that unknowingly had very tight security. One of the mistakes I made was going as a group of 6 (foreign-looking, and - okay -  maybe a little touristy with our appearances and expensive gadgets).Out of nowhere, once we started approaching consumers for interviews, and snapping photos with our cameras and phones, we could see the security teams walking towards us. Unfortunately, we were asked to leave the premises when we could not provide a permit.As luck would have it, we eyed a few consumers and approached them when they were further away from the original location. Success!Pro Tip #7 - Find Translators with People Skills and Interview Experience.Most of my immersion trips are overseas, where English is not the main language. I get annoyed at myself for not being able to interview non-English speaking consumers. Having seasoned, outgoing translators does help a lot! If you feel awkward standing around waiting for a translated answer, feel free to step away and let the translator interview the consumer without feeling pressured. Be sure it’s all recorded for transcription later.Insights + Action Plan = StrategyFindings are significant, it’s the basis of everything that you do while you are in immersion. But what’s more important is the ability to connect those dots and extract value from them. It’s similar to how we can amass tons of raw data but entirely pointless if nothing is done with it.A good strategy usually comes from good insights that are actionable.For example, we found out that a % of consumers that we interviewed did not know that GrabFresh has a pool of professional shoppers who pick grocery items for consumers. Their impression was that a driver would receive their order, drive to the location, get out of their vehicle and go into the store to do the picking. That’s not right. It hinders consumers from making their first purchase through the app.    Observing a personal shopper interacting with Grab driver-partner.&nbsp;  So, in this case, our hypothesis was: if consumers are aware of personal shoppers, the number of orders will increase.This opinion was a shared one that may have had an impact on our business. So we needed to take this back to the team, look at the data, brainstorm, and come up with a great strategy to improve the perception and its impact on our business (whether good or bad).Wrapping UpAfter a full immersion, it is always important to ask each and every member of some of these questions:“What went well? What did you learn?”“What can be improved? If you could change one thing, what would it be?”I’d usually document them and have a reflection for myself so that I can pick up what worked, what didn’t and continue to improve for my next immersion trip.Following the Double Diamond framework, immersion trips are part of the “Discover”phase where we gather consumer insights. Typically, I follow up with a Design sprint workshop where we start framing the problems. This is where we have a session where experts and researchers share their domain knowledge and research insights uncovered from various methodologies including immersions.Then, hopefully, we will have some actionable changes that we can execute confidently.So, good luck, bring some sunblock and see you on the ground!If you’d like to connect with Sherizan, you can find him on LinkedIn.",
        "url": "/why-you-should-organise-an-immersion-trip-for-your-next-project"
      }
      ,
    
      "preventing-pipeline-calls-from-crashing-redis-clusters": {
        "title": "Preventing Pipeline Calls from Crashing Redis Clusters",
        "author": "michael-cartmelljiahao-huangsandeep-kumar",
        "tags": "[&quot;Grab&quot;, &quot;Back End&quot;, &quot;Redis&quot;, &quot;Redis Cluster&quot;, &quot;Go&quot;]",
        "category": "",
        "content": "IntroductionOn Feb 15th, 2019, a slave node in Redis, an in-memory data structure storage, failed requiring a replacement. During this period, roughly only 1 in 21 calls to Apollo, a primary transport booking service, succeeded. This brought Grab rides down significantly for the one minute it took the Redis Cluster to self-recover. This behaviour was totally unexpected and completely breached our intention of having multiple replicas.This blog post describes Grab’s outage post-mortem findings.Understanding the InfrastructureWith Grab’s continuous growth, our services must handle large amounts of data traffic involving high processing power for reading and writing operations. To address this significant growth, reduce handler latency, and improve overall performance, many of our services use Redis - a common in-memory data structure storage - as a cache, database, or message broker. Furthermore, we use a Redis Cluster, a distributed implementation of Redis, for shorter latency and higher availability.Apollo is our driver-side state machine. It is on almost all requests’ critical path and is a primary component for booking transport and providing great service for consumer bookings. It stores individual driver availability in an AWS ElastiCache Redis Cluster, letting our booking service efficiently assign bookings to drivers. It’s critical to keep Apollo running and available 24/7.  Because of Apollo’s significance, its Redis Cluster has 3 shards each with 2 slaves. It hashes all keys and, according to the hash value, divides them into three partitions. Each partition has two replications to increase reliability.We use the Go-Redis client, a popular Redis library, to direct all written queries to the master nodes (which then write to their slaves) to ensure consistency with the database.  For reading related queries, engineers usually turn on the ReadOnly flag and turn off the RouteByLatency flag. These effectively turn on ReadOnlyFromSlaves in the Grab gredis3 library, so the client directs all reading queries to the slave nodes instead of the master nodes. This load distribution frees up master node CPU usage.  When designing a system, we consider potential hardware outages and network issues. We also think of ways to ensure our Redis Cluster is highly efficient and available; setting the above-mentioned flags help us achieve these goals.Ideally, this Redis Cluster configuration would not cause issues even if a master or slave node breaks. Apollo should still function smoothly. So, why did that February Apollo outage happen? Why did a single down slave node cause a 95+% call failure rate to the Redis Cluster during the dim-out time?Let’s start by discussing how to construct a local Redis Cluster step by step, then try and replicate the outage. We’ll look at the reasons behind the outage and provide suggestions on how to use a Redis Cluster client in Go.How to Set Up a Local Redis Cluster1. Download and install Redis from here.2. Set up configuration files for each node. For example, in Apollo, we have 9 nodes, so we need to create 9 files like this with different port numbers(x).// file_name: node_x.conf (do not include this line in file)port 600xcluster-enabled yescluster-config-file cluster-node-x.confcluster-node-timeout 5000appendonly yesappendfilename node-x.aofdbfilename dump-x.rdb3. Initiate each node in an individual terminal tab with:$PATH/redis-4.0.9/src/redis-server node_1.conf4. Use this Ruby script to create a Redis Cluster. (Each master has two slaves.)$PATH/redis-4.0.9/src/redis-trib.rb create --replicas 2127.0.0.1:6001..... 127.0.0.1:6009&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6001)M: 7b4a5d9a421d45714e533618e4a2b3becc5f8913 127.0.0.1:6001   slots:0-5460 (5461 slots) master   2 additional replica(s)S: 07272db642467a07d515367c677e3e3428b7b998 127.0.0.1:6007   slots: (0 slots) slave   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8S: 65a9b839cd18dcae9b5c4f310b05af7627f2185b 127.0.0.1:6004   slots: (0 slots) slave   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913M: 05363c0ad70a2993db893434b9f61983a6fc0bf8 127.0.0.1:6003   slots:10923-16383 (5461 slots) master   2 additional replica(s)S: a78586a7343be88393fe40498609734b787d3b01 127.0.0.1:6006   slots: (0 slots) slave   replicates 72306f44d3ffa773810c810cfdd53c856cfda893S: e94c150d910997e90ea6f1100034af7e8b3e0cdf 127.0.0.1:6005   slots: (0 slots) slave   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8M: 72306f44d3ffa773810c810cfdd53c856cfda893 127.0.0.1:6002   slots:5461-10922 (5462 slots) master   2 additional replica(s)S: ac6ffbf25f48b1726fe8d5c4ac7597d07987bcd7 127.0.0.1:6009   slots: (0 slots) slave   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913S: bc56b2960018032d0707307725766ec81e7d43d9 127.0.0.1:6008   slots: (0 slots) slave   replicates 72306f44d3ffa773810c810cfdd53c856cfda893[OK] All nodes agree about slots configuration.5. Finally, we try to send queries to our Redis Cluster, e.g.$PATH/redis-4.0.9/src/redis-cli -c -p 6001 hset driverID 100 state available updated_at 11111What Happens When Nodes Become Unreachable?Redis Cluster ServerAs long as the majority of a Redis Cluster’s masters and at least one slave node for each unreachable master are reachable, the cluster is accessible. It can survive even if a few nodes fail.Let’s say we have N masters, each with K slaves, and random T nodes become unreachable. This algorithm calculates the Redis Cluster failure rate percentage:if T &lt;= K:        availability = 100%else:        availability = 100% - (1/(N*K - T))If you successfully built your own Redis Cluster locally, try to kill any node with a simple command-c. The Redis Cluster broadcasts to all nodes that the killed node is now unreachable, so other nodes no longer direct traffic to that port.If you bring this node back up, all nodes know it’s reachable again. If you kill a master node, the Redis Cluster promotes a slave node to a temp master for writing queries.$PATH/redis-4.0.9/src/redis-server node_x.confWith this information, we can’t answer the big question of why a single slave node failure caused an over 95% failure rate in the Apollo outage. Per the above theory, the Redis Cluster should still be 100% available. So, the Redis Cluster server could properly handle an outage, and we concluded it wasn’t the failure rate’s cause. So we looked at the client side and Apollo’s queries.Golang Redis Cluster Client &amp; Apollo QueriesApollo’s client side is based on the Go-Redis Library.During the Apollo outage, we found some code returned many errors during certain pipeline GET calls. When Apollo tried to send a pipeline of HMGET calls to its Redis Cluster, the pipeline returned errors.First, we looked at the pipeline implementation code in the Go-Redis library. In the function defaultProcessPipeline, the code assigns each command to a Redis node in this line err:=c.mapCmdsByNode(cmds, cmdsMap).func (c *ClusterClient) mapCmdsByNode(cmds []Cmder, cmdsMap *cmdsMap) error {state, err := c.state.Get()        if err != nil {                setCmdsErr(cmds, err)                returnerr        }        cmdsAreReadOnly := c.cmdsAreReadOnly(cmds)        for_, cmd := range cmds {                var node *clusterNode                var err error                if cmdsAreReadOnly {                        _, node, err = c.cmdSlotAndNode(cmd)                } else {                        slot := c.cmdSlot(cmd)                        node, err = state.slotMasterNode(slot)                }                if err != nil {                        returnerr                }                cmdsMap.mu.Lock()                cmdsMap.m[node] = append(cmdsMap.m[node], cmd)                cmdsMap.mu.Unlock()        }        return nil}Next, since the readOnly flag is on, we look at the cmdSlotAndNode function. As mentioned earlier, you can get better performance by setting readOnlyFromSlaves to true, which sets RouteByLatency to false. By doing this, RouteByLatency will not take priority and the master does not receive the read commands.func (c *ClusterClient) cmdSlotAndNode(cmd Cmder) (int, *clusterNode, error) {        state, err := c.state.Get()        if err != nil {                return 0, nil, err        }        cmdInfo := c.cmdInfo(cmd.Name())        slot := cmdSlot(cmd, cmdFirstKeyPos(cmd, cmdInfo))        if c.opt.ReadOnly &amp;&amp; cmdInfo != nil &amp;&amp; cmdInfo.ReadOnly {                if c.opt.RouteByLatency {                        node, err:= state.slotClosestNode(slot)                        return slot, node, err                }                if c.opt.RouteRandomly {                        node:= state.slotRandomNode(slot)                        return slot, node, nil                }                node, err:= state.slotSlaveNode(slot)                return slot, node, err        }        node, err:= state.slotMasterNode(slot)        return slot, node, err}Now, let’s try and better understand the outage.  When a slave becomes unreachable, all commands assigned to that slave node fail.  We found in Grab’s Redis library code that a single error in all cmds could cause the entire pipeline to fail.  In addition, engineers return a failure in their code if err != nil. This explains the high failure rate during the outage.func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {        results := make([]gredisapi.ReplyPair, len(cmds))        var err error        for idx, cmd := range cmds {                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()                if results[idx].Err == goredis.Nil {                        results[idx].Err = nil                        continue                }                if err == nil &amp;&amp; results[idx].Err != nil {                        err = results[idx].Err                }        }        return results, err}Our next question was, “Why did it take almost one minute for Apollo to recover?”. The Redis Cluster broadcasts instantly to its other nodes when one node is unreachable. So we looked at how the client assigns jobs.When the Redis Cluster client loads the node states, it only refreshes the state once a minute. So there’s a maximum one minute delay of state changes between the client and server. Within that minute, the Redis client kept sending queries to that unreachable slave node.func (c *clusterStateHolder) Get() (*clusterState, error) {        v := c.state.Load()        if v != nil {                state := v.(*clusterState)                if time.Since(state.createdAt) &gt; time.Minute {                        c.LazyReload()                }                return state, nil        }        return c.Reload()}What happened to the write queries? Did we lose new data during that one min gap? That’s a very good question! The answer is no since all write queries only went to the master nodes and the Redis Cluster client with a watcher for the master nodes. So, whenever any master node becomes unreachable, the client is not oblivious to the change in state and is well aware of the current state. See the Watcher code.How to Use Go Redis Safely?Redis Cluster ClientOne way to avoid a potential outage like our Apollo outage is to create another Redis Cluster client for pipelining only and with a true RouteByLatency value. The Redis Cluster determines the latency according to ping calls to its server.In this case, all pipelining queries would read through the master nodesif the latency is less than 1ms (code), and as long as the majority side of partitions are alive, the client will get the expected results. More load would go to master with this setting, so be careful about CPU usage in the master nodes when you make the change.Pipeline UsageIn some cases, the master nodes might not handle so much traffic. Another way to mitigate the impact of an outage is to check for  errors on individual queries when errors happen in a pipeline call.In Grab’s Redis Cluster library, the function Pipeline(PipelineReadOnly) returns a response with an error for individual reply.func (c *clientImpl) Pipeline(ctx context.Context, argsList [][]interface{}) ([]gredisapi.ReplyPair, error) {        defer c.stats.Duration(statsPkgName, metricElapsed, time.Now(), c.getTags(tagFunctionPipeline)...)        pipe := c.wrappedClient.Pipeline()        cmds := make([]goredis.Cmder, len(argsList))        for i, args := range argsList {                cmd := goredis.NewCmd(args...)                cmds[i] = cmd                _ = pipe.Process(cmd)        }        _, _ = pipe.Exec()        return c.wrappedClient.getResultFromCommands(cmds)}func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {        results := make([]gredisapi.ReplyPair, len(cmds))        var err error        for idx, cmd := range cmds {                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()                if results[idx].Err == goredis.Nil {                        results[idx].Err = nil                        continue                }                if err == nil &amp;&amp; results[idx].Err != nil {                        err = results[idx].Err                }        }        return results, err}type ReplyPair struct {        Value interface{}        Err   error}Instead of returning nil or an error message when err != nil, we could check for errors for each result so successful queries are not affected. This might have minimised the outage’s business impact.Go Redis Cluster LibraryOne way to fix the Redis Cluster library is to reload nodes’ status when an error happens.In the go-redis library, defaultProcessor has this logic, which can be applied to defaultProcessPipeline.In ConclusionWe’ve shown how to build a local Redis Cluster server, explained how Redis Clusters work, and identified its potential risks and solutions. Redis Cluster is a great tool to optimise service performance, but there are potential risks when using it. Please carefully consider our points about how to best use it. If you have any questions, please ask them in the comments section.",
        "url": "/preventing-pipeline-calls-from-crashing-redis-clusters"
      }
      ,
    
      "poi-entrances-venues-door-to-door": {
        "title": "Guiding You Door-to-Door via Our Superapp!",
        "author": "neeraj-mishralara-pureum-yimsufyan-selametnagur-hassansummit-saurav",
        "tags": "[&quot;Grab&quot;, &quot;Data&quot;, &quot;Tech&quot;, &quot;Maps&quot;, &quot;App&quot;]",
        "category": "",
        "content": "Remember landing at an airport or going to your favourite mall and the hassle of finding the pickup spot when you booked a cab? When there are about a million entrances, it can get particularly annoying trying to find the right pickup location!Rolling out across Southeast Asia  is a brand new booking experience from Grab, designed  to make it easier for you to make a booking at large venues like airports, shopping centers, and tourist destinations! With the new booking flow, it will not only be easier to select one of the pre-designated Grab pickup points, you can also find text and image directions to help you navigate your way through the venue for a smoother rendezvous with your driver!Inspiration Behind the WorkFinding your pick-up point closest to you, let alone predicting it, is incredibly challenging, especially when you are inside huge buildings or in crowded areas. Neeraj Mishra, Product Owner for Places at Grab explains: “We rely on GPS-data to understand user’s location which can be tricky when you are indoors or surrounded by skyscrapers. Since the satellite signal has to go through layers of concrete and steel, it becomes weak which adds to the inaccuracy. Furthermore, ensuring that passengers and drivers have the same pick-up point in mind can be tricky, especially with venues that have multiple entrances. ”    Grab’s data analysis revealed that “rendezvous distance” (walking distance between the selected pick-up point and where the car is waiting) is more than twice the Grab average when the booking is made from large venues such as airports.To solve this issue, Grab launched “Entrances” (the green dots on the map) last year, which lists the various pick-up points available at a particular building, and shows them on the map, allowing users to easily choose the one closest to them, and ensuring their drivers know exactly where they want to be picked up from. Since then, Grab has created more than 120,000 such entrances, and we are delighted to inform you that average of rendezvous distances across all  countries have been steadily going down!  One Problem RemainedBut there was still one common pain-point to be solved. Just because a passenger has selected the pick-up point closest to them, doesn’t mean it’s easy for them to find it. This is particularly challenging at very large venues like airports and shopping centres, and especially difficult if the passenger is unfamiliar with the venue, for example - a tourist landing at Jakarta Airport for the very first time. To deliver an even smoother booking and pick-up experience, Grab has rolled out a new feature called Venues - the first in the region - that will give passengers in-app photo and text directions to the pick-up point closest to them.Let’s Break it Down! How Does it Work?Whether you are a local or a foreigner on holiday or business trip, fret not if you are not too familiar with the place that you are in!Let’s imagine that you are now at Singapore Changi Airport: your new booking experience will look something like this!Step 1: Fire the Grab app and click on Transport. You will see a welcome screen showing you where you are!  Step 2: On booking screen, you will see a new pickup menu with a list of available pickup points. Confirm the pickup point you want and make the booking!  Step 3: Once you’ve been allocated a driver, tap on the bubble to get directions to your pick-up point!  Step 4: Follow the landmarks and walking instructions and you’ve arrived at your pick-up point!  Curious About How We Got This Done?Data-driven DecisionsBased on a thorough data analysis of historical bookings, Grab identified key venues across our markets in Southeast Asia. Then we dispatched our Operations team to the ground, to identify all pick up points and perform detailed on-ground survey of the venue.Operations Team’s Leg WorkNagur Hassan, Operations Manager at Grab, explains the process: “For the venue survey process, we send a team equipped with the tools required to capture the details, like cameras, wifi and bluetooth scanners etc. Once inside the venue, the team identifies strategic landmarks and clear direction signs that are related to drop-off and pick-up points. Team also captures turn-by-turn walking directions to make it easier for Grab users to navigate – For instance, walk towards Starbucks and take a left near H&amp;M store. All the photos and documentations taken on the sites are then brought back to the office for further processing.”Quality AssuranceOnce the data is collected, our in-house team checks the quality of the images and data. We also mask people’s faces and number plates of the vehicles to hide any identity-related information. As of today, we have collected 3400+ images for 1900+ pick up points belonging to 600 key venues! This effort took more than 3000 man-hours in total! And we aim to cover more than 10,000 such venues across the region in the next few months.This is Only the BeginningWe’re constantly striving to improve the location accuracy of our passengers by using advanced Machine Learning and constant feedback mechanism. We understand GPS may not always be the most accurate determination of your current location, especially in crowded areas and skyscraper districts. This is just the beginning and we’re planning to launch some very innovative features in the coming months! So stay tuned for more!",
        "url": "/poi-entrances-venues-door-to-door"
      }
      ,
    
      "loki-dynamic-mock-server-http-tcp-testing": {
        "title": "Loki, a Dynamic Mock Server for HTTP/TCP Testing",
        "author": "thuy-nguyentmayank-guptavishal-prakashvineet-nair",
        "tags": "[&quot;Back End&quot;, &quot;Service&quot;, &quot;Mobile&quot;, &quot;Testing&quot;]",
        "category": "",
        "content": "BackgroundIn a previous article we introduced Mockers - an innovative tool for local box testing at Grab. Mockers used a Shift Left testing strategy, making testing more effective and cheaper for development teams. Mockers’ popularity and success motivated us to create Loki - a one-stop dynamic mock server for local box testing of mobile apps.There are some unique challenges in mobile apps testing at Grab. End-to-end testing of an app is difficult due to high dependency on backend services and other apps. Staging environment, which hosts a plethora of backend services, is tough to manage and maintain. Issues such as staging downtime, configuration mismatches, and data corruption can affect staging adding to the testing woes. Moreover, our apps are fairly complex, utilising multiple transport protocols such as HTTP, HTTPS, TCP for various business flows.The business flows are also complex, requiring exhaustive set up such as credit card payments set up, location spoofing, etc resulting in high maintenance costs for automated testing. Loki simulates these flows and developers can easily test use cases that take longer to set up in a real backend staging.Loki is our attempt to address challenges in mobile app testing by turning every developer local box into a full fledged pseudo backend environment where all mobile workflows can be tested without any external dependencies. It mocks backend services on developer local boxes, decoupling the mobile apps from real backend services, which provides several advantages such as:No need to deploy frequently to stagingTesting is blocked if the app receives a bad response from staging. In these cases, code changes have to be deployed on staging to fix issues before resuming tests. In contrast, using Loki lets developers continue testing without any immediate need to deploy code changes to staging.Allows parallel frontend and backend developmentLoki acts as a mock backend service when the real backend is still evolving. It lets the frontend development run in parallel with backend development.Overcome time limitationsIn a one week regression-and-release scenario, testing time is limited. However, the application UI rendering and functionality still needs reasonable testing. Loki lets developers concentrate on testing in the available time instead of fixing dependencies on backend services.Loki - Grab’s Solution to Simplify Mobile Apps TestingAt Grab, we have multiple mobile apps that are dependent on each other. For example, our passenger and driver apps are two sides of a coin; the driver gets a job card only when a passenger requests a booking. These apps are developed by different teams, each with its own release cycle. This can make it tricky to confidently and repeatedly test the whole business flow across apps. Apps also depend on multiple backend services to execute a booking or food order and communicate over different protocols.Here’s a look at how our mobile apps interact with backend services over different protocols:  Loki is a dynamic mock server, written in Golang, running in a Docker container on the local box or in CI. It is easy to set up and run through standard Docker commands. In the context of mobile app testing, it plays the role of backend services, so you no longer need to set up an extensive staging environment.The Loki architecture looks like this:  The Technical Challenges We Had to OvercomeWe wanted a comprehensive mocking solution so that teams don’t need to integrate multiple tools to achieve independent testing. It turned out that mocking TCP was most challenging because:  It is a long running client-server connection, and it doesn’t follow an HTTP-like request/response pattern.  Messages can be sent to the app without an incoming request as well, hence we had to expose a way via Loki to set a mock expectation which can send messages to the app without any request triggering it.  As TCP is a long running connection, we needed a way to delimit incoming requests so we know when we can truncate and deserialise the incoming request into JSON.We engineered the Loki backend to support both HTTP and TCP protocols on different ports. Yet, the mock expectations are set up using RESTful APIs over HTTP for both protocols. A single point of entry for setting expectations made it more intuitive for our developers.An in-memory cron implementation pushes scheduled messages to the app over a TCP connection. This enabled testing of complex use cases such as drivers getting new job cards, driver and passenger chat workflows, etc. The delimiter for TCP protocol is configurable at start up, so each team can decide when to truncate the request.To enable Loki on our CI, we had to reduce its memory footprint. Hence, we built Loki with pluggable storages. MySQL is used when running on local and on CI we switch seamlessly to in-memory cache or Redis.For testing apps locally, developers must validate complex use cases such as:      Payment related flows, which require the response to include the same payment ID as sent in the request. This is a case of simple mapping of request fields in the response JSON.        Flows requiring runtime logic execution. For example, a job card sent to a driver must have a valid timestamp, requiring runtime computation on Loki.  To support these cases and many more, we added JavaScript injection capability to Loki. So, when we set an expectation for an HTTP request/response pair or for TCP events, we can specify JavaScript for computing the dynamic response. This is executed in a sandbox by an in-house JS execution library.Grab follows a transactional workflow for bookings. Over the life of a ride, bookings go through different statuses. So, Loki had to address multiple HTTP requests to the same endpoint returning different responses. This feature is required for successfully mocking a whole ride end-to-end.Loki uses  an HTTP API “httpTimesAndOrder” for this feature. For example, using “httpTimesAndOrder”, you can configure the same status endpoint (/ride/status) to return different ride statuses such as “PICKING” for the first five requests, “IN_RIDE” for the next three requests, and so on.Now, let’s look at how to use Loki to mock HTTP requests and TCP events.Mocking HTTP RequestsTo mock HTTP requests, developers first point their app to send requests to the Loki mock server. Then, they set up expectations for all requests sent to the Loki mock server.  For example, the Passenger app calls an HTTP dependency GET /closeby/drivers/ to get nearby drivers. To mock it with Loki, you set an expected response on the Loki mock server. When the GET /closeby/drivers/ request is actually made from the Passenger app, Loki returns the set response.This snippet shows how to set an expected response for the GET /closeby/drivers/request:Loki API: POST `/api/v1/expectations`Request Body :{  \"uriToMock\": \"/closeby/drivers\",  \"method\": \"GET\",  \"response\": {    \"drivers\": [      1001,      1002,      1010    ]  }}Workflow for Setting Expectations and Receiving Responses  Mocking TCP EventsDevelopers point their app to Loki over a TCP connection and set up the TCP expectations. Loki then generates scheduled events such as sending push messages (job cards, notifications, etc) to the apps pointing at Loki.For example, if the driver app, after it starts, wants to get a job card, you can set an expectation in Loki to push a job card over the TCP connection to the driver app after a scheduled time interval.This snippet shows how to set the TCP expectation and schedule a push message:Loki API: POST `/api/v1/tcp/expectations/pushmessage`Request Body :{  \"name\": \"samplePushMsg\",  \"msgSequence\": [    {      \"messages\": {        \"body\": {          \"jobCardID\": 1001        }      }    },    {      \"messages\": {        \"body\": {          \"jobCardID\": 1002        }      }    }  ],  \"schedule\": \"@every 1m\"}Workflow for Scheduling a Push Message Over TCP  Some Sample Use CasesNow that you know about Loki, let’s look at some sample use cases.Generating a Custom Response at RuntimeOur first example is customising a runtime response for both HTTP and TCP requests. This is helpful when developers need dynamic responses to requests. For example, you can add parameters from the request URL or request body to the runtime response.It’s simple to implement this with a JavaScript function. Assume you want to embed a message parameter in the request URL to the response. To do this, you first use a POST method to set up the expectation (in JSON format) for the request on Loki:Loki API: POST `/api/v1/feature/expectations`Request Body :{  \"expectations\": [{    \"name\": \"Sample call\",    \"desc\": \"v1/test/{name}\",    \"tags\": \"v1/test/{name}\",    \"resource\": \"/v1/test?name=user1\",    \"verb\": \"POST\",    \"response\": {      \"body\": \"{ \\\"msg\\\": \\\"Hi \\\"}\",      \"status\": 200    },    \"clientOptions\": {\"javascript\": \"function main(req, resp) { var url = req.RequestURI; var captured = /name=([^&amp;]+)/.exec(url)[1]; resp.msg =  captured ? resp.msg + captured : resp.msg + 'myDefaultValue'; return resp }\"    },    \"isActive\": 1  }]}When Loki receives the request, the JavaScript function used in the clientOptionskey, adds name to the response at runtime. For example, this is the request’s fixed response:{    \"msg\": \"Hi \"}But, after using the JavaScript function to add the URL parameter, the dynamic response is:{    \"msg\": \"Hi user1\"}Similarly, you can use JavaScript to add other dynamic responses such as modifying the response’s JSON array, adding parameters to push messages, etc.Defining a Response Sequence for Mocked API EndpointsHere’s another interesting example - defining the response sequence for API endpoints.A response sequence is useful when you need different responses from the same API endpoint. For example, a status endpoint should return different ride statuses such as ‘allocating’, ‘allocated’, ‘picking’, etc. depending on the stage of a ride.To do this, developers set up their HTTP expectations on Loki. Then, they easily define the response sequence for an API endpoint using a Loki POST method.In this example:  times - specifies the number of times the same response is returned.  after - specifies one or more expectations that must match before a specified expectation is matched.Here, the expectations are matched in this sequence when a request is made to an endpoint - Allocating &gt; Allocated &gt; Pickuser &gt; Completed. Further, Completed is set to two times, so Loki returns this response two times.Loki API: POST `/api/v1/feature/sequence`Request Body :  \"httpTimesAndOrder\": [      {          \"name\": \"Allocating\",          \"times\": 1      },      {          \"name\": \"Allocated\",          \"times\": 1,          \"after\": [\"Allocating\"]      },      {          \"name\": \"Pickuser\",          \"times\": 1,          \"after\": [\"Allocated\"]      },      {          \"name\": \"Completed\",          \"times\": 2,          \"after\": [\"Pickuser\"]      }  ]}In ConclusionSince Loki’s inception, we have set up a full range CI with proper end-to-end app UI tests and, to a great extent, decoupled our app releases from the staging backend. This improved delivery cycles, and we did faster bug catching and more exhaustive testing. Moreover, both developers and QAs can easily play with apps to perform exploratory testing as well as manual functional validations. Teams are also using Loki to run automated scripts (Espresso and XCUItests) for validating the mobile app pages.Loki’s adoption is growing steadily at Grab. With our frequent release of new mobile app features, Loki helps teams meet our high quality bar and achieve huge productivity gains.If you have any feedback or questions on Loki, please leave a comment.",
        "url": "/loki-dynamic-mock-server-http-tcp-testing"
      }
      ,
    
      "correcting-restaurant-locations-harnessing-wisdom-of-the-crowd": {
        "title": "How We Harnessed the Wisdom of Crowds to Improve Restaurant Location Accuracy",
        "author": "pravin-kakar",
        "tags": "[&quot;Data Science&quot;]",
        "category": "",
        "content": "While studying GPS ping data to understand how long our driver-partners needed to spend at restaurants during a GrabFood delivery, we came across an interesting observation. We realised that there was a significant proportion of restaurants where our driver-partners were waiting for abnormally short durations, often for just seconds.Considering that it typically takes a driver a few minutes to enter the restaurant, pick up the order and then leave, we decided to dig further into this phenomenon. What we uncovered was that these super short pit stops were restaurants that were registered at incorrect coordinates within the system due to reasons such as the restaurant had moved to a new location, or human error during onboarding the restaurants. Incorrectly registered locations within our system impact all involved parties - eaters may not see the restaurant because it falls outside their delivery radius or they may see an incorrect ETA, drivers may have trouble finding the restaurant and may end up having to cancel the order, and restaurants who may get fewer orders without really knowing why. So we asked ourselves - how can we improve this situation by leveraging the wealth of data that we have? The SolutionOne of the biggest advantages we have is the huge driver-partner fleet we have on the ground in cities across Southeast Asia. They know the roads and cities like the back of their hand, and they are resourceful. As a result, they are often able to find the restaurants and complete orders even if the location was registered incorrectly. Knowing this, we looked at GPS pings and timestamps from these drivers, and combined this information with when they indicated that they have ordered or collected food from the restaurant. This is then used to infer the “pick-up location” from which the food was collected. Inferring this location is not so straightforward though. GPS ping quality can vary significantly across devices and will be affected by whether the device is outdoors or indoors (e.g. if the restaurant is inside a mall). Hence we compute metrics from times and distances between pings, ping frequency and ping quality to filter out orders where the GPS quality is determined to be sub-par. The thresholds for such filtering are determined based on a statistical analysis of orders by regions and times of day. One of the outcomes of such an analysis is that we deemed it acceptable to consider a driver “at” a restaurant, if their GPS ping falls within a predetermined radius of the registered location of the restaurant. However, knowing that a driver is at the restaurant does not necessarily tell us “when” he or she  is actually at the restaurant. See the following figure for an example.   &nbsp;As you can see from the area covered by the green circle, there are 3 distinct occurrences or “streaks” when the driver can be determined to be at the restaurant location - once when they are approaching the restaurant from the southwest before taking two right turns, then again when they are actually at the restaurant coming in from the northeast, and again when they leave the restaurant heading southwest before making a U-turn and then heading northeast. In this case, if the driver indicates that they have collected the food during the second streak, chronology is respected - the driver reaches the restaurant, the driver collects the food, the driver leaves the restaurant. However if the driver indicates that they have collected the food during one of the other streaks, that is an invalid pick-up even though it is “at” the restaurant.Such potentially invalid pick-ups could result in noisy estimates of restaurant location, as well as hamper us in our parent task of accurately estimating how long drivers need to wait at restaurants. Therefore, we modify the definition of the driver being at the restaurant to only include the time of the longest streak i.e. the time when the driver spent the longest time within the registered location radius. Extending this across multiple orders and drivers, we can form a cluster of pick-up locations (both “at” and otherwise) for each restaurant. Each restaurant then gets ranked through a combination of:Order volume: Restaurants which receive more orders are likely to have more valid signals for any predictions we make. Increasing the confidence we have in our estimates.Fraction of the orders where the pick-up location was not “at” the restaurant: This fraction indicates the number of orders with a pick-up location not near the registered restaurant location (with near being defined both spatially and temporally as above). A higher value indicates a higher likelihood of the restaurant not being in the registered location subject to order volumeMedian distance between registered and estimated locations: This factor is used to rank restaurants by a notion of “importance”. A restaurant which is just outside the fixed radius from above can be addressed after another restaurant which is a kilometre away. This ranked list of restaurants is then passed on to our mapping operations team to verify. The team checks various sources to verify if the restaurant is incorrectly located which is then fed back to the GrabFood system and the locations updated accordingly.Results  We have a system to catch and fix obvious errorsThe table below shows a few examples of errors we were able to catch and fix. The image on the left shows the distance between an incorrectly registered address and the actual location of the restaurant.            Restaurant      Path from registered location to estimated location      Zoomed in view of estimated location                  Sederhana  Minang                          Papa Ron's Pizza                          Rich-O Donuts &amp; Cafe                  Fixing these errors periodically greatly reduced the median error distance (measured as the straight line distance between the estimated location and registered location) in each city as restaurant locations were corrected.            Bangkok      Ho Chi Minh                                We helped to reduce cancellationsWe also tracked the number of GrabFood orders cancelled because the restaurant could not be found by our driver-partners as indicated on the app. Once we started making periodic updates, we saw a 5x decrease in cancellations because of incorrect restaurant locations.     We discovered some interesting findings!In some cases, we were actually stumped when trying to correct some of the locations according to what the system estimated. One of the most interesting examples was the restaurant “Waroeng Steak and Shake” in Bekasi. According to our system, the restaurant’s location was further up Jalan Raya Jatiwaringin than we thought it to be.   Examining this on Google Maps, we noticed that both locations oddly seemed to have a branch of the restaurant. What was going on here?   By looking at Google Reviews (credit to my colleague Kenneth Loh for the idea), we realised that the restaurant seemed to have changed its location, and this is what our system was picking up on.   In summary, the system was able to respond to a change in location for the restaurant without any active action taken by the restaurant and while other data sources had duplicates. What’s Next?Going forward, we are looking to automate some aspects of this workflow. Currently, the validation part is handled by our mapping operations team and we are looking to feedback their validation and actions taken so that we can fine-tune various hyperparameters in our system (registered location radii, normalisation factors, etc) and/or train more advanced models that are cognizant of different geo and driver characteristics in different markets.Additionally while we know that we should expect poor results for some scenarios (e.g. inside malls due to poor GPS quality and often approximate registered locations), we can extract such information (restaurant is inside a mall in this case) through a combination of manual feedback from operations teams and drivers, as well as automated NLP techniques such as name and address parsing and entity recognition. In the end, it is always useful to question the predictions that a system makes. By looking at some abnormally small wait times at restaurants, we were able to discover, provide feedback and continually update restaurant locations within the GrabFood ecosystem resulting in an overall better experience for our eaters, driver-partners and merchant-partners.",
        "url": "/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd"
      }
      ,
    
      "beyond-retries-part-3": {
        "title": "Designing Resilient Systems Beyond Retries (Part 3): Architecture Patterns and Chaos Engineering",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;, &quot;Chaos Engineering&quot;]",
        "category": "",
        "content": "This post is the third of a three-part series on going beyond retries and circuit breakers to improve system resiliency. This whole series covers techniques and architectures that can be used as part of a strategy to improve resiliency. In this article, we will focus on architecture patterns and chaos engineering to reduce, prevent, and test resiliency.Reducing Failure Through Architecture PatternsResiliency is all about preparing for and handling failure. So the most effective way to improve resiliency is undoubtedly to reduce the possible ways in which failure can occur, and several architectural patterns have emerged with this aim in mind. Unfortunately these are easier to apply when designing new systems and less relevant to existing ones, but if resiliency is still an issue and no other techniques are helping, then refactoring the system is a good approach to consider.IdempotencyOne popular pattern for improving resiliency is the concept of idempotency. Strictly speaking, an idempotent endpoint is one which always returns the same result given the same parameters, no matter how many times it is called. However, the definition is usually extended to mean it returns the results and has no side-effects, or any side-effects are only executed once. The main benefit of making endpoints idempotent is that they are always safe to retry, so it complements the retry technique to make it more effective. It also means there is less chance of the system getting into an inconsistent or worse state after experiencing failure.If an operation has side-effects but cannot distinguish unique calls with its current parameters, it can be made to be idempotent by adding an idempotency key parameter. The classic example is money: a ‘transfer money to X’ operation may legitimately occur multiple times with the same parameters, but making the same call twice would be a mistake, so it is not idempotent. A client would not be able to retry a call that timed out, because it does not know whether or not the server processed the request. However, if the client generates and sends a unique ID as an idempotency key parameter, then it can safely retry. The server can then use this information to determine whether to process the request (if it sees the request for the first time) or return the result of the previous operation.    Using idempotency keys can guarantee idempotency for endpoints with side-effects&nbsp;Asynchronous ResponsesA second pattern is making use of asynchronous responses. Rather than relying on a successful call to a dependency which may fail, a service may complete its own work and return a successful or partial response to the client. The client would then have to receive the response in an alternate way, either by polling (‘pull’) until the result is ready or the response being ‘pushed’ from the server when it completes.From a resiliency perspective, this guarantees that the downstream errors do not affect the endpoint. Furthermore, the risk of the dependency causing latency or consuming resources goes away, and it can be retried in the background until it succeeds. The disadvantage is that this works against the ‘fail fast’ principle, since the call might be retried indefinitely without ever failing. It might not be clear to the client what to do in this case.Not all endpoints have to be made asynchronous, and the decision to be synchronous or not could be made by the endpoint dynamically, depending on the service health. Work that can be made asynchronous is known as deferrable work, and utilising this information can save resources and allow the more critical endpoints to complete. For example, a fraud system may decide whether or not a newly registered user should be allowed to use the application, but such decisions are often complex and costly. Rather than slow down the registration process for every user and create a poor first impression, the decision can be made asynchronously. When the fraud-decision system is available, it picks up the task and processes it. If the user is then found to be fraudulent, their account can be deactivated at that point.Preventing Disaster Through Chaos EngineeringIt is famously understood that disaster recovery is worthless unless it’s tested regularly. There are dozens of stories of employees diligently performing backups every day only to find that when they actually needed to restore from it, the backups were empty. The same thing applies to resiliency, albeit with less spectacular consequences.The emerging best practice for testing resiliency is chaos engineering. This practice, made famous by Netflix’s Chaos Monkey, is the idea of deliberately causing parts of a system to fail in order to test (and subsequently improve) its resiliency. There are many different kinds of chaos engineering that vary in scope, from simulating an outage in an entire AWS region to injecting latency into a single endpoint. A chaos engineering strategy may include multiple types of failure, to build confidence in the ability of various parts of the system to withstand failure.Chaos engineering has evolved since its inception, ironically becoming less ‘chaotic’, despite the name. Shutting off parts of a system without a clear plan is unlikely to provide much value, but is practically guaranteed to frustrate your consumers - and upper management! Since it is recommended to experiment on production, minimising the blast radius of chaos experiments, at least at the beginning, is crucial to avoid unnecessary impact to the system.Chaos Experiment ProcessThe basic process for conducting a chaos experiment is as follows:  Define how to measure a ‘steady state’, in order to confirm that the system is currently working as expected.  Decide on a ‘control group’ (which does not change) and an ‘experiment group’ from the pool of backend servers.  Hypothesise that the steady state will not change during the experiment.  Introduce a failure in one component or aspect of the system in the control group, such as the network connection to the database.  Attempt to disprove the hypothesis by analysing the difference in metrics between the control and experiment groups.If the hypothesis is disproved, then the parts of the system which failed are candidates for improvement. After making changes, the experiments are run again, and gradually confidence in the system should improve.Chaos experiments should ideally mimic real-world scenarios that could actually happen, such as a server shutting down or a network connection being disconnected. These events do not necessarily have to be directly related to failure - ordinary events such as auto-scaling or a change in server hardware or VM type can be experimented with, as they could still potentially affect the steady state.Finally, it is important to automate as much of the chaos experiment process as possible. From setting up the control group to starting the experiment and measuring the results, to automatically disabling the experiment if the impact to production has exceeded the blast radius, the investment in automating them will save valuable engineering time and allow for experiments to eventually be run continuously.ConclusionRetries are a useful and important part of building resilient software systems. However, they only solve one part of the resiliency problem, namely recovery. Recovery via retries is only possible under certain conditions and could potentially exacerbate a system failure if other safeguards aren’t also in place. Some of these safeguards and other resiliency patterns have been discussed in this article.The excellent Hystrix library combines multiple resiliency techniques, such as circuit-breaking, timeouts and bulkheading, in a single place. But even Hystrix cannot claim to solve all resiliency issues, and it would not be wise to rely on a single library completely. However, just as it can’t be recommended to only use Hystrix, suddenly introducing all of the above patterns isn’t advisable either. There is a point of diminishing returns with adding more; more techniques means more complexity, and more possible things that could go wrong.Rather than implement all of the resiliency patterns described above, it is recommended to selectively apply patterns that complement each other and cover existing gaps that have previously been identified. For example, an existing retry strategy can be enhanced by gradually switching to idempotent endpoints, improving the coverage of API calls that can be retried.A microservice architecture is a good foundation for building a resilient system, but it requires careful planning and implementation to achieve. By identifying the possible ways in which a system can fail, then evaluating and applying the tried-and-tested patterns to withstand them, a reliable system can become one that is truly resilient.I hope you found this series useful. Comments are always welcome.",
        "url": "/beyond-retries-part-3"
      }
      ,
    
      "beyond-retries-part-2": {
        "title": "Designing Resilient Systems Beyond Retries (Part 2): Bulkheading, Load Balancing, and Fallbacks",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;, &quot;Bulkheading&quot;, &quot;Load Balancing&quot;, &quot;Fallbacks&quot;]",
        "category": "",
        "content": "This post is the second of a three-part series on going beyond retries to improve system resiliency. We’ve previously discussed about rate-limiting as a strategy to improve resiliency. In this article, we will cover these techniques: bulkheading, load balancing, and fallbacks.Introducing Bulkheading (Isolation)Bulkheading is a fundamental pattern which underpins many other resiliency techniques, especially where microservices are concerned, so it’s worth introducing first. The term actually comes from an ancient technique in ship building, where a ship’s hull would be partitioned into several watertight compartments. If one of the compartments has a leak, then the water fills just that compartment and is contained, rather than flooding the entire ship. We can apply this principle to software applications and microservices: by isolating failures to individual components, we can prevent a single failure from cascading and bringing down the entire system.Bulkheads also help to prevent single points of failure, by reducing the impact of any failures so services can maintain some level of service.Level of BulkheadsIt is important to note that bulkheads can be applied at multiple levels in software architecture. The two highest levels of bulkheads are at the infrastructure level, and the first is hardware isolation. In a cloud environment, this usually means isolating regions or availability zones. The second is isolating the operating system, which has become a widespread technique with the popularity of virtual machines and now containerisation. Previously, it was common for multiple applications to run on a single (very powerful) dedicated server. Unfortunately, this meant that a rogue application could wreak havoc on the entire system in a number of ways, from filling the disk with logs to consuming memory or other resources.    Isolation can be achieved by applying bulkheading at multiple levels&nbsp;This article focuses on resiliency from the application perspective, so below the system level is process-level isolation. In practical terms, this isolation prevents an application crash from affecting multiple system components. By moving those components into separate processes (or microservices), certain classes of application-level failures are prevented from causing cascading failure.At the lowest level, and perhaps the most common form of bulkheading to software engineers, are the concepts of connection pooling and thread pools. While these techniques are commonly employed for performance reasons (reusing resources is cheaper than acquiring new ones), they also help to put a finite limit on the number of connections or concurrent threads that an operation is allowed to consume. This ensures that if the load of a particular operation suddenly increases unexpectedly (such as due to external load or downstream latency), the impact is contained to only a partial failure.Bulkheading Support in the Hystrix LibraryThe Hystrix library for Go supports a form of bulkheading through its MaxConcurrentRequests parameter. This is conveniently tied to the circuit name, meaning that different levels of isolation can be achieved by choosing an appropriate circuit name. A good rule of thumb is to use a different circuit name for each operation or API call. This ensures that if just one particular endpoint of a remote service is failing, the other circuits are still free to be used for the remaining healthy endpoints, achieving failure isolation.Load Balancing    Global rate-limiting with a central server&nbsp;Load balancing is where network traffic from a client may be served by one of many backend servers. You can think of load balancers as traffic cops who distribute traffic on the road to prevent congestion and overload. Assuming the traffic is distributed evenly on the network, this effectively increases the computing power of the backend. Adding capacity like this is a common way to handle an increase in load from the clients, such as when a website becomes more popular.Almost always, load balancers provide high availability for the application. When there is just a single backend server, this server is a ‘single point of failure’, because if it is ever unavailable, there are no servers remaining to serve the clients. However, if there is a pool of backend servers behind a load balancer, the impact is reduced. If there are 4 backend servers and only 1 is unavailable, evenly distributed requests would only fail 25% of the time instead of 100%. This is already an improvement, but modern load balancers are more sophisticated.Usually, load balancers will include some form of a health check. This is a mechanism that monitors whether servers in the pool are ‘healthy’, ie. able to serve requests. The implementations for the health check vary, but this can be an active check such as sending ‘pings’, or passive monitoring of responses and removing the failing backend server instances.As with rate-limiting, there are many strategies for load balancing to consider.There are four main types of load balancer to choose from, each with their own pros and cons:  Proxy. This is perhaps the most well-known form of load-balancer, and is the method used by Amazon’s Elastic Load Balancer. The proxy sits on the boundary between the backend servers and the public clients, and therefore also doubles as a security layer: the clients do not know about or have direct access to the backend servers. The proxy will handle all the logic for load balancing and health checking. It is a very convenient and popular approach because it requires no special integration with the client or server code. They also typically perform ‘SSL termination’, decrypting incoming HTTPS traffic and using HTTP to communicate with the backend servers.  Client-side. This is where the client performs all of the load-balancing itself, often using a dedicated library built for the purpose. Compared with the proxy, it is more performant because it avoids an extra network ‘hop.’ However, there is a significant cost in developing and maintaining the code, which is necessarily complex and any bugs have serious consequences.  Lookaside. This is a hybrid approach where the majority of the load-balancing logic is handled by a dedicated service, but it does not proxy; the client still makes direct connections to the backend. This reduces the burden of the client-side library but maintains high performance, however the load-balancing service becomes another potential point of failure.  Service mesh with sidecar. A service mesh is an all-in-one solution for service communication, with many popular open-source products available. They usually include a sidecar, which is a proxy that sits on the same server as the application to route network traffic. Like the traditional proxy load balancer, this handles many concerns of load-balancing for free. However, there is still an extra network hop, and there can be a significant development cost to integrate with existing systems for logging, reporting and so on, so this must be weighed against building a client-side solution in-house.    Comparison of load-balancer architectures&nbsp;Grab’s Load-balancing ImplementationAt Grab, we have built our own internal client-side solution called CSDP, which uses the distributed key-value store etcd as its backend store.FallbacksThere are scenarios when simply retrying a failed API call doesn’t work. If the remote server is completely down or only returning errors, no amount of retries are going to help; the failure is unrecoverable. When recovery isn’t an option, mitigation is an alternative. This is related to the concept of graceful degradation: sometimes it is preferable to return a less optimal response than fail completely, especially for user-facing applications where user experience is important.One such mitigation strategy is fallbacks. This is a broad topic with many different sub-strategies, but here are a few of the most common:Fail SilentlyStarting with the easiest to implement, one basic fallback strategy is fail silently. This means returning an empty or null response when an error is encountered, as if the call had succeeded. If the data being requested is not critical functionality then this can be considered: missing part of a UI is less noticeable than an error page! For example, UI bubbles showing unread notifications are a common feature. But if the service providing the notifications is failing and the bubble shows 0 instead of N notifications, the user’s experience is unlikely to be significantly affected.Local ComputationA second fallback strategy when a downstream dependency is failing could be to compute the value locally instead. This could mean either returning a default (static) value, or using a simple formula to compute the response. For example, a marketplace application might have a service to calculate shipping costs. If it is unavailable, then using a default price might be acceptable. Or even $0 - users are unlikely to complain about errors that benefit them, and it’s better than losing business!Cached ValuesSimilarly, cached values are often used as fallbacks. If the service isn’t available to calculate the most up to date value, returning a stale response might be better than returning nothing. If an application is already caching the value with a short expiration to optimise performance, it can be reused as a fallback cache by setting two expiration times: one for normal circumstances, and another when the service providing the response has failed.Backup ServiceFinally, if the response is too complex to compute locally or if major functionality of the application is required to have a fallback, then an entirely new service can act as a fallback; a backup service. Such a service is a big investment, so to make it worthwhile some trade-offs must be accepted. The backup service should be considerably simpler than the service it is intended to replace; if it is too complex then it will require constant testing and maintenance, not to mention documentation and training to make sure it is well understood within the engineering team. Also, a complex system is more likely to fail when activated. Usually such systems will have very few or no dependencies, and certainly should not depend on any parts of the original system, since they could have failed, rendering the backup system useless.Grab’s Fallback ImplementationAt Grab, we make use of various fallback strategies in our services. For example, our microservice framework Grab-Kit has built-in support for returning cached values when a downstream service is unresponsive. We’ve even built a backup service to replicate our core functionality, so we can continue to serve consumers despite severe technical difficulties!Up Next, Architecture Patterns and Chaos Engineering…We’ve covered various techniques in designing reliable and resilient systems in the previous articles. I hope you found them useful. Comments are always welcome.In our next post, we will look at ways to prevent and reduce failures through architecture patterns and testing.Please stay tuned!",
        "url": "/beyond-retries-part-2"
      }
      ,
    
      "beyond-retries-part-1": {
        "title": "Designing Resilient Systems Beyond Retries (Part 1): Rate-Limiting",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;, &quot;Rate-limiting&quot;]",
        "category": "",
        "content": "This post is the first of a three-part series on going beyond retries to improve system resiliency. In this series, we will discuss other techniques and architectures that can be used as part of a strategy to improve resiliency. To start off the series, we will cover rate-limiting.Software engineers aim for reliability. Systems that have predictable and consistent behaviour in terms of performance and availability. In the electricity industry, reliability may equate to being able to keep the lights on. But just because a system has remained reliable up until a certain point, does not mean that it will continue to be. This is where resiliency comes in: the ability to withstand or recover from problematic conditions or failure. Going back to our electricity analogy - resiliency is the ability to turn the lights back on quickly when say, a natural disaster hits the power grid.Why We Value ResiliencyBeing resilient to many different failures is the best way to ensure a system is reliable and - more importantly - stays that way. At Grab, our architecture features hundreds of microservices, which is constantly stressed in an increasing number of different ways at higher and higher volumes. Failures that would be rare or unusual become more likely as our scale increases. For that reason, we proactively focus on - and require our services to think about - resiliency, even if they have historically been very reliable.As software systems evolve and become more complex, the number of potential failure modes that software engineers have to account for grows. Fortunately, so do the techniques for dealing with them. The circuit-breaker pattern and retries are two such techniques commonly employed to improve resiliency specifically in the context of distributed systems. In pursuit of reliability, this is a fine start, but it would be wrong to assume that this will keep the service reliable forever. This article will discuss how you can use rate-limiting as part of a strategy to improve resilience, beyond retries.Challenges with Retries and Circuit BreakersA common risk when introducing retries in a resiliency strategy is ‘retry storms’. Retries by definition increase the number of requests from the client, especially when the system is experiencing some kind of failure. If the server is not prepared to handle this increase in traffic, and is possibly already struggling to handle the load, it can quickly become overwhelmed. This is counter-productive to introducing retries in the first place!When using a circuit-breaker in combination with retries, the application has some form of safety net: too many failures and the circuit will open, preventing the retry storms. However, this can be dangerous to rely on. For one thing, it assumes that all clients have the correct circuit-breaker configurations. Knowing how to configure the circuit-breaker correctly is difficult because it requires knowledge of the downstream service’s configurations too.Introducing Rate-limitingIn a large organisation such as Grab with hundreds of microservices, it becomes increasingly difficult to coordinate and maintain the correct circuit-breaker configurations as the number of services increases.Secondly, it is never a good idea for the server to depend on its clients for resiliency. The circuit-breaker could fail or simply be bypassed, and the server would have to deal with all requests the client makes.It is therefore desirable to have some form of rate-limiting/throttling as another line of defence. There are many strategies for rate-limiting to consider.Types of Thresholds for Rate-limitingThe traditional approach to rate-limiting is to implement a server-side check which monitors the rate of incoming requests and if it exceeds a certain threshold, an error will be returned instead of processing the request. There are many algorithms such as ‘leaky bucket’, fixed/sliding window and so on. A key decision is where to set the thresholds: usually by client, endpoint, or a combination of both.Rate-limiting by client or user account is the approach taken by many public APIs: Each client is allowed to make a certain number of requests over a period, say 1000 requests per hour, and once that number is exceeded then their requests will be rejected until the time window resets. In this approach, the server must ensure that it has enough capacity (or can scale adequately) to handle the maximum allowed number of requests for each client. If new clients are added frequently, the overhead of maintaining and adjusting the limits may be significant. However, it can be a good way to guarantee a service-level agreement (SLA) with your clients.An alternative to per-client thresholds is to use per-endpoint thresholds. This limit is applied across all clients and can be set according to the server’s true capacity using benchmarks. Compared with per-client limits this is easier to configure and more reliable in preventing the server from becoming overloaded. However, one misbehaving client may be able to consume the entire quota, blocking other clients of the service.A rate-limiting strategy may use different levels of thresholds, and this is the best approach to get the benefits of both per-client and per-endpoint thresholds. For example, the following rules might be applied (in order):  Per-client, per-endpoint: For example, client A accessing the sendEmail endpoint. It is not necessary to configure thresholds at this granularity, but may be useful for critical endpoints.  Per-client: In addition to any per-client per-endpoint settings, client A could have a global threshold of 1000 requests/hour to any API.  Per-endpoint: This is the server’s catch-all guard to guarantee that none of its endpoints become overloaded. If client limits are properly configured, this limit should never be reached.  Server-wide: Finally, a limit on the number of requests a server can handle in total. This is important because even if endpoints can meet their limits individually, they are never completely isolated: the server will have some overhead and limited resources for processing any kind of request, opening and closing network connections etc.Local vs Global Rate-limitingAnother consideration is local vs global rate-limiting. As we saw in the previous section, backend servers are usually pooled together for resiliency. A naive rate-limiting solution might be implemented at the individual server instance level. This sounds intuitive because the thresholds can be calculated exactly according to the instance’s computing power, and it scales automatically as the number of instances increases. However, in a microservice architecture, this is rarely correct as the bottlenecks are unlikely to be so closely tied to individual instance hardware.More often, the capacity is reached when a downstream resource is exhausted, such as a database, a third-party service or another microservice. If the rate-limiting is only enforced at the instance level, when the service scales, the pressure on these resources will increase and quickly overload them. Local rate-limiting’s effectiveness is limited.Global rate-limiting on the other hand monitors thresholds and enforces limits across the entire backend server pool. This is usually achieved through the use of a centralised rate-limiting service to make the decisions about whether or not requests should be allowed to go through. While this is much more desirable, implementing such a service is not without challenges.Considerations When Implementing Rate-limitingCare must be taken to ensure the rate-limiting service does not become a single point of failure. The system should still function when the rate-limiter itself is experiencing problems (perhaps by falling back to a local limiter). Since the rate-limiter must be in the request path, it should not add significant latency because any latency would be multiplied across every endpoint being monitored. Grab’s own Quotas service is an example of a global rate-limiter which addresses these concerns.    Global rate-limiting with a central server. The servers send information about the request volumes, and the rate-limiting service responds with the rate-limiting decisions. This is done asynchronously to avoid introducing a point of failure.&nbsp;Generally, it is more important to implement rate-limiting at the server side. This is because, once again, assuming that clients have correct implementation and configurations is risky. However, there is a case to be made for rate-limiting on the client as well, especially if the clients can be trusted or share a common SDK.With server-side limiting, the server still has to accept the initial connection, process the rate-limiting logic and return an appropriate error response. With sufficient load, this overhead can be enough to render the system unresponsive; an unintentional denial-of-service (DoS) effect.Client-side limiting can be implemented by using a central service as described above or, more commonly, utilising response headers from the server. In this approach, the server response may include information about the client’s remaining quota and/or a timestamp at which the quota is reset. If the client implements logic for these headers, it can avoid sending requests at all if it knows they will be rate-limited. The disadvantage of this is that the client-side logic becomes more complex and another possible source of bugs, so this cost has to be considered against the simpler server-only method.Up Next, Bulkheading, Load Balancing, and Fallbacks…So we’ve taken a look at rate-limiting as a strategy for having resilient systems. I hope you found this article useful. Comments are always welcome.In our next post, we will look at the other resiliency techniques such as bulkheading (isolation), load balancing, and fallbacks.Please stay tuned!",
        "url": "/beyond-retries-part-1"
      }
      ,
    
      "context-deadlines-and-how-to-set-them": {
        "title": "Context Deadlines and How to Set Them",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;]",
        "category": "",
        "content": "At Grab, our microservice architecture involves a huge amount of network traffic and inevitably, network issues will sometimes occur, causing API calls to fail or take longer than expected. We strive to make such incidents a non-event by designing with the expectation of such incidents in mind. With the aid of Go’s context package, we have improved upon basic timeouts by passing timeout information along the request path. However, this introduces extra complexity, and care must be taken to ensure timeouts are configured in a way that is efficient and does not worsen problems. This article explains from the ground up a strategy for configuring timeouts and using context deadlines correctly, drawing from our experience developing microservices in a large scale and often turbulent network environment.TimeoutsTimeouts are a fundamental concept in computer networking. Almost every kind of network communication will have some kind of timeout associated with it, often configurable with a parameter. The idea is to place a time limit on some event happening, often a network response; after the limit has passed, the operation is aborted rather than waiting indefinitely. Examples of useful places to put timeouts include connecting to a database, making a HTTP request or on idle connections in a pool.    Figure 1.1: How timeouts prevent long API calls&nbsp;Timeouts allow a programme to continue where it otherwise might hang, providing a better experience to the end user. Often the default way for programs to handle timeouts is to return an error, but this doesn’t have to be the case: there are several better alternatives for handling timeouts which we’ll cover later.While they may sound like a panacea, timeouts must be configured carefully to be effective: too short a timeout will result in increased errors from a resource which could still be working normally, and too long a timeout will risk consuming excess resources and a poor user experience. Furthermore, timeouts have evolved over time with new concepts such as Go’s context package, and the trend towards distributed systems has raised the stakes: timeouts are more important, and can cause more damage if misused!Why Timeouts are UsefulIn the context of microservices, timeouts are useful as a defensive measure against misbehaving or faulty dependencies. It is a guarantee that no matter how badly the dependency is failing, your call will never take longer than the timeout setting (for example 1 second). With so many other things to worry about, that’s a really nice thing to have! So there’s an instant benefit to your service’s resiliency, even if you do nothing more than set the timeout.However, a service can choose what to do when it encounters a timeout, which can make them even more useful. Generally there are three options:  Return an error. This is the simplest, but unless you know there is error handling upstream, this can actually deliver the worst user experience.  Return a fallback value. We can return a default value, a cached value, or fall back to a simpler computed value. Depending on the circumstances, this can offer a better user experience.  Retry. In the best case, a retry will succeed and deliver the intended response to the caller, albeit with the added timeout delay. However, there are other complexities to consider for retries to be effective. For a full discussion on this topic, see Circuit Breaker vs Retries Part 1and Circuit Breaker vs Retries Part 2.At Grab, our services tend towards using retries wherever possible, to make minor errors as transparent as possible.The main advantage of timeouts is that they give your service time to do something else, and this should be kept in mind when considering a good timeout value: not only do you want to allow the remote call time to complete (or not), but you need to allow enough time to handle the potential timeout as well.Different Types of TimeoutsNot all timeouts are the same. There are different types of timeouts with crucial differences in semantics, and you should check the behaviour of the timeout settings in the library or resource you’re using before configuring them for production use.In Go, there are three common classes of timeouts:  Network timeouts: These come from the net package and apply to the underlying network connection. These are the best to use when available, because you can be sure that the network call has been cancelled when the call returns to your function.  Context timeouts: Context is discussed later in this article, but for now just note that these timeouts are propagated to the server. Since the server is aware of the timeout, it can avoid wasted effort by abandoning computation after the timeout is reached.  Asynchronous timeouts: These occur when a goroutine is executed and abandoned after some time. This does not automatically cancel the goroutine (you can’t really cancel goroutines without extra handling), so it risks leaking the goroutine and other resources. This approach should be avoided in production unless combined with some other measures to provide cancellation or avoid leaking resources.Dangers of Poor Timeout Configuration for Microservice CallsThe benefits of using timeouts are enticing, but there’s no free lunch: relying on timeouts too heavily can lead to disastrous cascading failure scenarios. Worse, the effects of a poor timeout configuration often don’t become evident until it’s too late: it’s peak hour, traffic just reached an all-time high and… all your services froze up at the same time. Not good.To demonstrate this effect, imagine a simple 3-service architecture where each service naively uses a default timeout of 1 second:    Figure 1.2: Example of how incorrect timeout configuration causes cascading failure&nbsp;Service A’s timeout does not account for the fact that Service B calls C. If B itself is experiencing problems and takes 800ms to complete its work, then C effectively only has 200ms to complete before service A gives up. But since B’s timeout to C is also 1s, that means that C could be wasting up to 800ms of computational effort that ‘leaks’ - it has no chance of being used. Both B and C are blissfully unaware at first that anything is wrong - they happily return successful responses that A never receives!This resource leak can soon be catastrophic, though: since the calls from B to A are timing out, A (or A’s clients) are likely to retry, causing the load on B to increase. This in turn causes the load on C to increase, and eventually all services will stop responding.The same thing happens if B is healthy but C is experiencing problems: B’s calls to C will build up and cause B to become overloaded and fail too. This is a common cause of cascading failure.How to Set a Good TimeoutGiven the importance of correctly configuring timeout values, the question remains as to how to decide upon a ‘correct’ timeout value. If the timeout is for an API call to another service, a good place to start would be that service’s service-level agreements (SLAs). Often SLAs are based on latency percentiles, which is a value below which a given percentage of latencies fall. For example, a system might have a 99th percentile (also known as P99) latency of 300ms; this would mean that 99% of latencies are below 300ms. A high-order percentile such as P99 or even P99.9 can be used as a ballpark worst-case value.Let’s say a service (B)’s endpoint has a 99th percentile latency of 600ms. Setting the timeout for this call at 600ms would guarantee that no calls take longer than 600ms, while returning errors for the rest and accepting an error rate of at most 1% (assuming the service is keeping to their SLA). This is an example of how the timeout can be combined with information about latencies to give predictable behaviour.This idea can be taken further by considering retries too. If the median latency for this service is 50ms, then you could introduce a retry of 50ms for an overall timeout of 50ms + 600ms = 650ms:Service BService B P99 latency SLA = 600msService B median latency = 50msService ARequest timeout = 600msNumber of retries = 1Retry request timeout = 50msOverall timeout = 50ms+600ms = 650msChance of timeout after retry = 1% * 50% = 0.5%  Figure 1.3: Example timeout configuration settings based on latency data&nbsp;This would still cut off the top 1% of latencies, while optimistically making another attempt for the median latency. This way, even for the 1% of calls that encounter a timeout, our service would still expect to return a successful response within 650ms more than half the time, for an overall success rate of 99.5%.Context PropagationGo officially introduced the concept of context in Go 1.7, as a way of passing request-scoped information across server boundaries. This includes deadlines, cancellation signals and arbitrary values. Let’s ignore the last part for now and focus on deadlines and cancellations. Often, when setting a regular timeout on a remote call, the server side is unaware of the timeout. Even if the server is notified indirectly when the client closes the connection, it’s still not necessarily clear whether the client timed out or encountered another issue. This can lead to wasted resources, because without knowing the client timed out, the server often carries on regardless. Context aims to solve this problem by propagating the timeout and context information across API boundaries.    Figure 1.4: Context propagation cancels work on B and C&nbsp;Server A sets a context timeout of 1 second. Since this information spans the entire request and gets propagated to C, C is always aware of the remaining time it has to do useful work - work that won’t get discarded. The remaining time can be defined as (1 - b), where b is the amount of time that server B spent processing before calling C. When the deadline is exceeded, the context is immediately cancelled, along with any child contexts that were created from the parent.The context timeout can be a relative time (eg. 3 seconds from now) or an absolute time (eg. 7pm). In practice they are equivalent, and the absolute deadline can be queried from a timeout created with a relative time and vice-versa.Another useful feature of contexts is cancellation. The client has the ability to cancel the request for any reason, which will immediately signal the server to stop working. When a context is cancelled manually, this is very similar to a context being cancelled when it exceeds the deadline. The main difference is the error message will be ‘context cancelled’ instead of ‘context deadline exceeded’. This is a common cause of confusion, but context cancelled is always caused by an upstream client, while deadline exceeded could be a deadline set upstream or locally.The server must still listen for the ‘context done’ signal and implement cancellation logic, but at least it has the option of doing so, unlike with ordinary timeouts. The most common reason for cancelling a request is because the client encountered an error and no longer needs the response that the server is processing. However, this technique can also be used in request hedging, where concurrent duplicate requests are sent to the server to decrease the impact of an individual call experiencing latency. When the first response returns, the other requests are cancelled because they are no longer needed.Context can be seen as ‘distributed timeouts’ - an improvement to the concept of timeouts by propagating them. But while they achieve the same goal, they introduce other issues that must be considered.Context Propagation and Timeout ConfigurationWhen propagating timeout information via context, there is no longer a static ‘timeout’ setting per call. This can complicate debugging: even if the client has correctly configured their own timeout as above, a context timeout could mean that either the remote downstream server is slow, or that an upstream client was slow and there was insufficient time remaining in the propagated context!Let’s revisit the scenario from earlier, and assume that service A has set a context timeout of 1 second. If B is still taking 800ms, then the call to C will time out after 200ms. This changes things completely: although there is no longer the resource leak (because both B and C will terminate the call once the context timeout is exceeded), B will have an increase in errors whereas previously it would not (at least until it became overloaded). This may be worse than completing the request after A has given up, depending on the circumstances. There is also a dangerous interaction with circuit breakers which we will discuss in the next section.If allowing the request to complete is preferable to cancelling it even in the event of a client timeout, the request should be made with a new context decoupled from the parent (ie. context.Background()). This will ensure that the timeout is not propagated to the remote service. When doing this, it is still a good idea to set a timeout, to avoid waiting indefinitely for it to complete.Context and Circuit-breakersA circuit-breaker is a software library or function which monitors calls to external resources with the aim of preventing calls which are likely to fail, ‘short-circuiting’ them (hence the name). It is a good practice to use a circuit-breaker for all outgoing calls to dependencies, especially potentially unreliable ones. But when combined with context propagation, that raises an important question: should context timeouts or cancellation cause the circuit to open?Let’s consider the options. If ‘yes’, this means the client will avoid wasting calls to the server if it’s repeatedly hitting the context timeout. This might seem desirable at first, but there are drawbacks too.Pros:  Consistent behaviour with other server errors  Avoids making calls that are unlikely to succeed  It is obvious when things are going wrong  Client has more time to fall back to other behaviour  More lenient on misconfigured timeouts because circuit-breaking ensures that subsequent calls will fail fast, thus avoiding cascading failureCons:  Unpredictable  A misconfigured upstream client can cause the circuit to open for all other clients  Can be misinterpreted as a server errorIt is generally better not to open the circuit when the context deadline set upstream is exceeded. The only timeout allowed to trigger the circuit-breaker should be the request timeout of the specific call for that circuit.Pros:  More predictable  Circuit depends mostly on server health, not client  Clients are isolatedCons:  May be confusing for clients who expect the circuit to open  Misconfigured timeouts are more likely to waste resourcesNote that the above only applies to propagated contexts. If the context only spans a single individual call, then it is equivalent to a static request timeout, and such errors should cause circuits to open.How to Set Context DeadlinesLet’s recap some of the concepts covered in this article so far:  Timeouts are a time limit on an event taking place, such as a microservice completing an API call to another service.  Request timeouts refer to the timeout of a single individual request. When accounting for retries, an API call may include several request timeouts before completing successfully.  Context timeouts are introduced in Go to propagate timeouts across API boundaries.  A context deadline is an absolute timestamp at which the context is considered to be ‘done’, and work covered by this context should be cancelled when the deadline is exceeded.Fortunately, there is a simple rule for correctly configuring context timeouts:The upstream timeout must always be longer than the total downstream timeouts including retries.The upstream timeout should be set at the ‘edge’ server and cascade throughout.In our scenario, A is the edge server. Let’s say that B’s timeout to C is 1s, and it may retry at most once, after a delay of 500ms. The appropriate context timeout (CT) set from A can be calculated as follows:CT(A) = (timeout to C * number of attempts) + (retry delay * number of retries)CT(A) = (1s * 2) + (500ms * 1) = 2,500ms    Figure 1.5: Formula for calculating context timeouts&nbsp;Extra time can be allocated for B’s processing time and to allow B to return a fallback response if appropriate.Note that if A configures its timeout according to this rule, then many of the above issues disappear. There are no wasted resources, because B and C are given the maximum time to complete their requests successfully. There is no chance for B’s circuit-breaker to open unexpectedly, and cascading failure is mostly avoided: a failure in C will be handled and be returned by B, instead of A timing out as well.A possible alternative would be to rely on context cancellation: allow A to set a shorter timeout, which cancels B and C if the timeout is exceeded. This is an acceptable approach to avoiding cascading failure (and cancellation should be implemented in any case), but it is less optimal than configuring timeouts according to the above formula. One reason is that there is no guarantee of the downstream services handling the timeout gracefully; as mentioned previously, the service must explicitly check for ctx.Done() and this is rarely followed in practice. It is also impractical to place checks at every point in the code, so there could be a considerable delay between the client cancellation and the server abandoning the processing.A second reason not to set shorter timeouts is that it could lead to unexpected errors on the downstream services. Even if B and C are healthy, a shorter context timeout could lead to errors if A has timed out. Besides the problem of having to handle the cancelled requests, the errors could create noise in the logs, and more importantly could have been avoided. If the downstream services are healthy and responding within their SLA, there is no point in timing out earlier. An exception might be for the edge server (A) to allow for only 1 attempt or fewer retries than the downstream service actually performs. But this is tricky to configure and weakens the resiliency. If it is desirable to shorten the timeouts to decrease latency, it is better to start adjusting the timeouts of the downstream resources first, starting from the innermost service outwards.A Model Implementation for Using Context Timeouts in Calls Between MicroservicesWe’ve touched on several useful concepts for improving resiliency in distributed systems: timeouts, context, circuit-breakers and retries. It is desirable to use all of them together in a good resiliency strategy. However, the actual implementation is far from trivial; finding the right order and configuration to use them effectively can seem like searching for the holy grail, and many teams go through a long process of trial and error, continuously improving their implementation. Let’s try to formally put together an ideal implementation, step by step.Note that the code below is not a final or production-ready implementation. At Grab we have developed independent circuit-breaker and retry libraries, with many settings that can be configured for fine-tuning. However, it should serve as a guide for writing resilient client libraries.Step 1: Context propagation  The skeleton function signature includes a context object as the first parameter, which is the best practice intended by Google. We check whether the context is already done before proceeding, in which case we ‘fail fast’ without wasting any further effort.Step 2: Create child context with request timeout  Our service has no control over the parent context. Indeed, it could have no deadline at all! Therefore it’s important to create a new context and timeout for our own outgoing request as well, using WithTimeout. It is mandatory to call the returned cancel function to ensure the context is properly cancelled and avoid a goroutine leak.Step 3: Introduce circuit-breaker logic  Next, we wrap our call to the external service in a circuit-breaker. The actual circuit-breaker implementation has been omitted for brevity, but there are two important points to consider:  It should only consider opening the circuit-breaker when requestTimeout is reached, not on ctx.Done().  The circuit name should ideally be unique for this specific endpoint  Step 4: Introduce retriesThe last step is to add retries to our request in the case of error. This can be implemented as a simple for loop, but there are some key things to include in a complete retry implementation:  ctx.Done() should be checked after each retry attempt to avoid wasting a call if the client has given up.  The request context should be cancelled before the next retry to avoid duplicate concurrent calls and goroutine leaks.  Not all kinds of requests should be retried.  A delay should be added before the next retry, using exponential backoff.  See Circuit Breaker vs Retries Part 2 for a thorough guide to implementing retries.Step 5: The complete implementation  And here we have arrived at our ‘ideal’ implementation of an external call including context handling and propagation, two levels of timeout (parent and request), circuit-breaking and retries. This should be sufficient for a good level of resiliency, avoiding wasted effort on both the client and server.As a future enhancement, we could consider introducing a ‘minimum time per request’, which the retry loop should use to check for remaining time as well as ctx.Done() (but not instead - we need to account for client cancellation too). Of course metrics, logging and error handling should also be added as necessary.Important TakeawaysTo summarise, here are a few of the best practices for working with context timeouts:Use SLAs and latency data to set effective timeoutsHaving a default timeout value for everything doesn’t scale well. Use available information on SLAs and historic latency to set timeouts that give predictable results.Understand the common error messagesThe context canceled (context.Canceled) error occurs when the context is manually cancelled. This automatically cancels any child contexts attached to the parent. It is rare for this error to surface on the same service that triggered the cancellation; if cancel is called, it is usually because another error has been detected (such as a timeout) which would be returned instead. Therefore, context canceled is usually caused by an upstream error: either the client timed out and cancelled the request, or cancelled the request because it was no longer needed, or closed the connection (this typically results in a cancelled context from Go libraries).The context deadline exceeded error occurs only when the time limit was reached. This could have been set locally (by the server processing the request) or by an upstream client. Unfortunately, it’s often difficult to distinguish between them, although they should generally be handled in the same way. If a more granular error is required, it is recommended to use child contexts and explicitly check them for ctx.Done(), as shown in our model implementation.Check for ctx.Done() before starting any significant workDon’t enter an expensive block of code without checking the context; if the client has already given up, the work will be wasted.Don’t open circuits for context errorsThis leads to unpredictable behaviour, because there could be a number of reasons why the context might have been cancelled. Only context errors due to request timeouts originating from the local service should lead to circuit-breaker errors.Set context timeouts at the edge service, using a cascading timeout budgetThe upstream timeout must always be longer than the total downstream timeouts. Following this formula will help to avoid wasted effort and cascading failure.In ConclusionGo’s context package provides two extremely valuable tools that complement timeouts: deadline propagation and cancellation. This article has shown the benefits of using context timeouts and how to correctly configure them in a multi-server request path. Finally, we have discussed the relationship between context timeouts and circuit-breakers, proposing a model implementation for integrating them together in a common library.If you have a Go server, chances are it’s already making heavy use of context. If you’re new to Go or had been confused by how context works, hopefully this article has helped to clarify misunderstandings. Otherwise, perhaps some of the topics covered will be useful in reviewing and improving your current context handling or circuit-breaker implementation.",
        "url": "/context-deadlines-and-how-to-set-them"
      }
      ,
    
      "peak-shift-demand-travel-trends": {
        "title": "Recipe for Building a Widget: How We Helped to “Peak-Shift” Demand by Helping Passengers Understand Travel Trends",
        "author": "lara-pureum-yimprashant-kumarraghav-gargpreeti-kotamarthiajmal-afifcalvin-ng-tjioerenrong-weng",
        "tags": "[&quot;Analytics&quot;, &quot;Data&quot;, &quot;Data Analytics&quot;]",
        "category": "",
        "content": "  Credits: Photo by rawpixel on Unsplash&nbsp;Stuck in traffic in a Grab ride? Pass the time by opening your Grab app and checking out the Feed - just scroll down! You’ll find widgets for games, polls, videos, news and even food recommendations!Beyond serving your everyday needs, we want to provide our users with information that is interesting, useful and relevant. That’s why we’re always coming up with new widgets.Building each widget takes close collaboration across multiple different teams - from Product Management to Design, Engineering, Behavioural Science, and Data Science and Analytics. Sounds like a lot of people, doesn’t it? But you’ll be surprised to hear that this behind-the-scenes collaboration works rapidly, usually in the span of one month! Which means we’re often moving from ideation phase to product release in just a few weeks.  This fast-and-furious process is anchored on one word - “consumer-centric”. And that’s how it all began with our  “Travel Trends Widget” - a widget that provides passengers with an overview of historical supply and demand trends for their current location and nearby time periods.Because we had so much fun developing this widget, we wanted to write a blog post to share with you what we did and how we did it!Inspiration: Where it All StartedTransport demand can be rather lumpy. Owing to organic patterns (e.g. office hours), a lot of passengers tend to request for cars around the same time. In periods like this, the increase in demand could outpace the arrival of driver supply, increasing the waiting time for passengers.Our goal at Grab is to make sure people get a ride when they want it and at the price they want, so we got to thinking about how we can ease this friction by leveraging our treasure trove - Big Data! - to help our passengers better plan their trips.As we were looking at the data, we noticed that there is a seasonality to demand and supply: at certain times and days, imbalances appear, peak and disappear, and the process repeats itself. Studies say that humans in general, unless shown a compelling reason or benefit for change, are habitual beings subject to inertia. So we set out to achieve exactly that: To create a widget to surface information to our passengers that may help them alter their decisions on when they choose to book a ride, thereby redistributing some of the present peak demands to periods just before and after peak - also known as “peak shifting the demand”!While this widget is the first-of-its-kind in the ride-hailing industry, “peak-shifting” was actually coined and introduced long ago!  As you can see from this post from the London Transport Museum (Source: Transport for London), London tube tried peak-shifting long before anyone else: Original Ad from 1928 displayed on the left, and Ad from 2015 displayed on the right, comparing the trends to 1928.  You may also have seen something similar at the last hotel you stayed at. Notice here a poster in an elevator at a Beijing hotel, announcing the best times to eat breakfast in comfort and avoid the crowd. (Photo credits to Prashant, our Product Manager, who saw this on holiday.)How the Travel Trends Widget WorksTo apply “peak-shifting” and help our users better plan their trips, we decided to dig in and leverage our data. It was way more complex than we had initially thought, as market conditions could be different on different days. This meant that  generic statements like “5PM-8PM are peak hours and prices will be hight” would not hold true. Contrary to general perception, we observed that even during peak hours, there are buckets of time when there is no surge or low surge.For instance, plot 1 and plot 2 below shows how a typical Monday and Tuesday surge looks like in a given month respectively. One of the key insights is that the surge trends during peak hour is different on Monday from Tuesday. It reinforces our initial hypothesis that every day is unique.So we used machine learning techniques to build a forecasting widget which can help our users and give them the power to plan their trips beforehand. This widget is able to provide the pricing trends for the next 2 hours. So with a bit of flexibility, riders can ride the tide!  So How Exactly does This Widget Work?!  It pulls together historically observed imbalances between supply and demand, for the consumer’s current location and nearby time periods. Aggregated data is displayed to consumers in easily interpreted visualisations, so that they can plan to leave at times when there are more supply, and with potentially more savings for fares.How did We Build the Widget? Loop, Agile Working Process, POC &amp; WorkstreamWidget-building is an agile, collaborative, and simultaneous process. First, we started the process with analysis from Product Analytics team, pulling out data on traffic trends, surge patterns, and behavioural insights of both passengers and drivers in Singapore.When we noticed the existence of seasonality for each day of the week, we came up with more precise analytical and business questions to dig deeper into the data. Upon verification of hypotheses, we decided that we will build a widget.Then joined the Behavioural Science, UX (User Experience) Design and the Product Management teams, who started giving shape to the problem we are solving. Our Behavioural Scientists shared their expertise on how information, suggestions and choices should be presented to enable easy assimilation and beneficial action. Daily whiteboarding breakouts, endless back-and forth conversations, and a healthy amount of challenge-and-accept culture ensured that we distilled the idea down to its core. We then presented the relevant information with just the right level of detail, and with the right amount of messaging, to allow users to take the intended action i.e. shift his/her demand outside of peak periods if possible.Our amazing regional Copywriting team then swung in to put our intent into words in 7 different languages for our users across South-East Asia. Simultaneously, our UX designers and Full-stack Engineers started exploring the best visual components to communicate data on time trends to users. More on this later, but suffice to say that plenty of ideas were explored and discarded in a collaborative process, which aimed to create something that’s intuitive and engaging while being robust and scalable to work across all types of devices.While these designs made their way up to engineering, the Data Science team worked on finding the most rigorous method to deduce the historical trend of surge across all our cities and areas, and time periods within them. There were discussions on how to best store and update this data reliably so that the widget itself can access it with great performance.Soon after, we went into the development process, and voila! We had the first iteration of the widget ready on our staging (internal testing) servers in just 2 weeks! This prototype was opened up to the core team for influx of feedback.And just two weeks later, the widget made its way to our Singapore and Jakarta Feeds, accessible to the world at large! Feedback from our users started pouring in almost immediately (thanks to the rich feedback functionality that comes with each widget), ranging from great to sometimes not-so-great, and we listened to all of it with a keen ear! And thus began a new cycle of iterations and continuous improvement, more of which we will share in a subsequent post.In the Trenches with the Creators: How Multiple Teams Got Together to Make this Come TrueVarious disciplines within our cross functional team came together to whip out this widget by quipping their expertise to the end product.Using Behavioural Science to Simplify Choices and Design Good OutcomesBehavioural Science helped to explore many facets of consumer behaviour in order to plan and design the widget: understanding how consumers think and conceptualising a widget that can be easily understood and used by the consumers.While fares are governed entirely by market conditions, it’s important for us to explain the economics to consumers. As a consumer-centric company, we aim to make the consumers feel like they own their decisions, which they can take based on full information. And this is the role of Behavioural Scientists at Grab!In guiding the consumers through the information, Behavioural Science team had the following three objectives in mind while building this Travel Trends widget:  Offer transparency on the fares: By exposing our historic surge levels for a 4 hour period, we wanted to ensure that the passenger is aware of the surge levels and does not treat the fare as a nasty shock.  Give information that helps them plan: By showing them surge levels for the future 2 hours, we wanted to help consumers who have the flexibility, plan for a better time, hence, giving them the power to decide based on transparent information.  Provide helpful tips: Every bar gives users tips on the conditions at that time and the immediate future. For instance, a low surge bar, followed by a high surge bar gives the tip “Psst… Leave now, It might get busy later!”, helping people understand the graph better and nudging them to take an action. If you are interested in saving fares, may we suggest tapping around all the bars to reveal the secret pro-tips?Designing Interfaces that Lead to Consumer Success by Abstracting ComplexityDesign team is the one behind the colours and shapes that make up the widget that you see and interact with! The team took inspiration from Google’s Popular Times.    Source/Credits: Google Live Popular Times&nbsp;Right from the offset, our content and product teams were keen to surface additional information and actions with each bar to keep the widget interactive and useful. One of the early challenges was to arrive at the right gesture that invites the user to interact and intuitively navigate the bars on the widget but also does not conflict with other gestures (eg scrolling and scrubbing) that the user was pre-trained to perform on the feed. We found out that tapping was simultaneously an unused and yet intuitive gesture that we could use for interaction with the bars.We then went into rounds of iteration on the visual design of the widget. In this process, multiple stakeholders were involved ranging from Product to Content to Engineering. We had to overcome a number of constraints i.e. the limited canvas of a widget and the context of a user when she is exploring the feed. By re-using existing libraries and components, we managed to keep the development light and ship something fast.  Dozens of revisions and four iterations later, we landed with a design that we felt equipped the feature for its user-facing goal, and did so in a manner which was aesthetically appealing!And finally we managed to deliver on the feature’s goal, by surfacing just the right detail of information in a manner that is intuitive yet effective to peak-shift demand.  Bringing All of This to Fruition Through High Performance EngineeringOur Development Engineering team was in charge of developing the widget and making it available to our users in just a few weeks’ time - materialising the work of the other teams.One of their challenges was to find the best way to process the vast amount of data (millions of database entries) so it can be visualised simply as bar charts. Grab’s engineers had to achieve this while making sure performance is as resilient as possible.There were two options in doing this:a) Fetch the data directly from the DB for each API call; orb) Store the data in an in-memory data structure on a timely basis, so when a user calls the API will no longer have to hit the DB.After considering that this feature will likely expect a lot of traffic thus high QPS, we decided that the former option would be too costly. Ultimately, we chose the latter option since it is more performant and more scalable.At the frontend, the challenge was to cater to the intricate request from our designers. We use chart libraries to increase our development speed, and not all of the requirements were readily supported by these libraries.For instance, let’s say this library makes visualising charts easy, but not so much for customising them. If designers wanted to have an average line in a dotted form, the library did not support this so easily. Also, the moving arrow pointers as you move between bar chart, changing colours of the bars changes when clicked – all required countless CSS tweaks.    Closing the Product Loop with User Feedback and Data Driven InsightsOne of the most crucial parts of launching any product is to ensure that consumers are engaging with the widget and finding it useful.To understand what consumers think about the widget, whether they find it useful and whether it is helping them to plan better,  we delved into the huge mine of clickstream data.  We found that 1 in 3 users who make a booking every day interact with the widget. And of these people, more than 70% users have given positive rating for the widget. This validates our initial hypothesis that if given an option, our consumers will love the freedom to plan their trips and inculcate more transparent ecosystem.These users also indicate the things they like most about the widget. 61% of users gave positive rating for usefulness, 20% were impressed by the design (Kudos to our fantastic designer Ajmal!!) and 13% for usability.  Beyond internal data, our widget made some rounds on social media channels. For Example, here is screenshot of what our users have to say on Twitter.We closely track these metrics on user engagement and feedback to ensure that we keep improving and coming up with new iterations which helps us to serve our consumers in a better way.ConclusionWe hope you enjoyed reading about how we went from ideation, through iterations to a finished widget in the hands of the user, all in 1 month! Many hands helped along the way. If you are interested in joining this hyper-proactive problem-solving team, please check out Grab’s career site!And if you have feedback for us, we are here to listen! While we cannot be happier to see some positive reaction from the public, we are also thrilled to hear your suggestions and advice. Please leave us a memo using the Widget’s comment function!EpilogueWe just released an upgrade to this widget which allows users to set reminders and be notified about availability of good fares in a time period of their choosing. We will keep a watch and come knocking! Go ahead, find the widget on your Grab feed, set a reminder and save on fares on your next ride!",
        "url": "/peak-shift-demand-travel-trends"
      }
      ,
    
      "structured-logging": {
        "title": "Structured Logging: The Best Friend You’ll Want When Things Go Wrong",
        "author": "aditya-praharaj",
        "tags": "[&quot;Logging&quot;]",
        "category": "",
        "content": "IntroductionEvery day millions of people around Southeast Asia count on Grab to get themselves or what they need from point A to B in a safe, comfortable and reliable manner. In fact, just very recently we crossed our 3 billion transport rides milestone, gaining the last billion in just a mere 6 months!We take this responsibility very seriously, and as we continue to grow and expand, it’s important for us to maintain a sophisticated backend system that is capable of sustaining the kind of scale needed to support all our consumers in Southeast Asia. This backend system is comprised of multiple services that interact with each other in many different ways. As Grab evolves, maintaining them becomes a significantly larger and harder task as developers continuously develop new features.To maintain these systems well, it’s important to have better observability; data that helps us better understand what is happening in the system by having good monitoring (metrics), event logs, and tracing for request scope data. Out of these, logs provide the most complete picture of what happened within the system - and is typically the first and most engaged point of contact. With good logs, the backend becomes much easier to understand, maintain, and debug. Without logs or with bad logs - we have a recipe for disaster; making it nearly impossible to understand what’s happening.In this article, we focus on a form of logging called structured logging. We discuss what it is, why is it better, and how we built a framework that integrates well with our current Elastic stack-based logging backend, allowing us to do logging better and more efficiently.Structured Logging is a part of a larger endeavour which will enable us to reduce the Mean Time To Resolve (MTTR), helping developers to mitigate issues faster when outages happen.What are Logs?Logs are lines of texts containing some information about some event that occurred in our system, and they serve a crucial function of helping us understand what’s happening in the backend. Logs are usually placed at points in the code where a significant event has happened (for example, some database operation succeeded or a passenger got assigned to a driver) or at any other place in the code that we are interested in observing.The first thing that a developer would normally do when an error is reported is check the logs - sort of like walking through the history of the system and finding out what happened. Therefore, logs can be a developer’s best friend in times of service outages, errors, and failed builds.Logs in today’s world have varying formats and features.  Log Format: These range from simple key-value based (like syslog) to quite structured and detailed (like JSON). Since logs are mostly meant for developer eyes, how detailed or structured a log is dictates how fast the developer can query the logs, as well as read them. The more structured the data is - the larger the size is per log line, although it’s more queryable and contains richer information.  Levelled Logging (or Log Levels): Logs with different severities can be logged at different levels. The visibility can be limited to a single level, limiting all logs only with a certain severity or above (for example, only logs WARN and above). Usually log levels are static in production environments, and finding DEBUG logs usually requires redeploying.  Log Aggregation Backend: Logs can have different log aggregation backends, which means different backends (i.e. Splunk, Kibana, etc.) decide what your logs might look like or what you might be able to do with them. Some might cost a lot more than others.  Causal Ordering: Logs might or might not preserve the exact time in which they are written. This is important, as how exact the time is dictates how accurately we can predict the sequence of events via logs.  Log Correlation: We serve countless requests from our backend services. Being able to see all the logs relevant to a particular request or a particular event helps us drill down to relevant  information for a specific request (e.g. for a specific passenger trying to book a ride).Combine this with the plethora of logging libraries available and you easily have a developer who is holding his head in confusion, unable to decide what to use. Also, each library has their own set of advantages and disadvantages, so the discussion might quickly become subjective and polarised - therefore it is crucial that you choose the appropriate library and backend pair for your applications.We at Grab use different types of logging libraries. However, as requirements changed  - we also found ourselves re-evaluating our logging strategy.The State of Logging at GrabThe number of Golang services at Grab has continuously grown. Most services used syslog-style key-value format logs, recognised as the most common format of logs for server-side applications due to its simplicity and ease for reading and writing. All these logs were made possible by a handful of common libraries, which were directly imported and used by different services.We used a cloud-based SaaS vendor as a frontend for these logs, where application-emitted logs were routed to files and sent to our logging vendor, making it possible to view and query them in real time. Things were pretty great and frictionless for a long time.However, as time went by, our logging bills started mounting to unprecedented levels and we found ourselves revisiting and re-evaluating how we did logging. A few issues surfaced:  Logging volume reduction efforts were successful to some extent - but were arduous and painful. Part of the reason was that almost all the logs were at a single log level - INFO.    Figure 1: Log Level Usage&nbsp;This issue was not limited to a single service, but pervasive across services. For mitigation, some services added sampling to logs, some removed logs altogether. The latter is only a recipe for disaster, so it was known that we had to improve levelled logging.  The vendor was expensive for us at the time and also had a few concerns - primarily with limitations around DSL (query language). There were many good open source alternatives available - Elastic stack to name one. Our engineers felt confident that we could probably manage our logging infrastructure and manage the costs better - which led to the proposal and building of Elastic stack logging cluster. Elasticsearch is vastly more powerful and rich than our vendor at the time and our current libraries weren’t enough to fully leverage its capabilities, so we needed a library which can leverage structure in logs better and easily integrate with Elastic stack.  There were some minor issues in our logging libraries namely:          Singleton initialisation pattern that made unit-testing harder      Single logger interface that reduced the possibility of extending the core logging functionality as almost all the services imported the logger interface directly      No out-of-the-box support for multiple writers            If we were to write a library, we had to fix these issues - and also encourage usage of best practices.    Grab’s critical path (number of services traversed by a single booking flow request) has grown in size. On average, a single booking request touches multiple microservices - each of which does something different. At the large scale at which we operate, it’s necessary therefore to easily view logs from all the services for a single request - however this was not something which was done automatically by the library. Hence, we also wanted to make log correlation easier and better.  Logs are events which happened at some point of time. The order in which these events occurred gives us a complete history of what happened in the system. However, the core logging library which formed the base of the logging across our Golang services didn’t preserve the log generation time (it instead used write time). This led to jumbling of logs which are generated in a span of a few microseconds - which not only makes the lives of our developers harder, but makes it near impossible to get an exact history of the system. This is why we wanted to also improve and enable causal ordering of logs - one of the key steps in understanding what’s happening in the system.Why Change?As mentioned, we knew there were issues with how we were logging. To best approach the problem and be able to solve it as much as possible without affecting existing infrastructure and services, it was decided to bootstrap a new library from the ground up. This library would solve known issues, as well as contain features which would not have been possible by modifying existing libraries. For a recap, here’s what we wanted to solve:  Improve levelled logging  Leverage structure in logs better  Easily integrate with Elastic stack  Encourage usage of best practices  Make log correlation easier and better  Improve and enable causal ordering of logs for a better understanding of service distributionEnter Structured Logging. Structured Logging has been quite popular around the world, finding widespread adoption. It was easily integrable with our Elastic stack backend and would also solve most of our pain points.Structured LoggingKeeping our previous problems and requirements in mind, we bootstrapped a library in Golang, which has the following features:Dynamic Log LevelsThis allows us to change our initialised log levels at runtime from a configuration management system - something which was not possible and encouraged before.This makes the log levels actually more meaningful now - developers can now deploy with the usual WARN or INFO log levels, and when things go wrong, just with a configuration change they can update the log level to DEBUG and make their services output more logs when debugging. This also helps us keep our logging costs in check. We made support for integrating this with our configuration management system easy and straightforward.Consistent Structure in LogsLogs are inherently unstructured unlike database schema, which is rigid, or a freeform text, which has no structure. Our Elastic stack backend is primarily based on indices (sort of like tables) with mapping (sort of like a loose schema). For this, we needed to output logs in JSON with a consistent structure (for example, we cannot output integer and string under the same JSON field because that will cause an indexing failure in Elasticsearch). Also, we were aware that one of our primary goals was keeping our logging costs in check, and since it didn’t make sense to structure and index almost every field - adding only the structure which is useful to us made sense.For addressing this, we built a utility that allows us to add structure to our logs deterministically. This is built on top of a schema in which we can add key-value pairs with a specific key name and type, generate code based on that - and use the generated code to make sure that things are consistently formatted and don’t break. We called this schema (a collection of key name and type pairs) the Common Grab Log Schema (CGLS). We only add structure to CGLS which is important - everything included in CGLS gets formatted in the different field and everything else gets formatted in a single field in the generated JSON. This helps keeps our structure consistent and easily usable with Elastic stack.    Figure 2: Overview of Common Grab Log Schema for Golang backend servicesPlug and Play Support with Grab-KitWe made the initialisation and use easy and out-of-the-box with our in-house support for Grab-Kit, so developers can just use it without making any drastic changes. Also, as part of this integration, we added automatic log correlation based on request IDs present in traces, which ensured that all the logs generated for a particular request already have that trace ID.Configurable Log FormatOur primary requirement was building a logger expressive and consistent enough to integrate with the Elastic stack backend well - without going through fancy log parsing in the downstream. Therefore, the library is expressive and configurable enough to allow any log format (we can write different log formats for different future use cases. For example, readable format in development settings and JSON output in production settings), with a default option of JSON output. This ensures that we can produce log output which is compatible with Elastic stack, but still be configurable enough for different use cases.Support for Multiple Writes with Different FormatsAs part of extending the library’s functionality, we needed enough configurability to be able to send different logs to different places at different settings. For example, sending FATAL logs to Slack asynchronously in some readable format, while sending all the usual logs to our Elastic stack backend. This library includes support for chaining such “cores” to any arbitrary degree possible - making sure that this logger can be used in such highly specialised cases as well.Production-like Logging Environment in DevelopmentDevelopers have been seeing console logs since the dawn of time, however having structured JSON logs which are only meant for production logs and are more searchable provides more power. To leverage this power in development better and allow developers to directly see their logs in Kibana, we provide a dockerised version of Kibana which can be spun up locally to accept structured logs. This allows developers to directly use the structured logs and see their logs in Kibana - just like production!Having this library enabled us to do logging in a much better way. The most noticeable impact was that our simple access logs can now be queried better - with more filters and conditions.    Figure 3: Production-like Logging Environment in DevelopmentCausal OrderingHaving an exact history of events makes debugging issues in production systems easier - as one can just look at the history and quickly hypothesise what’s wrong and fix it. To this end, the structured logging library adds the exact write timestamp in nanoseconds in the logger. This combined with the structured JSON-like format makes it possible to sort all the logs by this field - so we can see logs in the exact order as they happened - achieving causal ordering in logs. This is an underplayed but highly powerful feature that makes debugging easier.    Figure 4: Causal ordering of logs with Y'ALLBut Why Structured Logging?Now that you know about the history and the reasons behind our logging strategy, let’s discuss the benefits that you reap from it.On the outset, having logs well-defined and structured (like JSON) has multiple benefits, including but not limited to:  Better root cause analysis: With structured logs, we can ingest and perform more powerful queries which won’t be possible with simple unstructured logs. Developers can do more informative queries on finding the logs which are relevant to the situation. Not only this, log correlation and causal ordering make it possible to gain a better understanding of the distributed logs. Unlike unstructured data, where we are only limited to full-text or a handful of log types, structured logs take the possibility to a whole new level.  More transparency or better observability: With structured logs, you increase the visibility of what is happening with your system - since now you can log information in a better, more expressive way. This enables you to have a more transparent view of what is happening in the system and makes your systems easier to maintain and debug over longer periods of time.  Better consistency: With structured logs, you increase the structure present in your logs - and in turn, make your logs more consistent as the systems evolve. This allows us to index our logs in a system like Elastic stack more easily as we can be sure that we are sticking to some structure. Also with the adoption of a common schema, we can be rest assured that we are all using the same structure.  Better standardisation: Having a single, well-defined, structured way to do logging allows us to standardise logging - which reduces cognitive overhead of figuring out what happened in systems via logs and allows easier adoption. Instead of going through 100 different types of logs, you instead would only have a single format. This is also one of the goals of the library - standardising the usage of the library across Golang backend services.We get some additional benefits as well:  Dynamic Log Levels: This allows us to have meaningful log levels in our code - where we can deploy with baseline warning settings and switch to lower levels (debug logs) only when we need them. This helps keep our logging costs low, as well as reduces the noise that developers usually need to go through when debugging.  Future-proof Consistency in Logs: With the adoption of a common schema, we make sure that we stick with the same structure, even if say tomorrow our logging infrastructure changes - making us future-ready. Instead of manually specifying what to log, we can simply expose a function in our loggers.  Production-Like Logging Environment in Development: The dockerised Kibana allows developers to enjoy the same benefits as the production Kibana. This also encourages developers to use Elastic stack more and explore its features such as building dashboards based on the log data, having better watchers, and so on.I hope you have enjoyed this article and found it useful. Comments and corrections are always welcome.Happy Logging!",
        "url": "/structured-logging"
      }
      ,
    
      "data-ingestion-transformation-product-insights": {
        "title": "How We Simplified Our Data Ingestion &amp; Transformation Process",
        "author": "yichao-wangroman-atachiantsoscar-cassetticorey-scott",
        "tags": "[&quot;Big Data&quot;, &quot;Data Pipeline&quot;]",
        "category": "",
        "content": "IntroductionAs Grab grew from a small startup to an organisation serving millions of consumers and driver-partners, making day-to-day data-driven decisions became paramount. We needed a system to efficiently ingest data from mobile apps and backend systems and then make it available for analytics and engineering teams.Thanks to modern data processing frameworks, ingesting data isn’t a big issue. However, at Grab’s scale, it is a non-trivial task. We had to prepare for two key scenarios:  Business growth, including organic growth over time and expected seasonality effects.  Any unexpected peaks due to unforeseen circumstances. Our systems have to be horizontally scalable.We could ingest data in batches, in real time, or a combination of the two. When you ingest data in batches, you can import it at regularly scheduled intervals or when it reaches a certain size. This is very useful when processes run on a schedule, such as reports that run daily at a specific time. Typically, batched data is useful for offline analytics and data science.On the other hand, real-time ingestion has significant business value, such as with reactive systems. For example, when a consumer provides feedback for a Grab superapp widget, we re-rank widgets based on that consumer’s likes or dislikes. Note that when information is very time-sensitive, you must continuously monitor its data.This blog post describes how Grab built a scalable data ingestion system and how we went from prototyping with Spark Streaming to running a production-grade data processing cluster written in Golang.Building the System Without Reinventing the WheelThe data ingestion system:  Collects raw data as app events.  Transforms the data into a structured format.  Stores the data for analysis and monitoring.In a previous blog post, we discussed dealing with batched data ETL with Spark. This post focuses on real-time ingestion.We separated the data ingestion system into 3 layers: collection, transformation, and storage. This table and diagram highlights the tools used in each layer in our system’s first design.      LayerTools              Collection      Gateway, Kafka              Transformation      Go processing service, Spark Streaming              Storage      TalariaDB      Our first design might seem complex, but we used battle-tested and common tools such as Apache Kafka and Spark Streaming. This let us get an end-to-end solution up and running quickly.Collection LayerOur collection layer had two sub-layers:  Our custom built API Gateway received HTTP requests from the mobile app. It simply decoded and authenticated HTTP requests, streaming the data to the Kafka queue.  The Kafka queue decoupled the transformation layer (shown in the above figure as the processing service and Spark streaming) from the collection layer (shown above as the Gateway service). We needed to retain raw data in the Kafka queue for fault tolerance of the entire system. Imagine an error where a data pipeline pollutes the data with flawed transformation code or just simply crashes. The Kafka queue saves us from data loss by data backfilling.Since it’s robust and battle-tested, we chose Kafka as our queueing solution. It perfectly met our requirements, such as high throughput and low latency. Although Kafka takes some operational effort such as self-hosting and monitoring, Grab has a proficient and dedicated team managing our Kafka cluster.Transformation LayerThere are many options for real-time data processing, including Spark Streaming, Flink, and Storm. Since we use Spark for all our batch processing, we decided to use Spark Streaming.We deployed a Golang processing service between Kafka and Spark Streaming. This service converts the data from Protobuf to Avro. Instead of pointing Spark Streaming directly to Kafka, we used this processing service as an intermediary. This was because our Spark Streaming job was written in Python and Spark doesn’t natively support protobuf decoding. We used Avro format, since Grab historically used it for archiving streaming data. Each raw event was enriched and batched together with other events. Batches were then uploaded to S3.Storage LayerTalariaDB is a Grab-built time-series database. It ingests events as columnar ORC files, indexing them by event name and time. We use the same ORC format files for batch processing. TalariaDB also implements the Presto Thrift connector interface, so our users could query certain event types by time range. They did this by connecting a Presto to a TalariaDB hosting distributed cluster.ProblemsBuilding and deploying our data pipeline’s MVP provided great value to our data analysts, engineers, and QA team. For example, our mobile app team could monitor any abnormal change in the real-time metrics, such as the screen load time for the latest released app version. The QA team could perform app side actions (book a ride, make payment, etc.) and check which events were triggered and received by the backend. The latency between the ingestion and the serving layer was only 4 minutes instead of the batch processing system’s 60 minutes. The streaming processing’s data showed good business value.This prompted us to develop more features on top of our platform-collected real-time data. Very soon our QA engineers and the product analytics team used more and more of the real-time data processing system. They started instrumenting various mobile applications so more data started flowing in. However, as our ingested data increased, so did our problems. These were mostly related to operational complexity and the increased latency.Operational ComplexityOnly a few team members could operate Spark Streaming and EMR. With more data and variable rates, our streaming jobs had scaling issues and failed occasionally. This was due to checkpoint issues when the cluster was under heavy load. Increasing the cluster size helped, but adding more nodes also increased the likelihood of losing more cluster nodes. When we lost nodes, our latency went up and added more work for our already busy on-call engineers.Supporting Native ProtobufTo simplify the architecture, we initially planned to bypass our Golang-written processing service for the real-time data pipeline. Our plan was to let Spark directly talk to the Kafka queue and send the output to S3. This required packaging the decoders for our protobuf messages for Python Spark jobs, which was cumbersome. We thought about rewriting our job in Scala, but we didn’t have enough experience with it.Also, we’d soon hit some streaming limits from S3. Our Spark streaming job was consuming objects from S3, but the process was not continuous due to S3’s  eventual consistency. To avoid long pagination queries in the S3 API, we had to prefix the data with the hour in which it was ingested. This resulted in some data loss after processing by the Spark streaming. The loss happened because the new data would appear in S3 while Spark Streaming had already moved on to the next hour. We tried various tweaks, but it was just a bad design. As our data grew to over one terabyte per hour, our data loss grew with it.Processing LagOn average, the time from our system ingesting an event to when it was available on the Presto was 4 to 6 minutes. We call that processing lag, as it happened due to our data processing. It was substantially worse under heavy loads, increasing to 8 to 13 minutes. While that wasn’t bad at this scale (a few TBs of data), it made some use cases impossible, such as monitoring. We needed to do better.Simplifying the Architecture and Rewriting in GolangAfter completing the MVP phase development, we noticed the Spark Streaming functionality we actually used was relatively trivial. In the Spark Streaming job, we only:  Partitioned the batch of events by event name.  Encoded the data in ORC format.  And uploaded to an S3 bucket.To mitigate the problems mentioned above, we tried re-implementing the features in our existing Golang processing service. Besides consuming the data and publishing to an S3 bucket, the transformation service also needed to deal with event partitioning and ORC encoding.One key problem we addressed was implementing a robust event partitioner with a large write throughput and low read latency. Fortunately, Golang has a nice concurrent map package. To further reduce the lock contention, we added sharding.We made the changes, deployed the service to production, and discovered our service was now memory-bound as we buffered data for 1 minute. We did thorough benchmarking and profiling on heap allocation to improve memory utilisation. By iteratively reducing inefficiencies and contributing to a lower CPU consumption, we made our data transformation more efficient.PerformanceAfter revamping the system, the elapsed time for a single event to travel from the gateway to our dashboard is about 1 minute. We also fixed the data loss issue. Finally, we significantly reduced our on-call workload by removing Spark Streaming.ValidationAt this point, we had both our old and new pipelines running in parallel. After drastically improving our performance, we needed to confirm we still got the same end results. This was done by running a query against each of the pipelines and comparing the results. Both systems were registered to the same Presto cluster.We ran two SQL “excerpts” between the two pipelines in different order. Both queries returned the same events, validating our new pipeline’s correctness.select count(1) from (( select uuid, time from grab_x.realtime_new where event = 'app.metric1' and time between 1541734140 and 1541734200) except ( select uuid, time from grab_x.realtime_old where event = 'app.metric1' and time between 1541734140 and 1541734200))/* output: 0 */ConclusionsScaling a data ingestion system to handle hundreds of thousands of events per second was a non-trivial task. However, by iterating and constantly simplifying our overall architecture, we were able to efficiently ingest the data and drive down its lag to around one minute.Spark Streaming was a great tool and gave us time to understand the problem. But, understanding what we actually needed to build and iteratively optimise the entire data pipeline led us to:  Replacing Spark Streaming with our new Golang-implemented pipeline.  Removing Avro encoding.  Removing an intermediary S3 step.Differences between the old and new pipelines are:          Old Pipeline    New Pipeline        Languages    Python, Go    Go        Stages    4 services    3 services        Conversions    Protobuf → Avro → ORC    Protobuf → ORC        Lag    4-13 min    1 min  Systems usually become more and more complex over time, leading to tech debt and decreased performance. In our case, starting with more steps in the data pipeline was actually the simple solution, since we could re-use existing tools. But as we reduced processing stages, we’ve also seen fewer failures. By simplifying the problem, we improved performance and decreased operational complexity. At the end of the day, our data pipeline solves exactly our problem and does nothing else, keeping things fast.",
        "url": "/data-ingestion-transformation-product-insights"
      }
      ,
    
      "understanding-supply-demand-ride-hailing-data": {
        "title": "Understanding Supply &amp; Demand in Ride-hailing Through the Lens of Data",
        "author": "aayush-garglara-pureum-yimchunkai-phang",
        "tags": "[&quot;Analytics&quot;, &quot;Data&quot;, &quot;Data Analytics&quot;, &quot;Data Visualisation&quot;, &quot;Data Storytelling&quot;]",
        "category": "",
        "content": "The #1 Goal in Ride-Hailing: AllocationGrab’s ride-hailing business in its simplest form is about matchmaking Passengers looking for a comfortable mode of transport and Drivers looking for a flexible earning opportunity.Over the last 6 years, Grab has repeatedly fine-tuned its machine learning algorithms with the goal of ensuring that passengers get a ride when they want it, and that they are matched to the drivers that are closest to them.But drivers are constantly on the move, and at any one point there could be hundreds of passengers requesting a ride within the same area. This means that sometimes, the closest available drivers might still be too far away.The Analytics team at Grab attempts to analyse these instances at scale via clearly-defined metrics. We study the gaps so that we can identify potential product and operational solutions that may guide supply and demand towards geo-temporal alignment and better experience.In this article, we give you a glimpse of one of our analytics initiatives - to measure the supply and demand ratio at any given area and time.Defining Supply and DemandA single unit of Supply is considered as a driver who is Online and Idle (not currently on a job) at the beginning of an x seconds slot, where x is a minuscule unit of time. The driver’s GPS ping at the beginning of this x seconds slot is considered to be his or her location.A single unit of Demand is considered as a passenger who is checking fares for a ride via our app within the same x seconds slot. We consider the passenger’s location to be the pick up address entered.Mapping Supply and DemandFor the purpose of analysis, each location is aggregated to a geohash (a geographic location encoded into a string of letters and digits) with a precision of y where y refers to a very small polygon space of dimensions on the map. Each unit of Supply is then mapped to all units of Demand within the supply’s neighbouring geohashes as displayed in Figure 1.Figure 1: Illustration depicting a supply unit distributed among the demand units in its neighbouring geohashesA fraction of each unit of Supply is assigned to each unit of Demand in the neighbouring geohashes inversely weighted by Distance. Essentially, this means that a driver is more available to nearer passengers compared to further ones.To keep things simple for this article, we have used Straight Line Distance instead of Route Distance as a proxy to reduce the complexity of the algorithms.Summation of fractions of available drivers for each passenger would give the effective supply for each passenger. This is depicted in figure 1 where each passenger shares a small fraction of the supply.For this analysis, we have aggregated demand and effective supply for every geohash i and a time slot j combination, resulting in two simple aggregated metrics: Supply Demand Ratio and Supply Demand Difference (Figure 2).Figure 2: The metrics aggregated for any area and time slotProcessing the DataWhile the resulting metrics may look like a simple ratio and difference, calculating effective supply, which requires mapping every driver and passenger in neighbouring space, is a considerably heavy computation.Across the region, there could be hundreds of thousands of passengers looking for a ride at any given point in time. Our algorithms not only identify each Demand and Supply unit and its location, but also maps every Supply unit to all the Demand units in the same neighbourhood to output the fractional supply available to each passenger.Simply put, the complexity can be summarised as the following: Every extra Supply or Demand unit exponentially increases the algorithm’s computation power.This is just one of many high-computation problems that the Analytics team handles on a daily basis. So the problem solving doesn’t necessarily end with developing a network-representative algorithm or metric, but to be able to make it performant and usable, even as the business scales.Visualising the Metrics on a MapWith the metrics we discussed above, we can map out how gaps between demand and supply can evolve throughout the day. The GIF below displays Singapore’s supply demand gap on a typical day.Each bubble indicates a minuscule area on the map. The size of each bubble indicates the Supply Demand difference in that area - the bigger the bubble, the bigger the gap. We’ve also coloured the bubbles to indicate the Supply Demand Ratio where Red signifies Undersupplied and Green signifies Oversupplied.To meet our goals of ensuring that passengers can always find a ride whenever they want it, we need to balance demand and supply. At Grab, we do this in many ways, including on one hand - finding ways to move oversupply to areas where there is higher demand, and on the other - shifting less time-sensitive demand away from peak time-slots.Identifying Spatial Opportunities to Position SupplyFigure 3: Supply Demand Distribution in Singapore on a typical weekdayAt any given time of the day, there may be an oversupply of drivers in one area while there is undersupply in others.As shown in Figure 3, this is common in Singapore after morning peak hours when most rides end in CBD which results in an oversupply in the area. Such scenarios are also common at queueing spots such as Changi Airport.To address this geo-temporal misalignment, Grab recently updated the Heatmap on the Grab driver app to encourage drivers to move away from oversupplied areas to areas where there is higher demand.Identifying Temporal Opportunities to Move DemandFigure 4: Typical Supply Demand Distribution in a small residential area in Singapore across the day.Figure 4 is an aggregated representation of supply and demand on a typical weekday in a small residential area in Singapore.The highlighted region in Figure 4 depicts a time period when demand and supply are mismatched. Based on historical data, we know that demand can peak due to various factors both expected (usual peak hours) and unexpected (sudden heavy rain). However, supply amplifies at a delayed time period, usually when the demand is already subsiding.Figure 5: Travel Trends Widget on the passenger app showing best times to book in River Valley (Singapore)To address this imbalance, Grab recently launched a Travel Trends Widget (Figure 5) on the passenger app to let our riders know of the predicted demand distribution across hours.This widget shows you demand trends, based on the summation of historical data for a passenger’s specific location. The goal here is to encourage time-insensitive demand (passengers who don’t need a ride immediately) to book slightly later, helping passengers with more urgent needs to get allocated with higher ease.As testimony to its usefulness, the Travel Trends Widget is now ranked #1 among all of Grab’s widgets! With the highest number of click-throughs, we have observed that hundreds of thousands of people are finding it useful for their daily use! Watch out for the next upgraded version as we continue to improve it to be more contextual and smart!Stay Tuned for More!Given the continuously-changing reality where there is a constantly-fluctuating supply and demand, Grab’s Transportation team’s ultimate goal boils down to just one thing: to ensure that our passengers can get a ride when and where they need it, as fast and easy as possible; while providing our drivers better livelihood, through rewarding experience.To get this right - balancing demand and supply is crucial. There are many ways we do it. We have shared a couple in this piece, but another important element is dynamic pricing - where fares respond to shifts in supply and demand.We’ll be taking a closer look at this topic in another article. So stay tuned!Interested? Join Us!Grab’s Analytics team provides integral support to all of Grab’s services and products.This is only a glimpse of the Analytics team’s efforts to deeply understand our data, use it to evaluate the platform’s performance and continuously iterate to build better data driven products.If you are interested in solving problems like this, join us! We are hiring! Visit our career website to check out the openings!AcknowledgementsWe would like to thank the many contributors to the work mentioned above: Ashwin Madelil (Product Manager), Shrey Jain (Product Manager), Brahmasta Adipradana (Product Manager), Prashant Kumar (Product Manager), and Ajmal Jamal (Designer).",
        "url": "/understanding-supply-demand-ride-hailing-data"
      }
      ,
    
      "experimentation-platform-data-pipeline": {
        "title": "A Lean and Scalable Data Pipeline to Capture Large Scale Events and Support Experimentation Platform",
        "author": "oscar-cassettiroman-atachiants",
        "tags": "[&quot;Big Data&quot;, &quot;Data Pipeline&quot;, &quot;Experiment&quot;]",
        "category": "",
        "content": "IntroductionFast product development and rapid innovation require running many controlled online experiments on large user groups. This is challenging on multiple fronts, including cultural, organisational, engineering, and trustworthiness.To address these challenges, we need a holistic view of all our systems and their interactions:    For a holistic view, don’t just track systems closely related to your experiments. This mitigates the risk of a positive outcome on specific systems translating into a negative global outcome.  When developing new products, we need to know how events interact.For example, imagine we plan to implement a new feature to increase user engagement. We can design a simple A/B test that measures the user engagement with our product for two randomised groups of users. Let’s assume we ran the experiment and the test shows the engagement significantly increased for the treatment group. Is it safe to roll out this feature? Not necessarily, since our experiment only monitored one metric without considering others.Let’s assume an application where click through rate is a target metric we want to keep optimal since its value impacts our bottom line. Suppose we add a new feature and want to make sure our metric improves. We experiment and find it does improve our target metric. However, our DevOps team tells us the server load metrics degraded. Therefore, our next question is “are the server load metrics different between treatment and control?”.Obviously, it gets complicated when you have many experiments and metrics. Manually keeping track of all the metrics and interactions is neither practical nor scalable. Therefore, we need a system that lets us build metrics, measure and track interactions, and also allows us to develop features enabling global optimisation across our various product verticals.To build such a system, we must capture, ingest, and process data, and then serve the insights as part of our experiment results. In 2017, we started building the various layers to support this goal. In this post, we describe our progress, and lessons learned in building a system that ingests and processes petabytes of data for analytics.Data Lakes and Data PipelinesThe data pipeline concept is closely related to data lakes. Just like a lake that rivers and smaller streams flow into, a data lake is where various data streams and sources are collected, stored and utilised. Typically, a data pipeline destination is a data lake.[image source]Just as people use lakes for different purposes, Product Analytics and Data Scientists use data lakes for many purposes, ranging from data mining to monitoring and alerting.In contrast, a data pipeline is one way data is sourced, cleansed, and transformed before being added to the data lake. Moving data from a source to a destination can include steps such as copying the data, and joining or augmenting it with other data sources. A data pipeline is the sum of all the actions taken from the data source to its destination. It ensures the actions happen automatically and in a reliable way.Let’s consider two types of data pipelines: batch and stream. When you ingest data in batches, data is imported at regularly scheduled intervals. On the other hand, real-time ingestion or streaming is necessary when information is very time-sensitive.This post focuses on the lessons we learned while building our batch data pipeline.Why We Built Our Own Data PipelineAt the beginning of 2018, we designed the first part of our Mobile event Collector and Dispenser system (McD) that lets our mobile and backend applications send data to a data pipeline. We started with a small number of events (few thousand per second). But with Grab’s rapid growth, scaling our data pipeline was challenging. At the time of writing, the McD service ingests approximately400,000 events per second. Designing, implementing, and scaling our pipeline in less than a year was not easy. Also, we are a small and lean team. This affected the technologies we could use and how we developed and deployed the various components.Most importantly, we needed to keep things operationally simple and reliable. For instance, we decided to seek frameworks that support some form of SQL and a high-level language, since SQL is popular among Grabbers.Design RequirementsTo kick off the process, we first interviewed the project’s potential stakeholders, including both product owners and engineers.  The two questions we asked were:  Who will access the data?  What were their expectations in terms of lag between data being captured at source and the data being available through the serving layer?This second question is often missed when building data warehouses and ETL jobs. But for us, its answers were the cornerstone for future decisions.From the answers, we realised we needed to support access patterns from different users:  Data analysts performing analytical tasks such as querying the data for counts, averages within specific date ranges (one day, one week), and specific granularity (i.e. one hour). As we need to provide new data daily, this use case has an SLA of one day.  Data scientists doing Exploratory Data Analysis, building a dataset for training machine learning models, running optimisation algorithms, and inferring simulation parameters.  Quality assurance and support engineers searching for specific events who require very fine granular level access. Their SLA is at most a few hours.  Advanced monitoring and anomalies detection systems requiring a time series at different granularity depending on the type of monitoring.  Expert systems require both coarse and granular data while searching and aggregating across a dynamic set of variables.For each use case, we asked whether batch or streaming made more sense. We concluded that a one hour lag was acceptable for most of the applications, at least for the initial rollout. For the data analysts, an SLA of a few hours was acceptable.These initial conclusions gave us a lot of food for thought, particularly in regard to the data’s layout in the data lake and what storage format we planned to use.Our next question was: how would the various applications and stakeholders access data? All Grab analysts and data scientists use SQL and our backend applications talk to databases with SQL. It was clear we should access data through an SQL interface.Our final question was about democratising access to our data. We knew we had core applications and users we wanted to support. But we also knew the collected data could be strategic to other stakeholders and future use cases. Since we are a small team, we would not be able to support thousands of concurrent ad-hoc queries. For this reason, we surface this data using the Grab’s general data lake which is able to serve approximately 3 million queries per month.The Experimentation Platform (ExP) Data PipelineFollowing our initial information gathering sessions, we decided on these objectives:  Develop a pipeline for batch data, making sure it is highly available.  Allow analytical queries that aggregate on a wide range of attributes.  Allow building time series by specific event types.  Allow an SQL-supporting query engine.  Democratise the data access.Our batch data pipeline’s high-level architecture is pretty simple. It follows the pattern of most data warehouse ETL jobs except that we do not need to export data. In our data pipeline, we perform two operations, Load and Transform, and write the result data into our data lake.At a high level, we can think of the data pipeline as performing three key operations:  Load the data the ingestion layer has written on Amazon S3.  Transform the data by ordering and partitioning according to patterns discussed below.  Write data to Amazon S3 and metadata to a metastore.We use standard technologies: Apache Spark for compute, Apache Hive for metastore, and Apache Airflow as the workflow engine. We run Apache Spark on top of AWS Elastic MapReduce (EMR) with external AWS RDS and EC2 instances for Hive and Airflow.Particular topics of interest here are:  How we partition the data to enable the different access patterns discussed above.  How we used EMR and Airflow to achieve resilience and high availability.Let’s first look at what partitioning data means.For simplicity, we can think of data in a simple tabular format, just like a spreadsheet. Each row is a record and each column is an attribute. The columns can have a different range of values.We can organise data stored in the storage layer in hierarchical groups, called partitions, based on rows or columns. The serving layer can use this structure to filter data that needs to be served. For large-scale data, it is convenient to define partitions based on the attributes of one or more columns.Within a partition, data can be sorted depending on other attributes. Most data processing frameworks, including Apache Spark, support various partitioning schemes and sorting data within a partition (see https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/). In our pipeline, we use these Spark features to minimise the data processed by the serving layer.In our data, the time and event types are the key attributes. Every single event has the time that it was ingested and its associated event type.Our goal is to minimise the data the query engine needs to process and serve a specific query. Each query’s workload is the combination of the data that needs to be accessed and the complexity of the operation performed on the data. For analytical queries, common operations are data aggregation and transformations.Most of our analytical workloads span across a small number of event types (between 2 to 10) and a time range from one hour to a few months. Our expert system and time series systems workloads focus on a single event type. In these workloads, the time range can vary from a few hours to one day. A data scientist’s typical workloads require accessing multiple event types and specific time ranges. For these reasons, we partitioned data by event type and ingestion time.This hierarchical structure’ key advantages are:  When retrieving data for a specific event, we don’t need to scan other events or any index. The same applies for time ranges.  We do not need to maintain separate indexes and can easily reprocess part of the data.  Workloads across multiple events and/or time ranges can be easily distributed across multiple processing systems, which can process a specific sub-partition in parallel.  It is easier to enforce an Access Control List (ACL) by using the storage layer ACL system to restrict access to specific events and a time range.  We can reprocess a specific partition or a set of partitions without having to reprocess the full data.For storing the actual data in each partition, we considered two common storage formats and chose Apache ORC. We compared Apache Parquet against ORC for our workloads. We found an increase in performance (time saved in retrieving the data and storage utilised) between 12.5% and 80% across different use cases when using ORC with Snappy compression vs equivalent data store in Parquet with Snappy compression.Another key aspect was addressing the problem of High Availability of an AWS EMR. As of November 2018, AWS EMR does not support hot-standby and Spark multi-master deployment. We considered deploying Spark on top of Kubernetes but the initial deployment’s overhead as well as operating a Kubernetes cluster appeared more complex than our adopted solution. We do plan to revisit Spark on Kubernetes.The alternative approach we used was AWS EMR, which leverages the distributed nature of the airflow workers. We run one or more totally independent clusters for each availability zone. On the cluster’s master node, we run the Apache Airflow worker, which pulls any new job from a queue. The Spark jobs are defined as Airflow tasks bundled into a DAG.If a task fails, we automatically retry up to four times to overcome any transitory issues such as S3 API or KMS issues, availability of EC2 instances, or any other temporary issue with underlying resources.Tasks are scheduled across different clusters and therefore different availability zones. If an availability zone fails, generally there is no impact on other tasks’ executions. If two zones fail, then generally the impact is just a delay in when the data is available for serving.For deployments requiring an upgrade of the EMR version or of internal libraries, we roll out the new version to a random availability zone. This lets us perform canary deployments on our core processing infrastructure. It also lets us rollback very quickly as the remaining availability zones suffice to execute the pipeline’s workload. To do this, we use terraform and our GitLab CI.Packaging and Deployment of JobsWe believe our code’s architecture is also of interest. We use Apache Spark and write our Spark jobs in Python. However, to avoid performance penalties, we avoid processing the data within the Python VM. We do most of the processing using Spark SQL and the PySpark APIs. This lets us have comparable performance with the same job written in Scala or Java while using a programming language most of us are familiar with.A key aspect we addressed from the beginning was the package of the Spark jobs.The Spark documentation lacks information on how you should package your Python application. This resulted in misleading assumptions on how to write complex applications in Python. Often, Pyspark jobs are written using a single file where all the logic and data models are defined. Another common approach is to package the libraries and install them as part of the EMR bootstrap process where custom libraries can be installed on each node.We took a slightly different approach. We package our application using this pattern:  lib.zip, a zip file containing all the internal modules and the defined data models. These files are shared across different jobs.  Each Spark job has a Python file which defines the job’s core logic and submits the job.  Any configuration file is placed in S3 or in HDFS and loaded at runtime by the job.We deploy all the files on S3 using our deployment pipeline (Gitlab CI). This pattern gave us greater re-usability of our code across different Spark jobs. We can also deploy new job versions without re-deploying the full set of EMR clusters.Lessons LearntThroughout our data pipeline’s development, we learned important lessons that improved our original design. We also better understand what we can improve in the future.The first lesson relates to the size of the master node and task node in EMR.Our initial clusters had this setup:  Master node was using either an m4.xlarge or m5.xlarge instance.  One core node was using an m4.2xlarge or m5.2xlarge instance.  A dynamic number of task nodes were using m4.2xlarge or m5.2xlarge instances.The number of task nodes scaled up depending on the number of containers pending. They could go from 5 initial task nodes to 200 at a pace of 25 nodes added every 5 minutes, done automatically using our scaling policy. As we reached 100 nodes running on a single cluster, we noticed more task nodes failing during processing due to network timeout issues. This impacted our pipeline’s reliability since a Spark task failing more than 4 times aborted the full Spark job.To understand why the failure was happening, we examined the resource manager logs. We closely monitored the cluster while running a sample job of similar scale.To monitor the cluster, we used EMR’s default tools (Ganglia) (as shown below) and custom monitoring tools for CPU, memory, and network on top of Datadog. We noticed the overall cluster was heavily used and sometimes the master node even failed to load the metrics.EMR cluster CPU monitoring with GangliaInitially, we thought this would not have had any impact on the Spark job as the EMR master node in our settings is not the Spark driver node. Our reasoning was:  We deployed our applications in cluster mode and therefore the Spark job’s driver would have been one of the task nodes.  If the master was busy running the other services, such as the Spark history server and the Resource manager, it should have had no impact.Our reasoning was incorrect because, despite correctly assuming the Spark driver was a task node, we did not consider that Spark on EMR relies on YARN for all its resource allocation.By looking more carefully at the logs on the Spark task, we noticed the tasks nodes were failing to communicate their status to the master node and would then shut themselves down. This was happening at the same time as high CPU and high I/O on the master node.We rethought our deployment configuration. We used bigger master instances (m5.2xlarge) as well as much bigger task instances in lower numbers (r4.2xlarge) - up to 100 of them.After a few weeks of initial deployment, we noticed our EMR clusters’ core nodes failed quite regularly. This prevented the Spark job from being submitted, and would often require a full cluster redeployment to get the system healthy. The error in the job indicated an HDFS issue (see error log below). In our case, HDFS is only used to store the job’s metadata, such as the libraries, the configurations, and the main scripts. YARN also uses HDFS to store the logs.We monitored the core nodes more closely and tried to replicate the issue by running an equal number of Spark jobs to the total number of jobs processed by failed clusters. In our test, we monitored the core node directly, meaning we connected to the node and monitored it with tools such as iostat and iotop.We noticed that after a while the Spark jobs’ logs were using a considerable amount of the HDFS resources. We checked the defaults configuration in EMR for ‘spark-defaults.confs’ and tweaked the original configuration with:{      “Classification”: “spark-defaults”,      “Properties”:{        ”spark.history.fs.cleaner.enabled” : “true” ,        “spark.history.fs.cleaner.maxAge”:  ”72h”,        “spark.history.fs.cleaner.interval” : “1h”}}py4j.protocol.Py4JJavaError: An error occurred while calling o955.start.: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /mnt1/yarn/usercache/hadoop/appcache/application\\_1536570288257\\_0010/container\\_1536570288257\\_0010\\_01\\_000001/tmp/temporary-2a637804-562e-47f2-8e76-bd3d83f79eae/metadata could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1735)        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2561)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:422)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)HDFS failure on spark-submitConclusionsWe have processed over 2.5 PB of data in the past 3 and a half months while minimising the storage used on S3 (500 TB) as shown below. The storage saving is related to both ORC and our partition scheme. After this initial batch data pipeline, our focus has shifted to the streaming data pipeline and serving layer. We plan to improve this setup, especially as new Spark releases improve Kubernetes and Apache Spark support.",
        "url": "/experimentation-platform-data-pipeline"
      }
      ,
    
      "designing-resilient-systems-part-2": {
        "title": "Designing Resilient Systems: Circuit Breakers or Retries? (Part 2)",
        "author": "corey-scott",
        "tags": "[&quot;Resiliency&quot;, &quot;Circuit Breakers&quot;]",
        "category": "",
        "content": "This post is the second part of the series on Designing Resilient Systems. In Part 1, we looked at use cases for implementing circuit breakers. In this second part, we will do a deep dive on retries and its use cases, followed by a technical comparison of both approaches.Introducing RetryRetry is a software mechanism that monitors a request, and if it detects failure, automatically repeats the request.Let’s take the following example:Assuming our load balancer is configured to perform round-robin load balancing, this means that with two hosts in our upstream service, the first request will go to one host and the second request will go to the other host.If our request was unlucky and we were routed to the broken host, then our interaction would look like this:However, with retries in place, our interaction would look like this:You’ll notice a few things here. Firstly, because of the retry, we have successfully completed our processing; meaning we are returning fewer errors to our users.Secondly, while our request succeeded, it required more resources (CPU and time) to complete. We have to attempt the first request, wait for and detect the failure, before repeating and succeeding on our second attempt.Lastly, unlike the circuit breaker (discussed in Part 1), we are not tracking the results of our requests. We are, therefore, not doing anything to prevent ourselves from making requests to the broken host in the future. Additionally, in our example, our second request was routed to the working host. This will not always be the case, given that there will be multiple concurrent requests from our service and potentially even requests from other services. As such, we are not guaranteed to get a working host on the second attempt. In fact, the chance for us to get a working host is equal to the number of working hosts divided by the total hosts, in this case 50%.Digging a little deeper, we had a 50% chance to get a bad host on the first request, and a 50% chance on the retry. By extension we therefore have a 50% x 50% = 25% chance to fail even after 1 retry. If we were to retry twice, this becomes 12.5%.Understanding this concept will help you determine your max retries setting.Should We Retry for All Errors?The short answer is no. We should consider retrying the request if it has any chance of succeeding (i.e. error codes 503 - Service Unavailable and 500 - Internal Server Error). For example, for error code 503, a retry may work if the retry resulted in a call to a host that was not overloaded. Conversely, for errors like 401 - Unauthorised or 400 - Bad Request, retrying these wastes resources as they will never work without the user changing their request.There are two key points to consider: Firstly, the upstream service must return sensible and informative errors and secondly, our retry mechanism must be configured to react to different types of errors differently.IdempotencyA process (function or request) is considered to be idempotent if it can be repeated any number of times (i.e. one or more) and have the same result.Let’s say you have a REST endpoint that loads a city. Every time you call this method, you should receive the same outcome. Now, let’s say we have another endpoint, but this one reserves a ticket. If we call this endpoint twice, then we will have reserved 2 tickets. How does this relate to retries?Examine our retry interaction from above again:What happens if our first call to the broken host actually reserves at ticket but fails to respond to us correctly. Our retry to the second working host would then reserve a second ticket, and we would have no idea that we had made a mistake.This is because our reserve a ticket endpoint is not idempotent.Please do not take the above example to imply that only read operations can be retried and that all write/mutate changes cannot be; the example was chosen carefully.A reserve a ticket operation is almost always going to involve some finite amount of tickets, and in such a situation it is imperative that 1 request only results in 1 reservation. Other similar situations might include charging a credit card or incrementing a counter.Some write operations, like saving a registration or updating a record to a provided value (without calculation) can be repeated. Saving multiple registrations will cause messy data, but that can be cleaned up by some other non-consumer related process. In this case, it’s better to ensure we fulfil the consumers request at the expense of extra work for us rather than failing, leaving the system in an unknown state and making it the consumer’s problem. For example, let’s say we were updating the user’s password to abc123, this end state which was provided by the user is fixed and so, therefore, repeating the process only wastes the resources of the data store.In cases where retries are possible, but you want to be able to detect and prevent duplicate transactions (like in our ticket reservation example) it is possible to introduce a cryptographic nonce. This topic would require an article all of its own, but the short version is: a cryptographic nonce is a random number introduced into a request that helps us detect that two requests are actually one.If that didn’t make much sense, here’s an example:Let’s say we receive a ticket registration request from our consumer, and we append to it a random number. Now, when we call our upstream service, we can pass the request data together with the nonce. This request is partially processed but then fails and returns an HTTP 500 - Internal Server Error. We retry this request with another upstream service host and again supply the request data and the exact same nonce. The upstream host is now able to use this nonce and other identifying information in the request (e.g. consumer id, amount, ticket type, etc.) to determine that both requests originate from the same user request and therefore should be treated as one. In our case, this might mean we return the tickets reserved by the first partially processed request and complete the processing.For more information on cryptographic nonces, start here.BackoffIn our previous example, when we failed, we immediately tried again and because the load balancer gave us a different host, the second request succeeded. However, this is not actually how it works. The actual implementation includes a delay/wait in-between request like this:This amount of wait time between requests is called the backoff.Consider what happens when all hosts of the upstream service are down. Remember, the upstream service could be just one host (like a database). If we were to retry immediately, we would have a high chance to fail again and again and again, until we exceeded our maximum number of attempts.Viewed simply, the backoff is a process that changes the wait time between attempts based on the number of previous failures.Going back to our example, let’s assume that our backoff delay is 100 milliseconds.            Retry Attempt            Delay                      1            1 x 100ms = 100ms                  2            2 x 100ms = 200ms                  5            5 x 100ms = 500ms                  10            10 x 100mx = 1,000ms           The underlying theory here is that if a request has already failed a few times, then it has an increased likelihood of failing again. We, therefore, want to give the upstream service the greater chance to recover and be able to fulfil our request.By increasing the delay, we are not only giving it more time to recover, but we are spreading out the load of our requests and retries. In cases where the request failure is caused by the upstream service being overloaded, this spreading out of the load also gives us a greater chance of success.JitterWith backoff in-place, we have a way to spread out the load we are sending to our upstream service. However, the load will still be spiky.Let’s say we make 10,000 requests and they all fail because the upstream service cannot handle that amount of simultaneous requests. Following our simple backoff implementation from earlier, after 100ms delay we would retry all 10,000 requests which would also fail for the same reason. To avoid this, the retry implementation includes jitter. Jitter is the process of increasing or decreasing the delay from the standard to further spread out the load. In our example, this might mean that our 10,000 requests are delayed between 70-150ms (for the first retry attempt) by a random amount.The goal here is similar to above, which is to smooth out the request load.Note: For purposes of this article we’ve provided a vastly over-simplified definition of backoff and jitter. If you would like a more in-depth description, please read this article.SettingsIn Grab, we have implemented our own retry library inspired by this AWS blog article. In this library we have the following settings:Maximum RetriesThis value indicates how many times a request can be retried before giving up (failing).Retry FilterThis is a function that processes the returned error and decides if the request should be retried.Base and Max DelayWhen we combine the concepts of backoff and jitter, we are left with these two settings.The base delay is the minimum backoff delay between attempts, while the max delay is the maximum delay between attempts.Note: The actual delay will always be between these the values for Base and Max Delays and will also be based on the attempt number (number of previous failures).Time-boxing RequestsWhile the underlying goal of the retry mechanism is to do everything possible to fulfil our user’s request by retrying until we successfully complete the request, we cannot try forever.At some point, we need to give up and allow the failure.When configuring the retry mechanism, it is essential to tune the Maximum Retries, Request Timeout, and Maximum Delay together. The target to keep in mind when tuning these values is the worst-case response time to our consumer.The worst-case response time can be calculated as: (maximum retries x request timeout) + (maximum retries x maximum delay)For example:     Max Retries      Request Timeout (ms)      Maximum Delay (ms)      Total Time for first attempt and retries      Total time for delays      Total Time Overall (ms)          2      100      200      3 x 100      2 x 200      800          5      100      200      6 x 100      5 x 200      1,600          3      500      200      4 x 500      3 x 200      2,600     You can see from this table how the total amount of time taken very quickly escalates.Circuit Breakers vs RetriesSome of the original discussions that started this series was centred around one question “why use a circuit-breaker when you can just retry?” Let’s dig into this a little deeper.Communication with Retries onlyAssuming we take sufficient time to plan, track and tune our retry settings, a system that only has retries will have an excellent chance of successfully achieving our goals by merely retrying.Consider our earlier example:In this simple example, setting our retry count to 1 would ensure that we would achieve our goals. If the first attempt went to the broken host, we would merely retry and be load-balanced to the other working host.Sounds good right? So where is the downside? Let’s consider a failure scenario where our broken host does not throw an error immediately but instead never responds. This means:  When routed to the working host first then the response time would be fast, whatever the processing time of the working host is.  When routed to the broken host first then the response time would be equal to our Request Timeout setting plus the processing time of the working host.As you can imagine, if we had more hosts and in particular more broken hosts, then we would require a higher setting for Maximum Retries, and this would result in higher potential response time (i.e. multiples of the Request Timeout setting).Now consider the worst-case scenario -  when all the upstream hosts are down. All of our requests will take at least Maximum Retries x Request Timeout to complete. This situation is referred to as cascading failure (more info).Another form of cascading failure occurs when the load that should be handled by a broken host is added to a working host, causing the working host to become overloaded.For example, if in our above example, we have 2 hosts that are capable of handling 10k requests/second each. If we currently have 15k requests/second, then our load balancer has spread the load, and we have 7.5k requests/second on each.However, because all requests to the broken host are retried on the working host, our working host suddenly has to handle its original 7.5k requests plus 7.5k retries giving it 15k requests/second to handle, which it cannot.Communication with Circuit Breaker OnlyBut what if you only implemented a circuit breaker and no retries? There are two factors to note in this scenario. Firstly, the error rate of our system is the error rate that is seen by our users. For example, if our system has a 10% error rate then 10% of our users would receive an error.Secondly, should our error rate exceed the Error Percent Threshold then the circuit would open, and then 100% of our users would get an error even though there are hosts that could successfully process the request.Circuit Breaker and RetriesThe third option is of course to adopt both circuit breaker and retry mechanisms.Taking the same example we used in the previous section, if we were to retry the 10% of requests that failed once, 90% of those requests would pass on the second attempt. Our success rate would then go from the original 90% to 90% + ( 90% x 10%) = 99%Perhaps another interesting side-effect of retrying and successfully completing the request is the effect that it has on the circuit itself. In our example, our error rate has moved from 10% to 1%. This significant reduction in our error rate means that our circuit is far less likely to open and prevent all requests.Circuit Breaker Inside Retries / Retries Inside Circuit BreakerIt might seem strange but it is imperative that you spend some time considering the order in which you place the mechanisms.For example, when you have the retry mechanism inside the circuit breaker, then when the circuit breaker sees a failure, this means that we have already attempted retries several times and still failed. An error in this situation should be rather unlikely. By extension then we should consider using a very low Error Percent Threshold as the trigger to open the circuit.On the other hand, when we have a circuit breaker inside a retry mechanism, then when the retry mechanism sees a failure, this means either the circuit is open, or we have failed an individual request. In this configuration, the circuit breaker is monitoring all of the individual requests instead of the batch in the previous. As such, errors are going to be much more frequent. We, therefore, should consider a high Error Percent Threshold before opening the circuit. This configuration is also the only way to achieve circuit breaker per host.The second configuration is by far my preferred option. I prefer it because:  The circuit breaker is monitoring all requests.  The circuit is not unduly influenced by one bad request. For example, a request with a large payload might fail when sent to all hosts, but all other requests are fine. If we have a low Error Percent Threshold setting, this might unduly influence the circuit.  I like to ensure that bulwark inside our circuit breaker implementation also protects the upstream service from excessive requests, which it does more effectively when tracking individual requests  If I set the Timeout setting on my circuit to some huge number (e.g. 1 hour), then I can effectively ignore it and the calculation of my maximum possible time spent calling the upstream service is simplified to (maximum retries x request timeout) + (maximum retries x maximum delay).  Yes, this is not so simple, but it is one less setting to worry about.Final ThoughtsIn this two-part series, we have introduced two beneficial software mechanisms that can increase the reliability of our communications with external upstream services.We have discussed how they work, how to configure them, and some of the less obvious issues that we must consider when using them.While it is possible to use them separately, for me, it should never be a question of if you should have a circuit breaker or a retry mechanism. Where possible you should always have both. With the bulwark thrown in for free in our circuit breaker implementation, it gets even better.The only thing that could make working with an upstream service even better for me, (e.g. more reliable and potentially faster) would be to add a cache in front of it all. But we’ll save that for another article.I hope you have enjoyed this series and found it useful. Comments, corrections, and even considered disagreements are always welcome.Happy Coding!",
        "url": "/designing-resilient-systems-part-2"
      }
      ,
    
      "big-data-real-time-presto-talariadb": {
        "title": "Querying Big Data in Real-time with Presto &amp; Grab's TalariaDB",
        "author": "roman-atachiantsoscar-cassetti",
        "tags": "[&quot;Big Data&quot;, &quot;Real-Time&quot;, &quot;Database&quot;, &quot;Presto&quot;, &quot;TalariaDB&quot;]",
        "category": "",
        "content": "IntroductionEnabling the millions and millions of transactions and connections that take place every day on our platform requires data-driven decision making. And these decisions need to be made based on real-time data. For example, an experiment might inadvertently cause a significant increase of waiting time for riders.Without the right tools and setup, we might only know the reason for this longer waiting time much later. And that would negatively impact our driver-partners’ livelihoods and our consumers’ Grab experience.To overcome the challenge of retrieving information from large amounts of data, our first step was to adopt the open-source Facebook’s Presto, that makes it possible to query petabytes with plain SQL. However, given our many teams, tools, and data sources, we also needed a way to reliably ingest and disperse data at scale throughout our platform.To cope with our data’s scale and velocity (how fast data is coming in), we built two major systems:      McD: Our scalable data ingestion and augmentation service.        TalariaDB: A custom data store used, along with Presto and S3, by a scalable data querying engine.  In this article, we focus on TalariaDB, a distributed, highly available, and low latency time-series database that stores real-time data. For example, logs, metrics, and click streams generated by mobile apps and backend services that use Grab’s Experimentation Platform SDK. It “stalks” the real-time data feed and only keeps the last one hour of data.TalariaDB addresses our need to query at least 2-3 terabytes of data per hour with predictable low query latency and low cost. Most importantly, it plays very nicely with the different tools’ ecosystems and lets us query data using SQL.The figure below shows how often a particular event happened within the last hour. The query scans through almost 4 million rows and executes in about 1 second.Design GoalsTalariaDB attempts to solve a specific business problem by unifying cold and hot storage data models. This reduces overall latency, and lets us build a set of simple services that queries and processes data. TalariaDB does not attempt to be a general-purpose database. Simplicity was a primary design goal. We also set the following functional and non-functional requirements.Functional Requirements      Time-Series Metrics. The system can store thousands of different time-series metrics.        Data Retention. Keep the most recent data. This is configurable so we can extend the retention period on the fly.        Query or Aggregate by any dimension. We will build very complex queries using the full power of SQL and the Presto query engine for graphing, log retrieval, Grab Splainer, analytics, and other use-cases.  Non-functional Requirements      Linear, Horizontal Scalability. The hot data layer can scale to a multi-terabyte or even multi-petabyte scale.        Low Latency. The system responds and retrieves data for a particular combination of metric name and time window. The query executes within a few seconds at most, even if there is a petabyte of data.        Simplicity. The system is simple, easy to write, understand, and maintain.        Availability. The system is an Available &amp; Partition tolerant system (AP in CAP terms), always responding to queries even when some nodes are unavailable. For our purposes, partial data is better than no data.        Zero Operation. The system “just works”, with zero manual intervention. It needs to scale for the years to come.        High Write Throughput. Since both read and write throughput are high, we support at least one million events per second on a cluster.        Cost. Given the scale, the system should be as low cost as possible. Ideally, it should be as cheap as the SSDs and still be able to query terabytes or even petabytes of data with predictable, low latency.  Where TalariaDB Sits in Our Data PipelineThe figure below shows where TalariaDB fits in our event ingestion data pipeline’s architecture.To help you understand this schema, let’s walk through what happens to a single event published from mobile app or a backend service.xsdk.Track(ctx, \"myEvent\", 42, sdk.NewFacets().    Passenger(123).    Booking(\"ADR-123-2-001\").    City(10)First, using the Track() function in our Golang, Android or iOS SDKs, an engineer tracks a metric as follows:      The tracked event goes into our McD Gateway service. It performs authentication if necessary, along with some basic enrichment (e.g. adding a unique event identifier). It then writes these events into our Kafka topic.        The McD Consumer service reads from Kafka and prepares a columnar ORC file which is then partitioned by event name. In the example above, myEvent is pushed into its own file together with all the other myEvents which are ingested at more or less the same time. This happens in real time and is written to an S3 bucket every 30 seconds.        A Spark hourly job kicks in every hour to create massive columnar files used for cold/warm storage retrieval.        The Presto query engine has both schemas registered letting users (people or systems) to perform sub-second queries on the data, and even combine the two schemas together by having a unified SQL layer.  How TalariaDB is DesignedNow, let’s look at TalariaDB and its main components.One of TalariaDB’s goals is simplicity. The system itself is not responsible for data transformation and data re-partitioning but only ingests and serves data to Presto.To make sure TalariaDB scales to millions of events per second, it needs to leverage batching. A single event in TalariaDB is not stored as a single row. Instead, we store a pre-partitioned batch of events in a binary, columnar format. Spark streaming takes care of partitioning by event name (metric name) before writing to S3, making our design more streamlined and efficient.You can see from the schema above, that the system really does only a few things:      Listens to SQS S3 notifications of Put Object, downloading each file and writing it to an internal LSM Tree with expiration.        Performs periodic compaction and garbage collection to evict expired data. This is essentially done by the underlying LSM Tree.        Exposes an API for Presto by implementing PrestoThriftConnector.  We experimented with several different storage backends, and Badger key-value store ended up winning our hearts. It’s an efficient and persistent log structured merge (LSM) tree based key-value store, purely written in Go. It is based upon the WiscKey paper from USENIX FAST 2016. This design is highly SSD-optimised and separates keys from values to minimise I/O amplification. It leverages both the sequential and the random performance of SSDs.TalariaDB specifically leverages two of Badger’s unique features:      Very fast key iteration and seek. This lets us store millions of keys and quickly figure out which ones need to be retrieved.        Separation of keys and values. We keep the full key space in memory for fast seeks. But iteration and our values are memory-mapped for faster retrieval.  Columnar Time-series DatabaseAs mentioned, a single event in TalariaDB is not stored as a single row, but as a pre-partitioned batch of events in binary, columnar format. This achieves fast ingestion and fast retrieval. As data will be aligned on disk, only that column needs to be selected and sent to Presto. The illustration in the next section shows the difference. That being said, it is inefficient to store large amounts of data in a single column. For fast iteration, TalariaDB stores millions of individual columnar values (smaller batches) and exposes a combined “index” of metric name and time.The query pattern we serve is key to understand why we do this. We need to answer questions such as:      How many of a given event types are in a time window?        What is an aggregate for a given metric captured on a specific event (e.g. count, average)?        What are all the events for a passenger / driver-partner / merchant?  These use cases can be served with various trickery using a row based storage, but they require fairly complex and non-standard access patterns. We want to support anyone with an SQL client and SQL basic knowledge.Data Layout &amp; QueryTalariaDB combines a log-structured merge tree (LSMT) and columnar values to provide fast iteration and retrieval of an individual event type within a given time window. The keys are lexicographically ordered. When a query comes, TalariaDB essentially seeks to the first key for that metric and stops iterating when either it finds the next metric or reaches the time bound. The diagram below shows how the query is processed.During the implementation, we had to reduce memory allocations and memory copies on read, which led us to implementing a zero-copy decoder. In other words, when a memory-mapped value is decoded, no data is copied around and we simply send it to PrestoDB as quickly and efficiently as possible.Integrating with PrestoTalariaDB is queryable using the Presto query engine (or a thrift client implementing the Presto protocol) so we can keep things simple. To integrate TalariaDB and Presto, we leveraged the Presto Thrift Connector. To use the Thrift connector with an external system, you need to implement the PrestoThriftService interface. Next, configure the Thrift Connector to point to a set of machines, called Thrift servers, that implement the interface. As part of the interface implementation, the Thrift servers provide metadata, splits, and data. The Thrift server instances are assumed to be stateless and independent from each other.What Presto essentially does is query one of the TalariaDB nodes and requests “data splits”. TalariaDB replies with a list of machines containing the query’s data. In fact, it simply maintains a membership list of all of the nodes (using the reliable Gossip protocol) and returns to Presto a list of all the machines in the cluster. We solve the bootstrapping problem by simply registering the full membership list at a random period in Route53.Next, Presto hits every TalariaDB instance in parallel for data retrieval. Interestingly enough, by adding a new machine in the TalariaDB cluster we gain data capacity and reduce query latency at the same time. This is provided the Presto cluster has an equal or larger amount of executors to process the data.Scale and ElasticityWhile scaling databases is not a trivial task, by sacrificing some of the requirements (such as strong consistency as per CAP), TalariaDB can scale horizontally by simply adding more hardware servers.TalariaDB is not only highly available but also tolerant to network partitions. If a node goes down, data residing on the node becomes unavailable but new data will still be ingested and presented. We would much rather serve our consumers some data than no data at all. Going forward, we plan to transition the entire system to a Kubernetes StatefulSet integration. This lets us auto-heal the TalariaDB cluster without data loss, as Kubernetes manages the data volumes.We do upscaling by adding a new machine to the cluster. It automatically joins the cluster by starting gossiping with one of the nodes (discovery is done using a DNS record, Route53 in our case). Once the instance joins the cluster, it starts polling from a queue the files it has to ingest.Downscaling must be graceful, given we currently don’t replicate data. However, we can exploit that TalariaDB only stores data for the trailing time period. A graceful downscaling might be implemented by simply stopping ingesting new data but still serving data until everything the node holds is expired and storage is cleared. This is similar to how EMR deals with downscaling.ConclusionWe have been running TalariaDB in production for a few months. Together with some major improvements in our data pipeline, we have built a global real-time feed from our mobile applications for our analysts, data scientists, and mobile engineers by helping them monitor and analyse behaviour and diagnose issues.We achieved our initial goal of fast SQL queries while ingesting several terabytes of data per hour on our cluster. A query of a single metric typically takes a few seconds, even when returning several million rows. Moreover, we’ve also achieved one minute of end-to-end latency: when we track an event on the mobile app, it can be retrieved from TalariaDB within one minute of its happening.",
        "url": "/big-data-real-time-presto-talariadb"
      }
      ,
    
      "designing-resilient-systems-part-1": {
        "title": "Designing Resilient Systems: Circuit Breakers or Retries? (Part 1)",
        "author": "corey-scott",
        "tags": "[&quot;Resiliency&quot;, &quot;Circuit Breakers&quot;]",
        "category": "",
        "content": "This post is the first of a two-part series on Circuit Breakers and Retries, where we will introduce and compare these two often used service reliability concepts. For Part 1, we will focus on the use cases for implementing circuit breakers including the different options related to the configuration of circuits.Things should just work. That is the most fundamental expectation that any consumer has towards a service provider. But just as poor weather is inevitable and often unpredictable, so are software and hardware failures. That is why it’s important for software engineers to plan and account for failures.In this first article of a two-part series, we will begin to introduce and compare two frequently used service reliability mechanisms: Circuit Breakers and Retries. At Grab, we use both of these mechanisms extensively throughout our many software systems to ensure that we can weather failures and continue to provide our consumers with the services they expect from us. But are both mechanisms equal? Where and how do we choose one over the other?In this series we will take a close look at both approaches and their use cases, to help you make an informed decision regarding if and when to apply each method. But let’s start by looking at the common reasons for failures. With our services communicating with numerous external resources, failures can be caused by:  Networking issues  System overload  Resource starvation (e.g. out of memory)  Bad deployment/configuration  Bad request (e.g. lack of authentication credentials, missing request data)But rather than thinking of all the ways a call to an upstream service could fail, it is often easier to  consider what a successful request is. It should be timely, in the expected format, and contain the expected data. If we go by this definition, then everything else is therefore some kind of failure, whether it’s:  a slow response  no response at all  a response in the wrong format  a response that does not contain the expected dataIn planning for failures, we should strive to be able to handle each of these errors, just as we should try to prevent our service from emitting them. So let’s start looking at the different techniques for addressing these errors.(Note: All the examples and tools mentioned in this article are in Go. However, prior knowledge of Go is not required, only advantageous.)Introducing the Circuit BreakerHas your electricity ever shorted out? Perhaps you switched on a faulty appliance, plunging your entire house into darkness. Darkness may be inconvenient, but it’s certainly better than things catching fire or getting electrocuted!The device in your electrical box that is protecting you is called a circuit breaker. Instead of letting the electricity through the faulty appliance and potentially causing more problems, it has detected a fault and broken the connection.Software circuit breakers work the same way. A software circuit breaker is a mechanism that sits between 2 pieces of code and monitors the health of everything flowing through it. However, instead of stopping electricity when there’s a fault, it blocks requests.A typical “happy path” request from a service to an upstream service looks like this:Our \"main\" calls the circuit breaker (also inside our code), which in turn makes the request to the upstream service. The upstream service then processes the request and sends a response. The circuit breaker receives the response, and if there was no error, returns it to the original caller.So, let’s look at what happens when the upstream service fails.The request path is the same. And at this point, you might be wondering what we have gained from this as our request still failed. You are right, for this specific request, we gained nothing. However, let’s assume that all of the requests for the past 3 seconds have failed. The circuit breaker has been monitoring these requests and keeping track of how many passed and how many failed. It notices that all the requests are failing, so instead of making any further requests, it opens the circuit, which prevents any more requests from being made. Our flow now looks like this:It might look like we still haven’t achieved anything. But we have.Consider our previous discussion on how services can break: Services can break when they are overwhelmed with requests. Once a service is overloaded, making any further requests could result in two issues. Firstly, making the request is likely pointless, as we are not going to get a valid and/or timely response. Secondly, by creating more requests, we are not allowing the upstream service to recover from being overwhelmed and in fact, most likely overloading it more.But circuit breakers are not just about being a good user and protecting our upstream services. They are also beneficial for our service as we will see in the next sections.FallbackCircuit breakers, like Hystrix, include the ability to define a fallback. The flow with a fallback in place looks like this:So what does that get us? Let’s consider an example. Assume you are writing a service that requires the road travel distance between 2 locations.If things are working as they should, we would call the “distance calculator service”, providing it with the start and end locations, and it will return the distance. However, that service is down at the moment. A reasonable fallback in this situation might therefore be to estimate the distance by using some trigonometry.  Of course, calculating distance in this manner would be inaccurate, but using an inaccurate value which allows us to continue processing the user’s request is far better than to fail the request completely.In fallback processing, using an estimated value instead of the real value not the only option, other common options include:  Retrying the request using a different upstream service  Scheduling the request for some later time  Loading potentially out of date data from a cacheThere are, of course, cases where there is no reasonable fallback. But even in these situations, using a circuit breaker is still beneficial.Consider the cost of making and waiting for a request that eventually fails. There are CPU, memory and network resources, all being used to make the request and wait for the response. Then there is the delayed response to your user.All of these costs are avoided when the circuit is open, as the request is not made but instead immediately failed. While returning an error to our users is not ideal, returning the fastest possible error is the best worst option.Should the Circuit Breaker Track All Errors?The short answer is no. We should only track errors that are not caused by the user (i.e. HTTP error codes 400 and 401), but by the network or infrastructure (i.e. HTTP error codes 503 and 500).If we tracked errors caused by users, then it would be possible for one malicious user to send a large number of bad requests, causing our circuit to open and creating a service disruption for everyone.Circuit RecoveryWe have talked about how the circuit breaker can open the circuit and cut requests when there have been too many errors. We should also be aware of how the circuit becomes closed again.Unlike the electricity example we used above, with a software circuit breaker, you don’t need to find the fuse box in the dark and close the circuit manually. The software circuit breaker can close the circuit by itself.After the circuit breaker opens the circuit, it will wait for a configurable period, called a Sleep Window, after which it will test the circuit by allowing some requests through. If the service has recovered, it will close the circuit and resume normal operations. If the requests still return an error, then it will repeat the sleep/try process until recovery.BulwarkAt Grab, we use the Hystrix-Go circuit breaker, and this implementation includes a bulwark. A bulwark is a software process that monitors the number of concurrent requests and is able to prevent more than the configured maximum number of concurrent requests from being made.  This is a very cheap form of rate-limiting.In our case, the prevention of too many requests is achieved by opening the circuit (as we saw above). This process does not count towards the errors and will not directly influence other circuit calculations.So why is this important? As we talked about earlier, it’s possible for services to become unresponsive (or even crash) when it receives too many concurrent requests.Consider the following scenario: A hacker has decided to attack your service with a DOS attack. All of a sudden your service is receiving 100x the usual amount of requests. Your service could then make 100x the amount of requests to your upstream.If your upstream does not implement some form of rate-limiting, with this many requests, it would crash. By introducing a bulwark between your service and the upstream, you achieve two things:  You do not crash the upstream service because you limit the amount of requests that it cannot process.  The “extra” requests that are failed by the bulwark have both the ability to fallback and the ability to fail fast.Circuit Breaker SettingsThe Hystrix-Go circuit breaker has five settings, they are:TimeoutThis duration is the maximum amount of time a request is allowed to take before being considered an error. This takes into consideration that not all calls to upstream resources will fail promptly.With this, we can limit the total amount of time it takes us to process a request by defining how long we are willing to wait for our upstream.Max Concurrent RequestsThis is the bulwark setting (as mentioned above).Consider that the default value (10) indicates simultaneous requests and not “per second”. Therefore, if requests are typically fast (completed in a few milliseconds) then there is no need to allow more.Additionally, setting this value too high can cause your service to become starved of the resources (memory, CPU, ports) that it needs to make the requests.Request Volume ThresholdThis is the minimum number of requests that must be made within the evaluation (rolling window) period before the circuit can be opened.This setting is used to ensure that a small number of errors during low request volume does not open the circuit.Sleep WindowThis is the duration the circuit waits before the circuit breaker will attempt to check the health of the requests (as mentioned above).Setting this too low limits the effectiveness of the circuit breaker, as it opens/checks often. However, setting this duration too high limits the time to recovery.Error Percent ThresholdThis is the percentage of requests that must fail before the circuit is opened.Many factors should be considered when setting this value, including:  Number of hosts in the upstream service (more info in the next section)  Reliability of the upstream service and your connection to it  Service’s sensitivity to errors  Personal preferenceCircuit ConfigurationIn the next few sections, we will be discussing some different options related to the configuration of circuits, in particular, the per host and per service configuration, and how do we as programmers define the circuit.In Hystrix-Go, the typical usage pattern looks like this:hystrix.Go(\"my_command\", func() error {    // talk to other services    return nil}, func(err error) error {    // do this when services are down    return nil})The very first parameter “my_command” is the circuit name. The first thing to notice here is that because the circuit name is a parameter, the same value can be supplied to multiple invocations of the circuit breaker.This has some interesting side effects.Let’s say your service calls multiple endpoints of an upstream service called ‘list’, ‘create’, ‘edit’ and ‘delete’. If we want to track the error rates of each of these endpoints separately, you can define the circuit like this:func List() {   hystrix.Go(\"my_upstream_list\", func() error {      // call list endpoint      return nil   }, nil)}func Create() {   hystrix.Go(\"my_upstream_create\", func() error {      // call create endpoint      return nil   }, nil)}func Update() {   hystrix.Go(\"my_upstream_update\", func() error {      // call update endpoint      return nil   }, nil)}func Delete() {   hystrix.Go(\"my_upstream_delete\", func() error {      // call delete endpoint      return nil   }, nil)}You will notice that I have prefixed all of the circuits with “my_upstream_” and then appended the name of the endpoint. This gives me 4 circuits for 4 endpoints.On the other hand, if we want to track all the errors relating to one destination together, we can define our circuits like this:func List() {   hystrix.Go(\"my_upstream\", func() error {      // call list endpoint      return nil   }, nil)}func Create() {   hystrix.Go(\"my_upstream\", func() error {      // call create endpoint      return nil   }, nil)}func Update() {   hystrix.Go(\"my_upstream\", func() error {      // call update endpoint      return nil   }, nil)}func Delete() {   hystrix.Go(\"my_upstream\", func() error {      // call delete endpoint      return nil   }, nil)}In the above example, all of the different calls use the same circuit name.So how do we decide which to go with? In an ideal world, one circuit per upstream destination is sufficient. This is because all failures are infrastructure (i.e. network) related and in these cases when calls to one endpoint fail, all are certain to fail. This approach would result in the circuit being opened in the quickest possible time, thereby reducing our error rates.However, this approach assumes that our upstream service cannot fail in such a way that one endpoint is broken and the others remain working. It also assumes that our processing of the upstream responses never make a mistake processing the errors returned from the upstream service. For example, if we were to accidentally track user errors on one of our circuit breaker calls, we could quickly find ourselves prevented from making any calls to our upstream.Therefore, even though having one circuit per endpoint results in circuits that are slightly slower to open, it is my recommended approach. It is better to make as many successful requests as possible than inappropriately open the circuit.One Circuit Per ServiceWe have talked about upstream services as if they are a single destination, and when dealing with databases or caches, they might be. But when dealing with APIs/services, this will seldom be the case.But why does this matter? Think back to our earlier discussions regarding how a service can fail. If the machine running our upstream service has a resource issue (out of memory, out of CPU, or disk full), these are issues that are localised to that particular machine. So, if one machine is resource-starved, this does not mean that all of the other machines supporting that service will have the same issue.When we have one circuit breaker for all calls to a particular resource or service, we are using the circuit breaker in a “per service” model. Let’s look at some examples to examine how this affects the circuit breaker’s behaviour.Firstly, when we only have 1 destination, as is typically the case for databases:If all calls to the single destination (e.g. database) fail, then our error rate will be 100%.The circuit is sure to open, and this is desirable as the database is unable to respond appropriately and further requests will waste resources.Now let’s look at what happens when we add a load balancer and more hosts:Assuming a simple round-robin load balancing, all calls to one host succeed and all calls to the other fail. Giving us: 1 bad host / 2 total hosts = 50% error rate.If we were to set our Error Percent Threshold to anything over 50%, then the circuit would not open, and we would see 50% of our requests fail. Alternatively, if we were to set our Error Percent Threshold to less than 50%, the circuit would open and all requests shortcut to fallback processing or fail.Now, if we were to add additional hosts to the upstream service, like this:Then the calculation and the impact of one bad instance change dramatically. Our results become: 1 bad hosts / 6 total hosts = 16.66% error rate.There are a few things we can derive from this expanded example:  One bad instance will not cause the circuit to open (which would prevent all requests from working)  Setting a very low error rate (e.g. 10%), which would cause the circuit to open because of our one bad host would be foolish as we have 5 other hosts that are able to service the requests  Circuit breakers in a “per service” configuration should only have an open circuit when most (or all) of the destination hosts are unhealthyOne Circuit Per HostAs we have seen above, it is possible for one bad host to impact your circuit, so you might then consider having one circuit for each upstream destination host.However, to achieve this, our service has to be aware of the number and identity of upstream hosts. In the previous example, it was only aware of the existence of the load balancer. Therefore, if we remove the load balancer from our previous example, we are left with this:With this configuration, our one bad host cannot influence the circuits that are tracking the other hosts. Feels like a win.However, with the load balancer removed, our service will now need to take on its responsibilities and perform client-side load balancing.To be able to perform client-side load balancing, our service must track the existence and health of all the hosts in our upstream service and balance the requests across the hosts. At Grab, many of our gRPC-based services are configured in this way.With our new configuration, we have incurred some additional complexity, relating to client-side load balancing, and we have also gone from 1 circuit to 6. These additional 5 circuits also incur some amount of resource (i.e. memory) cost. In this example, it might not seem like a lot, but as we adopt additional upstream services and the numbers of these upstream hosts grow, the cost does multiply.The last thing we should consider is how this configuration will influence our ability to fulfil requests. When the host first goes bad, our request error rate will be the same as before: 1 bad host / 6 total hosts = 16.66% error rateHowever, after sufficient errors have occurred to open the circuit to our bad host, then we will be able to avoid making requests to that host, and we would resume having a 0% error rate.Final Thoughts on Per Service vs Per HostBased on the discussion above, you may want to rush off and convert all of your circuits to per host. However, the additional complexity of doing so should not be underestimated.Additionally, we should also consider what response our per service load balancer might have when the bad host is failing. If the load balancer in our per service example is configured to monitor the health of service running on each host (and not just the health of the host itself), then it is able to detect and remove that host from the load balancer and potentially replace it with a new host.It is possible to use both per service and per host at the same time (although I have never tried). In this configuration, the per service circuit should only open when there is little chance there are any valid hosts and by doing so it would save the request processing time taken to run through the retry cycle. The configuration for this has to be:  Circuit Breaker (per service) → Retry → Circuit Breaker (per host).My advice is to consider how and why your upstream service could fail and then use the simplest possible configuration for your situation.Up Next, Retries…So we’ve taken a look at the first common mechanism used in designing for reliability, which is Circuit Breakers. I hope you have enjoyed this post and found it useful. Comments, corrections, and even considered disagreements are always welcome.In our next post, we will look at the other service reliability mechanism on the spotlight, which is Retries. We will see how it works, how to configure it, and tackle some implementations with backoff and jitter. We will also discuss when we should use circuit breakers versus retries, or even a combination of both.Stay tuned!",
        "url": "/designing-resilient-systems-part-1"
      }
      ,
    
      "chaos-engineering": {
        "title": "Orchestrating Chaos Using Grab's Experimentation Platform",
        "author": "roman-atachiantstharaka-wijebandaraabeesh-thomas",
        "tags": "[&quot;Chaos Engineering&quot;, &quot;Resiliency&quot;, &quot;Microservice&quot;]",
        "category": "",
        "content": "BackgroundTo everyday users, Grab is an app to book a ride, order food, or make a payment. To engineers, Grab is a distributed system of many services that interact via remote procedure call (RPC), sometimes called a microservice architecture. Hundreds of Grab services run on thousands of machines with engineers making changes every day. In such a complex setup, things can always go wrong. Fortunately, many of the Grab app’s internal services are not critical for user actions like booking a car. For example, bookmarks that recall the user’s previous destination add user value, but if they don’t work, the user should still enjoy a reasonable user experience.Partial availability of services is not without risk. Engineers must have an alternative plan if something goes wrong when making RPC calls against non-critical services. If the contingency strategy is not implemented correctly, non-critical service problems can lead to an outage.So how do we make sure that Grab users can complete critical functions, such as booking a taxi, even when non-critical services fail? The answer is Chaos Engineering.At Grab, we practice chaos engineering by intentionally introducing failures in a service or component in the overall business flow. But the failed’ service is not the experiment’s focus.  We’re interested in testing the services dependent on that failed service.Ideally, the dependent services should be resilient and the overall flow should continue working. For example, the booking flow should work even if failures are put in the driver location service. We test whether retries and exponential fallbacks are configured correctly, if the circuit breaker configs are set properly, etc.To induce chaos into our systems, we combined the power of our Experimentation Platform (ExP) and Grab-Kit.Chaos ExP injects failures into traffic-serving server middleware (gRPC or HTTP servers). If the system behaves as expected, you can be confident that services will degrade gracefully when non-critical services fail.Chaos ExP simulates different types of chaos, such as latencies and memory leaks within Grab’s infrastructure. This ensures individual components return something even when system dependencies aren’t responding or respond with unusually high latency. It ensures our resilience to instance failures, as threats to availability can come from microservice level disruptions.Setting Up for ChaosTo build our chaos engineering system, we identified the two main areas for inducing chaos:      Infrastructure: By randomly shutting down instances and other infrastructure parts        Application: By introducing failures during runtime at a granular level (e.g. endpoint/request level)  You then enable chaos randomly or intentionally via experiments:      Randomly                  More suitable for ‘disposable’ infrastructure (e.g. ec2 instances)                    Tests redundant infrastructure for impact on end-users                    Used when impact is well-understood                  Experiments                  Accurately measure impact                    Control over experimental parameters                    Can limit impact on end-users                    Suitable for complex failures (e.g. latency) when impact is not well understood            Finally, you can categorise failure modes as follows:      Resource: CPU, memory, IO, disk        Network: Blackhole, latency, packet loss, DNS        State: Shutdown, time, process killer  Many of these modes can be applied or simulated at the infrastructure or app level, as shown:For Grab, it was important to comprehensively test application-level chaos and carefully measure the impact. We decided to leverage an existing experimentation platform to orchestrate application-level chaos around the system, shown in the purple box, by injecting it in the underlying middleware such as Grab-Kit.Why Use the Experimentation Platform?There are several chaos engineering tools. However, using them often requires an advanced level of infrastructure and operational skill, the ability to design and execute experiments, and resources to manually orchestrate the failure scenarios in a controlled manner. Chaos engineering is not as simple as breaking things in production.Think of chaos engineering as a controlled experiment. Our ExP SDK provides resilient and asynchronous tracking. Thus, we can potentially attribute business metrics to chaos failures directly. For example, by running a chaos failure that introduces 10 second latencies in a booking service, we can determine how many rides were negatively affected and how much money was lost.Using ExP as a chaos engineering tool means we can customise it based on the application or environment’s exact needs so that it deeply integrates with other environments like the monitoring and development pipelines.There’s a security benefit as well. With ExP, all connections stay within our internal network, giving us control over the attack surface area. Everything can be kept on-premise, with no reliance on the outside world. This also potentially makes it easier to monitor and control traffic.Chaos failures can be run ad-hoc, programmatically, or scheduled. You can also schedule them  to execute on certain days and within a specified time window. You can also set the maximum number of failures and customise them (e.g. number of MBs to leak, seconds to wait).ExP’s core value proposition is allowing engineers to initiate, control, and observe how a system behaves under various failure conditions. ExP also provides a comprehensive set of failure primitives for designing experiments and observing what happens when issues occur within a complex distributed system. Also, by integrating ExP with chaos testing, we did not require any modifications to a deployment pipeline or networking infrastructure. Thus the combination can be utilised more easily for a range of infrastructure and deployment paradigms.How We Built the Chaos SDK and UITo build the chaos engineering SDK, we leveraged a property of our existing ExP SDK - single-digit microsecond-level variable resolution, which does not require a network call. You can read more about ExP SDK’s implementation here. This let us build two things:      A smaller chaos SDK on top of ExP SDK. We’ve integrated this directly in our existing middleware, such as Grab-Kit and DB layers.        A dedicated web-based UI for creating chaos experiments  Thanks to our Grab-Kit integration, Grab engineers don’t actually need to use the Chaos SDK directly. When Grab-Kit serves an incoming request, it first checks with the ExP SDK. If the request “should fail”, it applies the appropriate failure type. It then forwards it to the handler of the specified endpoint.We currently support these failure types:      Error - fails the request with an error        CPU Load - creates a load on the CPU        Memory Leak - creates some memory which is never freed        Latency - pauses the request’s execution for a random amount of time        Disk Space - creates some temporary files on the machine        Goroutine Leak - creates and leaks goroutines        Panic - creates a panic in the request        Throttle - creates a rate limiter inside the request that rejects limit-exceeding requests  As an example, if a booking request goes to our booking service, we call GetVariable(“chaosFailure”) to determine if this request should succeed. The call contains all of the information required to make this decision (e.g. the request ID, IP address of the instance, etc). For Experimentation SDK implementation details, visit this blog post.To promote chaos engineering among our engineers we built a great developer experience around it. Different engineering teams at Grab have expertise in different technologies and domains. So some might not have knowledge and skills to perform proper chaos experiments. But with our simplified user interface, they don’t have to worry about the underlying implementation.Also, engineers who run chaos experiments are different experimentation platform users compared with our users like Product Analysts and Product Managers. Because of that, we provide a different experiment creation experience with a simple and specialised UI to configure new chaos experiments.In the chaos engineering platform, an experiment has four steps:      Define the ideal state of the system’s normal behaviour.        Create a control configuration group and a treatment configuration group. A control group’s variables are assigned existing values. A treatment group’s variables are assigned new values.        Introduce real-world failures, like an increase in CPU load.        Find the statistically significant difference between the system’s correct and failed states.  To create a chaos experiment, target the service you want the experiment to break. You can further fine-grain this selection by providing the environment, availability zone, or a specific list of instances.Next, specify a list of services affected by breaking the target service. You will closely monitor these services during the experiment. It helps to analyse the impact of the experiment later, though we continue tracking overall metrics indicating overall system health.Next, we provide a UI to specify a strategy for dividing control and treatment groups, failure types, and configurations for each treatment. For the final step, provide a time duration and create the experiment. You’ve now added a chaos failure to your system and can monitor how it affects system behaviour.ConclusionsAfter running a chaos experiment, there are typically two potential outcomes. You’ve verified your system is resilient to the introduced failure, or you’ve found a problem you need to fix. Both of these are good outcomes if the chaos experiment was first run on a staging environment. In the first case, you’ve increased your confidence in the system and its behaviour. In the other case, you’ve found a problem before it caused an outage.Chaos Engineering is a tool to make your job easier. By proactively testing and validating your system’s failure modes you reduce your operational burden, increase your resiliency, and will sleep better at night.",
        "url": "/chaos-engineering"
      }
      ,
    
      "feature-toggles-ab-testing": {
        "title": "Reliable and Scalable Feature Toggles and A/B Testing SDK at Grab",
        "author": "roman-atachiants",
        "tags": "[&quot;Experiment&quot;, &quot;Back End&quot;, &quot;Front End&quot;, &quot;Feature Toggle&quot;, &quot;A/B Testing&quot;]",
        "category": "",
        "content": "Imagine this scenario. You’re on one of several teams working on a sophisticated ride allocation service. Your team is responsible for the core booking allocation engine. You’re tasked with increasing the efficiency of the booking allocation algorithm for allocating drivers to passengers. You know this requires a fairly large overhaul of the implementation which will take several weeks. Meanwhile other team members need to continue ongoing work on related areas of the codebase. You need to be able to ship this algorithm in an incomplete state, but dynamically enable it in the testing environment while keeping it disabled in the production environment.How do you control releasing a new feature like this, or hide a feature still in development? The answer is feature toggling.Grab’s Product Insights &amp; Experimentation platform provides a dynamic feature toggle capability to our engineering, data, product, and even business teams. Feature toggles also let teams modify system behaviour without changing code.Grab uses feature toggles to:      Gate feature deployment in production to keep new features hidden until product and marketing teams are ready to share.        Run experiments (A/B tests) by dynamically changing feature toggles for specific users, rides, etc. For example, a feature can appear only to a particular group of people while running an experiment (treatment group).  Feature toggles, for both experiments and rollouts, let Grab substantially mitigate the risk of releasing immature functionality and try new features safely. If a release has a negative impact, we roll it back. If it’s doing well, we keep rolling it out.Product and marketing teams then use a web portal to turn features on/off, set up user targeting rules, set various configurations, perform percentage rollouts, and test in production.Engineers use our solution to run experiments in their server-side application logic. This includes search and recommendation algorithms, pricing &amp; fees, site architecture, outbound marketing campaigns, transactional messaging, and product rollouts.With experiments, you can perform tests to find out which changes actually work:      A/B tests to determine which out of two or more variations, usually minor improvements, produces the best results.        Feature tests to safely test a significant change, such as trying out a new feature on a limited audience.        Feature rollout to launch a feature (independent of a test). At this stage, you also make the feature available to more users by increasing the traffic allocation to 100%.  Legacy ExperimentationBefore 2017, all our experiments were done manually with custom code written here and there in every backend service. As our engineering team grew, this became unsustainable and resulted in excessive friction and endless meetings. The figure below describes problems we used to face before having a centralised experimentation platform. This was an iterative process which sometimes took weeks, slowing down the organisation altogether.We needed to solve our A/B testing issues and let Grabbers easily integrate and retrieve feature toggle values dynamically. And we needed to that without having network calls and without subjecting our services to unnecessary network jitter, potential latency, and reliability issues.Moreover, we also needed to track metrics and results of dynamic retrieval. For example, if an A/B test is running on a specific feature toggle, we needed to track what choice was made (i.e. users that got A and those that got B).Legacy Feature RolloutOur legacy feature toggling system was essentially a library shipped with all of our Golang services that wrapped calls to a shared Redis. Retrieving values involved network calls and local caching to support our scale, but slowly, as the number of backend microservices grew, it started to become a single point of failure.// Retrieve a feature flag using our legacy systemsitevar.GetFeatureFlagOrDefault(\"someFeatureFlagKey\", 10, false)Design Goals of Our SDKNote: We call a specific feature toggle a variable. In this section, the word “variable” refers to a feature toggle.To overcome these challenges, we designed an SDK with capabilities to:      Retrieve values of variables dynamically        Track every retrieval made along with an experiment which might have potentially been applied to the variable. For example, if a user retrieves a value of a variable for a particular passenger, this value along with the context (e.g. passenger, country, time) will be tracked throughout our data logging system.  On the non-functional requirements side, we needed our SDK to be scalable, reliable, and have virtually no latency on the variable retrieval. This meant that we could not make a network call every time we needed a variable. Also, this had to be done asynchronously.We ended up designing a very simple Go API for our SDK to be used by backend services. The API essentially contains two functions GetVariable() and Track() which are rather self-explanatory - one gets a value of the variable and the other lets users track anything they want.type Client interface {    // GetVariables with name is either domain or experiment name    GetVariables(ctx context.Context, name string, facets Facets) Variables    // Track an event with a value and metadata*    Track(ctx context.Context, eventName string, value float64, facets Facets)}We started the design of the entire platform by designing the APIs first. We wanted to make it simple to use for developers without requiring them to change code each time experiment conditions change or have to move from testing to rollout, and so on. Making the API simple was also crucial as our engineering team grew significantly and the code needed to be very simple to read and understand.We have also introduced a concept of “facets” which is essentially a set of well-defined attributes used for many different purposes within the platform, from making decisions to tracking and analysing metrics.Passenger  int64  // The passenger identifierDriver     int64  // The driver identifierService    int64  // The equivalent to a vehicle typeBooking    string // The booking codeLocation   string // The location (geohash or coordinates)Session    string // The session identifierRequest    string // The request identifierDevice     string // The device identifierTag        string // The user-defined tag...Making Sub-microsecond DecisionsThe retrieval of feature toggles is done using the GetVariable() method of the client which takes few parameters:      The name of the variable to retrieve. This is essentially the feature toggle name that uniquely identifies a specific product feature or a configuration.        The facets representing contextual information about this event and are sent to our data pipeline. In fact, every time GetVariable() is called, an event is automatically generated and reported.  threshold :=  client.GetVariable(ctx, \"myFeature\", sdk.NewFacets().    Driver(driverID).    City(cityID)).Int64(10)From the code above, note there’s a second step required to actually retrieve the value. In the example we use the method Int64(). It checks if a variable is part of the experiment, converts it to int64, and returns a value.      The default value is used when:                  no experiment and no rollout are configured for that variable or                    the experiment or rollout are not valid or do not match constraints or                    some errors occurred.            It is important to note that no network I/O happens during the GetVariables() call, as everything is done in the client. The variable tracking is done behind the scenes. The analyst sees it being reflected directly in our data lake, which consists of Simple Storage Service (S3) &amp; Presto.To make sure no network I/O happens on each GetVariable(), we made our SDK intelligent and formalised both dynamic configurations (we call them rollouts) and experiments. The SDK periodically fetches configurations from S3 and constructs internal, in-memory models to execute.Let’s start with a rollout definition example. It’s essentially a JSON document with a set of constraints the SDK can evaluate.{  \"variable\": \"automatedMessageDelay\",  \"rolloutBy\": \"city\",  \"rollouts\": [{    \"value\" : 60,    \"string\": \"60s delay\",    \"constraints\": [      {\"target\": \"city\", \"op\": \"=\", \"value\": \"6\"},      {\"target\": \"svc\", \"op\": \"in\", \"value\": \"[302, 11]\"}    ]  },{    \"value\" : 90,    \"string\": \"90s delay\",    \"constraints\": [      {\"target\": \"city\", \"op\": \"=\", \"value\": \"10\"},      {\"target\": \"pax\", \"op\": \"/\", \"value\": \"0.25\"}    ]  }],  \"default\": 30,  \"version\": 1515051871,  \"schema\": 1}This definition contains the rollout of the automatedMessageDelay variable.      The City facet configures the rollout. This means each city becomes a feature on its own for this variable. We also provide a web-based UI for configuring everything, as shown in the figure below.        There are two specific rollouts and one default rollout:    a. For Singapore (City = 6) and Vehicle types 302 and 11, the variable is set to 60.    b. For Jakarta (City = 10) and 25% of Passengers, the variable is set to 90.    c. For everything else, the default rollout value is 30.        The rollout definition has a version for auditing and a schema for possible evolution.  Our SDK uses an internal configuration service to store configurations (the Universal Configuration Manager, or UCM, which uses Amazon S3 behind-the-scenes). All of our backend services poll from UCM and get notified when a configuration is updated. The figure below demonstrates the overall system architecture.Similarly, we have an experiment configuration with more advanced features such as assignment strategy and values changing dynamically. In the example below, we define an experiment that randomly changes the value between 0 and 1 every 30 seconds..{  \"domain\": \"primary\",  \"name\": \"primary.testTimeSlicedShuffleStrategy\",  \"variables\": [{    \"name\": \"timeSlicedShuffleTest1\",    \"salt\": \"primary.testTimeSlicedShuffleStrategy\",    \"facets\": [\"ts\"],    \"strategy\": \"timeSliceShuffle\",    \"choices\": [{        \"value\": 0,        \"span\": 30      }, {        \"value\": 1,        \"span\": 30      }]    }],  \"constraints\": [    { \"target\": \"ts\", \"op\": \"&gt;\", \"value\": \"1528714601\" },    { \"target\": \"ts\", \"op\": \"&lt;\", \"value\": \"1528801001\" },    { \"target\": \"city\", \"op\": \"=\", \"value\": \"5\" }],  \"schemaVersion\": 1,  \"state\": \"COMPLETED\",  \"slotting\": {    \"byPercentage\": 0.5  }}Similar to the formalisation of feature toggles, we formalised our experiments as JSON files and configured through our configuration store. Everything is done asynchronously and reliably as our services only depend on a Tier-0 AWS Simple Storage Service (S3). Our goal was to keep everything simple and reliable.Embracing the BinaryAs mentioned earlier, our users need the ability to track things. In the SDK, GetVariable() tracks its specified variable value whenever it’s called.The experimentation platform SDK provides an easy way to track any variable from the code and directly surface it in the presto table for data analysts. Use the client’s Track() method which takes several parameters:      The name of the event, which gets prefixed by the service name.        The value of the event, which currently can be only a numeric value.        The facets representing contextual information about this event. Users are encouraged to provide as much information as possible, for example, passenger ID, booking code, driver ID.  client.Track(ctx, \"myEvent\", 42, sdk.NewFacets().    Passenger(123).    Booking(\"ADR-123-2-001\").    City(10))We use tracking for reporting when a decision is made. For example, when GetVariable() is called, we need to report whether control or treatment was applied to a particular passenger or booking code. Since there’s no direct network call to get a variable, we internally track every decision and send it to our data pipeline periodically and asynchronously. We also use tracking for capturing important metrics such as the duration of taxi pickup, whether a promotion applied, etc.When designing tracking, a major goal was to minimise network traffic while keeping performance impact of event reporting small. While this isn’t very important for backend services, we also use the same design for our mobile and web applications. In Southeast Asia, mobile networks may not be great. Also, data can be expensive for our drivers who cannot afford the fastest network plan and the latest iPhone. These business needs must be translated in the design.So how do we design an efficient protocol for telemetry transmission which keeps both CPU and network use down? We kept it simple, embracing the binary and batch events. We use variable size integer encoding and a minimisation technique for each batch, where once a string is written, it is assigned to an auto-incremented integer value and is written only once to the batch.This technique did miracles for us and kept network overhead at bay while still keeping our encoding algorithm relatively simple and efficient. It was more efficient than using generic serialisations such as Protocol Buffers, Avro, or JSON.ResultsWe have described our feature toggles SDK, but what benefits have we seen? We’ve seen fast adoption of the platform in the company, product managers rolling out features, and data scientists/analysts able to run experiments autonomously. Engineers are happy and things move faster inside the company. This makes us more competitive as an organisation and focused on our consumer’s needs, instead of spending time in meetings and on communication.",
        "url": "/feature-toggles-ab-testing"
      }
      ,
    
      "mockers": {
        "title": "Mockers - Overcoming Testing Challenges at Grab",
        "author": "mayank-guptavineet-nairshivkumar-krishnanthuy-nguyentvishal-prakash",
        "tags": "[&quot;Back End&quot;, &quot;Service&quot;, &quot;Testing&quot;]",
        "category": "",
        "content": "Grab serves millions of consumers in Southeast Asia, taking care of their everyday needs such as rides, food delivery, logistics, financial services, and cashless payments.To delight our consumers, we’re always looking at new features to launch, or how we can improve existing features. This means we need to develop fast, but also at high quality - which is not an easy balance to strike. To tackle this, we innovated around testing, resulting in Mockers - a tool to expand the scope of local box testing. In local box testing, developers can test their microservices without depending on an integrated test environment. It is an approach to implement Shift Left testing, in which testing is performed earlier in the product life cycle. Early testing makes it easier and less expensive to fix bugs.Grab employs a microservice architecture with over 250 microservices working together. Think of our application as a clockwork with coordinating gears. Each gear may evolve and change over time, but the overall system continues to work.Each microservice runs on its own and communicates with others through lightweight protocols like HTTP and gRPC, and each has its own development life cycle. This architecture allows Grab to quickly scale its applications. It takes less time to implement and deploy a new feature as a microservice.However the complexity of a microservices architecture makes testing much harder. Here are some common challenges:      Each team is responsible only for its microservices, so there’s little centralised management.        Teams use different programming languages, data stores, etc. for each microservice, so it’s hard to construct and maintain a good test environment that covers everything.        Some microservices have been around since the start, some were created last week, some were refactored a month ago. This means they may be at very different maturity levels.        As the number of microservices keeps growing, so does the number of tests needed for coverage.  Why Conventional Testing is Not EnoughThe conventional approach to testing involves heavy unit testing on local boxes, and maintains one or more test environments with all microservices  - these are usually called staging environments. Teams run integration, contract, and other tests on the staging environment, making it the primary test area. After comprehensive testing on staging, teams promote the microservice to production. Once it reaches production, very little or no testing is done.(The testing terminologies used here such as unit tests, integration tests, contract tests, etc are defined in http://www.testingstandards.co.uk/bs_7925-1_online.htm and https://martinfowler.com/bliki/ContractTest.html.)However, testing on a staging environment has its limitations:      Ambiguity of ownership - Staging is usually nobody’s responsibility as there is no centralized management. Issues on staging take longer to resolve because questions such as ‘who fixes’, ‘who coordinates’, and ‘who maintains’ can go unanswered. Further, one failed microservice results in a testing blocker as many microservices may depend on it.        High cost of finding and fixing bugs - Staging is where teams try to uncover complex bugs. Quite often, the cost of testing and debugging is high and confidence over results is low because:                  State of test environment is constantly changing as independent teams deploy their microservices, leading to false test failures                    Data and configuration become inconsistent over time due to:                              Orphaned testing that leaves data inconsistent across microservices                                Multiple users overusing staging for different purposes such as manual testing, providing demos and training, etc                                Manually hacking or hard coding data to simulate dependent functionality                                    Difficulty in testing negative cases - How would my microservice respond if the dependency times out or returns a bad response, or if the response payload is too big? Such negative cases are hard to simulate in staging as they either require extensive data set up or an intentional dependency failure.  Introducing Mockers - Now You can Run Your Tests LocallyMockers is a Go SDK coupled with a CLI tool for managing a central monorepo of mock servers at Grab.Mockers simulates a staging environment on developer local boxes. It expands the scope of testing at the local box level, and lets you run functional, resiliency, and contract tests on local boxes or on your CI (Continuous Integration) such as Jenkins. This enables you to catch complex bugs at lower costs as bugs are now found much earlier in the development life cycle. This key advantage makes Mockers a better testing tool than the conventional approach where testing primarily happens on staging, resulting in higher costs.It lets you create mock servers for mimicking the behaviour of your microservice dependencies, and you can easily set positive or negative expectations in your tests to simulate complex scenarios.The idea is to have one standard mock server per microservice at Grab, kept up-to-date with the microservice definition. Our monorepo makes any new mock server available to all teams for testing.Mockers generates mock servers for both HTTP and gRPC microservices. To set up a mock server for a HTTP microservice, you need to provide its API Swagger specification. For a gRPC mock server, you need to provide the protobuf file.Simple CLI commands in Mockers let you generate or update mock servers with the latest microservice definitions, as well as list all available mock servers.Here is an example for generating a gRPC mock server. The path to the protobuf file, in this case &lt;gitlab.net/…/pb&gt;, is provided in the servergen command.The Go SDK sets expectations for testing, and manages the mock server life cycle.For example, there’s a microservice, booking, that has a mock server. To test your microservice, which depends on booking,you start booking’s mock server and set up test expectations (requests to booking and responses from booking) in your test file. If the mock server is not in sync with the latest changes to booking, you use a simple CLI command to update it, and then run your tests and evaluate the results.Note that mock servers provide responses to requests from a microservice being tested, but do not process the requests with any internal logic. They just return the specified response for that request.What’s Great About MockersHere are some of Mockers’ features and their benefits:  Automatic contract verification - We generate a mock server based on a microservice’s API specification. It provides code hooks to set expectations using the microservice defined request and response structs. In the code below, assume the CarType struct field is deleted from the booking microservice. Now, when we update the mock server using CLI, this test generates a compile time error saying “CarType” struct field is unknown, indicating a contract mismatch.      Out-of-the-box resiliency testing with repeatable tests - Building on top of our in-house chaos SDK, we inject a middleware into the mock server enabling developers to bring all sorts of chaos scenarios to life. Want to check if your retries are working properly or code fallback actually works? Just add a resiliency test to fail the mock server by 50% and check if retries work, or fail 100% to check if code fallback actually executes. You can also simulate latency, memory leaks, CPU spike, etc.        No overhead of maintaining code mocks when dependent microservices change; a simple CLI command updates the mock server.        As shown in the following examples, it’s simple to write tests. As mock servers send their mock results at the network level, you don’t have to expose your microservice’s guts to inject code mocks. Developers can treat their microservice as a black box. Note that these are not complete test cases, but they show the basics of testing using mock servers.  Here is an example test.Here is an example of resiliency testing.Common Questions about Mockers  How is this different from my unit tests with code mocks for dependencies?In unit tests, you mock the interfaces for your microservice dependencies. With Mockers, you avoid this overhead and use mock servers started on network ports. This tests your outbound API calls over the network to dependent mock servers, and tests your microservice at a layer closer to integration.  How do I know my mock server is up-to-date with the latest API contracts?Currently, you need to update the mock server using the CLI. If there is an API contract mismatch after the update, your Mockers based tests start to break. In future, we will add the last updated info for each mock server in the mockers ls CLI command.  I ran functional tests locally using Mockers. Should I still write and maintain integration tests for my microservice on staging?Yes. The integration tests run against real microservice dependencies with real data, and other streaming infrastructure on a distributed Staging environment. Hence, you should have integration tests for your microservice to catch issues before promoting to production.Road AheadWe’ve identified Grab’s mobile app as a candidate to benefit from using mock servers. To this end, we are working on building a microservice that acts as a mock server supporting both HTTP and TCP protocols.With this microservice, mobile developers and test engineers can use our user interface (UI) to set their expected responses to mobile app calls. The mobile app is then pointed to the mock server for sending and receiving responses.Benefits:      Mobile teams can test an app’s rendering and functionality aspects without being fully dependent on an integrated staging environment for the backend.        Backend flows currently under production can be easily tested using custom JSON responses during the mobile app development phase.  In ConclusionMockers deals with microservice testing challenges, which helps you meet your consumers’ demands.Mockers adoption has seen a steady growth among our critical microservices. Since Mockers was launched at Grab, many teams have adopted it to test their microservices. Our adoption rate has increased every month in 2018 so far, and we see no reason why this won’t continue until we run out of non-Mockers using microservices.Depth rather than breadth of Mockers usage has increased. In the last few months, teams adopting Mockers wrote a large number of tests that use mock servers.If you have any comments or questions about Mockers, please leave a comment.",
        "url": "/mockers"
      }
      ,
    
      "journey-tourist-grab": {
        "title": "Journey of a Tourist via Grab",
        "author": "lara-pureum-yim",
        "tags": "[&quot;Analytics&quot;, &quot;Data&quot;, &quot;Data Analytics&quot;, &quot;Tourism&quot;, &quot;Tourists&quot;]",
        "category": "",
        "content": "IntroductionThe recent premiere of the “made-with-Singapore” blockbuster movie “Crazy Rich Asians” has garnered real hype around the city-state as a lavish tourist destination. Do tourists travel on Grab to outlandishly fancy places like those you see in the movie? What were their favourite local places? Other than major attractions and shopping destinations, where do they go? Here are some exciting travel patterns that we found about our tourists’ rides in Singapore!1. Tourists Arrival Pattern in SingaporeLet’s look at the composition of tourist-passengers on Grab platform.More than 60% of total tourist-passengers on Grab come from Southeast Asia, mainly from Malaysia, Indonesia, Philippines, Vietnam, and Thailand.With our data that covers millions of passengers from more than 150 countries (the list includes passengers from Seychelles, Madagascar, and even North Korea!), we found that more than half of Grab’s international tourists come from China, USA, and India, outside of Southeast Asia.In terms of seasonality, we found distinguished differences between those who come from countries around the equator and those from places with four-seasons.Tourists coming from tropical region tend to follow the usual festivity season, resembling a trimodal distribution - where Grab sees high peaks of tourist-passengers in start-of-year, mid-year and end-of-year. South/Southeast Asian and Middle Easterners seem to leverage on school holidays and major public holidays to travel to Singapore.Meanwhile, those who seek to avoid cold weather in Europe, North America, as well as Northeast Asia, tend to come to Singapore and ride with Grab during their winter time, mainly from September to January. Singapore’s warm weather tends to provide the much needed sunlight to those tourists-passengers while Grab provides them with convenient and reliable transportation to various attractions.2. Tourists Mobility in Singapore with GrabWhere do they like to visit and enjoy in the Lion City? Which of Singapore’s most iconic landmarks do they like to travel to via Grab? At which pick-up points do Grab serve the tourist-passengers the most?2.1. Connecting Tourist-Passengers from Airport to HotelsOnce they land in Singapore, tourists-passengers’ first ride out of Changi offers evergreen view of the “Garden City.” When Grab cruises out of the Skytrax World’s Best Airport, almost 90% of our tourist-passengers headed straight to hotels, according to our data.Most of these hotels are in the central region - in fact, a cluster of green circles on the map below is where 80% of tourists go to via Grab. Sure enough, Orchard, Bugis, Singapore River, Downtown Core areas are heavily crowded with excellent hotels, exotic restaurants, and exciting nightlife scenes.What’s interesting is the large volume of Grab bookings from the airport to Kallang where Kampong glam is located. It nests just as many tourists as all the other smaller areas combined, with high concentration of affordable hotels as well as bustling streets filled with colourful shophouses, mosques, temples, and local eateries.2.2. Connecting Tourist-Passengers from Airport to Cruises, MICE, and Major AttractionsThose who were too excited to skip the hotel check-in, where do they go from the airport? Our data shows that tourists craving for crazy-rich-Asian shopping went to Singapore’s iconic Marina Bay Sands as well as prominent shopping malls in downtown. Island-hoppers’ destinations were either Harbourfront Cruise and Ferry Terminal or Tanah Merah Ferry Terminal.The list goes on to show that some people went straight to Sentosa from the airport for their exclusive holidays, while some went to MICE (Meetings, Incentives, Conferences, Exhibitions) venues to attend to business.2.3. Singapore Shopping SpreeA significant proportion of our passengers took a ride with us to major shopping areas, especially Orchard, Downtown, Bugis, Harbourfront, and Kallang.Undoubtedly, these places offer a lively and picturesque array of shopping options, ranging from antique jewelries to boutique luxurious handbags.Given the popular timing of their pick-ups from these malls -which peaks at 2pm, 4pm, and 9pm-, we are delighted to know that Grab is there to complete our tourist-passengers’ shopping or dining experience.2.4. Food, food, food!Hawker centers are an indispensable part of Singapore’s food culture. And our data shows that tourists value that too! Also known as the place where “Crazy Rich Asians” local delicacies dining scene was filmed, Newton Food Centre is one of the most sought-after dining places for our tourist-passengers.Chinatown and Chijmes are other venues that topped the list for Grab-riding-tourists’ favourite dining list. Our data shows that Grab rides are more than doubled during the late night hours, and especially so on Friday, Saturday, Sunday. And yes, Chijmes was where the wedding was held in the movie – but in reality, it’s a friendly heritage building that houses many dining options.2.5. Medical TourismWe observed that hospitals and medical centers (both private and public) are popular destinations for our tourist-passengers.What’s even more striking is its growth on Grab platform which has increased over 500% over 2015-2017. According to data from a medical tourism index in 2017, Singapore was ranked the most attractive among seven Asian countries in terms of “patient experience.”  Majority of these medical-tourists on our platform come from Southeast Asian countries.ConclusionGrab’s services to tourists are an integral part of connecting tourists to various destinations and attractions. Our data shows that there is a plethora of captivating locations in Singapore that are both uniquely local and vibrantly modern. Our tourist-passengers were a mixed bunch who seemed to know how to enjoy Singapore!This is only the beginning of Grab’s effort to interpret more about tourists’ mobility patterns.Grab is dedicated to making the tourists’ experience on our platform more convenient and delightful. By delving into Singapore-loving visitors’ behavioural patterns through our data, we hope to serve you better.If you are curious about how tourists are travelling via Grab in other countries, let us know! We will drill down into our data to discover something interesting for you!",
        "url": "/journey-tourist-grab"
      }
      ,
    
      "quotas-service": {
        "title": "How We Designed the Quotas Microservice to Prevent Resource Abuse",
        "author": "jim-zhangao-chao",
        "tags": "[&quot;Quota&quot;, &quot;Back End&quot;, &quot;Service&quot;]",
        "category": "",
        "content": "How We Designed the Quotas Microservice to Prevent Resource AbuseAs the business has grown, Grab’s infrastructure has changed from a monolithic service to dozens of microservices. And that number will soon be expressed in hundreds. As our engineering team grows in parallel, having a microservice framework provides benefits such as higher flexibility, productivity, security, and system reliability. Teams define Service Level Agreements (SLA) with their clients, meaning specification of their service’s API interface and its related performance metrics. As long as the SLAs are maintained, individual teams can focus on their services without worrying about breaking other services.However, migrating to a microservice framework can be tricky - due to the the large number of services and having to communicate between them. Problems that are simple to solve or don’t exist for a monolithic service such as service discovery, security, load balancing, monitoring, and rate limiting are challenging for a microservice based framework. Reliable, scalable, and high performing solutions for common system level issues are essential for microservice success, and there is a Grab-wide initiative to provide those common solutions.As an important component of the initiative, we wrote a microservice called Quotas, a highly scalable API request rate limiting solution to mitigate the problems of service abuse and cascading service failures. In this article, we discuss the challenges Quotas addresses, how we designed it, and the end results. What Quotas Try to AddressRate limiting is a well known concept used by many companies for years. For example, telecommunication companies and content providers frequently throttle requests from abusive users by using popular rate-limiting algorithms such as leaky bucket, fixed window, sliding log, sliding window, etc. All of these avoid resource abuse and protect important resources. Companies have also developed rate limiting solutions for inter-service communications, such as Doorman (https://github.com/youtube/doorman/blob/master/doc/design.md), Ambassador (https://www.getambassador.io/reference/services/rate-limit-service), etc, just to name a few.Rate limiting can be enforced locally or globally. Local rate limiting means an instance accumulates API request information and makes decisions locally, with no coordination required. For example, a local rate limiting strategy can specify that each service instance can serve up to 1000 requests per second for an API, and the service instance will keep a local time-aware request counter. Once the number of received requests exceeds the threshold, it will reject new requests immediately until the next time bucket with available quota. Global rate limiting means multiple instances share the same enforcement policy. With global rate limiting, regardless of the service instance a client calls, it will be subjected to the same global API quota. Global rate limiting ensures there is a global view and it is preferred in many scenarios. In a cloud context, with auto scaling policy setup, the number of instances for a service can increase significantly during peak traffic hours. If only local rate limiting is enforced, the accumulative effect can still put great pressure on critical resources such as databases, network, or downstream services and the cumulative effects can cause service failures.However, to support global rate limiting in a distributed environment is not easy, and it becomes even more challenging when the number of services and instances increases. To support a global view, Quotas needs to know how many requests a client service A (i.e., service A is a client of Quotas) is getting now on an endpoint comparing to the defined thresholds. If the number of requests is already over the thresholds, Quotas service should help to block a new request before service A executes its main logic. By doing that, Quotas service helps service A protect resources such as CPU, memory, database, network, and its downstream services, etc. To track the global request counts on service endpoints, a centralised data store such as Redis or Dynamo is generally used for the aggregation and decision making. In addition, decision latency and scalability become major concerns if each request needs to make a call to the rate limiting service (i.e., Quotas) to decide if the request should be throttled. And if that is the case, the rate limiting service will be on the critical path of every request and it will be a major concern for services. That is the scenario we absolutely wanted to avoid when designing Quotas service.Designing QuotasQuotas ensures Grab internal services can guarantee their service level agreement (SLA) by throttling “excessive” API requests made to them, thereby avoiding cascading failures . By rejecting these calls early through throttling, services can be protected from depleting critical resources such as databases, computation resources, etc.The two main goals for Quotas are:      Help client services throttle excessive API requests in a timely fashion.        Minimise latency impacts on client services, i.e., client services should only see negligible latency increase on API response time.  We followed these design guidelines:      Providing a thin client implementation. Quotas service should keep most of the processing logic at the service side. Once we release a client SDK, it’s very hard to track who’s using what version and to update every client service with a new client SDK version. Also, more complex client side logic increases the chances of introducing bugs.        To allow scaling of Quotas service, we use an asynchronous processing pipeline instead of a synchronous one (i.e., client service makes calls Quotas for every API request). By asynchronously processing events, a client service can immediately decide whether to throttle an API request when it comes in, without delaying the response too much.        Allowing for horizontal scaling through config changes. This is very important since the goal is to onboard all Grab internal services.  Figure 1 is a high-level system diagram for Quotas’ client and server side interactions. Kafka sits at the core of the system design. Kafka is an open-source distributed streaming platform under the Apache license and it’s widely adopted by the industry (https://kafka.apache.org/intro). Kafka is used in Quotas system design for the following purposes:      Quotas client services (i.e., services B and C in Figure 1) send API usage information through a dedicated Kafka topic and Quotas service consumes the events and performs its business logic.        Quotas service sends rate-limiting decisions through application-specific Kafka topics and the Quotas client SDKs running on the client service instances consume the rate-limiting events and update the local in-memory cache for rate-limiting decisions. For example, Quotas service uses topic names such as “rate-limiting-service-b” for rate-limiting decisions with service B and “rate-limiting-service-c” for service C.        An archiver is running with Kafka to archive the events to AWS S3 buckets for additional analysis.      Figure 1: Quotas High-level System DesignThe details of Quotas client side logic is shown in Figure 2 using service B as an example. As it shows, when a request comes in (e.g., from service A), service B will perform the following logic:\tQuotas middleware running with service B\t\t\t\t  intercepts the request and calls Quotas client SDK for the rate limiting decision based on API and client information.\t\t  \t\t\t  \t\tIf it throttles the request, service B returns a response code indicating the request is throttled.\t\t  \t\tIf it doesn't throttle the request, service B handles it with its normal business logic.\t\t  \t\t\t  \t\t  asynchronously sends the API request information to a Kafka topic for processing.\t\t\t\tQuotas client SDK running with service B\t\t\t\t\tconsumes the application-specific rate-limiting Kafka stream and updates its local in-memory cache for new rate-limiting decisions. For example, if the previous decision is true (i.e., enforcing rate limiting), and the new decision from the Kafka stream is false, the local in-memory cache will be updated to reflect the change. After that, if a new request comes in from service A, it will be allowed to go through and served by service B.\t\t\tprovides a single public API to read the rate limiting decision based on API and client information. This public API reads the decisions from its local in-memory cache.\t\t\t    Figure 2: Quotas Client Side LogicFigure 3 shows the details of Quotas server side logic. It performs the following business logic:      Consumes the Kafka stream topic for API request information        Performs aggregations on the API usages        Stores the stats in a Redis cluster periodically        Makes a rate-limiting decision periodically        Sends the rate-limiting decisions to an application-specific Kafka stream        Sends the stats to DataDog for monitoring and alerting periodically  In addition, an admin UI is available for service owners to update thresholds and the changes are picked up immediately for the upcoming rate-limiting decisions.    Figure 3: Quotas Server Side LogicImplementation Decisions and OptimisationsOn the client service side (service B in the above diagrams), the Quotas client SDK is initialised when service B instance is initialised. The Quotas client SDK is a wrapper that consumes Kafka rate-limiting events and writes/reads the in-memory cache. It exposes a single API to check the rate-limiting decisions on a client with a given API method. Also, service B is hooked up with Quotas middleware to intercept API requests. Internally, it calls the Quotas client SDK API to determine if it should allow/reject the requests before the actual business logic. Currently, Quotas middleware supports both gRPC and REST protocols.Quotas utilises a company-wide streaming solution called Sprinkler for the Kafka stream Producer and Consumer implementations. It provides streaming SDKs built on top of sarama (an MIT-license Go library for Apache Kafka), providing asynchronous event sending/consuming, retry, and circuit breaking capabilities.Quotas provides throttling capabilities based on the sliding window algorithm on the 1-second and 5-second levels. To support extremely high TPS demands, most of Quotas intermediate operations are designed to be done asynchronously. Internal benchmarks show the delay for enforcing a rate-limiting decision is up to 200 milliseconds. By combining 1-second and 5-second level settings, client services can more effectively throttle requests.During system implementation, we find that if Quotas instances make a call to the Redis cluster every time it receives an event from the Kafka API usage stream, the Redis cluster will quickly become a bottleneck due to the amount of calculations. By aggregating API usage stats locally in-memory and calling Redis instances periodically (i.e., every 50 ms), we can significantly reduce Redis usage and still keep the overall decision latency at a relatively low level. In addition, we designed the hash keys in a way to make sure requests are evenly distributed across Redis instances.Evaluation and BenchmarksWe did multiple rounds of load tests, both before and after launching Quotas, to evaluate its performance and find potential scaling bottlenecks. After the optimisation efforts, Quotas now gracefully handles 200k peak production TPS. More importantly, critical system resource usage for Quotas’ application server, Redis and Kafka are still at a relatively low level, suggesting that Quotas can support much higher TPS before the need to scale up.Quotas current production settings are:      12 c5.2xlarge (8 vCPU, 16GB) AWS EC2 instances        6 cache.m4.large (2 vCPU, 6.42GB, master-slave) AWS ElasticCaches        Shared Kafka cluster with other application topics  Figures 4 &amp; 5 show a typical day’s CPU usage for the Quotas application server and Redis Cache respectively. With 200k peak TPS, Quotas handles the load with peak application server CPU usage at about 20% and Redis CPU usage of 15%. Due to the nature of Quotas data usage, most of the data stored in Redis cache is time sensitive and stored with time-to-live (TTL) values.However, because of how Redis expires keys (https://redis.io/commands/expire) and the amount of time-sensitive data Quotas stores in Redis, we have implemented a proprietary cron job to actively garbage collect expired Redis keys. By running the cron job every 15 minutes, Quotas keeps the Redis memory usage at a low level.    Figure 4: Quotas CPU Usage    Figure 5: Quotas Redis CPU UsageWe have conducted load tests to identify the potential issues for scaling Quotas. The tests have shown that we can horizontally scale Quotas to support extremely high TPS using only configuration changes:      Kafka is well known for its high throughput, low-latency, high scalability characteristics. By either increasing the number of partitions on Quotas API usage topic or adding more Kafka nodes, the system can evenly distribute and handle additional load.        All Quotas application servers form a consumer group (CG) to consume the Kafka API usage topic (partitioned based on the number of instance expectations). Whenever an instance starts or goes offline, the topic partitions are re-distributed among the application servers. This allows balanced topic partition consumptions and thus somewhat evenly distributed application server CPU and memory usages.         We have also implemented a consistent hashing based algorithm to support multiple Redis instances. It supports easy Redis instances addition or removal by configuration changes. With well chosen hash keys, load can be evenly distributed to the Redis instances.  With the above design and implementations, all the critical Quotas components can be easily scaled and extended when a bottleneck occurs either at Kafka, application server, or Redis levels.Roadmap for QuotasQuotas is currently used by more than a dozen internal Grab services, and soon all Grab internal services will use it.Quotas is part of the company-wide ServiceMesh effort to handle service discovery, load balancing, circuit breaker, retry, health monitoring, rate-limiting, security, etc. consistently across all Grab services.",
        "url": "/quotas-service"
      }
      ,
    
      "boh-prize": {
        "title": "Grab Senior Data Scientist Liuqin Yang Wins Beale-Orchard-Hays Prize",
        "author": "yang-liuqin",
        "tags": "[&quot;Data Science&quot;, &quot;BOH&quot;]",
        "category": "",
        "content": "The Beale-Orchard-Hays PrizeGrab Senior Data Scientist Dr. Liuqin Yang (along with Professor Defeng Sun and Professor Kim-Chuan Toh) wins the 2018 Beale-Orchard-Hays Prize, the highest honour in the field of Computational Mathematical Optimisation. This is the first time an Asian team wins the Beale-Orchard-Hays Prize. The award is presented once every three years by Mathematical Optimisation Society in memory of Martin Beale and William Orchard-Hays, pioneers in computational mathematical optimisation. Previous winners include world leading figures in computational optimisation such as Professor Stephen P. Boyd and Professor William J. Cook.Mathematical optimisation is widely used in many fields, for example, vast majority of the models in machine learning are essentially optimisation problems.The Award-winning Paper and SoftwareThe award was presented at the opening ceremony of the 23rd International Symposium for Mathematical Programming (ISMP) in France in July 2018. ISMP takes place every three years and is the flagship conference in the field of mathematical optimisation. The prize was awarded for a paper and the software SDPNAL+ that it refers.    In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.The software is designed for solving semidefinite programming (SDP) but the optimisation methods presented in the paper can be applied to more general mathematical optimisation problems. SDP is an important subfield of mathematical optimisation and its applications are growing rapidly. Many practical problems in operations research and machine learning can be modelled or approximated as SDP problems.Traditional optimisation methods can only solve small and medium scale (say, matrix dimension is less than 2000 and the number of constraints is less than 5000) SDP. Fortunately, large-scale SDP can be solved efficiently by SDPNAL+ now. Numerical experiments in the paper and other benchmark tests show that SDPNAL+ is a state-of-the-art solver for large-scale SDP and it is the only viable software to solve many large-scale SDPs at present. The largest SDP problem that is solved has matrix dimension 9261 and the number of constraints more than 12 million, which boosts the solvable scale to thousands of times. In particular, the prize jury chair Dr. Michael Grant presented a concrete example shared by the nominator. It takes 122 hours for the traditional solver to solve a problem in a cluster with 56 cores CPU and 128 GPUs while SDPNAL+ solves it within 1.5 hours in a normal desktop PC.Applications in Data Science and GrabThe novel technology of the software SDPNAL+ also contributes to data science and AI (Artificial Intelligence) community. Mathematical optimisation is the essential foundation of machine learning and AI. Many large-scale machine learning problems can be solved by the algorithms used in the software, for example, Lasso problems, support vector machine and deep learning. Consequently, the novel technology can be applied to voice search, voice-activated assistants, face perception, automatic translation, cancer detection, and so on.Grab is a leading technology company that offers ride-hailing, ride sharing and logistics services in Southeast Asian. It is also a data-driven company and millions of rides are booked on the app daily. Grab needs to solve a lot of large-scale optimisation problems, e.g., allocation optimisation, carpool optimisation and logistics optimisation; and a lot of large-scale machine learning problems, e.g., supply and demand forecasting. The optimisation technology has been used in Grab to speed up the key algorithms to hundreds of times faster and achieve a cost reduction of millions of dollars.A significant project we are working on in Grab is allocation optimisation system, which matches the passengers and the drivers in an optimal way. The drivers are always moving, and we need to choose the optimal driver for each passenger based on distance and many other factors to maximise the system efficiency and user experience. The allocation efficiency can be increased to dozens of times by using the optimisation techniques. Thousands of requests are booked in Grab each minute on average and we need to allocate the bookings every few seconds by dozens of millions of computations. The computational optimisation techniques can accelerate the allocation algorithms to run hundreds of times faster.Prize CitationThe text of the award citation is below:Liuqin Yang, Defeng Sun and Kim-Chuan Toh, SDPNAL+: a majorised semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints, Mathematical Programming Computation, 7 (2015), 331-366.Biography of the WinnersProfessor Kim-Chuan Toh is a Provost’s Chair Professor at the Department of Mathematics, National University of Singapore (NUS). He is one of the world’s leading figures in computational optimisation and the winner of the 2017 INFORMS Optimisation Society Farkas Prize for his fundamental contributions to the theory, practice, and application of convex optimisation. His current research focuses on designing efficient algorithms and software packages for large-scale machine learning problems and matrix optimisation problems.Professor Defeng Sun is Chair Professor of Applied Optimisation and Operations Research, The Hong Kong Polytechnic University. He is one of the world’s leading figures in semismooth Newton methods for optimisation. He currently focuses on building up the new field of matrix optimisation and establishing the foundation for the next generation methodologies for big data optimisation and applications.Dr. Liuqin Yang is Senior Data Scientist at Grab and a computational optimisation expert. He obtained his PhD degree in Mathematics from NUS in 2015 under the direction of Professor Toh and Professor Sun. The award-winning paper and software SDPNAL+ is one of his PhD research topics. He has published three papers in the top optimisation journals. Currently, he works on big data optimisation, machine learning and business applications in data science.",
        "url": "/boh-prize"
      }
      ,
    
      "building-grab-s-experimentation-platform": {
        "title": "Building Grab’s Experimentation Platform",
        "author": "abeesh-thomasroman-atachiants",
        "tags": "[&quot;Experiment&quot;, &quot;Back End&quot;, &quot;Front End&quot;]",
        "category": "",
        "content": "ExP OverviewAt Grab, we continuously strive to improve the user experience of our app for both our passengers and driver-partners.To do that, we’re constantly experimenting, and in fact, many of the improvements we roll out  to the Grab app are a direct result of successful experiments.However, running many experiments at the same time can be a messy, complicated and expensive process. That is why we created the Grab  experimentation platform (ExP), to provide clean and simple ways to identify opportunities, create prototypes, perform experiments, refine, and launch products. Before rolling out new Grab features, ExP enables us to run controlled experiments to test the effectiveness of the new feature. The goal of ExP is to make sure  that new features roll out without any hiccups and causal relationships are analysed correctly.Experimentation helps development teams determine if they’re building the right product. It allows the team to scrap an idea early on if it doesn’t make a positive impact. This avoids wastage of precious resources. By adopting experimentation, teams eliminate uncertainty and guesswork from their product  development process; thus avoiding long development cycles. By introducing a new version of our app to only a select group of consumers, teams can quickly assess if their new updates are improvements or regressions. This allows for better recovery and damage control if necessary.    Figure: Experimentation Platform PortalWhy We Built ExPIn the early days, experiments were performed on a small scale that allowed users to define metrics, and then compute and surface those metrics for a small set of experiments. The process around experimentation was rather painful. When product managers wanted to run an experiment, they set up a meeting with product analysts, data scientists, and engineers. Experiments were designed, custom logging pipelines were built and services were modified to support each new experiment. It was an expensive and time-consuming process.To overcome these challenges, we wanted to build a platform with the following goals in mind:      Create a unified experimentation platform across the organisation that prevents multiple concurrent experiments from interfering with one another and allows engineers and data scientists to work on the same set of tools.        Allow simple, fast, and cost-effective experiments        Automate the selection of representative cohorts of drivers and passengers to perform A/A testing.        Support power analysis to perform appropriate significance tests        Enable a fully automated data pipeline where experimental data is streamed out in real-time, then tagged and stored in S3        Create platform for plugging in custom analysis modules        Create Event Triggers/Alerts set up on important business metrics to identify adverse effects of a change        Design single centralised online UI for creating and managing the experiments, which is constantly being improved - long term vision is to allow anyone in the organization to create and run experiments  Since implementing ExP, we have seen the number of experiments grow from just a handful to about 25 running concurrently. More impressively, the number of metrics computed per day has grown exponentially to ~2500 distinct metrics per day and roughly 50,000 distinct experiment/metric combinations.With this scale comes some issues we needed to address. Here is the architectural approach we have taken to address them:Prevention of network effects - At Grab, we have several types of users: our driver-partners, passengers, and merchants. Unlike most experimentation platforms out there that deal with a single website visitor, our user types can and do interact with each other which leads to network effects in some cases. For example, an experiment on promotions can lead to a surge of demand more than the supply.Control and treatment assignment strategies - Various teams within the organisation have different requirements and ways of setting up experiments. Some simple aesthetic experiments can be simply randomised by a user ID while other, algorithmic experiments may use a time-slicing strategies with bias minimisation. So we built many different strategies for different use-cases to address the challenging task of having all of these be both random and deterministic at the same time.Prevention of experiment interference - We also attempt to gate for inter-experiment interference by providing a mechanism similar to Google’s Domains &amp; Layers combined with an expert system for experiment design validation. We attempt to prevent interference of experiments by introducing a geo-temporal segmentation for concurrent experiments running together with advanced validation and suggestions to users on how experiments need to be setup.Components of ExPGrab’s ExP allows internal users (engineers, product managers, analysts, and others) to toggle various features on or off, adjust thresholds, change configurations dynamically without restarting anything. To achieve this, we’ve introduced a couple of cornerstone concepts in our UI and SDKs.Variables and MetricsThe basic components of every experimentation platform are variables and metrics.      A Variable is something we can change, for example, different payment methods can be enabled for a particular user or a city.        A Metric is something we want to improve and keep observing. For example, cancellation rate or revenue. In our platform, we constantly keep track of metric changes.  RolloutsOur rollout process consists of deploying a feature first to a small portion of users and then gradually ramping up in stages to larger groups. Eventually, we reach 100 percent of all users that fall under a target specification (for instance, geographic location, which can be as small as a district of a city.The goal of a feature rollout is to make the deployment of new features as stable and reliable as possible by controlling user exposure in the early stages and monitoring the impact of the feature on key business metrics at each stage.GroupsOur platform lets internal users define custom groups (also known segments). A group is a set of identifiers such as passenger IDs, geohashes, cities, and others. We use this to logically group a set of things that we can then conduct rollouts and experiments on.ExperimentsAt Grab, we have formalised an “experiment definition” which is essentially a time-bound (with start and end time) configuration which can be split between control and treatment(s) for one or multiple variables. This configuration is stored as a JSON document and contains the entire experiment setup.It is important to highlight that having a formal experiment definition actually brings several benefits to the table:      Machines can understand it and can automatically and autonomously execute experiments, even in distributed systems.        Communication between teams (engineering, product and data science) is simplified as formal documents to ensure everyone is on the same page.  Structured Experimental DesignWith a formalised experiment definition, we then provide Android, iOS and Golang SDKs which consume experiment definitions and apply experiments.Experiment definitions allow our SDKs to intelligently apply experiments without actually doing any costly network calls. Experiments get delivered to the SDKs through our configuration management platform, which supports dynamic reconfiguration.Our SDKs implement various algorithms that enable experiment designers to set up experiments and define an assignment strategy (algorithm), which determines the value to be returned for a particular variable, and for a particular user.Overall, we support two major and frequently used strategies:      Randomised sampling with uniform or weighted probability. This is useful when we want to randomly sample between control and treatment(s), for example, if we want 50% of passengers to get one value and 50% of passengers to get another value for the given variable.        Time-sliced experiments where control and treatment(s) are split by time (for example, 10 minutes control, then 10 minutes for treatment).  Sample ExperimentSince its rollout, ExP and its staged rollout framework have proven to be indispensable to many Grab feature deployments.Take the GrabChat feature, for example. Booking cancellations were a key problem and the team believed that with the right interventions in place, some cancellations could be prevented.One of the ideas we had was to use GrabChat to establish a conversation between the driver and the passenger by sending automated messages. This transforms the service from a mere transaction to something more human and personal. By adding this human touch to the service, it reduced perceived waiting time, making passengers and driver-partners more patient and accepting of any unavoidable delays that might arise.When we deployed this new feature for app users in a specific geographic area, we noticed a drop in cancellations. To validate this, we conducted a series of iterative experiments using ExP. Check out this blog to find out more: https://engineering.grab.com/experiment-chat-booking-cancellationsLastly, we used the platform to perform a staged rollout of this functionality to different users in different countries across Southeast Asia.ConclusionBuilding our own experimentation platform hasn’t been an easy process, but it  has helped  promote a culture of experimentation within the organisation. It has allowed data scientists and product teams to analyse the quality of new features and perform iterations more frequently, with our team working closely with them to support various assignment strategies and hypothesis.Looking ahead, there is more we can do to evolve ExP. We’re looking at building automated and real-time dashboards and funnels with slice and dice functionality for our experiments and further increasing experimental capacity while maintaining strict boundaries in order to maintain validity of experiments. Ultimately, to keep improving, we must keep experimenting.",
        "url": "/building-grab-s-experimentation-platform"
      }
      ,
    
      "introducing-grab-kit": {
        "title": "Introducing Grab-Kit: Distributed Service Design at Grab",
        "author": "karen-kuemichael-cartmell",
        "tags": "[&quot;Back End&quot;, &quot;Engineering&quot;, &quot;Golang&quot;]",
        "category": "",
        "content": "As Grab rapidly expands its services, we at Engineering continue to look for ways to work smarter and deliver qualitative and relevant services quickly and efficiently. This helps us to stay true to our commitment to outserve our partners and consumers.As we evolved from a single monolithic application to a microservices-based architecture, we were faced with a new challenge. How do we support exponential growth while maintaining consistency, coordination, and quality?Here is what we came up with.A Framework to Solve it AllOur Grab Developer Experience team came up with the following solution: Grab-Kit - a framework for building Go microservices. Grab-Kit is designed to create a fully functional microservice scaffolding in seconds, allowing engineers to focus on the business logic straight away!Grab-Kit provides abstraction from all aspects of distributed system design by simplifying the creation and operation of microservices through scaffolding, using smart library configuration defaults, automatic initialisation, context propagation, and runtime framework configuration. Moreover, it standardises communication across services.We no longer need to spend long hours generating boilerplate code, initialising common libraries, creating dashboards and alarms, or creating Data Access Objects (DAOs). Instead, we can concentrate on delivering scalable and agile services that are essential for the success of our engineers and in turn delight our consumers.The Heart of Grab-KitThe inspiration behind the Grab-Kit framework is Go-Kit. However, Grab-Kit goes beyond the ideas proposed by Go-Kit, for example, our Grab-Kit has added automatic code generation, which saves efforts required in writing boilerplate code for both server and client service sides. While Go-Kit proposes techniques for microservices, there is still a lot of manual work involved in implementing them. In contrast, Grab-Kit actually helps us focus on the business logic by doing this work for us while codifying all best engineering practices around distributed service design.Continue reading to see what we love most about Grab-Kit.StandardisationThe underlying intention of Grab-Kit is to gain consistency across services in the following components:Service DefinitionsProblemsServices have multiple sources and configurations, and produce inconsistent APIs, SDKs, error handling, transport layer, and so on.SolutionReaching a level of consistency relies on having a single source of truth. Grab-Kit defines the service definition in a proto definition file (.proto file) and considers this file as the single source of truth.How is it done?Grab-Kit automatically generates a .proto file when we run Grab-Kit create &lt;service&gt; for the first time; this file is then used by Grab-Kit to generate all other code such as boilerplates and data transfer objects (DTOs). Grab-Kit automatically generates DTOs for custom message types in the .proto file. It also generates the protocol buffers (protobuf) bindings for these types, so they can be converted between the Go DTO and protobuf types.Middleware StackProblemsTeams manage multiple logs in various locations. The logs were in many different formats, making it difficult to search and filter them.Traceability is another factor that prevented teams from monitoring service health efficiently. There was no indication on what happened to a request.SolutionGrab-Kit uses a consistent middleware stack across all clients. It uses middleware for logging, circuit breaker, stats, panic recovery, profiling, caching, and so on.Grab-Kit provides easy, automatic profiling with flame graphs and execution traces available in development mode. Further, all service related metrics and logs are generated automatically.How is it done?Grab-Kit wraps endpoints with a standard middleware for logging and stats. It also compacts stack traces using the panicparse library. Grab-Kit’s output is much more readable than the default output.In addition, the consistent middleware stack automatically starts the CPU profile and trace for each endpoint on developer mode.Automated Dashboard GenerationProblemsOur services are monitored on dashboards and monitoring is important to ensure that our services are working as they should. However, it can be time consuming to create meaningful dashboards without fully understanding the available metrics in our libraries, or how to even use them.Dashboards also need to be regularly maintained as changes to the metrics or keys used can lead to missing or inaccurate graphs.Missing alerts can lead to production incidents going unnoticed, consequently costing Grab business opportunities.SolutionWith Grab-Kit, we can automatically create dashboards and add graphs (for monitoring and observing) for our services and all its upstream dependencies. In addition, we can keep the graphs up to date as the codebase changes.How is it done?We enable libraries to define the metrics published in a declarative manner (metrics.yaml). A tool (grab-kit dash) reads these files and uses the DataDog API to automatically create a dashboard with the given metrics. If a dashboard already exists, Grab-Kit adds any missing metrics and updates the existing ones, ensuring that the dashboard is always complete and in sync.Following is an example workflow for creating dashboards and updating existing ones:We’ve gone with the modular approach because not all libraries -are relevant to a particular service. This means that Grab-Kit can selectively publish graphs from just the libraries used by the service. For example, if service X doesn’t use Elasticsearch, then it doesn’t need the Elasticsearch metrics.There is a group of ‘core’ metrics included by default, and additional ones can be selected by the service owner.Final ThoughtsWith Grab-Kit’s out-of-the-box support for microservice features such as authentication and authorisation, throttling, client-side load balancing, logging, metering, and so on, we’ve seen a huge increase in our productivity. Our friends in the GrabFood team now save up to 70% development time on creating a new service. We have also recorded improvements in stability and availability of our services.More and more teams have adopted Grab-Kit since the Grab Developer Experience team released it in November 2017. We see a marginal growth in adoption every month as illustrated in the following chart:At Grab, we are on a never-ending journey to deliver robust services that meet our consumers’ requirements. We continue to standardise and streamline our engineering best practices around distributed service design through Grab-Kit. The future is in Grab-Kit!Should you have any questions or require more details about Grab-Kit, please don’t hesitate to leave a comment.",
        "url": "/introducing-grab-kit"
      }
      ,
    
      "experiment-chat-booking-cancellations": {
        "title": "How Grab Experimented with Chat to Drive Down Booking Cancellations",
        "author": "ishita-parbatkaisen-wangjoseph-khanmike-tee",
        "tags": "[&quot;Chat&quot;, &quot;Booking&quot;, &quot;Experiment&quot;]",
        "category": "",
        "content": "Booking Cancellations are Frustrating and CostlyAt Grab, we consistently strive to build a platform that delivers excellent user experience to both our passengers and driver-partners. A major degradation to a seamless booking experience is the cancellation of that booking. A cancelled booking is an unpleasant experience and a costly event which only frustrates all parties involved.Cancellations at Grab    Figure 1 - High intent bookings not completed due to cancellationsPost allocation cancellations are particularly painful; these are cases where a strong intent to take a ride is expressed, the price is agreed upon but the trip eventually does not happen.  While we recognise that some cancellations are unavoidable, we wanted to know if there are instances where such cancellations, particularly the ones post allocation, can be prevented.Service Design as Part of Our Consumer-centric CultureService design is the process of generating a product, policy or any kind of enhancement that improves the user experience while hitting business metrics.Booking cancellations were a key problem of which the team was convinced that, with the right interventions in place, some cancellations could be prevented. To identify these scenarios the team conducted several rounds of user research to understand the root cause of cancellations and devise a valid and quality solution that would enhance the ride experience of both passengers and driver-partners.One interesting anecdote they heard from driver-partners was that when the driver-partner informed the passenger that he was on the way to the pick up point, the booking had a lower likelihood of being cancelled! A simple message from the driver-partner could help reduce perceived waiting time for Passengers and give them confidence in the service.This triggered us to dive deeper into understanding the correlation between a GrabChat message and cancellation rates.GrabChat is Indeed Correlated to Reduced Cancellation RatesGrabChat is our in-app messaging system that allows the matched passenger and driver-partner to chat with each other during the booking, saving on the costs of phone calls and/or SMSes while continuing to be on the app.We dug into our data to validate the feedback that Service Design team had received and discovered an interesting correlation; bookings which had a GrabChat conversation indeed had a lower likelihood of being cancelled. When the two parties established contact through chat, it transformed the service from a mere transaction to something more human. And this human touch to the service reduced perceived waiting time, making passengers and driver-partners more patient and accepting of any unavoidable delays that might arise.Building on this insight, we hypothesised that if we could encourage both parties to engage via a chat conversation upon getting a matched ride, we could potentially avoid a cancellation due to the prompted communication at no additional cost to the passenger, driver-partner or our platform. To validate this, we conducted a series of iterative experiments.Experimentation on Automated Messages and Delay TimeFirst, we tested with system-generated concise and informational messages sent at different delay-time intervals to test and validate if delays matter. We quickly observed that sending out a GrabChat automated-message sooner rather than later was more successful in preventing a booking cancellation. Once we identified the winning-variant on the delay-time to send a message, we explored a variety of message-verbiages, tones and styles across different cities to observe the varying effects.Test variations included:      open-ended questions        direct asks for specific details such as the pick-up location        first-person-speak (on the driver-partner’s behalf)        Inclusion of emojis      Figure 2 - Experiment Design for Varying delay time                                                                    Figure 3 - Examples of automated-messages in GrabChat    Figure 4 - We experimented on different localised messages based on local culturesSuccessful Experiments Yielded New LearningsAfter thoroughly testing in different cities and verticals, we observed that this small change to the user experience resulted in a reduction of booking cancellations by up to 2 percentage points. In the process, we learnt a lot more about our passengers! For example, it was amazing to observe how, in Kuala Lumpur, Passengers responded best to personalised questions in first-person-speak whereas a simple direct message worked better in Bangkok!Cancellation Rate during Experiment    Figure 5 - Cancellation rates consistently dropped in all the experimented citiesAnother key observation was that while the average number of messages per booking exchanged between a passenger and driver-partner was higher in the Control groups, cancellations still decreased in comparison to the Treatment groups. This showed us that quality, not quantity, of engagement through chat was the real metric mover. When we sent a clear directed question to the passengers, we were able to solicit a quick and meaningful response which made the conversation and the pick-up experience more efficient.ConclusionThrough these experiments and product enhancements, Grab is dedicated to making the experience on our platform more human-centric and context-specific. This is why we build hyper-local products like GrabChat which not only helps our Indonesian driver-partners save hundreds in call and SMS costs but also allows our chat-loving Filipino passengers to talk carefree!The process is scientific, iterative and often born out of the simplest of ideas - in this case, making people talk more to improve the booking experience.Learn more about GrabThis is one of a number of interesting showcases around Grab’s many services and features. We hope that this short story has piqued your interests in Grab - please feel free to contact us if you like to find out more or check out our Tech Blog here.",
        "url": "/experiment-chat-booking-cancellations"
      }
      ,
    
      "deep-dive-into-database-timeouts-in-rails": {
        "title": "Deep Dive into Database Timeouts in Rails",
        "author": "jia-hao-goh",
        "tags": "[&quot;Back End&quot;, &quot;Database&quot;, &quot;Distributed Systems&quot;, &quot;Ruby&quot;, &quot;Ruby on Rails&quot;]",
        "category": "",
        "content": "A couple of weeks ago, we had a production outage for one of our internal Ruby on Rails application servers. One of the databases that the application connects to had a failover event. It was expected that the server should continue functioning for endpoints which do not depend on this database, but it was observed that our server slowed down to a crawl, and was unable to function properly even after the failover completed, until we manually restarted the servers.BackgroundActiveRecord is the canonical ORM for Rails to access a database. Different requests are handled on different threads, so a connection pool is necessary to maintain a limited set of connections to the database and also to skip the additional latency of establishing a TCP connection.  A connection pool synchronises thread access to a limited number of database connections. The basic idea is that each thread checks out a database connection from the pool, uses that connection, and checks the connection back in.  It will also handle cases in which there are more threads than connections: if all connections have been checked out, and a thread tries to checkout a connection anyway, then ConnectionPool will wait until some other thread has checked in a connection.Source: The ActiveRecord::Connection Pool .Options for the Connection PoolIn Rails, database configurations are set in the config/database.yml file. These options are either native to the ActiveRecord::ConnectionPool module, or passed to the underlying adapter, depending on whether MySQL or PostgreSQL is used.ActiveRecord uses connection adapters to make database calls. For MySQL, it uses the mysql2 library, which depends on the libmysqlclient C library. The following options affect the behaviour of the library:            Option      Description      Source      Default                  pool      This specifies the maximum number of connections to the database that ActiveRecord will maintain per server.      Native to the ActiveRecord ConnectionPool      5 Source              checkout_timeout      When making a ActiveRecord call, ActiveRecord tries to checkout a database connection from the pool. If the pool is at maximum capacity, ActiveRecord will wait for this timeout to elapse before raising an ActiveRecord ConnectionTimeoutError exception.      Native to the ActiveRecord ConnectionPool      5 seconds Source.              connect_timeout      If there are no available connections to the database in the connection pool, a new connection will have to be established. connect_timeout, specifies the timeout to establish a new connection to the database before failing.      Native to the mysql2 library, passed to libmysqlclient as MYSQL_OPT_CONNECT_TIMEOUT      120 seconds Source.              read_timeout      Read timeout is used by the libmysqlclient library to identify whether the MySQL client is still alive and sending data. As we know that TCP sends data in chunks, the client waits for this timeout when reading from the socket, before deeming that there is an error and closing the connection.      Native to the mysql2 library, passed to libmysqlclient as MYSQL_OPT_READ_TIMEOUT      3 × 10 minutes Source      Connection Pooling AlgorithmThe following pseudocode is the algorithm for how ActiveRecord retrieves connections from the pool to perform database queries.if there are existing connections to the database available:    return one of the existing connectionsif the pool is at capacity:    wait on the queue, raise exception if `checkout_timeout` has elapsed    return one of the now available connections# pool is not at capacitytry to create a new connection, raise exception if `connect_timeout` has elapsed# connection to database establishedreturn new connectionThis is loosely translated from the source code.Replicating and DebuggingLet’s try to replicate the problem in a small Rails application. We will create a new Rails application, connect it to a database, run it in a Docker container and finally run some experiments to replicate the problem. In production, we use Puma to run our Rails server and connect to a few MySQL databases managed by Amazon Relational Database Service (RDS), so we will try to follow that on our local setup.Step 1: Create a New Rails ApplicationFirst, we will scaffold a fresh Rails application and connect it to two databases that we will call as db_main and db_other:# the flags removes unwanted boilerplate coderails new rails-mysql-timeouts --database=mysql --api -M -C -S -J -TFor simplicity, we will set the thread_count of our Puma server to 2, in config/puma.rb.threads_count = 2Using rails generate scaffold, we set up a Driver model to talk to our main database, and a Passenger model to talk to another database we want to test the failure on. This can be done by adding the following line to our Passengers model.class Passenger &lt; ApplicationRecord  # connect to #{Rails.env}_other database specified in the database.yml  establish_connection \"#{Rails.env}_other\".to_symendWe now have the following HTTP routes:# connects to db_mainGET /drivers/1# connects to db_otherGET /passengers/1Now, we will run our Rails server with the following environment variables:export RAILS_ENV=productionexport RAILS_LOG_TO_STDOUT=1rails serverBy using a docker container to run the Rails application, we can isolate the process namespace and focus directly on our application. We run ps and observe the two threads we have configured puma — puma 001 and puma 002.$ ps -T -e  PID  SPID TTY          TIME CMD    1     1 ?        00:00:00 sleep   30    30 pts/1    00:00:00 bash   63    63 pts/0    00:00:00 bash   97    97 pts/1    00:00:03 ruby   97    99 pts/1    00:00:00 ruby-timer-thr   97   105 pts/1    00:00:00 tmp_restart.rb*   97   106 pts/1    00:00:00 puma 001   97   107 pts/1    00:00:00 puma 002   97   108 pts/1    00:00:00 reactor.rb:152   97   109 pts/1    00:00:00 thread_pool.rb*   97   110 pts/1    00:00:00 thread_pool.rb*   97   111 pts/1    00:00:00 server.rb:327  112   112 pts/0    00:00:00 psNote that PID 1 is sleep because in docker-compose.yml, we specified that the container should start with cmd: sleep infinity so that we can attach to the running container at any time, not unlike a ssh to a machine.Step 2: Verify Our ApplicationWe make the following requests to ensure that our server is working correctly:$ curl localhost:3000/drivers[{\"id\":1,\"name\":\"test driver\",\"created_at\":\"2017-11-05T11:59:15.000Z\",\"updated_at\":\"2017-11-05T11:59:15.000Z\"}]$ curl localhost:3000/passengers[{\"id\":1,\"name\":\"test\",\"created_at\":\"2017-01-01T00:00:00.000Z\",\"updated_at\":\"2017-01-07T00:00:00.000Z\"}]Great! We are now able to see the records generated in the database by the above curl requests.The entire source code for this application can be found here.Step 3: Simulating the Production IssueWe will now try to simulate the production issue by using a proxy to monitor all our TCP connections from our Rails application to our database. Finally, we will run some experiments by sending requests that hit the backend database and analyse the behaviour of both connect_timeout and read_timeout settings.First, we use Toxiproxy as a transport layer proxy to db_other which allows us to manipulate the pipe between the client and the upstream database. The following command stops all data from getting the proxy, and closes the connection after timeout.toxiproxy-cli toxic add db_other_proxy --toxicName timeout --type timeout --attribute=timeout=100000Now we test if things are still working for endpoints that access the unaffected database.$ curl localhost:3000/drivers[{\"id\":1,\"name\":\"test driver\",\"created_at\":\"2017-11-05T11:59:15.000Z\",\"updated_at\":\"2017-11-05T11:59:15.000Z\"}]This is expected, as the db_main is still running. Let’s trigger a request to db_other.$ curl localhost:3000/passengersWe notice that the command does not exit and our terminal blocks while waiting for the command to terminate.Let’s trigger another call to db_main.$ curl localhost:3000/drivers[{\"id\":1,\"name\":\"test driver\",\"created_at\":\"2017-11-05T11:59:15.000Z\",\"updated_at\":\"2017-11-05T11:59:15.000Z\"}]Seems like it still works! Now let’s make another request to the db_other to lock up the two threads our server is configured to use.$ curl localhost:3000/passengersAnd make another request to db_main.$ curl localhost:3000/driversNotice that the call to /drivers is stuck and does not complete now. Because we have set the thread count to 2, and have two /passengers requests in flight, both threads are stuck waiting for the database and we do not have any more threads available to handle the new request, hence the stalled /drivers request.This is exactly what happened during our production outage, except on a much larger scale.ExperimentsLet’s perform some experiments to better understand how connect_timeout and read_timeout work. We will set the timeouts to the following:+ connect_timeout: 10+ read_timeout: 5In the following section, we will perform two experiments.Experiment 1: Application has No Existing Connections before Database Failure  Stop data transmission to db_other  Start Rails  GET /passengersWe first block data to db_other , so that on the first ActiveRecord call to retrieve some data from the database, there are no available connections in the connection pool and it needs to establish a fresh connection to the database when it receives the first GET /passengers request.Experiment 2: Application has Existing Connections before Database Failure  Start Rails  GET /passengers  Stop data transmission to db_other  GET /passengersWe’ve started Rails and made a call to GET /passengers. A connection to the database is established to retrieve the data, and checked back into the pool as an available connection after the request.Now, when the proxy stops sending data to db_other, ActiveRecord does not know that the database is unavailable and believes that the previously checked in connection is available for use with the second GET /passengers.We can use the ss command to observe the TCP connections. When Rails has just been started, there are no existing TCP connections .# shows TCP connections with the PID$ ss -tnpAfter a GET /passengers completes, a TCP connection can be seen in the ESTAB state.$ ss -tnpState  Recv-Q  Send-Q  Local Address:Port  Peer Address:PortESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:((\"ruby\",pid=11683,fd=13))Now, we stop the database and make another call to GET /passengers. We run ss when the request is in flight, and observe another TCP connection for the request to the port Rails listens on, port 3000.$ ss -tnpState  Recv-Q  Send-Q  Local Address:Port  Peer Address:PortESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:((\"ruby\",pid=11683,fd=13))ESTAB  0       0       172.18.0.4:3000     172.18.0.1:60878  users:((\"ruby\",pid=11683,fd=12))After read_timeout has elapsed, we see that a new connection is established to the database, and the first one has transitioned to a FIN-WAIT state. This new TCP connection is in the ESTAB state (line 3), because we have only stopped the database on the application layer, but the sockets to the container still accept the TCP handshake on the transport layer.$ ss -tnpState       Recv-Q  Send-Q  Local Address:Port  Peer Address:PortFIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306ESTAB       0       0       172.18.0.4:54308    172.18.0.3:3306   users:((\"ruby\",pid=11683,fd=13))ESTAB       0       0       172.18.0.4:3000     172.18.0.1:60878  users:((\"ruby\",pid=11683,fd=12))After connect_timeout has elapsed, the request terminates with a 500 error, and we observe that all the connections are in the FIN-WAIT state.$ ss -tnpState       Recv-Q  Send-Q  Local Address:Port  Peer Address:PortFIN-WAIT-2  0       0       172.18.0.4:54310    172.18.0.3:3306FIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306FIN-WAIT-2  0       0       172.18.0.4:54308    172.18.0.3:3306The experimental data can be found below.FindingsIt’s worth noting that when setting connect_timeout and read_timeout in the database.yml, there is a difference between empty values and the case where the key is missing entirely in the file. If the values are empty, scenario 1 will fail to terminate after 5 minutes, but if the keys are absent, scenario 1 will fail after 120 seconds, which is the default for connect_timeout.Experiment 1 FindingsThe request waits for connect_timeout to connect to the database, where the default value (when not specified) is indeed 120 seconds.As expected, connecting to the database with no existing connections is independent of the read_timeout.Experiment 2 FindingsThe request waits for read_timeout + connect_timeout before failing. This is because the connection pool waits for read_timeout on the existing connection before terminating it, and then waits for connect_timeout as it tries to establish a new connection to db_other.AnalysisWith these findings, we can try to understand how the lack of these timeouts affected our Rails server in production during and after the database failover.Establishing TermsOur application server constantly receives requests, out of which a certain percentage of requests will trigger the code to connect to the affected database, which we’ll call x-type requests. The other requests, that do not trigger a database connection, we’ll call x’-type requests.AnalysisWith the background knowledge gathered in our experiments, let’s try to analyse all the steps that happened during our production outage.  Rails started from a clean state, with no connections set up to the database initially  Rails handles the first few x request types, opens a connection to the database  Subsequent requests of x type can reuse the same connections from the connection pool  At a certain time, due to a hardware fault out of our control, a failover of the database is triggered  At the same time requests of x type comes in — and ActiveRecord reuses the same database connection from the pool, but there is no response. It then waits for read_timeout, causing the thread to be stuck waiting for the default timeout  Even though Rails can process requests of the x’ type normally, more and more requests of x type come in and cause more and more threads to be stuck waiting  Eventually, all the available threads to handle requests are stuck waiting on the TCP connection to the failed database, and Rails can no longer respond to new requests  After the default read_timeout has elapsed (3 × 10 minutes), some threads will be released to handle new requests  Subsequent requests of x type will cause a new connection to be opened to the database          If the failover is complete and the DNS records for the new instance has been updated, the new connections will be established      If the failover is not complete or the DNS records were not updated, the TCP connections will still try to connect to the old IP address with the failed database instance. The connections will wait for the connect_timeout (default 120 seconds) to elapse before failing        Finally, once all the threads are stuck, our Rails application stops responding to all requests until it was restarted manuallySolutionTo fix the problem, we have to prevent our database connections from being stuck in trying to read from an unresponsive socket, and trying to connect to a closed socket.This can be done by simply setting the read_timeout so that when the database fails, existing connections and threads will be released. The connect_timeout also has to be set so that when the existing connections are released, new connections and threads handling the requests will not be stuck trying to connect to the same unavailable database.We set the following values in our staging environment and manually triggered a database failover via the AWS console, and observed that requests of the x’ type are no longer stalled during the failover.The following is a snippet for our current database.yml configuration before the outage, and the changes to resolve the problem.# Config for the non-primary `db_other` databaseproduction_other:  adapter: mysql2  encoding: utf8  reconnect: false  database: …  pool: …  reaping_frequency: 120  username: …  password: …  host: …# New changes+ connect_timeout: 5+ read_timeout: 5ConclusionIn this post, we have gone over how timeouts are handled by the ActiveRecord ORM with our MySQL database and how failing to configure them brought down some of our production systems.Timeouts are very important configurations when setting up distributed systems and they are easily overlooked in the initial deployments of such applications.These principles are not just limited to Rails or MySQL, and the experiments and their findings can be easily extended to other technologies as well. Needless to say, these timeout settings are extremely important for the resiliency of applications in the world of micro services.References  ankane/the-ultimate-guide-to-ruby-timeouts  ankane/production_rails  MySQL Reference Manual  MySQL Source Code Mirror  TCP Connection StatesBig thanks to Joel Low for helping out with this investigation and clarifying ambiguities in Rails and MySQL, and my manager Amit Saini for his helpful review of this post!Source code for the test rails application can be found here.",
        "url": "/deep-dive-into-database-timeouts-in-rails"
      }
      ,
    
      "dealing-with-the-meltdown-patch-at-grab": {
        "title": "Dealing with the Meltdown Patch at Grab",
        "author": "althaf-hameez",
        "tags": "[&quot;AWS&quot;, &quot;Meltdown&quot;]",
        "category": "",
        "content": "Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments across a region of more than 620 million people.The meltdown attack reported recently had far reaching implications in terms of security as well as performance. This post is a quick rundown of what performance impacts we noted as well as how we went on to mitigate them.Most of our infrastructure runs on AWS. Initially, the only indicators we had were the slightly more than usual EC2 maintenance notices sent by AWS. However, as most of our EC2 fleet is stateless, we were able to simply terminate the required instances and spin up new ones. All the instances run on HVM across a variety of instance types running multiple Golang and Ruby applications and we didn’t notice any performance impact.The one place where we did notice a performance impact was on Elasticache. We use Elasticache, the managed service offered by AWS, to run hundreds of Redis nodes. These Redis instances are used by services in multiple ways and we run both the clustered version as well as the non-clustered version.On January 3rd, our automatic alerting triggered at around noon for high CPU utilisation on one of our critical Redis nodes. The CPU utilisation had jumped from around 36% to 76%. Now those numbers don’t look too bad until you realise that this is an m4.large instance which means it has 2 vCPUs. Combined with the fact that Redis is single-threaded, whenever we see CPU utilisation go past 50% it’s a cause for concern.The initial suspicions were a deployment / workload change causing the spike and our initial investigations focused on that. However, over the course of a few hours, multiple unrelated Redis nodes started displaying the exact same behaviour with sudden significant spikes in CPU utilisation.    Fig 1. Redis CPU UtilisationNotice the multiple sudden steep spikes in CPU utilisation and then plateauing as time goes on.Some of the Redis with CPU utilisation spikes were the replica nodes in the multi-az setup. As most services were having these replicas purely for HA and not actively using them, having the CPU utilisation on it spike without the master node spiking indicated that it was no longer a workload issue. At this point, we escalated to AWS with the data in hand.Later that night, we then attempted to perform Multi-AZ Failovers for certain nodes  where the master had exhibited a spike but the replica hadn’t. Our suspicions at this time was that there was some underlying hardware issue and failing over to a node that wasn’t affected would help us. It was successful as once the replica became the master the CPU utilisation went down to the original levels. We performed this operation for multiple nodes and then called it a night confident we’ve mitigated the problem.Alas, our success was short-lived as the example graph below shows.    Fig 2. CPU Utilisation of an affected Redis instanceInitially, prd-sextant-001 was the master and 002 was the replica. At noon on the 3rd, you see the CPU spike on master, the corresponding drop on the replica is still unexplained (The hypothesis is that a percentage of updates failed on the master node resulting in a smaller set of changes to be replicated). Early in the morning on the 4th is when we performed the failover, you see 002 now having utilisation equal to 001. On the evening of the 4th, however, you see that 002 has its CPU utilisation significantly spiked up.With information released from AWS that the EC2 maintenance was related to meltdown and benchmarks being released about the performance impact of the patches, the two were put together as the possible explanation of what we were seeing. AWS could be performing rolling patches to the Elasticache nodes. As a node gets patched the CPU spikes and our failovers were only successful in reducing the utilisation because we were failing over to a node that wasn’t yet patched. However, once that node got patched the CPU would spike again.Realising that this was now going to be the expected performance the teams quickly sprung into action on how to best spread the load.Clustered RedisWe would add additional shards so that the load gets spread evenly. This was complicated by the fact that we were running on the engine version 3.2.4 which didn’t support live re-sharding so we had to spin up a lot of new clusters with the additional shards, ensure that the cache gets warmed up before switching completely over and decommissioning the old one.    Fig 3. Old API Redis Cluster with nodes going up to 50% peak CPU    Fig 4. New API Redis Cluster with additional shards now hovering at about 30% peak CPU    Fig 5. Old Hot Data Redis Cluster with CPU peaking at around 48%    Fig 6. New Hot Data Redis Cluster with CPU peaking at around 24%Non-Clustered Redis      Some of our systems were already designed to use multiple Redis nodes. So provisioning additional nodes and updating the configs to start using these nodes was the easiest solution.        For certain Redis nodes that were able to utilise Redis Cluster with minimal code change, we switched them to use Redis Cluster.        The final few Redis nodes, the service teams made significant code changes so that they could shard the data onto multiple nodes.      Fig 7. Redis where code changes were made to shard the data across multiple nodes to reduce the overall load on a single critical nodeAll of these mitigations were done over a period of 24 hours to ensure that we go past our Friday peak (our highest traffic point during the week) without any consumer facing impact.ConclusionThis post was meant to give a quick glimpse of the impact that Meltdown has had at Grab as well as provide some real data on the performance impact of the patches.The design of our internal systems in their usage of Redis to quickly be able to horizontally scale-out was key in ensuring that there was minimal impact, if any to our consumers.We still have further investigation to conduct to truly understand why only certain Redis workloads were affected while others weren’t. We are planning to dive deeper into this and that may be the subject of a future blog post.",
        "url": "/dealing-with-the-meltdown-patch-at-grab"
      }
      ,
    
      "grabshare-at-the-intelligent-transportation-engineering-conference": {
        "title": "GrabShare at the Intelligent Transportation Engineering Conference",
        "author": "dominic-widdows",
        "tags": "[&quot;Data Science&quot;, &quot;GrabShare&quot;]",
        "category": "",
        "content": "We’re excited to share the publication of our paper GrabShare: The Construction of a Realtime Ridesharing Service, which was Grab’s contribution to the Intelligent Transportation Engineering Conference in Singapore last month.The ICITE conference was a terrific event for getting to know researchers and experts in transportation, with presentations ranging from improving battery life and security in autonomous vehicles, to predicting bus arrival times and traffic congestion in cities from Penang to Beijing. It’s inspiring to meet with such a wide range of scientists, committed in many different ways to improving the safety, quality, and sustainability of transportation throughout the world.GrabShare is Grab’s service that offers passengers going the same way a more cost effective fare for sharing the ride, and is one of the products for which Grab recently won a Digital Disruptor of the Year award. The paper itself gives quite a broad overview of how the GrabShare system works.GrabShare has to connect drivers and passengers who want to know if they can have a ride almost immediately. Passengers may also be using smartphones with spotty connections that may appear and disappear from the network at any time. These real-time demands make the system design somewhat different from that of a traditional transportation provider such as a railway network or airline. There’s an algorithm for matching rides together, which has to give very quick answers, deal with volatile supply and demand, and cope with the fact that any message to a driver or passenger might not get through. Good luck with that!To build a successful product, we need a lot more than this. Pricing needs to work well for both passengers and drivers. Traffic patterns need to be understood to give reliable travel time estimates - and the system uses hundreds of these estimates, because for every match that’s made, the scheduling system considers and rejects many others that turn out to be less promising. And just to make this part more challenging, we’re dealing with cities like Manila and Jakarta that have some of the world’s most notorious traffic jams.None of this could happen without the teams on the ground. A large part of building GrabShare has been about listening to feedback from these experts and turning it into code. When we hear a passenger or driver complain that a match wasn’t appropriate, our country teams analyse the problem, and often the engineering team gets involved directly in updating the online systems to make sure similar problems don’t happen again.We’ve come this far for GrabShare. It’s been a rewarding journey, and we will continue to iterate and innovate. According to our records and estimates, in the past month alone GrabShare saved over 4.5 million km in driving distance by using one car instead of two for thousands of shared journeys. In addition, the service has reduced congestion and pollution including CO2 and other emissions – by about as much as 1,000 flights from Singapore to Beijing, or about as much CO2 as what 5 square kilometres of forest absorbs in a month. (As far as we can tell from researching on the web – we’re tree enthusiasts, not tree scientists!) And the travel cost savings have been attracting new passengers to the platform – within just two weeks in August, more than 100,000 new users took GrabShare rides.It’s a good time for us to thank the organisers of the ICITE conference, and all the other contributors to the event. We hope some of our readers enjoy finding out more about GrabShare, and getting a more thorough understanding of how it’s built. And most importantly, thanks to our drivers, passengers, and dedicated teams across Southeast Asia who’ve  made this happen. Of all the research I’ve been involved in over the years, there’s never been anything that affected so many people or where the acknowledgements section was so heartfelt.",
        "url": "/grabshare-at-the-intelligent-transportation-engineering-conference"
      }
      ,
    
      "grabbing-growth-a-growth-hacking-story": {
        "title": "Grabbing Growth: A Growth Hacking Story",
        "author": "gaurav-sachdevahuan-yangjiaying-lim",
        "tags": "[&quot;Growth Hacking&quot;]",
        "category": "",
        "content": "Disrupt or be disrupted - that was exactly the spirit in which the Growth Hacking team was created this year (also a Grab principle that is recognised on the 2017 CNBC Disruptor 50 list). This was a deliberate decision to nurture our scrappy DNA, and ensure that we had a dedicated space to experiment and enable intelligent risk-taking.Focusing on initiatives with the highest impact to unlock exponential scaling, our lean and nimble Growth Hacking team tackles challenges considered either too niched or high-risk by business teams. We do this by delivering growth loops to Grab, with the ultimate aim to outserve our 68 million consumers across the region.What is a growth loop?A typical growth loop follows this path:      1. Actively acquire the right users through needs-based /observable traits segmentation.    2. Activate these users to change their behaviour through incentives or deterrents.    3. Engage these same users through an ongoing consumer lifecycle management programme.     4. Driving virality across the system to exponentially increase desired impact.             In order to create the most scalable and impactful growth loops, we chose to house our Growth Hacking team within our Technology organisation (instead of its traditional home: Marketing). This enables us to leverage our engineering expertise to increase the speed and scale of our experiments , A/B test frequently and deploy across different markets simultaneously.What results has the Growth team delivered since its inception?Since its formation earlier this year, we have completed several experiments across the Grab platform, testing the effects of gamification, multi-level marketing and local culture on user behaviour.We measure our success against a single metric, the Growth Factor: defined as increase in rides / increase in costs.  A growth factor greater than one indicates that we’re bringing a cost efficient increase in rides / market share.Being a data-driven business, we’re focused on how we can best define successful experiments. Having a single source of truth to prioritise and evaluate our experiments ensures that we can move fast and consistently.A successful Growth projects is our Spin-to-Win experiment. We started formulating this experiment by asking, “How can we better engage with drivers?”We knew that gamification is a proven growth strategy and wanted to leverage this concept to drive viral engagement on our platform. In particular, we were inspired by an experiment conducted by psychologist and behaviourist B.F Skinner in the 1960s. Skinner put pigeons in a box that issued a pellet of food when they pushed a lever. However, he altered the box, such that pellets were delivered randomly. This incentivised the pigeons to press the lever more often. This experiment created the “variable ratio enforcement” proof:With too little reward, people (or pigeons!) will disengage.With too much rewards, people (and pigeons!) will also disengage.Based on this theory, we wanted to find the right balance in delivering an incentive experience that was delightful yet unobtrusive. The result was the Spin-to-Win game. Because of its popularity, such a game was easily understood, and probabilistic enough to drive engagement.We developed an A/B test within three weeks and offered both monetary and merchandise rewards to drivers who completed a pre-determined number of rides per day.                            Spin-to-Win with Merchandise rewards                                          Spin-to-Win with Monetary rewards                                          Design Variations on Monetary version                                          Design Variations - Hyperlocal for Jakarta                                          Jackpot Prize Design 1                                          Jackpot Prize Design 2              To increase engagement, we sent reminders to drivers regularly. We also celebrated every win of the driver to encourage continual participation.  As we continue to experiment across the region, we take into account additional lenses, including driver acceptance, cancellation and driver ratings to further refine our results. And hopefully, this is something all of our drivers can enjoy very soon!",
        "url": "/grabbing-growth-a-growth-hacking-story"
      }
      ,
    
      "the-data-and-science-behind-grabshare-part-i": {
        "title": "The Data and Science Behind GrabShare Part I: Verifying Potential and Developing the Algorithm",
        "author": "tang-muchen",
        "tags": "[&quot;Data Science&quot;, &quot;GrabShare&quot;]",
        "category": "",
        "content": "Launching GrabShare was no easy feat. After reviewing the academic literature, we decided to take a different approach and build a new matching algorithm from the ground up. Not only did this really test our knowledge of fundamental data science principles, but it challenged our team to work together to develop something we had never seen before!Because we had so much fun learning and developing GrabShare, we wanted to write a two part blog post to share with you what we did and how we did it. After reading this, we hope that you might be more prepared to build your very own optimised [practical, effective and efficient] matching algorithm.We hope you enjoy the ride!I. A Little HistoryBy matching different travellers with similar itineraries in both time and their geographic locations, ride-sharing can improve driver utilisation and reduce traffic congestion. This concept of pooling (or called ride-sharing), has been a popular concept for decades due to its significant societal and environmental benefits. Tremendous interest in the real-time or dynamic pooling system has grown in recent years, either from a pooling matching algorithm (e.g., [2], [3]) or a system efficiency perspective [4]. We refer interested readers to [5]–[8] as a comprehensive overview on how optimisation and operations research models in academic literature can support the development of real-time pooling systems and innovative thinking on possible future ride-sharing modes.Leveraging on an internet-based platform that integrates passengers’ smart-phone data in real-time, we are able to provide a ride-sharing service that allows passengers to spend less while enabling drivers to earn more. Companies such as Didi, Grab, Lyft and Uber have managed to transform the concept of a real-time pooling service from imagination into reality. Even though the problem of how to match drivers and riders in real-time has been extensively studied by various optimisation technologies in literature (e.g., Avego’s ride-sharing system [5] and Lyft match making [9]), there has been a renewed interest in the problem and how we can solve it in practice.Let us turn the clock back to late 2015. This was when Grab’s Data Science (Optimisation) team was born. The team decided to eschew the literature and current state of the art, and challenged ourselves to design the GrabShare matching algorithm from the ground up, from basic principles. Indeed, its main task was to make ride matching decisions (which is combinatorial) in order to maximise the overall system efficiency, while satisfying specific constraints to guarantee good user experience (such as detour, overlap, trip angle, and efficiency). A general optimisation problem comprises of three main parts: Objective function, Constraints, and Decision Space. The constrained optimisation problem takes the usual form:Here X denotes a set of decision variables that correspond to real-world decisions we can adjust or control. The objective function f(X) is either a cost function that we want to minimise, or a value function that we want to maximise. The constraints are mathematical expressions of physical restrictions to decision variables on the possible solutions, which could have either inequality form: g(X) or equality form: h(X) or both.In this article, we discuss how the GrabShare matching algorithm is tackled as an optimisation problem and how its various formulations can have a different impact to Grab, passengers, and drivers. Differing from previous studies in literature, which mainly focus on improving overall system efficiency using conventional operations research methods, we approached the problem from a more data-driven perspective. Our key focus was on extracting critical insights from data to improve the GrabShare user experience, from the point of design and development of the matching algorithm and throughout subsequent continual efforts of product improvement.II. From GrabCar to GrabShareFrom 2012 onwards, Grab has had a mature product named “GrabCar” that serves millions of individual traveling requests by an integrated dispatching system. The drivers’ locations and other states are maintained in the system such that we can simultaneously find drivers and make assignments for thousands of traveling requests. With a GPS-enabled mobile device, the users (known as passengers) can use Grab’s passenger app to place transportation requests from specified origin to destination. In this article we use the term “booking” to denote a confirmed transportation request placed by a passenger, which contains explicit pickup and drop-off information. At the same time, drivers who have registered with their own or rented vehicles can login to Grab’s system through a driver app to indicate their readiness to take nearby passengers. The GrabCar service is similar to a traditional taxi service in that a completed GrabCar ride consists of three steps:      A passenger makes a booking;        A GrabCar driver is assigned to the booking;        The assigned driver picks up the passenger and ferries him/her to the destination and the ride is completed.  It is common for people to arrange for manual ride-sharing with our friends traveling in the same direction to save on travel cost as well as to socialize and connect during the trip. By making use of real-time integrated ride information in the Grab system, we aimed to automatically match strangers traveling in similar directions and assign the same vehicle to both their journeys, allowing them to effectively car-pool. Before promoting the concept of GrabShare however, we had to verify its potential from the existing GrabCar bookings. For example, during morning peak hours we mappped every single booking into a four-dimensional vector with the latitudes and longitudes of pickup and drop-off locations. In addition, the latitudes and longitudes were transformed into a Universal Transverse Mercator (UTM) format to map the earth’s surface to an 2-dimensional Cartesian Coordinate System for distance calculation. After applying a DBSCAN cluster method [10] with parameter “eps=300”, which means that only bookings with distance of less than 300 meters can be considered as neighbourhoods, we observed eight clear clusters of booking with close pickup and drop-off locations in Figure 1.    Figure 1. Morning booking clusters with similar itinerariesThe booking requests within each cluster can be allocated and fulfilled with less vehicles, through pooling. Even though not all of them may be willing to share vehicles with others, at least those with unallocated bookings (around 8%) may benefit. After repeating this analysis for different time periods, we observed that a certain percentage of the bookings could be covered with good performing clusters as seen in Table I. We observed that the coverage rate for different time periods fluctuates from 35% to 45% for most part of the day (coverage during mid-night and early morning hours is much smaller as the amount of bookings is much smaller). Because bookings in the same cluster are “near perfect matches” with very close pickup and drop-off location, the potential for GrabShare was found to be quite promising because we could expect even more opportunities for matching in the middle of a trip.            Hours      8-10      10-13      14-16      16-18      18-22      Others                  Coverage      46%      39%      35%      38%      43%      22%        Table 1. Cluster coverage of different time periodsThe assignment flow of typical GC bookings is stated in Algorithm I. For every newly arrived booking, we search for nearby drivers and check for their availability condition. If no driver is available, we recycle it to the next round of assignment. Otherwise we select the most suitable driver for them. Leveraging the current system structure, we planned to extend the GrabCar service to GrabShare by maintaining more detailed bookings and driver state information along with an additional check on seat reservation.    Algorithm I. GrabCar booking assignment flowSpecifically, Algorithm II gives the assignment flow of Grab- Share bookings. We can see that its overall structure is the same with GrabCar except for two differences. Firstly, the candidate driver set is different. For every new GrabShare booking, we search for in-transit GrabShare drivers who are currently serving at least one GrabShare booking. Therefore, we need to check seat availability condition to ensure that the vehicle has enough remaining seats to serve the new GrabShare booking. Mathematically, the following seat reservation constraint needs to be satisfied for a successful assignment between booking bki and driver drj:where s(drj) denotes the total capacity of the vehicle drj, op(drj) is one of the maintained variable that denotes the current occupied capacity of the vehicle drj and rp (bki) is the required capacity for booking bki. To make it consistent, we also need to update the vehicle occupied capacity variable op (drj) by adding the booking required capacity rp (bki) after every successful assignment or removing it if cancellation occurs.    Algorithm II. GrabShare booking assignment flow    Figure 2. GrabShare match case in SingaporeSecondly, GrabShare’s user experience is different from GrabCar due to the sharing concept. Here we defined some measures to evaluate the GrabShare matching, taking into consideration the trip angle, eta (short for Expected Time of Arrival), detour and efficiency. These measures are used to exclude unacceptable matches and to quantify how good the match is. For example, given a matching route scenario of two bookings (n = 1) as shown in Figure 2. At the first step the driver receives the first GrabShare booking from point A to D (25 minutes direct trip time). After the driver picks up the first passenger and reaches location B on his way to D, he/she is assigned to pickup the second booking from C to E (21 minutes direct trip time). A GrabShare match happens and the final route sequence is generated as A→B→C→D→E. With pooling, it takes 29.5 minutes for the first passenger and 27 minutes for the second passenger to reach their destinations, respectively. Overall it is a good match as the passengers are only delayed a little bit by pooling with a promising driver utilisation rate. In this case the driver only needs to drive 23.72km in total to serve two bookings, instead of a total of 39.13km if they were served separately. Not only does this allow passengers to be allocated rides, but drivers save considerable time and money through this efficiency, while increasing their earning power simultaneously.This is ultimately deemed a good match, but the details on how we quantify this and its corresponding optimisation model are explained in Part II.References[1]  Grab, “Grab extends GrabShare regionally with Malaysia’s first on-demand carpooling service,” 2017. [Online]. Available: https://www.grab.com/my/press/business/grabsharemalaysia/[2]  J. Alonso-Mora, S. Samaranayake, A. Wallar, E. Frazzoli, and D. Rus, “On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment,” Proceedings of the National Academy of Sciences, vol. 114, no. 3, pp. 462–467, Mar 2017.[3]  A. Conner-Simons, “Study: carpooling apps could reduce taxi traffic 75 percent,” 2016. [Online]. Available: http://www.csail.mit.edu/ridesharing_reduces_traffic_300_percent[4]  D. Dimitrijevic, N. Nedic, and V. Dimitrieski, “Real-time carpooling and ride-sharing: Position paper on design concepts, distribution and cloud computing strategies,” in Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on. IEEE, 2013, pp. 781–786.[5]  N. Agatz, A. Erera, M. Savelsbergh, and X. Wang, “Optimization for dynamic ride-sharing: A review,” European Journal of Operational Research, vol. 223, no. 2, pp. 295–303, 2012.[6]  A. Amey, J. Attanucci, and R. Mishalani, “Real-time ridesharing: opportunities and challenges in using mobile phone technology to improve rideshare services,” Transportation Research Record: Journal of the Transportation Research Board, no. 2217, pp. 103–110, 2011.[7]  N. D. Chan and S. A. Shaheen, “Ridesharing in north america: Past, present, and future,” Transport Reviews, vol. 32, no. 1, pp. 93–112, 2012.[8]  M. Furuhata, M. Dessouky, F. Ordonez, M.-E. Brunet, X. Wang, and S. Koenig, “Ridesharing: The state-of-the-art and future directions,” Transportation Research Part B: Methodological, vol. 57, pp. 28–46, 2013.[9]  Lyft, “Matchmaking in lyft line—part 1,” 2016. [Online]. Available: https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4[10]  M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A density-based algorithm for discovering clusters in large spatial databases with noise.” in Kdd, vol. 96, no. 34, 1996, pp. 226–231.",
        "url": "/the-data-and-science-behind-grabshare-part-i"
      }
      ,
    
      "the-art-of-hiring-good-engineers": {
        "title": "The Art of Hiring Good Engineers",
        "author": "rachel-lee",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Hiring the first five good engineers in your team requires a different approach to hiring the first twenty good engineers. The approach to designing this process will be even more different, when you want to hire to scale up to a 100 engineers… or even to 300.Should you start from the top hires?Or should you start from hiring a few really good engineers and then help them to scale?This post covers some concepts on designing efficient, useful processes to help Tech Leads who are actively involved in building fast-growing Engineering teams from scratch. This post may also be useful for people working in super niche fields who want to build and scale Engineering teams for continued growth.Start SmallThere is never only one way to start building a team of good engineers. Many of the mobile apps we use daily today, are known for having small teams of good engineers who built great products:      WhatsApp finished building their initial product with 32 engineers        Skype’s product was built by 5 engineers        Instagram’s first team only had 13 employees in total    Your small team of good engineers should be ready to fight every day for the right to be a player in the market where you are.They will need to be more eager, more hungry, and more consumer minded than others. They will need to always fight for the right to be a player in that market. To fight off the big guys, sometimes the good engineer needs to be battle-worn, battle-tried.The battle-tried good engineer has faced many of the situations you too will face:      How do you handle outages that cause the entire product to be down?        How do you release a lot of features simultaneously without disrupting users?  Follow Existing WisdomMany of the top technology companies today have had great success in hiring good engineers. They do put in plenty of effort and research to design processes that work for bringing in hundreds of good engineers each year.My recommendation to you would be to follow the wisdom of these best practices that have been adopted by the top companies.  Many of these processes are well documented and shared about on sites like Quora and Glassdoor. However, be mindful that there are weak points in these processes due to how top companies deal with volume.  (Too many interviews + tests and coding assignment rounds) * too little human touch = bad design for hiring a team of good engineersShatter IllusionsThe illusion that there is always more good engineers out there.When you have found a truly good engineer, he/she is simply irreplaceable. I believe this good engineer can help drive successful processes to help you hire 100 good engineers as I’ve personally witnessed this effort happen - and as they also carry great value in product knowledge, systems design, and design thinking, they will be able to exert an influence over their colleagues which, when lost, has an immeasurable impact on the internal culture.The illusion that small teams can afford to not think about diversity.Don’t forget about diversity, equality and inclusion. Even in small teams, diversity - this means hiring a good engineer coming from a completely different background from you and the other members - bring distinct advantages.Remember to give everyone an equal chance to join your team by eliminating as many obvious biases as possible - and to understand inclusion, simply: when you invite someone ‘different’ to the party, make them feel like they are not only invited, but make them feel like they are really one of you! There’s something to be said for teams that champion all three.Do a fast game, but not too fast though!If you bring in good engineers too fast and furious without a proper approach, some parts of the moving equation will prove to be detrimental to their success - if you are not setting them up for success, the good engineer will find it hard to match the fit prerogatives, and fail, fast. Being fast at hiring good engineers should not be the only success metric you hold yourself to.The Recipe so farStart Small + Follow Existing Wisdom + Shatter Illusions + … how about designing a process that works ?Now, if you are ready to design your process, consider these 4 steps for designing a robust process.Step 1: Make a REALLY Good ListIf you do decide to only hire the top 2 - 5% good engineers with a relevant tech stack / industry expertise, understand that you are making your process 100 times harder. Sometimes this means that you have to process 500 profiles in order to hire 5 - 10 good engineers. This will take months at least, unless you have super resources.Add to your list those good engineers who are open-source committers, top engineers from the leading technology companies who are in your location. Even if they do not join your team now, they will be able to recommend others - good engineers attract good engineers and these activities of yours will be discussed in engineering communities.Once you have already recognised all profiles from LinkedIn, GitHub is the next battleground to look up.Step 2: Determine Technical FitWhile I don’t recommend the technical phone screen for every single engineering role (as cybersecurity and niche data engineering processes can be designed differently, frontend, full-stack and product or mobile engineering hiring can benefit from a process to review their portfolio and design thinking), most top companies assign tests as the pre-screening round that can be a timed coding test with relevancy, a technical phone screen, a recruiter screen for the role, scope, and culture fit or, a take-home assignment involving designing elements crucial for success in the role.Most top companies design this first part to take 1-2 hours of the candidates’ time initially, the phone screen can range from 20 minutes to 1.5 hours.Step 3: Determine Culture and Team Fit Based off Group InterviewsThe outcome you should look for is ideally for every one of your warriors to feel comfortable with fighting alongside this battle-ready good engineer. Misgivings and possible gaps can always be improved on while working on the product together.Also, check if the good engineer can work well with others in Design, Product and Data functions as well as communicate reasonably well to someone outside your engineering organisation. Any HR or Recruitment professional can help check if the engineer possesses a few of these soft skills you need, such as communicating to stakeholders, business acumen or excitement in helping solve consumer issues. Don’t skip this step!Step 4: Optimise Your Process to ‘Always Be Closing’Most talent acquisition professionals abide by the ‘always be closing’ mantra - they are selective in the people they talk to and eventually feel proud to represent to top companies, they also choose the roles they want to focus on, usually these are the easier roles to fulfil, according to their expertise.It could be a good idea to identify the members in your team who are really ‘strict’ interviewers, we call them ‘bar-raisers’ and only send the super strong profiles across to them. The normal profiles can be sent to other good engineers for interviewing, so as not to burn out the strict bar-raiser.By following this method we can effectively predict the pipeline of candidates and expedite those who have passed well in the bar-raiser round. Always know your reasons for declining a candidate and always be involved in the interview rounds no matter how big your team gets. Your efforts will definitely be discussed by others, so it’s also a good idea to frequently check in with peers, board members and technical advisors, especially if you find a senior candidate who had overlapping tenures with them. Your board and peers could provide useful information about this candidate or other interesting details that enable you to enhance your decision making process and improve your future hiring process as well.If you did not manage to optimise your approach, a possible outcome is that you will be wasting valuable time of your existing Engineers. One hour spent in a bad interview is one hour less that could be spent on coding for your product.I think it is a good idea to personally spend more time with all good engineer candidates on an informal basis, if you feel that the 1 hour or 1.5 hour session did not suffice to determine if he/she is a suitable hire.Many good engineers will seem to possess all the right credentials but in the mid to long-term, there will always be some who are much better for your team.Final ThoughtsAim for a flat structure, even if you get big one day. Companies like Facebook and Grab still try to keep their Engineering structure as flat as possible.Company after company who rose to greatness often struggle with scale.The first point to the last point of interaction is always important for the candidate experience. If you want to build a strong team from the first hire to the last, your branding is important.Hire the really talented good engineer, and the rest will follow.Ensure your small team still stands for diversity, equality and inclusionAlways be closing but don’t forget to have fun: Your current challenge will always be to hire the people who really want to see you succeed!So You Need to Hire a Good EngineerThank you for reading and please share some ideas for future inspiration, I love challenges!",
        "url": "/the-art-of-hiring-good-engineers"
      }
      ,
    
      "migrating-existing-datastores": {
        "title": "Migrating Existing Datastores",
        "author": "nishant-gupta",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "At Grab we take pride in creating solutions that impact millions of people in Southeast Asia and as they say, with great power comes great responsibility. As an app with 55 million downloads and 1.2 million drivers, it’s our responsibility to keep our systems up-and-running. Any downtime causes drivers to miss earning and passengers to miss their appointments.It all started when in early 2017, Grab Identity team realised that given the rate at which our user base was growing, we wouldn’t be able to sustain the load with our existing single Redis node architecture. We used Redis as a cache to store authentication tokens required for secure mobile client to server communication. These tokens are permanently backed up in an underlying MySQL store. The existing Redis instance was filling at crazy speeds and we were growing at a rate at which we had a maximum of 2 months to react before we would start to ‘choke’ i.e. running out of memory to store more data or run operations on the above mentioned Redis node.It was the moment of truth for us, and forced us to re-evaluate the design and revisit architectural decisions. We had to move away from our existing Redis node and do it fast. We had several options:  Move to a larger Redis instance: While definitely an option, we now had the opportunity to solve for the existing flaw of a single point of failure in our design. In spite of having replication groups set up, in cases of failure it can take a few minutes before a slave gets promoted as master and until that happens, service write operations would remain impacted. Our priority was moving in the direction of higher availability.  Move away from Redis: Well, that was one of the options, but it was not the time to re-evaluate other caching solutions from scratch.  Setup a custom Redis cluster, backed by Redis Replication Groups: This option did address availability concerns, but raised additional concerns:          We had to rely on client-side sharding, so clients would be slightly more complex.      In case of having to add a new shard, the migration was going to be very tricky. Remember, it was a custom cluster so there would be no self-balancing offered. We might end up moving selected user information from existing nodes to new nodes, pretty much cherry picking via some custom logic for this one time migration.        Use AWS ElastiCache cluster:          Server-side data sharding was available, meaning AWS would take care of the sharding strategy for us.      Adding a new shard was not possible, oops!… BUT, anyhow a fresh setup might turn out to be more clean and deterministic than running custom rebalancing implementation as in the above option.      From all the mentioned options, it was clear to us that achieving a completely horizontally scalable model where data-sources could be increased on demand with ease, was not possible with the Redis-AWS combination (unless we ended up with a self-hosted Redis on EC2). This is when we started questioning some assumptions:Did we need horizontal scalability for all the operations?And we had the answer to this. In a typical authentication system, the scale of writes is significantly lower compared to that of reads. A token that was provisioned in 1 request, would end up being used to authenticate another N requests and our graphs validated this:Write loadVSRead loadIt was a clear difference of ~200 times in peak load. So, what if we can achieve horizontal scalability in read cases, and be a bit futuristic in provisioning shards to cover write load?We had our answer and our winner in the process. AWS ElastiCache did offer support for adding new nodes on demand. These new nodes would act as the read-replica of the master node in the same shard, meaning we can potentially provide horizontal scalability for read operations. To decide on the number of shards, we projected our rate of growth based on what we saw in the previous 6 months, factored in future plans with some additional buffer and decided to go with 3 shards, with 2 replicas for each master; 9 nodes in total.Now that we had finalised the direction, we had to move and define milestones for ourselves. We decided a few targets for this move:  No downtime: This was one of the audacious targets that we set for ourselves. We wanted to avoid even a single second of downtime of our systems and that was no easy thing. Why so? For some perspective: this service was handling a peak load of 20k per sec, which meant a 10 second downtime would impact ~200k requests, roughly translating to 50k users. Importantly, unlike other businesses, it was not an option to carry out maintenance tasks such as these at low load times. This policy stems from the belief that at odd hours our availability becomes even more critical for the consumers. They are more dependent on our services and rely on us to help them provide safe transport, when other means are probably not available. Imagine someone counting on us for his/her 4:00AM flight.  Zero collateral damage during this move, meaning that no existing tokens should be invalidated or missed in the new source. This implied that during the move, data in the new datasource had to be in perfect sync with the old datasource.  No security loopholes, we wanted to ensure that all the invalidated tokens remain invalid and not leave even a tiny window to reuse those.In a nutshell, we planned to switch the datasource for the 20k QPS system, without any user experience impact, while in a live running mode.We made our combat plan as comprehensive as possible; outlining each step with maximum precision and caution. Our migration plan comprised of the following six steps.Step 1: One time data migration from old Redis Node to Redis ClusterThis was relatively simple, since the new cluster was not handling live traffic. We just had to make sure that we did not end up impacting performance of the existing node during the migration. SCAN, DUMP and RESTORE did the trick for us, without any clear impact on performance.Step 2: Application changes to write to new Redis Cluster in asynchronous mode in request path (alongside the old datastore). Shadow writing to the new cluster did not add latency to existing requests and allowed us to validate that all the service to cluster interactions were working as expected. Even in case of failure, the requests will not be impacted.Step 3: Application to start writing to new Redis Cluster in synchronous mode in request path. Once step 2 was validated, it was time to make the next move. Any failure in cluster calls, would result in the failure of the API call in this step.Step 4: Application to start reading from Redis Cluster in asynchronous mode and validate values against old Redis Node. This was a validation step to ensure the data being written in the new data source was in sync with the old source. Respective validation results were being tracked as metrics. This validation was being carried out as part of existing read APIs.Step 5: Move all the Application reads from old Redis Node to new Redis Cluster. This was THE move, where we stopped reading from old data-source. By this point all the APIs were already backed by the redis-cluster.Step 6: Stop writing to the old Redis Node. This was just a cleanup step, to remove any interactions with the old source.Each step was controlled by configuration flags. In case of unforeseen events or drastic situation, we had levers to move the system back to its original state. Additionally, at each step we added extensive metrics to make sure that we had solid data-points backing our move to confidently move to the next step. We moved smoothly from one step to another and there came a time when we moved to Step 6 and there, we had defused the bomb, timely.What did we learn from this — in the software world, things are not always tough, problems may not require rocket-science tech all the time. Sometimes, it’s more about well thought-through planning, meticulous execution, coordinated steps, measured and data driven decision making, that’s all you need to have a winning strategy.",
        "url": "/migrating-existing-datastores"
      }
      ,
    
      "so-you-need-to-hire-good-engineers": {
        "title": "So You Need to Hire Good Engineers",
        "author": "rachel-lee",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "If you are in a fast growing tech startup, you’re probably actively interviewing and hiring engineers to scale teams.My question to you is, what hiring strategy are you using when interviewing engineering warriors?This post explores some intriguing concepts that are formed behind hiring processes for engineers and how these concepts shape processes to increase your probability of hiring that one good engineer.I have spoken with more than a hundred engineering leaders in tech companies about how they hire. I’ve asked them to share their thoughts with me on the most important factors that they look for when hiring a good engineer.This is what I found.1. Technical Fit vs Cultural Fit  “An engineer’s technical fit can be around 80% for our ‘on the job’ requirement. It can be difficult to find a 100% fit, and for those engineers who have some gaps, it’s personally motivating for me to have this opportunity to help the engineer achieve, and close the gaps”  - From a 15 years experienced senior leader in tech who has managed teams of up to 30It would surely be on everyone’s wish list to hire that engineer who has a perfect technical fit, but most of the time we don’t get so lucky. Certain factors play a part in this equation, e.g. your team’s location in a place where the pool of candidates could be of lower quality, the nature of your product may mean that you may not need such a perfect 100% technical fit, or because you are lean and you don’t have the luxury to wait.However, there are many different reasons to why an engineer who isn’t a perfect technical fit may be right for your team. One of the most important factors to assess when you cannot find the right technical fit is love. Does the engineer really love what s/he does? Do they try to do more than others and really push themselves harder? Do they want to work with the team and would they feel empowered?  “The fit I’m looking for includes having an appetite for risk taking and innovation. The people to hire should be someone who brings good ideas, someone who is also good at execution, who wants to challenge the status quo … and this person is incredibly hard to find!  - From an Engineering leader for backend teams in an on-demand, media streaming platformUltimately a good engineer is someone who is excited by things they do not know and is willing to learn. These engineers typically have the ability to:  Step forward without letting overthinking and over-analysis bite you… to not get distracted and mired by obstacles.  Iterate code, fast (bias for action that is scalable and proves to be so, over time, as opposed to quick-fixes).  Produce nice, clean, readable and debuggable code.  (Sometimes) take a deep breath and see the full picture.So which is better, technical ability or cultural fit? In reality it’s about finding the best balance for you and your team.2. Finding the “Smartest” Engineer  “I ask them if they have participated in hackathons and examine their CV closely to see what kind of career moves they have made. Were those decisions progressive? Did they look for opportunities to learn and grow? I check how they would solve problems and reach solutions.”  - Acting CTO in an autonomous vehicle startupMany confident managers and leaders make it their goal to hire the smartest people they can.A good question to ask yourself at the end of the process can be: For this person who is being hired, are they raising or lowering the average bar? In this scenario the goal is to make the team better. Really smart engineers are able to turn $1 million-worth complex problems into $100K simple ones. When this happens, whether or not the problem is able to be solved becomes far less important.To be an expert in everything is not required. In order to make your team better you need engineers to be smart in different ways.3. Finding the “Knowing-Asking-Learning” Engineer  “I look for candidates with deep understanding of the tools, technologies or problems that s/he has worked on before. I look for passion and ability to learn, as technology is changing at a greater pace than ever before, we need candidates who can and will keep expanding their knowledge. I look for candidates who can bring something different to the table so that the team can have a diversity of skill set, experiences, points of view and backgrounds.”  - Engineering leader of teams operating in Systems Reliability, Databases and Data Engineering, in an Asian ‘unicorn’ technology startupThis engineer has a deep understanding of knowing how it’s done and exactly why it should be done in this way. When they do not know, these good engineers will ask why, and keep asking why. You see they want to learn why people use particular technologies and why particular algorithms are being used for this solution in order to understand how deeply this solution has been thought through.If you hire based only on what an engineer knows right now you ask questions like these:  How long have you been coding in Ruby/Python/Golang/Javascript?  Explain how XMLFilter works in Java?  What is the default size of a Java HashMap?In order to get better insights then you should consider following up with this:  Tell me why you did this.  Then keep exploring the ‘why’ angle!Sure, this takes a bit more effort on your part, but you will actually be assessing their aptitude and future potential of the engineer.The Recipe So FarSo, we explored concepts of hiring these archetypes:  Technical fit vs Cultural fit  The Smartest Engineer  The Knowing-Asking-Learning EngineerExperience + coding ability + knowledge + ‘more than knowledge’ + love + … could this be the equation ?The Real Magic in the RecipeGetting good engineers into your team is critical to your success. It also takes time, and effort, and teamwork, and having a good plan.The good engineer you hire eventually, ends up being 5x or 10x more productive in your existing environment.It is important to start off with the right concepts, if your first few hires are not good engineers, you may eventually end up with a team of 100 no-good engineers.Good engineers are able to debug problems better, think of solutions better, understand a programme faster and assess potential impact and implications faster. They also will be likely to write bug-free code, consistently.Overall, they will help us to figure out how to make others on their team better engineers.Programming = Problem Solving.Yes.And now it is decision making time. 😊Names of people interviewed are omitted to retain confidentiality.",
        "url": "/so-you-need-to-hire-good-engineers"
      }
      ,
    
      "come-and-hackallthethings-at-grab": {
        "title": "Come and #hackallthethings at Grab",
        "author": "grab-engineering",
        "tags": "[&quot;Security&quot;]",
        "category": "",
        "content": "  For the longest time, security has been at the center of our priorities. There’s nothing more self-evident about the trust our millions of driving partners and consumers put in Grab. We strive every day to build the best tools available to ensure their data stays secure.For this reason, we launched our private bug bounty programme one year ago, allowing security researchers to scrutinise our code and flag vulnerabilities for handsome rewards. Over the past twelve months, we have been able to work with more than 350 talented researchers and have awarded nearly 200 bug reports. We would like to take this opportunity to thank everyone who submitted reports and helped us become more secure. As much as we have received some exceptional reports, we are looking for more!Today, we are excited to officially announce our public bug bounty program!Working with HackerOne, we want to continue to drive our security efforts forward. Are you up for the challenge to #hackallthethings and earn big rewards?!Come find our vulnerabilities and help us create one of the most secure platforms in the world! Are you sharp enough to identify any remote code execution, SQL injections, exportable XSS vulnerabilities or overall high impact security issues?We care about our users, so work with us to protect them as best we can. Help us resolve security issues to protect users with transparency, responsibility, and ethical practices. Depending on the impact and severity, our programme will reward up to $10,000 per bug report.We look forward to awarding some valid reports! Can’t wait to start?Visit https://hackerone.com/grab for complete guidelines, details, terms and conditions.Happy hacking!Grab Security Team",
        "url": "/come-and-hackallthethings-at-grab"
      }
      ,
    
      "how-we-scaled-our-cache-and-got-a-good-nights-sleep": {
        "title": "How We Scaled Our Cache and Got a Good Night's Sleep",
        "author": "gao-chao",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "Caching is arguably the most important and widely used technique in computer industry, from CPU to Facebook live videos, cache is everywhere.ProblemOur CDS (Common Data Service) relies heavily on caching too. It helps us reduce database load and generate faster responses to our consumers. But as our business grows, the load on our cache system grows too and it might eventually become a bottleneck.To solve this potential problem, we need to be able to horizontally scale our cache system. Why horizontally?  We want to have more caching space in order to accommodate more caches in future.  The caching system we are using is single-threaded (Redis provided by ElasticCache) which would only use one core even in a multicore system. Vertically scaling by adding more cores to one machine simply doesn’t help.The options available to us are as follow:  Use Redis master-slave model and make all writes go through master, all reads through multiple slaves.  Use Twemproxy as a middle layer of distributing caches to multiple backend ElasticCache machines.  Custom sharding the cache keys across multiple ElasticCache machines.There are a few known drawbacks of the first approach, especially when there is some trickiness that comes with replication and master fail-over scenarios, as described in this post.Moreover, the first approach doesn’t solve our first problem - to have more memory space in order to accommodate more caches. Naturally, we gave it up.The second approach of using Twemproxy isn’t a good solution either. It has been proven before that under a heavy load, Twemproxy will become the bottleneck as all the cache I/O will be going through there.DesignFinally, we decided to implement a custom sharding mechanism for our caches. Each CDS instance will hash each of the keys it needs to read or write, and based on the hashed value it will figure out which shard the key is possibly in and then access that shard for the interested key. This approach is essentially what Twemproxy does to CDS instances, thus distributing the load.Twemproxy  CDS Hashing  We wrote an internal Golang package to implement consistent hashing already and we have a fairly clean abstraction, so the work becomes pretty easy - wrap the components!ImplementationComing to the topic of implementation, the first thing we considered is that even though the logic of cache read / write is different in this sharding model, it’s still a cache from our server’s point of view. So we added an interface called ShardedCache which is composited with the original Cache interface (so it has the same exposed methods with Cache) that allows us to easily swap implementations where cache is used.The second thing we made sure of is that ShardedCache is only a thin wrapper on top of Cache. The core caching I/O features still happens in Cache implementation and what ShardedCache provides is hashing capability so it will be much easier for these 2 implementations evolve in parallel with minimum impact on each other.Furthermore, although we are using ketama as default hashing method, users can still inject their own hashing functions if needed. This facilitates tests and future extensions.DeploymentShipping a new software to production always comes with risk. Especially with such a critical system as caching.When switching to a new cache mechanism, some cache misses are inevitable, so we chose to deploy during the relatively peaceful hours at night, so that we can have some cache warm up time before the morning peak.Also, we have a cron job to populate caches for some heavy requests every 12 hours, so we need to make the cron job double write cache to the new systems beforehand in order to prevent high volume DB reads and possible data inconsistencies.Therefore, the steps are:  Configure cron job double writing to the new cache system – Need to deploy CDS because cron job is running within CDS.  Verify the populated caches in new system and configure CDS to read from there – Need to deploy CDS again for the configuration changes.This process took 2 days to finish, a little tedious but worth doing for a max degree of reliability.OutcomeWith everything is in place, we deployed it and here’s what happened:As you can see from the graphs below, although we are experiencing more load, after a period of warmup. The sharded caching solution offers much better P99 latency comparing to the old single system.    Many thanks to Jason Xu for his awesome consistent hashing package and Nguyen Qui Hieu for his discussion of this solution and help in setting up new ElasticCache nodes.P.S. If you or your friends are interested in the work we are doing in Engineering Data and want to explore more, you are welcome to talk to us! We are eagerly looking for good engineers to grow our team!References  Consistent Hashing",
        "url": "/how-we-scaled-our-cache-and-got-a-good-nights-sleep"
      }
      ,
    
      "grabs-front-end-study-guide": {
        "title": "Grab's Front End Study Guide",
        "author": "tay-yang-shun",
        "tags": "[&quot;Front End&quot;, &quot;JavaScript&quot;, &quot;Web&quot;]",
        "category": "",
        "content": "  The original post can be found on GitHub. Future updates to the study guide will be made there. If you like what you are reading, give the repository a star! 🌟Grab is Southeast Asia (SEA)’s leading transportation platform and our mission is to drive SEA forward, leveraging on the latest technology and the talented people we have in the company. As of May 2017, we handle 2.3 million rides daily and we are growing and hiring at a rapid scale.To keep up with Grab’s phenomenal growth, our web team and web platforms have to grow as well. Fortunately, or unfortunately, at Grab, the web team has been keeping up with the latest best practices and has incorporated the modern JavaScript ecosystem in our web apps.The result of this is that our new hires or back end engineers, who are not necessarily well-acquainted with the modern JavaScript ecosystem, may feel overwhelmed by the barrage of new things that they have to learn just to complete their feature or bug fix in a web app. Front end development has never been so complex and exciting as it is today. New tools, libraries, frameworks and plugins emerge every other day and there is so much to learn. It is imperative that newcomers to the web team are guided to embrace this evolution of the front end, learn to navigate the ecosystem with ease, and get productive in shipping code to our users as fast as possible. We have come up with a study guide to introduce why we do what we do, and how we handle front end at scale.This study guide is inspired by “A Study Plan to Cure JavaScript Fatigue” and is mildly opinionated in the sense that we recommend certain libraries/frameworks to learn for each aspect of front end development, based on what is currently deemed most suitable at Grab. We explain why a certain library/framework/tool is chosen and provide links to learning resources to enable the reader to pick it up on their own. Alternative choices that may be better for other use cases are provided as well for reference and further self-exploration.If your company is exploring a modern JavaScript stack as well, you may find this study guide useful to your company too! Feel free to adapt it to your needs. We will update this study guide periodically, according to our latest work and choices.- Grab Web TeamPre-requisites  Good understanding of core programming concepts.  Comfortable with basic command line actions and familiarity with source code version control systems such as Git.  Experience in web development. Have built server-side rendered web apps using frameworks like Ruby on Rails, Django, Express, etc.  Understanding of how the web works. Familiarity with web protocols and conventions like HTTP and RESTful APIs.Table of Contents  Single-page Apps (SPAs)  New-age JavaScript  User Interface  State Management  Coding with Style  Maintainability          Testing      Linting JavaScript      Linting CSS      Types        Build System  Package ManagementCertain topics can be skipped if you have prior experience in them.Single-page Apps (SPAs)Web developers these days refer to the products they build as web apps, rather than websites. While there is no strict difference between the two terms, web apps tend to be highly interactive and dynamic, allowing the user to perform actions and receive a response for their action. Traditionally, the browser receives HTML from the server and renders it. When the user navigates to another URL, a full-page refresh is required and the server sends fresh new HTML for the new page. This is called server-side rendering.However in modern SPAs, client-side rendering is used instead. The browser loads the initial page from the server, along with the scripts (frameworks, libraries, app code) and stylesheets required for the whole app. When the user navigates to other pages, a page refresh is not triggered. The URL of the page is updated via the HTML5 History API. New data required for the new page, usually in JSON format, is retrieved by the browser via AJAX requests to the server. The SPA then dynamically updates the page with the data via JavaScript, which it has already downloaded in the initial page load. This model is similar to how native mobile apps work.The benefits:  The app feels more responsive and users do not see the flash between page navigations due to full-page refreshes.  Fewer HTTP requests are needed to the server, as the same assets do not have to be downloaded again for each page load.  Clear separation of the concerns between the client and the server; you can easily build new clients for different platforms (e.g. mobile, chatbots, smart watches) without having to modify the server code. You can also modify the technology stack on the client and server independently, as long as the API contract is not broken.The downsides:  Heavier initial page load due to loading of framework, app code, and assets required for multiple pages 1.  There’s an additional step to be done on your server which is to configure it to route all requests to a single entry point and allow client-side routing to take over from there.  SPAs are reliant on JavaScript to render content, but not all search engines execute JavaScript during crawling, and they may see empty content on your page. This inadvertently hurts the SEO of your app 2.While traditional server-side rendered apps are still a viable option, a clear client-server separation scales better for larger engineering teams, as the client and server code can be developed and released independently. This is especially so at Grab when we have multiple client apps hitting the same API server.As web developers are now building apps rather than pages, organization of client-side JavaScript has become increasingly important. In server-side rendered pages, it is common to use snippets of jQuery to add user interactivity to each page. However, when building large apps, jQuery is not sufficient. After all, jQuery is primarily a library for DOM manipulation and it’s not a framework, it does not define a clear structure and organization for your app.JavaScript frameworks have been created to provide higher-level abstractions over the DOM, allowing you to keep state in memory, out of the DOM. Using frameworks also brings the benefits of reusing recommended concepts and best practices for building apps. A new engineer on the team who has experience with a framework but not the app will find it easier to understand the code because it is organized in a structure that he is familiar with. Popular frameworks have a lot of tutorials and guides, and tapping on the knowledge and experience from colleagues and the community will help new engineers get up to speed.Study Links  Single Page App: advantages and disadvantages  The (R)Evolution of Web DevelopmentNew-age JavaScriptBefore you dive into the various aspects of building a JavaScript web app, it is important to get familiar with the language of the web - JavaScript, or ECMAScript. JavaScript is an incredibly versatile language which you can also use to build web servers, native mobile apps and desktop apps.Prior to 2015, the last major update was ECMAScript 5.1, in 2011. However, in the recent years, JavaScript has suddenly seen a huge burst of improvements within a short span of time. In 2015, ECMAScript 2015 (previously called ECMAScript 6) was released and a ton of syntactic constructs were introduced to make writing code less unwieldy. Auth0 has written a nice history of JavaScript. Till this day, not all browsers have fully implemented the ES2015 specification. Tools such as Babel enable developers to write ES2015 in their apps and Babel transpiles them down to ES5 to be compatible for browsers.Being familiar with both ES5 and ES2015 is crucial. ES2015 is still relatively new and a lot of open source code and Node.js apps are still written in ES5. If you are doing debugging in your browser console, you might not be able to use ES2015 syntax. On the other hand, documentation and example code for many modern libraries that we will introduce later below are written in ES2015. At Grab, we use ES2015 (with Babel Stage-0 preset) to embrace the syntactic improvements the future of JavaScript provides and we have been loving it so far.Spend a day or two revising ES5 and exploring ES2015. The more heavily used features in ES2015 include “Arrows and Lexical This”, “Classes”, “Template Strings”, “Destructuring”, “Default/Rest/Spread operators”, and “Importing and Exporting modules”.Estimated Duration: 3-4 days. You can learn/lookup the syntax as you learn the other libraries and try building your own app.Study Links  Learn ES5 on Codecademy  Learn ES2015 on Babel  Free Code Camp  ES6 Katas  You Don’t Know JS (Advanced content, optional for beginners)User Interface - React  If any JavaScript project has taken the front end ecosystem by storm in recent years, that would be React. React is a library built and open-sourced by the smart people at Facebook. In React, developers write components for their web interface and compose them together.React brings about many radical ideas and encourages developers to rethink best practices. For many years, web developers were taught that it was a good practice to write HTML, JavaScript and CSS separately. React does the exact opposite, and encourages that you write your HTML and CSS in your JavaScript instead. This sounds like a crazy idea at first, but after trying it out, it actually isn’t as weird as it sounds initially. Reason being the front end development scene is shifting towards a paradigm of component-based development. The features of React:      Declarative - You describe what you want to see in your view and not how to achieve it. In the jQuery days, developers would have to come up with a series of steps to manipulate the DOM to get from one app state to the next. In React, you simply change the state within the component and the view will update itself according to the state. It is also easy to determine how the component will look like just by looking at the markup in the render() method.        Functional - The view is a pure function of props and state. In most cases, a React component is defined by props (external parameters) and state (internal data). For the same props and state, the same view is produced. Pure functions are easy to test, and the same goes for functional components. Testing in React is made easy because a component’s interfaces are well-defined and you can test the component by supplying different props and state to it and comparing the rendered output.        Maintainable - Writing your view in a component-based fashion encourages reusability. We find that defining a component’s propTypes make React code self-documenting as the reader can know clearly what is needed to use that component. Lastly, your view and logic is self-contained within the component, and should not be affected nor affect other components. That makes it easy to shift components around during large-scale refactoring, as long as the same props are supplied to the component.        High Performance - You might have heard that React uses a virtual DOM (not to be confused with shadow DOM) and it re-renders everything when there is a change in state. Why is there a need for a virtual DOM? While modern JavaScript engines are fast, reading from and writing to the DOM is slow. React keeps a lightweight virtual representation of the DOM in memory. Re-rendering everything is a misleading term. In React it actually refers to re-rendering the in-memory representation of the DOM, not the actual DOM itself. When there’s a change in the underlying data of the component, a new virtual representation is created, and compared against the previous representation. The difference (minimal set of changes required) is then patched to the real browser DOM.        Ease of Learning - Learning React is pretty simple. The React API surface is relatively small compared to this; there are only a few APIs to learn and they do not change often. The React community is one of the largest, and along with that comes a vibrant ecosystem of tools, open-sourced UI components, and a ton of great resources online to get you started on learning React.        Developer Experience - There are a number of tools that improves the development experience with React. React Devtools is a browser extension that allows you to inspect your component, view and manipulate its props and state. Hot reloading with webpack allows you to view changes to your code in your browser, without you having to refresh the browser. Front end development involves a lot of tweaking code, saving and then refreshing the browser. Hot reloading helps you by eliminating the last step. When there are library updates, Facebook provides codemod scripts to help you migrate your code to the new APIs. This makes the upgrading process relatively pain-free. Kudos to the Facebook team for their dedication in making the development experience with React great.  Over the years, new view libraries that are even more performant than React have emerged. React may not be the fastest library out there, but in terms of the ecosystem, overall usage experience and benefits, it is still one of the greatest. Facebook is also channeling efforts into making React even faster with a rewrite of the underlying reconciliation algorithm. The concepts that React introduced has taught us how to write better code, more maintainable web apps and made us better engineers. We like that.We recommend going through the tutorial on building a tic-tac-toe game on the React homepage to get a feel of what React is and what it does. For more in-depth learning, check out the highly-rated free course, React Fundamentals by the creators of React Router, who are experts from the React community. It also covers more advanced concepts that are not covered by the React documentation. Create React App by Facebook is a tool to scaffold a React project with minimal configuration and is highly recommended to use for starting new React projects.React is a library, not a framework, and does not deal with the layers below the view - the app state. More on that later.Estimated Duration: 3-4 days. Try building simple projects like a to-do list, Hacker News clone with pure React. You will slowly gain an appreciation for it and perhaps face some problems along the way that isn’t solved by React, which brings us to the next topic…Study Links  React Official Tutorial  React Fundamentals  Simple React Development in 2017  Presentational and Container ComponentsAlternatives  Angular  Ember  Vue  CycleState Management - Flux/Redux  As your app grows bigger, you may find that the app structure becomes a little messy. Components throughout the app may have to share and display common data but there is no elegant way to handle that in React. After all, React is just the view layer, it does not dictate how you structure the other layers of your app, such as the model and the controller, in traditional MVC paradigms. In an effort to solve this, Facebook invented Flux, an app architecture that complements React’s composable view components by utilising a unidirectional data flow. Read more about how Flux works here. In summary, the Flux pattern has the following characteristics:  Unidirectional data flow - Makes the app more predictable as updates can be tracked easily.  Separation of concerns - Each part in the Flux architecture has clear responsibilities and are highly decoupled.  Works well with declarative programming - The store can send updates to the view without specifying how to transition views between states.As Flux is not a framework per se, developers have tried to come up with many implementations of the Flux pattern. Eventually, a clear winner emerged, which was Redux. Redux combines the ideas from Flux, Command pattern and Elm architecture and is the de facto state management library developers use with React these days. Its core concepts are:  App state is described by a single plain old JavaScript object (POJO).  Dispatch an action (also a POJO) to modify the state.  Reducer is a pure function that takes in current state and action to produce a new state.The concepts sound simple, but they are really powerful as they enable apps to:  Have their state rendered on the server, booted up on the client.  Trace, log and backtrack changes in the whole app.  Implement undo/redo functionality easily.The creator of Redux, Dan Abramov, has taken great care in writing up detailed documentation for Redux, along with creating comprehensive video tutorials for learning basic and advanced Redux. They are extremely helpful resources for learning Redux.Combining View and StateWhile Redux does not necessarily have to be used with React, it is highly recommended as they play very well with each other. React and Redux have a lot of ideas and traits in common:  Functional composition paradigm - React composes views (pure functions) while Redux composes pure reducers (also pure functions). Output is predictable given the same set of input.  Easy To Reason About - You may have heard this term many times but what does it actually mean? Through our experience, React and Redux makes debugging simpler. As the data flow is unidirectional, tracing the flow of data (server responses, user input events) is easier and it is straightforward to determine which layer the problem occurs.  Layered Structure - Each layer in the app / Flux architecture is a pure function, and has clear responsibilities. It is pretty easy to write tests for them.  Development Experience - A lot of effort has gone into creating tools to help in debugging and inspecting the app while development, such as Redux DevTools.Your app will likely have to deal with async calls like making remote API requests. redux-thunk and redux-saga were created to solve those problems. They may take some time to understand as they require understanding of functional programming and generators. Our advice is to deal with it only when you need it.react-redux is an official React binding for Redux and is very simple to learn.Estimated Duration: 4 days. The egghead courses can be a little time consuming but they are worth spending time on. After learning Redux, you can try incorporating it into the React projects you have built. Does Redux solve some of the state management issues you were struggling with in pure React?Study Links  Flux Homepage  Redux Homepage  Egghead Course - Getting Started with Redux  Egghead Course - Build React Apps with Idiomatic Redux  React Redux Links  You Might Not Need ReduxAlternatives  MobXCoding with Style - CSS Modules  Writing good CSS is hard. It takes many years of experience and frustration of shooting yourself in the foot before one is able to write maintainable and scalable CSS. CSS, having a global namespace, is fundamentally designed for web documents, and not really for web apps that favor a components architecture. Hence, experienced front end developers have designed methodologies to guide people on how to write organized CSS for complex projects, such as using SMACSS, BEM, SUIT CSS, etc. However, the encapsulation of styles that these methodologies bring about are artificially enforced by conventions and guidelines. They break the moment developers do not follow them.Fortunately, the front end ecosystem is saturated with tools, and unsurprisingly, tools have been invented to partially solve some of the problems with writing CSS at scale. “At scale” means that many developers are working on the same project and touching the same stylesheets. There is no community-agreed approach on writing CSS in JS at the moment, and we are hoping that one day a winner would emerge, just like Redux did, among all the Flux implementations. For now, we are banking on CSS Modules. CSS modules is an improvement over existing CSS that aims to fix the problem of global namespace in CSS; it enables you to write styles that are local by default and encapsulated to your component. This feature is achieved via tooling. With CSS modules, large teams can write modular and reusable CSS without fear of conflict or overriding other parts of the app. However, at the end of the day, CSS modules are still being compiled into normal globally-namespaced CSS that browsers recognize, and it is still important to learn raw CSS.If you are a total beginner to CSS, Codecademy’s HTML &amp; CSS course will be a good introduction to you. Next, read up on the Sass preprocessor, an extension of the CSS language which adds syntactic improvements and encourages style reusability. Study the CSS methodologies mentioned above, and lastly, CSS modules.Estimated Duration: 3-4 days. Try styling up your app using the SMACSS/BEM approach and/or CSS modules.Study Links  Learn HTML &amp; CSS course on Codecademy  Intro to HTML/CSS on Khan Academy  SMACSS  BEM  SUIT CSS  CSS Modules Specification  Sass Homepage  A pattern for writing CSS to scaleAlternatives  JSS  Styled ComponentsMaintainabilityCode is read more frequently than it is written. This is especially true at Grab, where the team size is large and we have multiple engineers working across multiple projects. We highly value readability, maintainability and stability of the code and there are a few ways to achieve that: “Extensive testing”, “Consistent coding style” and “Typechecking”.Testing - Jest + Enzyme  Jest is a testing library by Facebook that aims to make the process of testing pain-free. As with Facebook projects, it provides a great development experience out of the box. Tests can be run in parallel for faster speed and during watch mode, only the tests for the changed files are run. One particular feature we like is “Snapshot Testing”. Jest can save the generated output of your React component and Redux state and save it as serialised files, so you wouldn’t have to manually come up with the expected output yourself. Jest also comes with built-in mocking, assertion and test coverage. One library to rule them all!React comes with some testing utilities, but Enzyme by Airbnb makes it easier to generate, assert, manipulate and traverse your React components’ output with a jQuery-like API. It is recommended that Enzyme be used to test React components.Jest and Enzyme makes writing front end tests fun and easy. It also helps that React components and Redux actions/reducers are relatively easy to test because of clearly defined responsibilities and interfaces. For React components, we can test that given some props, the desired DOM is rendered, and that callbacks are fired upon certain simulated user interactions. For Redux reducers, we can test that given a prior state and an action, a resulting state is produced.The documentation for Jest and Enzyme are pretty concise, and it should be sufficient to learn them by reading it.Estimated Duration: 2-3 days. Try writing Jest + Enzyme tests for your React + Redux app!Study Links  Jest Homepage  Testing React Apps with Jest  Enzyme Homepage  Enzyme: JavaScript Testing utilities for ReactAlternatives  AVA  KarmaLinting JavaScript - ESLint  A linter is a tool to statically analyse code and finds problems with them, potentially preventing bugs/runtime errors and at the same time, enforcing a coding style. Time is saved during pull request reviews when reviewers do not have to leave nitpicky comments on coding style. ESLint is a tool for linting JavaScript code that is highly extensible and customisable. Teams can write their own lint rules to enforce their custom styles. At Grab, we use Airbnb’s eslint-config-airbnb preset, that has already been configured with the common good coding style in the Airbnb JavaScript style guide.For the most part, using ESLint is as simple as tweaking a configuration file in your project folder. There’s nothing much to learn about ESLint if you’re not writing new rules for it. Just be aware of the errors when they surface and Google it to find out the recommended style.Estimated Duration: 1/2 day. Nothing much to learn here. Add ESLint to your project and fix the linting errors!Study Links  ESLint Homepage  Airbnb JavaScript Style GuideAlternatives  Standard  JSHintLinting CSS - stylelint  As mentioned earlier, good CSS is notoriously hard to write. Usage of static analysis tools on CSS can help to maintain our CSS code quality and coding style. For linting CSS, we use stylelint. Like ESLint, stylelint is designed in a very modular fashion, allowing developers to turn rules on/off and write custom plugins for it. Besides CSS, stylelint is able to parse SCSS and has experimental support for Less, which lowers the barrier for most existing code bases to adopt it.  Once you have learnt ESLint, learning stylelint would be effortless considering their similarities. stylelint is currently being used by big companies like Facebook, GitHub and Wordpress.One downside of stylelint is that the autofix feature is not fully mature yet, and is only able to fix for a limited number of rules. However, this issue should improve with time.Estimated Duration: 1/2 day. Nothing much to learn here. Add stylelint to your project and fix the linting errors!Study Links  stylelint Homepage  Lint your CSS with stylelintAlternatives  Sass Lint  CSS LintTypes - Flow  Static typing brings about many benefits when writing apps. They can catch common bugs and errors in your code early. Types also serve as a form of documentation for your code and improves the readability of your code. As a code base grows larger, we see the importance of types as they gives us greater confidence when we do refactoring. It is also easier to onboard new members of the team to the project when it is clear what kind of values each object holds and what each function expects.Adding types to your code comes with the trade-off of increased verbosity and a learning curve of the syntax. But this learning cost is paid upfront and amortised over time. In complex projects where the maintainability of the code matters and the people working on it change over time, adding types to the code brings about more benefits than disadvantages.The two biggest contenders in adding static types to JavaScript are Flow (by Facebook) and TypeScript (by Microsoft). As of date, there is no clear winner in the battle. For now, we have made the choice of using Flow. We find that Flow has a lower learning curve as compared to TypeScript and it requires relatively less effort to migrate an existing code base to Flow. Being built by Facebook, Flow has better integration with the React ecosystem out of the box. Anyway, it is not extremely difficult to move from Flow to TypeScript as the syntax and semantics are quite similar, and we will re-evaluate the situation in time to come. After all, using one is better than not using any at all.Flow recently revamped their documentation site and it’s pretty neat now!Estimated Duration: 1 day. Flow is pretty simple to learn as the type annotations feel like a natural extension of the JavaScript language. Add Flow annotations to your project and embrace the power of type systems.Study Links  Flow Homepage  TypeScript vs FlowAlternatives  TypeScriptBuild System - webpack  This part will be kept short as setting up webpack can be a tedious process and might be a turn-off to developers who are already overwhelmed by the barrage of new things they have to learn for front end development. In a nutshell, webpack is a module bundler that compiles a front end project and its dependencies into a final bundle to be served to users. Usually, projects will already have the webpack configuration set up and developers rarely have to change it. Having an understanding of webpack is still a good to have in the long run. It is due to webpack that features like hot reloading and CSS modules are made possible.We have found the webpack walkthrough by SurviveJS to be the best resource on learning webpack. It is a good complement to the official documentation and we recommend following the walkthrough first and referring to the documentation later when the need for further customisation arises.Estimated Duration: 2 days (Optional).Study Links  webpack Homepage  SurviveJS - Webpack: From apprentice to masterAlternatives  Rollup  BrowserifyPackage Management - Yarn  If you take a peek into your node_modules directory, you will be appalled by the number of directories that are contained in it. Each babel plugin, lodash function, is a package on its own. When you have multiple projects, these packages are duplicated across each project and they are largely similar. Each time you run npm install in a new project, these packages are downloaded over and over again even though they already exist in some other project in your computer.There was also the problem of non-determinism in the installed packages via npm install. Some of our CI builds fail because at the point of time when the CI server installs the dependencies, it pulled in minor updates to some packages that contained breaking changes. This would not have happened if library authors respected semver and engineers assumed that API contracts are respected all the time.Yarn solves these problems. The issue of non-determinism of installed packages via a yarn.lock file and it ensures that every install results in the exact same file structure in node_modules across all machines. Yarn utilises a global cache directory within your machine, and packages that have been downloaded before do not have to be downloaded again. This also enables offline installation of dependencies!The most common Yarn commands can be found here. Most other yarn commands are similar to the npm equivalents and it is fine to use the npm versions instead. One of our favourite commands is yarn upgrade-interactive which makes updating dependencies a breeze especially when the modern JavaScript project requires so many dependencies these days. Do check it out!npm@5.0.0 was released in May 2017 and it seems to address many of the issues that Yarn aims to solve. Do keep an eye on it!Estimated Duration: 2 hours.Study Links  Yarn Homepage  Yarn: A new package manager for JavaScriptAlternatives  Good old npmThe Journey has Just BegunCongratulations on making it this far! Front end development today is hard, but it is also more interesting than before. What we have covered so far will help any new engineer to Grab’s web team to get up to speed with our technologies pretty quickly. There are many more things to be learnt, but building up a solid foundation in the essentials will aid in learning the rest of the technologies. This helpful front end web developer roadmap shows the alternative technologies available for each aspect.We made our technical decisions based on what was important to a rapidly growing Grab Engineering team - maintainability and stability of the front end code base. These decisions may or may not apply to smaller teams and projects. Do evaluate what works best for you and your company.As the front end ecosystem grows, we are actively exploring, experimenting and evaluating how new technologies can make us a more efficient team and improve our productivity. We hope that this post has given you insights into the front end technologies we use at Grab. If what we are doing interests you, we are hiring!Many thanks to Joel Low, Li Kai and Tan Wei Seng who reviewed drafts of this article.The original post can be found on GitHub. Future updates to the study guide will be made there. If you like what you are reading, give the repository a star! 🌟  More ReadingGeneral  State of the JavaScript Landscape: A Map for Newcomers  The Hitchhiker’s guide to the modern front end development workflow  How it feels to learn JavaScript in 2016  Roadmap to becoming a web developer in 2017  Modern JavaScript for Ancient Web DevelopersOther Study Guides  A Study Plan To Cure JavaScript Fatigue  JS Stack from Scratch  A Beginner’s JavaScript Study PlanFootnotes            This can be solved via webpack code splitting. &#8617;              Universal JS to the rescue! &#8617;      ",
        "url": "/grabs-front-end-study-guide"
      }
      ,
    
      "dns-resolution-in-go-and-cgo": {
        "title": "DNS Resolution in Go and Cgo",
        "author": "ryan-law",
        "tags": "[&quot;Golang&quot;, &quot;Networking&quot;]",
        "category": "",
        "content": "This article is part two of a two-part series (part one). In this article, we will talk about RFC 6724 (3484), how DNS resolution works in Go and Cgo, and finally explaining why disabling IPv6 also disables the sorting of IP Addresses.As a quick recap of our journey so far, we walked you through our investigative process of a load balancing issue on our AWS Elastic Load Balancer (ELB) nodes and how we temporarily fixed it by using Cgo and disabling IPv6. In this part of the series, we will be diving deeper into RFC 6724 (3484), exploring DNS Resolution in Go and Cgo, explaining why disabling IPv6 “fixes” the IP addresses sorting and how the permanent fix requires modifying the Go source code. If you already understand RFC 6274 (3484), please feel free to jump to the section titled “Further Investigation” and if you are short on time, the “Summary” is also provided at the end of the article.BackgroundRFC 6724 (3484)RFC 6724 and its earlier revision – RFC 3484, defines how connections between two systems over the internet should be established when there is more than one possible IP address on the source and destination systems. And because of the way the internet works, if you connect to a website by entering a domain name instead of a IP address, it is almost guaranteed that you will execute an implementation of the RFC. When you enter a domain name in your browser, behind the scenes, your browser will send a DNS A (for IPv4) or AAAA (for IPv6) query to a DNS server to get a list of IP addresses that it should connect to. Because nowadays, almost all websites have two or more servers behind them, it’s very likely for you to get at least two IP addresses back from the DNS. The question is then, what happens when you get two IP addresses? Which one should you choose? This is exactly the question that the RFC is attempting to address. (For more detailed information, please refer to the RFC itself. The sorting rules for the source and destination address are located on page 9 and 13 respectively)Go and CgoDuring the early days of Go, Cgo was introduced as a way for Go programs to embed C code inside of Go. Cgo allows Go to tap into the vast amount of C libraries, an ability that is especially useful in situations where you want to execute some low level operation that you know works really well in C and is non-trivial to rewrite in Go. However, with Go maturing, the Go maintainers have decided to move away from C implementations to native Go implementations. When Go executes C code, it will actually run the C code on an OS thread instead of goroutines that are orders of magnitude cheaper.Further InvestigationNow that we have fixed the problem on our production systems by forcing the use of the Cgo DNS resolver and disabling IPv6, we are able to comfortably explore the problem and figure out why the unintuitive solution of using Cgo and disabling IPv6 works. Seeing how the Go source code in general has decent documentation, we decide to investigate that first. From the section titled “Name Resolution” of the documentation of the net package, we can see that by default, Go uses the Go DNS Resolver. In cases where it is not supported, it falls back to Cgo or some other implementation that is the default on the OS. In our case, our production servers run on Ubuntu so the default DNS resolver is the native Go DNS Resolver and if we were to enable Cgo, we will be either using the getaddrinfo or getnameinfo functions in glibc.Being armed with that knowledge, we write up a small Go programme that calls the net.LookupHost function and a simple C programme that calls getaddrinfo to make sure that our understanding is accurate and to test out the behaviour of both these programs in different situations.package mainimport (        \"log\"        \"net\"        \"net/http\")const (        astrolabe = \"astrolabe.ap-southeast-1.elb.amazonaws.com\")func lookup() {        log.Println(net.LookupHost(astrolabe))}# Modified from http://www.binarytides.com/hostname-to-ip-address-c-sockets-linux/#include&lt;stdio.h&gt; //printf#include&lt;string.h&gt; //memset#include&lt;stdlib.h&gt; //for exit(0);#include&lt;sys/socket.h&gt;#include&lt;errno.h&gt; //For errno - the error number#include&lt;netdb.h&gt; //hostent#include&lt;arpa/inet.h&gt;int hostname_to_ip(char *  , char *);int main(int argc , char *argv[]){    char *hostname = \"astrolabe.ap-southeast-1.elb.amazonaws.com\";    char ip[100];    hostname_to_ip(hostname , ip);    printf(\"astrolabe elb resolved to %s\", ip);    printf(\"\\n\");}/*    Get ip from domain name*/int hostname_to_ip(char *hostname , char *ip){    int sockfd;    struct addrinfo hints, *servinfo, *p;    struct sockaddr_in *h;    int rv;    memset(&amp;hints, 0, sizeof hints);    hints.ai_family = AF_UNSPEC; // use AF_INET6 to force IPv6    hints.ai_socktype = SOCK_STREAM;    if ((rv = getaddrinfo( hostname , \"http\" , &amp;hints , &amp;servinfo)) != 0)    {        fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(rv));        return 1;    }    // loop through all the results and connect to the first we can    for (p = servinfo; p != NULL; p = p-&gt;ai_next)    {        h = (struct sockaddr_in *) p-&gt;ai_addr;        strcat(ip, \" \");        strcat(ip , inet_ntoa( h-&gt;sin_addr ) );        strcat(ip, \" \");    }    freeaddrinfo(servinfo); // all done with this structure    return 0;}First of all, to see the default state of the source system, we run the ip address show command to show the list of network interfaces available on the source system.root@ip-172-21-2-90:~# ip address show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link       valid_lft forever preferred_lft foreverAnd because we are only interested in the outgoing network interface, we will be using the command ip address show dev eth0 from this point onwards.root@ip-172-21-2-90:~# ip address show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link       valid_lft forever preferred_lft foreverNow to run the Go, Cgo and C DNS resolvers.root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:07:31 [172.21.2.108 172.21.2.144 172.21.1.152 172.21.1.97] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.ap-southeast-1.elb.amazonaws.com) = Cgo2017/01/18 02:08:08 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.2.108  172.21.2.144  172.21.1.97  172.21.1.152As you can see, they all have the exact same sorting order with 172.21.2.108 being the first and 172.21.1.152 being the last, which is exactly as defined in Rule 9 of the RFC’s destination address sorting algorithm – addresses are sorted based on the longest matching prefix first.Source172.21.2.90:  10101100.00010101.00000010.01011010Destination172.21.2.108: 10101100.00010101.00000010.01101100172.21.2.144: 10101100.00010101.00000010.10010000172.21.1.97:  10101100.00010101.00000001.01100001172.21.1.152: 10101100.00010101.00000001.10011000To make it clearer, we have converted the IP addresses to their binary form for easier comparison. We can see that 172.21.2.108 has the longest matching prefix with our source interface of 172.21.2.90 and because the IP addresses in the 172.21.1.* subnet has the same matching prefix length, they can actually show up in a different order in which either 172.21.1.97 or 172.21.1.152 comes first.Now let’s see what happens when we disable IPv6. This can be done with the following commands:# We can either disable IPv6 completelysh -c 'echo 1 &gt; /proc/sys/net/ipv6/conf/eth0/disable_ipv6'# or we can just remove IPv6 from the outgoing interfacesip -6 addr del fe80::b4:d4ff:fe24:bbad/64 dev eth0After disabling IPv6, we run the ip address show dev eth0 command again to verify that the IPv6 address is no longer attached to the source interface.root@ip-172-21-2-90:~# ip address show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft foreverNow we run the programs again to see what has changed. For the sake of clarity, we are showing 2 runs of each of the programs.root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:14:39 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:14:40 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.ap-southeast-1.elb.amazonaws.com) = Cgo2017/01/18 02:15:41 [172.21.1.97 172.21.1.152 172.21.2.108 172.21.2.144] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.elb.amazonaws.com) = Cgo2017/01/18 02:15:43 [172.21.2.144 172.21.1.97 172.21.1.152 172.21.2.108] &lt;nil&gt;root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.1.152  172.21.2.108  172.21.2.144  172.21.1.97root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.1.97  172.21.1.152  172.21.2.108  172.21.2.144And from the results, you can see that it has no impact on the native Go DNS resolver but both the Cgo and C DNS resolvers are starting to return the IP addresses in a random order, as expected from our learnings in part one.Ok, Disabling IPv6 and Using Cgo/C Works, Now What?Now that we have established that disabling IPv6 does indeed solve the problem for us in Cgo and C (both use the same underlying getaddrinfo function in glibc), it is time for us to explore the Go source code to see if there is anything that stands out in its implementation of a DNS resolver.Being Go programmers, we can quickly navigate around the Go source code to reach the native Go DNS resolver (net/addrselect.go) and from the source code, we can see that it only implements part of the rules in the RFC. It does not provide a way to override the rules and, most importantly, it does not do any form of source address selection but instead relies on processing the Rule 9 sorting based on a couple of selected and reserved CIDR blocks (Reserved CIDR Blocks).Knowing what we have done so far, we had strong reasons to believe that it is the lack of source address selection that is causing the Go DNS resolver to behave differently from the DNS resolver in glibc.Source Address SelectionReferring back to the RFC, the part on source address selection states that the source address selection should be configurable by the system administrators. A quick google search shows us that for Ubuntu systems, the file is /etc/gai.conf. To isolate the changes that we are making, we re-enable IPV6 before proceeding further. First, we try to move IPv4 addresses to the top of the list. We suspect that for some weird reason, the IPv6 source address is somehow being used to make the outgoing connection, otherwise why would disabling IPv6 do anything at all? Surprisingly, all of our different attempts at modifying /etc/gai.conf do not do anything (Well, one of the attempts does, by adding a 172.21.2.90/26 prefix. It works because the common prefix for the addresses in the 172.21.2.* subnet would now be the same). Welp, we are now back at square one.After hours and hours of research by talking to people with networking experience and going through pages and pages of Google search results that touch on this topic (Microsoft’s blog posts on Vista, Debian mailing list, etc.), we finally come across a series of article on Linux Hacks (Part 1, Part 2). Guess what? The article actually tells us that source address selection is not configured through /etc/gai.conf but is done through the kernel instead! Aha!Off we go, once again making a bunch of different configuration changes to the network interface that bring us nowhere. Also, because the Go DNS resolver does not actually do any sort of source address selection, spending more time on this avenue does not really help us in finding the problem.The Source Code We GoIf you have ever gotten stuck on trying to figure out how something works and all the googling is not giving you the right answers, you know that going through the source code is the next thing to try. It is almost never the first thing that any programmer wants to do though. Navigating someone else’s code is hard and it’s even harder when it’s not a language you’re very familiar with. Ultimately, we decide to bite the bullet and dive deep into the code in glibc to see how source address selection is done specifically and get an understanding of how it affects the sorting of the IP addresses.Funnily enough, even finding the source code of glibc is not as straightforward as we expect. Nowadays, when you want to find a piece of code, you will probably just google it and find it on GitHub. This isn’t the case for glibc as the main source code is hosted at sourceware and is unfortunately not easy to navigate. Luckily, we found a mirror on GitHub that provided us with a familiar interface. Again, finding the source code for getaddrinfo itself also isn’t easy. At first, we end up in the inet directory and we get completely confused as all the files only have macro definitions and no code at all. Only after some googling and stumbling around, we find that the source code for getaddrinfo is at sysdeps/posix.Being mostly Go or Ruby programmers, it takes a little bit of time to understand how the C-based code works. After getting a basic understanding, we decide to whip out good old gdb to start debugging the code step by step. Eventually, we find the issue. The way the prefix attributes of the source addresses are set disables the sorting of the IP addresses, since they are the only values that are different when we enable/disable IPv6. With some more research, we identify a file named check_pf.c where the source address selection is actually being done. In the end, we narrow it down to a block of code in check_pf.c that is the root cause of this whole thing. The block of code basically states that if there are no IPv6 source addresses on the outgoing interface, it will just return that there are no possible source addresses at all that in turn causes Rule 9 sorting of the RFC to be completely bypassed and give us back the default DNS ordering (round robin in most scenarios).Finally understanding how it works in glibc, we modify the Go source code and to add in the same behaviour. With the same weird logic in check_pf.c, the Go DNS resolver now works the same as the glibc DNS resolver. However, we’re not interested in maintaining a separate fork of Go and instead opened a ticket with the Go maintainers. Within a very short timeframe, the Go maintainers decided to skip RFC 6274 completely for IPv4 addresses and merge this patch into the current upstream with release in Go 1.9. Eventually, the fix is also backported to Go 1.8.1 a release on April 7, 2017. The image below shows the effects of this change on one of our systems running on Go 1.8.1  SummaryTo summarise, in the first part of the series, we walked through our process investigating why we were receiving ELB HTTP 5xx alerts on Astrolabe (our driver location processing service) and how we fixed it by forcing Go to use the Cgo DNS resolver while IPv6 was disabled. In the second part of the series, we dived deeper into the problem to figure out why our solution in part 1 worked. In the end, it turns out that it was because of some undocumented behaviour in glibc that allowed the internet to continue working as it did.A couple of takeaways that we had from this investigation:  It is never easy to reimplement something that is already working, as in the case of Go’s reimplementation of glibc’s getaddrinfo. Because of a couple of lines of undocumented code in glibc, the Go maintainers did not manage to replicate glibc exactly and that caused strange and hard to understand problems.  Software is something that we can always reason with. With enough time, you will almost always be able to find the root cause and fix it.That’s it, we hope that you enjoyed reading our journey as much as we enjoyed going through it!Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.",
        "url": "/dns-resolution-in-go-and-cgo"
      }
      ,
    
      "driving-southeast-asia-forward-with-aws": {
        "title": "Driving Southeast Asia Forward with AWS",
        "author": "arul-kumaravel",
        "tags": "[&quot;AWS&quot;]",
        "category": "",
        "content": "  My name is Arul Kumaravel, VP of Engineering at Grab. Grab’s mission is to drive Southeast Asia (SEA) forwards. Today I would like to share with you how Amazon Web Services (AWS) is helping us with this mission. Grab was started in 2012 by our founders Anthony Tan and Tan Hooi Ling when they were in Harvard Business School. Both are from Malaysia. They started Grab, known as MyTeksi then, with a simple idea: to make Grab simple and easy to use for the people. We’ve come a long way since our humble beginnings.Today, we offer the most comprehensive suite of transport services in SEA, including taxis, cars and bikes. We have services that cater to every transport need, preferences and price points of our consumers. The numbers tell a story. We’re currently in 40 cities in 7 countries, the largest land fleet of 780,000 drivers in the region. Our app is installed in 38 million devices. We’re no longer just a taxi app, we’re much more than that. We’ve built a market-leading transportation platform. So whether you need a car, limo or a bike, whether you want to pay with cash, with credit, you just have to go to one place.Our journey doesn’t stop here. We continue to outserve our consumers by launching new products and services, such as social sharing, which is GrabHitch, parcel delivery, GrabExpress, and GrabFood. We are able to build the best and most widely-used app because of our talented pool of developers spread across all our six development centres. Our largest center is here in Singapore. We also have centres in Bangalore, Beijing, Ho Chi Minh, Jakarta and Seattle. Our engineers love that they are making an impact on the lives of SEA. A lot of these have been made possible thanks to our work with AWS.Grab started using Amazon Web Services since its inception in 2012. Our initial application was built using Ruby on Rails, which we ran on Amazon EC2. We used Amazon RDS MySQL for our storage. Of course we used VPC and other networking infrastructure for running our application. We have since evolved our app architecture from a single monolithic application to microservices-based architecture. We have grown quickly over the years and our usage of AWS increased tremendously. We used a number of AWS services that helps Grab team save time and resources up to 40%. There are so many services that we use today and you might be wondering why. Each of the services has its own use case. Let me give you a concrete example of how we used AWS. AWS has enabled us to build strong capabilities to review real-time data. We use this capability to make matching drivers to passengers efficiently. For example, we pro-actively push information, telling drivers where the demand is high during certain time of the day. What you’re seeing is a demand heat map created on a Monday morning for Singapore at around 8.45am. This is the time that most people leave for work. As you can see, the red dot here in the map represents that the demand is high. As you can see, the demand is high in the center part of Singapore. For those who are familiar with Singapore, you’ll know that’s where most of the housing estates are. We monitor changing custom demands in real-time, and send drivers notifications to go to areas with higher booking demand. For example, there’s another heat map on a Friday evening after work. We can clearly see the difference between Friday night and Monday morning. Friday night hot spots are in the central business district. Monday morning when people go to work, high demand is mostly in the residential areas. this seems obvious, but demand is not always where we expect it to be. We have to track in real-time, so that we can respond quickly when there are unforeseen like weather and public transportation breakdowns. What this means is our drivers get to get increased revenue or they can reduce the numbers they are driving. For consumers, this means that they can book the fastest ride, without having to stand at the side of the road trying to hail a taxi.By using big data, we have been able to increase our allocation rate, which is the matching of drivers to passengers by up to 30%. beyond using data to make Grab bookings more efficient, we want to solve bigger problems of traffic congestion, and also help with urban planning. What do we do with the 100 of millions of GPS data points we get from our drivers fleet? Here’s a screenshot of our open traffic platform, a collaboration between grab and World Bank. In this image, the red means the traffic moves less than 10 km per hour while the dark blue means the traffic moves more than 70-80 km per hour. This screenshot is taken on a peak hour on Tuesday in Singapore’s Central Business Distract. It’s easy to see which roads are smooth flowing and which roads to avoid. City governments have free access to open traffic. They can get real-time traffic condition in the city at one glance. open traffic helps government save costs and manpower on manual monitoring and focus  on issues that matter. It can identify roads to help manage traffic beside areas that need more infrastructure and identify roads with high action rates. AWS has enabled us to manage this multi-petabyte flow of data and leverage it to improve our consumer experience.We’ve been using AWS since our inception and there are many benefits to using AWS but I want to pick three that I would like to call out here. The first one being lean operation scheme. We have fewer than 10 engineers full-time maintaining all the services mentioned before. as a startup, the speed of innovation and growth is key. AWS has allowed us to focus on our users and consumers and not spend time on infrastructure. That’s where AWS enterprise support came in. Even though our user count increased multiple fold, we didn’t have to increase our headcount.Second benefit is awesome scalability. We started small but have grown tremendously over the last 4 years. Our usage of AWS has increased 200 times over the last 4 years but it was never an artificial limitation for us to scale our business. With a couple of button touches, our infrastructure grew with us.Lastly, continuous innovation. We have been using AWS for our analytics platform. it has evolved over the years and gone through several iterations. we started with MySQL, later on we moved to Redshift, now our analytics platform runs on data lake on S3 with EMR and presto. All these was done in AWS without any need to look for another platform. Now we look forward to using Athena as well, this is something that we have been waiting for, looks like it’s coming to Singapore soon, so we’ll be using that as well.Using AWS has enabled Grab Engineering team to focus on consumers, innovating on new ideas, iterating on new features and rolling them out quickly into the hands of the consumers. This has given Grab a competitive advantage in transforming the consumer experience. SEA is growing at a tremendous pace. We have an unprecedented opportunity to build a platform that caters to the mobile-first environment and infrastructure needs. We are working on two main areas: making the baby travel easier, and we’re building a multi-modal transport system that offers the most affordable and convenient option across the mobility spectrum, making the way we pay easier. A payments platform, that is the most affordable and convenient platform to pay for services. Momentous challenges, but with AWS on our side, that’s a singular focus. We believe we are only scratching the surface of what’s possible with Grab.Grab is SEA’s largest homegrown technology company and we want to continue growing and provide better service to our consumers. We’re the number one transport app in the region, but more importantly, how does tomorrow look like? Grab is part of the first wave of the technology startups from SEA, for SEA. And we belong to the first group focused on building the tech ethos ecosystem and using innovation to improve peoples’ lives. We expect most startups to be creative and built in SEA. AWS platforms make barrier to entry low for startups, and to scale when the business scales. We believe to our very core, but we then we are in this journey together to build SEA’s Baidu, Alibaba, and Tencent. If China and India can do it, why cant we? I look forward to hearing success stories of aspiring entrepreneurs among you in the future for work like this. Good luck and thank you.",
        "url": "/driving-southeast-asia-forward-with-aws"
      }
      ,
    
      "how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps": {
        "title": "How to Go from a Quick Idea to an Essential Feature in Four Steps",
        "author": "huang-datan-sien-yi",
        "tags": "[&quot;Data Science&quot;, &quot;Product Management&quot;]",
        "category": "[&quot;Data Science&quot;, &quot;Product&quot;]",
        "content": "How do you work within a startup team and build a quick idea into a key feature for an app that impacts millions of people? It’s one of those things that is hard to understand when you just graduate as an engineer.Software engineer Huang Da and data scientist Tan Sien Yi can explain just that. Huang Da and his team first came up with the idea for a chat function in the Grab app in early 2016 and since the official roll out of GrabChat, the first messaging tool in a mobility services app, more than 78 million messages have been exchanged across the region. Here’s their story on how this feature evolved from a quick idea to an essential feature.1. Identify the ProblemHuang Da: Southeast Asia is a pretty challenging place for an app. We have countries with vastly different internet conditions and infrastructural capabilities. You don’t always have access to Wi-Fi. A lot of people are still using 2G, which has limited bandwidth, slow speeds and the high probability of data packets dropping due to congestion or interference affecting the Wi-Fi signal.With that context in mind, in January 2016, we first started thinking of a new, safe and automated way for drivers and passengers to communicate better. Cities in Southeast Asia change so fast, so being able to communicate makes a big difference if you’re trying to find your driver or passenger.In discussing the problem with my team, one idea jumped out: why don’t we build an in-app chat solution? It’s the safest and most anonymised way to allow passengers and drivers to communicate. Also, if there’s one thing we know, it’s that people in Southeast Asia love to chat, with applications such as WhatsApp, Facebook Messenger and Line being ubiquitous.2. Build an MVP SolutionHuang Da: Once we decided to build GrabChat, we started with a prototype. We could have integrated it with third parties, but building it yourself allows more flexibility and options, as well as the opportunity to scale up down the line.We started with a very simple TCP server, without making use of our architecture or entire back end, because we were expecting challenges to arise in any case. While the basic communication protocol is easy, making sure messages get delivered in the real world, is a different ordeal. The messages going through a TCP connection might get lost; we might have to get up with an ad-layer and that’s just two examples.As a next step, we built an architecture, which made use of the whole Grab infrastructure, extracting out the TCP layer and making it a stand-alone layer.  We decided to design GrabChat as a service: it opens interfaces for other services to create and manage the chat room. After a chat room is created, clients in the same chat room could send messages to each other through TCP messages. Services interacts with GrabChat through internal HTTPS requests, and clients interact with GrabChat through Message Exchange service via Gundam and Hermes, our TCP gateway and message dispatcher.The core component of a GrabChat conversation is the message exchange service, which oversees the delivery of messages to all the recipients. It implements a protocol that involves sufficient handshake acknowledgement to make sure the message arrives. There are multiple ways to design the protocol, but finally we agreed on implementing around the concept of “server only push once”.The difficult part of coming up with the protocol is to decide which part of the system, the client or the server, should handle the message loss. It essentially becomes a push or pull problem: If we handle it on the server, the server needs to keep pushing (spamming) the message until the client acknowledges it; on the other hand, if we handle it on the client’s side, the client needs to poll the server for the latest status and message.We chose not to do with the server push method because a message could remain unacknowledged for many reasons, key reason among them being network issues, but if a server pushes regardless, it might drop into a resend loop and never come out, resulting in a severe loss of resources.On the other hand, if we do it on the client side, we don’t need to worry too much about the extra resource consumption: we only process the requests that reach the backend. From the perspective of a client, it keeps trying to send a message until it receives a response from the server before it times out, or fails to maintain a keep-alive heartbeat with the server. When that happens, it terminates the connection and reconnects. In other words, clients only send requests when needed, which is more friendly to server.3. EvaluateAfter building the initial architecture is when the most time-intensive part comes in. There’s a lot of discussions across different teams, including product manager, team leads, front-end and design around the feature’s impact and ways to mature the design.Data scientist Sien Yi evaluated the impact of GrabChat to give the engineering team the analysis it needed to further improve the product. One hypothesis was that the use of GrabChat would lower the cancellation rates in the Grab app. Sien Yi tested this thesis.Sien Yi: Measuring the effect of GrabChat isn’t just about comparing the cancellations ratios on the Grab app, before and after implementation of the GrabChat feature. For all we know, those who use GrabChat could be the more engaged consumers who are less likely to cancel anyway — even without GrabChat.We approached testing the hypothesis from two sides.Comparing Non-chat vs Chat Bookings of Individual PassengersAs a first line of enquiry, we looked at a sample size of 20,000 passengers who had done a significant number of bookings before GrabChat and continued making a significant number of bookings after GrabChat was introduced.Our research showed that 8 out of 10 passengers cancelled less on bookings where GrabChat was used.  There were still some remaining issues with this analysis though:  One could say that even for the same passenger, they might already be more engaged at a booking level when they use GrabChat.  There might be a selection bias in that we necessarily sample passengers with more experience on the Grab platform in order to measure meaningful differences between their Chat and non-Chat bookings.  We haven’t accounted for driver cancels.Using the Cancellation Prediction ModelThis is where the cancellation prediction model came in. With the data science team, we’ve been building a model that predicts how likely an allocated booking will be cancelled. We trained the model on GrabCar data for September in Singapore (before GrabChat was ever used), and then ran the model on October data (after GrabChat was adopted).  We developed a calibration plot (see above), which put actual cancellation proportions against predicted cancellation figures. The plot above suggests the model predicted that many allocated bookings would have been cancelled had GrabChat not been used. In other words, the data implied the use of GrabChat correlated with a decrease in the likelihood of cancellations.Sien Yi and the data science team confirmed that the use of GrabChat is correlated with lower cancellation rates, meaning that the experience of passengers and drivers has been improved by the introduction of GrabChat.4. IterateHuang Da: While the first protocol was built in March 2016, we’ve had many evaluation and iteration sessions before and after GrabChat was made available to all users in September/October. Together with the product manager, we built a roadmap with updates far beyond the first set of protocols.For example, one of our insights from the first tests with the communications protocol was that the driver needs to be able to continue driving and not get distracted by the messages. To make it easier for our drivers to deal with the messages, we built template messages such as “I’m here” or “I’ll be there in 2 minutes”, which created a serious uptick in the volume of messages.Building a product which is essential to our business is a never-ending project. We’re never “done”. Instead, we continue to look for iterations and solutions which serve our passengers and drivers in the best way possible.",
        "url": "/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps"
      }
      ,
    
      "troubleshooting-unusual-aws-elb-5xx-error": {
        "title": "Troubleshooting Unusual AWS ELB 5XX Error",
        "author": "dharmarth-shahryan-law",
        "tags": "[&quot;AWS&quot;, &quot;Networking&quot;]",
        "category": "",
        "content": "This article is part one of a two-part series (part two). In this article, we explain the ELB 5XX errors which we experience without an apparent reason. We walk you through our investigative process and show you our immediate solution to this production issue. In the second article, we will explain why the non-intuitive immediate solution works and how we eventually found a more permanent solution.Triggered: [Gothena] Astrolabe failed (Warning), an alert from Datadog that we have been seeing very often in our #tech-operations slack channel. This alert basically tells us that Gothena 1 is receiving ELB 2 HTTP 5xx 3 errors when calling Astrolabe 4. Because of how frequently we update our driver location data, losing one or two updates of a single driver has never really been an issue for us at Grab. It was only when this started creating a lot of noise for our on call engineers, we decided that it was time to dig into it and fix it once and for all.Here is a high level walkthrough of the systems involved. The driver app would connect to the Gothena Service ELB. Requests are routed to Gothena service. Gothena sends location update related requests to Astrolabe.  Hopefully the above gives you a better understanding of the background before we dive into the problem.Clues from AWSIf you have ever taken a look at the AWS ELB dashboards, you will know that it shows a number of interesting metrics such as SurgeQueue 5, SpillOver 6, RequestCount, HealthyInstances, UnhealthyInstances and a bunch of other backend metrics. As you see below, every time we receive one of the Astrolabe failed alerts, the AWS monitors would show that the SurgeQueue is filling up, SpillOver of requests is happening and that the average latency 7 of the requests increase. Interestingly, this situation would only persist for 1-2 minutes during our peak hours and only in one of the two AWS Availability Zones (AZ) that our ELBs are located in.Cloudwatch Metrics                                                                                                                                                          Previous            Next  There are a few interesting points worth noting in above metrics:  There are no errors from backend i.e. no 5XX or 4XX errors.  Healthy and unhealthy instance count do not change i.e. all backend instances are healthy and serving the ELB.  Backend 2XX count drops significantly i.e requests are not reaching backend instances.  RequestCount drops significantly. It adds further proof of the above point that requests are not reaching the backend instances.By jumping into the more detailed CloudWatch metrics, we are able to further confirm from our side that there is an uneven distribution of requests across the two different AZs. When we reach out to AWS’ tech support, they confirm that one of the many ELB nodes is somehow preferred and is causing a load imbalance across ELB nodes that in turn causes a single ELB node to occasionally fail and results in the ELB 5xx errors that we are seeing.What is Happening?Having confirmation of the issue from AWS is a start. Now we can confidently say that our monitoring systems are working correctly – something that is always good to know. After some internal discussions, we then came up with some probable causes:  ELB is not load balancing correctly (Astrolabe ELB)  ELB is misconfigured (Astrolabe ELB)  DNS/IP caching is happening on the client side (Gothena)  DNS is misconfigured and is not returning IP(s) in a round-robin manner (AWS DNS Server)We once again reach out to AWS tech support to see if there are any underlying issues with ELB when running at high loads (we are serving upwards for 20k request per second on Astrolabe). In case you’re wondering, AWS ELB is just like any other web service, it can occasionally not work as expected . However, in this instance, they confirm that there are no such issues at this point.Moving on to the second item on the list – ELB configurations. When configuring ELBs, there are a couple of things that you would want to look out for: make sure that you are connecting to the right backend ports, your Route 53 8 configuration for the ELB is correct and the same goes for the timeout settings. At one point, we suspected that our Route 53 configuration was not using CNAME records when pointing to the ELB but it turns out that for the case of ELBs, AWS actually provides an Alias Record Set that is essentially the same as a CNAME but with the added advantages of being able to reflect IP changes on the DNS server more quickly and not incurring additional ingress/egress charges for resolving Alias Record Set. Please refer to this to learn more about CNAME vs Alias record set.Having eliminated the possibility of a misconfiguration on the ELB, we move on to see if Gothena itself is doing some sort of IP caching or if there is some sort DNS resolution misconfiguration that is happening on the service itself. While doing this investigation, we notice the same pattern in all other services that are calling Astrolabe (we record all outgoing connections from our services on Datadog). It just so happens that because Gothena is responsible for the bulk of the requests to Astrolabe that the problem is more prominent here than on other services. Knowing this, allows us to narrow the scope down to either a library that is used by all these services or some sort of server configuration that we were applying across the board. This is where things start to get a lot more interesting.A Misconfigured Server? Is it Ubuntu? Is it Go?Here at Grab, all of our servers are running on AWS with Ubuntu installed on them and almost all our services are written in Go, which means that we have a lot of common setup and code between services.The first thing that we check is the number of connections created from one single Gothena instance to each individual ELB node. To do this, we first use the dig command to get the list of IP addresses to look for:$ dig +short astrolabe.grab.com172.18.2.38172.18.2.209172.18.1.10172.18.1.37Then, we proceed with running the netstat command to get connection counts from the Gothena instance to each of the ELB IPs retrieved above.netstat | grep 172.18.2.38 | wc -l; netstat | grep 172.18.2.209 | wc -l; netstat | grep 172.18.1.10 | wc -l; netstat | grep 172.18.1.37 | wc -l;And of course, the output of the command above shows that 1 of the 4 ELB nodes is preferred and the numbers are heavily skewed towards that one single node.[0;32m172.18.1.9 | SUCCESS | rc=0 &gt;&gt;00580[0m[0;32m172.18.1.34 | SUCCESS | rc=0 &gt;&gt;00925[0m[0;32m172.18.2.137 | SUCCESS | rc=0 &gt;&gt;010000[0m[0;32m172.18.1.18 | SUCCESS | rc=0 &gt;&gt;00590[0m[0;32m172.18.1.96 | SUCCESS | rc=0 &gt;&gt;00495[0m[0;32m172.18.2.22 | SUCCESS | rc=0 &gt;&gt;100000[0m[0;32m172.18.2.66 | SUCCESS | rc=0 &gt;&gt;100000[0m[0;32m172.18.2.50 | SUCCESS | rc=0 &gt;&gt;100000[0mHere is the sum of total connections to each ELB node from all Gothena instances. This also explains an uneven distribution of requests across the two different AZs with 1b serving more requests than 1a.172.18.2.38 -&gt; 84172.18.2.209 -&gt; 66172.18.1.10 -&gt; 138172.18.1.37 -&gt; 87And just to make sure that we did not just end up with a random outcome, we ran the same netstat command across a number of different services that are running on different servers and codebases. Surely enough, the same thing is observed on all of them. This narrows down the potential problem to either something in the Go code, in Ubuntu or in the configurations. With this newfound knowledge, the first thing that we look into is whether Ubuntu is somehow caching the DNS results. This quickly turned into a dead end as DNS results are never cached on Linux by default, it would only be cached if we are running a local DNS server like dnsmasq.d or have a modified host file which we do not have.The next thing to do now is to dive into the code itself. And to do that, we spin up a new EC2 instance in a different subnet (this is important later on) but with the same configuration as the other servers to run some tests.To help narrow down the problem points, we do some tests using cURL and a programme in Go, Python and Ruby to try out the different scenarios and check consistency. While running the programs, we also capture the DNS TCP packets (by using the tcpdump command below) to understand how many DNS queries are being made by each of the program. This helps us to understand if any DNS caching is happening.$ tcpdump -l -n port 53Curiously, when running the 5 requests to a health check URL from Go, Ruby, and Python, we see that cURL, Ruby and Python make 5 different DNS queries while Go only makes 1 DNS query. It turned out that cURL, Ruby and Python create new connections for each request by default while Go uses the same connection for multiple requests by default. The tests show that the DNS is correctly returning the IP addresses list in a round robin manner as cURL, Ruby, Python and Go programs were all making connections to both the IPs in an even manner. Note: Because we are running the tests on a different isolated environment, there are only 2 Astrolabe ELB nodes instead of the earlier 4.For simplicity the curl and tcpdump output is shown here:dharmarth@ip-172-21-12-187:~$ dig +short astrolabe.grab.com172.21.2.115172.21.1.107dharmarth@ip-172-21-12-187:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.1.107...* Connected to astrolabe.grab.com (172.21.1.107) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Mon, 09 Jan 2017 11:19:00 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-12-187:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Mon, 09 Jan 2017 11:19:01 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-12-187:~$ sudo tcpdump -l -n port 53tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes09:29:37.906017 IP 172.21.12.187.37107 &gt; 172.21.0.2.53: 19598+ A? astrolabe.grab.com. (43)09:29:37.906030 IP 172.21.12.187.37107 &gt; 172.21.0.2.53: 41742+ AAAA? astrolabe.grab.com. (43)09:29:37.907518 IP 172.21.0.2.53 &gt; 172.21.12.187.37107: 41742 0/1/0 (121)09:29:37.909391 IP 172.21.0.2.53 &gt; 172.21.12.187.37107: 19598 2/0/0 A 172.21.1.107, A 172.21.2.115 (75)09:29:43.109745 IP 172.21.12.187.59043 &gt; 172.21.0.2.53: 13434+ A? astrolabe.grab.com. (43)09:29:43.109761 IP 172.21.12.187.59043 &gt; 172.21.0.2.53: 63973+ AAAA? astrolabe.grab.com. (43)09:29:43.110508 IP 172.21.0.2.53 &gt; 172.21.12.187.59043: 13434 2/0/0 A 172.21.2.115, A 172.21.1.107 (75)09:29:43.110575 IP 172.21.0.2.53 &gt; 172.21.12.187.59043: 63973 0/1/0 (121)The above tests make things even more interesting. We carefully kept the testing environment close to production in hopes of reproducing the issue yet everything seems to be working correctly. We run tests from the same OS image, same version of Golang, with the same HTTP client code and the same server configuration, but the issue of preferring a particular IP never happens.How about running the tests on one of the staging Gothena instance? For simplicity, we’ll show curl and tcpdump output which is indicative of the issue faced by our Go service.dharmarth@ip-172-21-2-17:~$ dig +short astrolabe.grab.com172.21.2.115172.21.1.107dharmarth@ip-172-21-2-17:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Fri, 06 Jan 2017 11:07:16 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-2-17:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.stg-myteksi.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Fri, 06 Jan 2017 11:07:19 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-2-17:~# tcpdump -l -n port 53 | grep -A4 -B1 astrolabetcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes11:10:00.072042 IP 172.21.0.2.53 &gt; 172.21.2.17.51937: 25522 2/0/0 A 172.21.3.78, A 172.21.0.172 (75)11:10:01.893912 IP 172.21.2.17.28047 &gt; 172.21.0.2.53: 11695+ A? astrolabe.grab.com. (43)11:10:01.893922 IP 172.21.2.17.28047 &gt; 172.21.0.2.53: 13413+ AAAA? astrolabe.grab.com. (43)11:10:01.895053 IP 172.21.0.2.53 &gt; 172.21.2.17.28047: 13413 0/1/0 (121)11:10:02.012936 IP 172.21.0.2.53 &gt; 172.21.2.17.28047: 11695 2/0/0 A 172.21.1.107, A 172.21.2.115 (75)11:10:04.242975 IP 172.21.2.17.51776 &gt; 172.21.0.2.53: 54031+ A? kinesis.ap-southeast-1.amazonaws.com. (54)11:10:04.242984 IP 172.21.2.17.51776 &gt; 172.21.0.2.53: 49840+ AAAA? kinesis.ap-southeast-1.amazonaws.com. (54)--11:10:07.397387 IP 172.21.0.2.53 &gt; 172.21.2.17.18405: 1772 0/1/0 (119)11:10:08.644113 IP 172.21.2.17.12129 &gt; 172.21.0.2.53: 27050+ A? astrolabe.grab.com. (43)11:10:08.644124 IP 172.21.2.17.12129 &gt; 172.21.0.2.53: 3418+ AAAA? astrolabe.grab.com. (43)11:10:08.644378 IP 172.21.0.2.53 &gt; 172.21.2.17.12129: 3418 0/1/0 (121)11:10:08.644378 IP 172.21.0.2.53 &gt; 172.21.2.17.12129: 27050 2/0/0 A 172.21.2.115, A 172.21.1.107 (75)11:10:08.999919 IP 172.21.2.17.12365 &gt; 172.21.0.2.53: 55314+ A? kinesis.ap-southeast-1.amazonaws.com. (54)11:10:08.999928 IP 172.21.2.17.12365 &gt; 172.21.0.2.53: 14140+ AAAA? kinesis.ap-southeast-1.amazonaws.com. (54)^C132 packets captured136 packets received by filter0 packets dropped by kernelIt didn’t work as expected in cURL. There is no IP caching, cURL is making DNS queries. We can see DNS is returning output correctly as per round robin. But somehow it’s still choosing the same one IP to connect to.With all that, we have indirectly confirmed that the DNS round robin behaviour is working as expected and thus leaving us with nothing else left on the list. Everybody that participated in the discussion up to this point was equally dumbfounded.After that long fruitless investigation, one question comes to mind. Which IP address will get the priority when the DNS results contain more than one IP address? A quick search on Google gives the following StackOverflow result with the following snippet:  A DNS server resolving a query, may prioritize the order in which it uses the listed servers based on historical response time data (RFC1035 section 7.2). It may also prioritize by closer sub-net (I have seen this in RFC but don’t recall which). If no history or sub-net priority is available, it may choose by random, or simply pick the first one. I have seen DNS server implementations doing various combinations of above.Well, that is disappointing, no new insights to preen from that. Having spent the whole day looking at the same issue, we were ready to call it a night while having the gut feeling that something must be misconfigured on the servers.If you are interested in finding the answers from the clues above, please hold off reading the next section and see if you can figure it out by yourself.BreakthroughComing in fresh from having a good night’s sleep, the issue managed to get the attention of even more Grab engineers that happily jumped in to help investigate the issue together. Then the magical clue happened, someone with an eye for networking spotted that the requests were always going to the ELB node that has the same subnet as the client that was initiating the request. Another engineer then quickly found RFC 3484 that talked about sorting of source and destination IP addresses. That was it! The IP addresses were always being sorted and that resulted in one ELB node getting more traffic than the rest.Then an article surfaced that suggests disabling IPv6 for C-based applications. We quickly try that with our Go programme which does not work. But when we then try running the same code with Cgo 9 enabled as the DNS resolver it leads to success! The request count to the different ELB nodes is now properly balanced. Hooray!If you have been following this post, you would have figured that the issue is impacting all of our internal services. But as stated earlier, the load on the other ELBs is not high as Astrolabe. So we do not see any issues with the other services, The traffic to Astrolabe has been steadily increasing over the past few months, which might have hit some ELB limits and causing 5XX errors.Alternatives Considered:  Move Gothena instances into a different subnet  Move all ELBs into a different subnet  Use service discovery to connect internal services and bypass ELB  Use weighted DNS + bunch of other config to balance the loadAll the 4 solutions could solve our problem too but seeing how disabling IPv6 and using Cgo for DNS resolution required the least effort, we went with that.Stay tuned for part 2 which will go into detail about the RFC, why disabling IPv6 and using Cgo works as well as what our plans are for the future.Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.Footnotes            Gothena – An internal service that is in-charge of all driver communications logic. &#8617;              AWS ELB – AWS Elastic Load Balancer, a load balancing service that is offered by AWS. There can be more than one instance representing an AWS ELB. DNS RoundRobin is used to distribute connections among AWS ELB instances. &#8617;              ELB HTTP 5xx errors – An HTTP 5xx error that is returned by the ELB instead of the backend service. &#8617;              Astrolabe – An internal service that is in charge of storing and processing all driver location data. &#8617;              ELB SurgeQueue - The number of requests that are pending routing. &#8617;              ELB SpillOver - The total number of requests that were rejected because the surge queue is full. &#8617;              ELB Latency - The time elapsed, in seconds, after the request leaves the load balancer until the headers of the response are received. &#8617;              AWS Route 53 - A managed cloud DNS solution provided by AWS. &#8617;              Cgo - Cgo enables the creation of Go packages that call C code. &#8617;      ",
        "url": "/troubleshooting-unusual-aws-elb-5xx-error"
      }
      ,
    
      "scaling-like-a-boss-with-presto": {
        "title": "Scaling Like a Boss with Presto",
        "author": "aneesh-chandra",
        "tags": "[&quot;Analytics&quot;, &quot;AWS&quot;, &quot;Data&quot;, &quot;Storage&quot;]",
        "category": "",
        "content": "A year ago, the data volumes at Grab were much lower than the volume we currently use for data-driven analytics. We had a simple and robust infrastructure in place to gather, process and store data to be consumed by numerous downstream applications, while supporting the requirements for data science and analytics.Our analytics data store, Amazon Redshift, was the primary storage machine for all historical data, and was in a comfortable space to handle the expected growth. Data was collected from disparate sources and processed in a daily batch window; and was available to the users before the start of the day. The data stores were well-designed to benefit from the distributed columnar architecture of Redshift, and could handle strenuous SQL workloads required to arrive at insights to support out business requirements.  While we were confident in handling the growth in data, what really got challenging was to cater to the growing number of users, reports, dashboards and applications that accessed the datastore. Over time, the workloads grew in significant numbers, and it was getting harder to keep up with the expectations of returning results within required timelines. The workloads are peaky with Mondays being the most demanding of all. Our Redshift cluster would struggle to handle the workloads, often leading to really long wait times, occasional failures and connection timeouts. The limited workload management capabilities of Redshift also added to the woes.In response to these issues, we started conceptualising an alternate architecture for analytics, which could meet our main requirements:  The ability to scale and to meet the demands of our peaky workload patterns  Provide capabilities to isolate different types of workloads  To support future requirements of increasing data processing velocity and reducing time to insightSo We Built the Data LakeWe began our efforts to overcome the challenges in our analytics infrastructure by building out our Data Lake. It presented an opportunity to decouple our data storage from our computational modules while providing reliability, robustness, scalability and data consistency. To this effect, we started replicating our existing data stores to Amazon’s Simple Storage Service (S3), a platform proven for its high reliability, and widely used by data-driven companies as part of their analytics infrastructure.The data lake design was primarily driven by understanding the expected usage patterns, and the considerations around the tools and technologies allowing the users to effectively explore the datasets in the data lake. The design decisions were also based on the data pipelines that would collect the data and the common data transformations to shape and prepare the data for analysis.The outcome of all those considerations were:  All large datasets were sharded/partitioned based on the timestamps, as most of the data analysis involved a specific time range and it gave an almost even distribution of data over a length of time. The granularity was at an hour, since we designed the data pipelines to perform hourly incremental processing. We followed the prescribed technique to build the S3 keys for the partitions, which is using the year, month, day and hour prefixes that are known to work well with big data tools such as Hive and Spark.  Data was stored as AVRO and compressed for storage optimisations. We considered several of the available storage formats - ORC, Parquet, RC File, but AVRO emerged as the elected winner mainly due to its compatibility with Redshift. One of the focus points during the design was to offload some of the heavy workloads run on Redshift to the data lake and have the processed data copied to Redshift.  We relied on Spark to power our data pipelines and handle the important transformations. We implemented a generic framework to handle different data collection methodologies from our primary data sources - MySQL and Amazon Kinesis. The existing workloads in Redshift written in SQL were easy enough to be replicated on Spark SQL with minimal syntax changes. For everything else we relied on the Spark data frame API.  The data pipelines were designed to perform, what we started to term as RDP, Recursive Data Processing. While majority of the data sets handled were immutable such as driver states, availability and location, payment transactions, fare requests and more, we still had to deal with the mutable nature of our most important datasets - bookings and candidates. The life cycle of a passenger booking request goes through several states from the starting point of when the booking request was made, through the assignment of the driver, to the length of the ride until completion. Since we collected data at hourly intervals we had to reprocess the bookings previously collected and update the records in the data lake. We performed this recursively until the final state of the data was captured. Updating data stored as files in the data lake is an expensive affair and our strategy to partition, format and compress the data made it achievable using Spark jobs.  RDP posed another interesting challenge. Most of the data transformation workloads, for example - denormalising the data from multiple sources, required the availability of the individual hourly datasets before the workloads were executed. Managing the workloads to orchestrate complex dependencies at hourly frequencies required a suitable scheduling tool. We were faced with the classic question - to adapt, or to build our own? We chose to build a scheduler that fit the bill.Once we had the foundational blocks defined and the core components in place, the actual effort in building the data lake was relatively low and the important datasets were available to the users for exploration and analytics in a matter of few days to weeks. Also, we were able to offload some of the workload from Redshift to the data lake with EMR + Spark as the platform and computational engine respectively. However, retrospectively speaking, what we didn’t take into account was the adaptability of the data lake and the fact that majority of our data consumers had become more comfortable in using a SQL-based data platform such as Redshift for their day-to-day use of the data stores. Working with the data using tools such as Spark and Zeppelin involved a larger learning curve and was limited to the skill sets of the data science teams.And more importantly, we were yet to tackle our most burning challenge, which was to handle the high workload volumes and data requests that was one of our primary goals when we started. We aimed to resolve some of those issues by offloading the heavy workloads from Redshift to the data lake, but the impact was minimal and it was time to take the next steps. It was time to presto.Gusto with PrestoSQL on Hadoop has been an evolving domain, and is advancing at a fast pace matching that of other big data frameworks. A lot of commercial distributions of the Hadoop platform have taken keen interest in providing SQL capabilities as part of their ecosystem offerings. Impala, Stinger, Drill appear to be the frontrunners, but being on the AWS EMR stack, we looked at Presto as our SQL engine over the data lake in S3.The very first thing we learnt was the lack of support for the AVRO format in Presto. However, that seemed to be the only setback as it was fairly straightforward to adapt Parquet as the data storage format instead of AVRO. Presto had excellent support for Hive metastore, and our data lake design principles were a perfect fit for that. AWS EMR had a fairly recent version of Presto when we started (they have upgraded to more recent versions since). Presto supports ANSI SQL. While the syntax was slightly different to Redshift, we had no problems to adapt and work with that. Most importantly, our performance benchmarks showed results that were much better than anticipated. A lot of online blogs and articles about Presto always tend to benchmark its performance against Hive which frankly doesn’t provide any insights on how well Presto can perform. What we were more interested in was to compare the performance of Presto over Redshift, since we were aiming to offload the Redshift workloads to Presto. Again, this might not be a fair enough comparison since Redshift can be blazingly fast with the right distribution and sort keys in place, and well written SQL queries. But we still aimed to hit at-least 50-60% of the performance numbers with Presto as compared to Redshift, and were able to achieve it in a lot of scenarios. Use cases where the SQL only required a few days of data (which was mostly what the canned reports needed), due to the partitions in the data, Presto performed as well as (if not better than) Redshift. Full table scans involving distribution and sort keys in Redshift were a lot faster than Presto for sure, but that was only needed as part of ad-hoc queries that were relatively rare.We compared the query performance for different types of workloads:  A. Aggregation of data on the entire table (2 Billion records)          Sort key column used in Redshift        B. Aggregation of data with a specific data range (1 week)          Partitioning fields used in Presto        C. Single record fetch  D. Complex SQL query with join between a large table (with date range) and multiple small tables  E. Complex SQL query with join between two large tables (with date range) and multiple small tables  Notes on the performance comparison:  The Presto and Redshift clusters had similar configurations  No other workloads were being executed when the performance tests were run.Although Presto could not exceed the query performance of Redshift in all scenarios, we could divide the workloads across different Presto clusters while maintaining a single underlying storage layer. We wanted to move away from a monolithic multi-tenant to a completely different approach of shared-data multi-cluster architecture, with each cluster catering to a specific application or a type of usage or a set of users. Hosting Presto on EMR provided us with the flexibility to spin up new clusters in a matter of minutes, or scale existing clusters during peak loads.With the introduction of Presto to our analytics stack, the architecture now stands as depicted:  From an implementation point of view, each Presto cluster would connect to a common Hive metastore built on RDS. The Hive metastore provided the abstraction over the Parquet datasets stored in the data lake. Parquet is the next best known storage format suited for Presto after ORC, both of which are columnar stores with similar capabilities. A common metastore meant that we only had to create a Hive external table on the datasets in S3 and register the partitions once, and all the individual presto clusters would have the data available for querying. This was both convenient and provided an excellent level of availability and recovery. If any of the cluster went down, we would failover to a standby Presto cluster in a jiffy, and scale it for production use. That way we could ensure business continuity and minimal downtime and impact on the performance of the applications dependant on Presto.The migration of workloads and canned SQL queries from Redshift to Presto was time consuming, but all in all, fairly straightforward. We built custom UDFs for Presto to simplify the process of migration, and extended the support on SQL functions available to the users. We learnt extensively about writing optimised queries for Presto along the way. There were a few basic rules of thumb listed below, which helped us achieve the performance targets we were hoping for.  Always rely on the time-based partition columns whenever querying large datasets. Using the partition columns restricts the amount of data being read from S3 by Presto.  When joining multiple tables, ordering the join sequences based on the size of the table (from largest to the smallest) provided significant performance benefits and also helped avoid skewness in the data that usually leads to “exceeds memory limit” exceptions on Presto.  Anything other than equijoin conditions would cause the queries to be extremely slow. We recommend avoiding non equijoin conditions as part of the ON clause, and instead apply them as a filter within the WHERE clause wherever possible.  Sorting of data using ORDER BY clauses must be avoided, especially when the resulting dataset is large.  If a query is being filtered to retrieve specific partitions, use of SQL functions on the partitioning columns as part of the filtering condition leads to a really long PLANNING phase, during which Presto is trying to figure out the partitions that need to be read from the source tables. The partition column must be used directly to avoid this effect.Back on the HighwayIt has been a few months since we have adopted Presto as an integral part of our analytics infrastructure, and we have seen excellent results so far. On an average we cater to 1500 - 2000 canned report requests a day at Grab, and support ad-hoc/interactive query requirements which would most likely double those numbers. We have been tracking the performance of our analytics infrastructure since last year (during the early signs of the troubles). We hit the peak just before we deployed Presto into our production systems, and the migration has since helped us achieve a 400% improvement in our 90th percentile numbers. The average execution times of queries have also improved significantly, and we have successfully eliminated the high wait times that were associated with the Redshift workload manager during periods with large numbers of concurrent requests.ConclusionAdding Presto to our stack has give us the boost we needed to scale and meet the growing requirements for analytics. We have future-proofed our infrastructure by building the data lake, and made it easier to evaluate and adapt new technologies in the big data space. We hope this article has given you insights in Grab’s analytics infrastructure. We would love to hear your thoughts or your experience, so please do leave a note in the comments below.Many thanks to Edwin Law who reviewed drafts and waited patiently for it to be published.",
        "url": "/scaling-like-a-boss-with-presto"
      }
      ,
    
      "deep-dive-into-ios-automation-at-grab-continuous-delivery": {
        "title": "Deep Dive into iOS Automation at Grab - Continuous Delivery",
        "author": "sun-xiangxinpaul-meng",
        "tags": "[&quot;Continuous Delivery&quot;, &quot;iOS&quot;, &quot;Mobile&quot;, &quot;Swift&quot;]",
        "category": "",
        "content": "This is the second part of our series “Deep Dive into iOS Automation at Grab”, where we will cover how we manage continuous delivery. The first article is available here.As a common solution to the limitations of an Apple developer account’s device whitelist, we use an enterprise account to distribute beta apps internally. There are 4 build configurations per target:Adhoc QA - Most frequently distributed builds for mobile devs and QAs whose devices present in the ad hoc provisioning profile.Hot Dogfood - Similar to adhoc QA (both have debug options to connect to a staging environment) but signed under an enterprise account. This build is meant for backend devs to test out their APIs on staging.Dogfood - Company-wide beta testing that includes both the online and offline team. This is often released when new features are ready or accepted by QA. It can also be a release candidate before we submit to the App Store.Testflight - Production regression testing for QA team. The accepted build will be submitted to the App Store for release.The first 3 are distributed through Fabric. The last one is, of course, distributed through iTunes Connect. Archiving is done simply through bash scripts. Why did we move away from Fastlane? First of all, our primary need is archiving. We don’t really need a bunch of other powerful features. The scripts simply perform clean build and archive actions using xcodebuild. Each of them is less than 100 lines. Secondly, it’s so much easier and flexible for us to customize our own script. E.g. final modifications to the code before archiving. Lastly, we have one less dependency. That means one less step to provision a new server.Server-side SwiftNow whenever we need a new build we simply execute a script. But the question is, who should do it? It’s clearly not an option to login to the build machine and do it manually. So again, as a whole bunch of in-house enthusiasts, we wrote a simple app using server-side Swift. The first version was implemented by our teammate Paul Meng. It has gone through a few iterations over time.The app integrates with SlackKit using Swift Package Manager and listens to the command from a Slackbot @iris. (In case you were wondering, Iris is not someone on the team. Iris is the reverse of Siri 🙊. We love Iris.)    Irisbot is a Swift class that conforms to messageEventsDelegate protocol offered by SlackKit. When it receives a message, we parse the message and enqueue a job into a customised serialised DispatchQueue. Here are a few lines of the main logic.func received(_ message: Message, client: Client) {  // Interpret message to get the command and sanitize user inputs...  // Schedule a job.  archiveQueue.async {    // Execute scripts based on command.    shell(\"bash\", \"Scripts/\\(jobType.executableFileName)\", branch)    // Notify Slack channel when job is done.    client.webAPI.sendMessage(channel: channel, text: \"job \\(jobID) completed\",  }  // Send ACK to the channel.  client.webAPI.sendMessage(channel: channel, text: \"building... your job ID is \\(jobID)\", ...)}Now if anyone needs a build they can trigger it themselves. 🎉    Literally anyoneDeploymentsWe sometimes add new features to @iris or modify build scripts. How to deploy those changes? We did it with a little help of Capistrano. Here is how:The plain Iris project looks like this:├── Package.swift├── Package.pins├── Packages├── Sources│   └── main.swift└── ScriptsAdditional files after Capistrano look like this:├── Gemfile├── Gemfile.lock├── Capfile├── config│   ├── deploy│   │   └── production.rb│   └── deploy.rb└── lib    └── capistrano            └── tasksIris doesn’t have a staging environment. So simply config the server IPs in production.rb:server 'x.x.x.x', user: 'XCode Server User Name'And then a set of variables in deploy.rb:set :application, \"osx-server\"set :repo_url, \"git@github.com:xxx/xxxxx.git\"set :deploy_to, \"/path/to/wherever\"set :keep_releases, 2ask :branch, `git rev-parse --abbrev-ref HEAD`.chompappend :linked_files, \"config.json\"linked_files will symlink any file in the shared/ folder on the server into the current project directory. Here we linked a config.json which consists of the path to the iOS passenger app repo on the server and where to put the generated .xcarchive and .ipa files. So that people can pass in a different value in their local machine when they want to test out their changes.We are all set. How simple is that! To deploy 🚀, simply execute cap production deploy.Screwed up? cap production deploy:rollback will rescue.ConclusionWhat Grab has now, isn’t the most mature setup (there is still a lot to consider. e.g. scaling, authorisation, better logging etc.), but it serves our needs at the moment. Setting up a basic working environment is not hard at all, it took an engineer slightly over a week. Every team and product has its unique needs and preferences, so do what works for you! We hope this article has given you some insights on some of the decisions made by the iOS team at Grab. We would love to hear about your experience in the comments below.Happy automating!",
        "url": "/deep-dive-into-ios-automation-at-grab-continuous-delivery"
      }
      ,
    
      "deep-dive-into-ios-automation-at-grab-integration-testing": {
        "title": "Deep Dive into iOS Automation at Grab - Integration Testing",
        "author": "sun-xiangxin",
        "tags": "[&quot;Continuous Integration&quot;, &quot;iOS&quot;, &quot;Mobile&quot;, &quot;Testing&quot;]",
        "category": "",
        "content": "This is the first part of our series “Deep Dive into iOS Automation at Grab”, where we will cover testing automation in the iOS team. The second article is available here.Over the past two years at Grab, the iOS passenger app team has grown from 3 engineers in Singapore to 20 globally. Back then, each one of us was busy shipping features and had no time to set up a proper automation process. It was common to hear these frustrations from the team:Travis Failed Again But it Passes in My LocalThere was a time when iOS 9 came out and Travis failed for us for every single integration. We tried emailing their support but the communication took longer than we would have liked, and ultimately we didn’t manage to fix the issue in time.Fastlane Chose the Wrong Provisioning Profile AgainWe relied on Fastlane for quite some time and it is a brilliant tool. However, there was a time when some of us had issues with provisioning profiles constantly. Why and how we moved away from Fastlane will be explained later.Argh, if More People Tested in Production Before the Release, This Crash Might have been CaughtPrior to the app release, we do regression testing in a production environment. In the past, this was done almost entirely by our awesome QA team via Testflight distributions exclusively. That meant it was hard to cover all combinations of OSes, device models, locations and passenger account settings. We had prior incidents that only happened to a particular phone model, operating system, etc. Those gave us motivation to install a company-wide dogfooding program.If you can relate to any of the above. This article is for you. We set up and developed most of the stuff below in-house, hence if you don’t have the time or manpower to maintain, it is still better to go with third-party services.Testing and distribution are two aspects that we put a lot of effort in automating. Part I will cover how we do integration tests at Grab.Testing - Xcode ServerBesides being a complete Apple fan myself, there are a couple of other reasons why we chose Xcode Server over Travis and Bitrise (which our Android team uses) to run our tests.Faster IntegrationUnlike most cloud services where every test is run in a random box from a macOS farm, at Grab, we have complete control of what machine we connect to. Provisioning a server (pretty much downloading Xcode, a macOS server, combined with some extremely simple steps) is a one-time affair and does not have to be repeated during each integration. e.g. Installing correct version of CocoaPods and command line libraries.Instead of fresh cloning a repository, Xcode Server simply checks out the branch and pulls the latest code. That can save time especially when you have a long commit history.Native Native NativeIt is a lot more predictable. It guarantees that it’s the same OS, same Xcode version, same Swift version. If the tests passes on your Xcode, and on your teammates’ Xcodes, it will pass on the server’s Xcode.Perfect UI Testing Process RecordingThis is the most important reason and is something Travis / Bitrise didn’t offer at the time I was doing my research. When a UI test fails, knowing which line number caused it to fail is simply not enough. You would rather know what exactly happened. Xcode Server records every single step of your integration just like Xcode. You can easily skim through the whole process and view the screenshots at each stage. Xcode 8 even allows you to view a live screen on the Xcode Server while an integration is running.For those of you who are familiar with UI testing on Xcode, you can view the results from the server in the exact same format. Clicking on the eye icon allows you to view the screenshots.  Sounds good! Let’s get started. On the day we got our server, we found creative ways to use it.    Our multi-purpose server ♻️WorkflowThe basic idea is to create a bot when a feature branch is pushed, trigger the bot on each commit and delete the bot after the feature is merged / branch is deleted. Grab uses Phabricator as the main code review tool. We wrote scripts to create and delete the bots as Arcanist post diff (branch is created/updated) and land (branch is merged) hooks.  Some PHP is still required. This is all of it 😹:$botCommand = \"ruby bot.rb trigger $remoteBranchName\";Creating a bot manually is simply a POST request to your server with the bot specifications in body and authentication in headers. You can totally use cURL. We wrote it in Ruby:response = RestClient::Request.execute(  url: XCODE_SERVER_URL,  method: 'post',  verify_ssl: false,  headers: @headers,  payload: body)if response.code == 201  puts \"Successfully created bot #{name}, uuid #{uuid}\"  return JSON.parse(response.body)['_id']else  puts \"Failed to create bot #{name}\"endAs you can see, XCODE_SERVER_URL is configurable. This is how we scale when the team expands.Now the only thing left is to figure out the body payload. It is simple, all the bots and their configurations can be viewed as JSON via the following API. Simply create a bot via Xcode UI and it will reveal all the secrets:curl -k -u username:password https://your.server.com:20343/api/botsApple doesn’t have a lot of documentation on this. For a list of Xcode Server APIs you can try out this list.GotchasWe have been happy with the server most of the time. However, along the way we did discover several downsides:  The simulator that the Xcode Server spins up does not necessarily have customised location enabled. You probably want to mock your locations in code in testing environment.      Installed builds are being updated during each integration and reused. There might be cache issues from previous integrations. Hence, deleting the app in your pre-integration script can be a good idea:    $ xcrun simctl uninstall booted your.bundle.id        Right after upgrading Xcode, you may face some transient issues. An example from what we’ve observed so far is that existing bots often can’t find the simulators that used to be attached to them. Deleting old simulators and configuring new ones will help. That may also require you to change your bot creation script depending on your configuration. Restarting the server machine sometimes helps too.  If you have one machine like us, there will be downtime during the software update. It either introduces inconvenience to your teammates or worse, someone could break master during the downtime.Stay tuned for the second part where we will cover on how we manage continuous delivery.Many thanks to Dillion Tan and Tay Yang Shun who reviewed drafts and waited patiently for it to be published.",
        "url": "/deep-dive-into-ios-automation-at-grab-integration-testing"
      }
      ,
    
      "a-key-expired-in-redis-you-wont-believe-what-happened-next": {
        "title": "A Key Expired in Redis, You Won't Believe What Happened Next",
        "author": "karan-kamath",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "One of Grab’s more popular caching solutions is Redis (often in the flavour of the misleadingly named ElastiCache 1), and for most cases, it works. Except for that time it didn’t. Follow our story as we investigate how Redis deals with consistency on key expiration.A recent problem we had with our ElastiCache Redis involving our Unicorn API, was that we were serving unusually outdated Unicorns to our clients.  Unicorns are in popular demand and change infrequently, and as a result, Grab Unicorns are cached at almost every service level. Unfortunately, consumers typically like having shiny new unicorns as soon as they are spotted, so we had to make sure we bound our Unicorn change propagation time. In this particular case, we found that apart from the usual minuscule DB replication lag, a region-specific change in Unicorns took up to 60 minutes to reach our consumers.  Considering that our Common Data Service (CDS) server cache (5 minutes), CDS client cache (1 minute), Grab API cache (5 minutes), and mobile cache (varies, but insignificant) together accounted for at most ~11 minutes of Unicorn change propagation time, this was a rather perplexing find. (Also, we should really consider an inter-service cache invalidation strategy for this 2.)How We Cache Unicorns at the API LevelSubsequently, we investigated why the Unicorns returned from the API were up to 45 minutes stale, as tested on production. Before we share our findings, let’s go through a quick overview of what the Unicorn API’s ElastiCache Redis looks like.  We have a master node used exclusively for writes, and 2 read-only slaves 3. This is also a good time to mention that we use Redis 2.x as ElastiCache support for 3.x was only added in October 2016.As Unicorns are region-specific, we were caching Unicorns based on locations, and consequently, have a rather large number of keys in this Redis (~5594518 at the time). This is also why we encountered cases where different parts of the same city inexplicably had different Unicorns.So What Gives?As part of our investigation, we tried monitoring the TTLs (Time To Live) on some keys in the Redis.Steps (on the master node):  Run TTL for a key, and monitor the countdown to expiry          Starting from 300 (seconds), it counted down to 0      After expiry, it returned -2 (expected behaviour)        Running GET on an expired key returned nothing  Running a GET on the expired key in a slave returned nothingInterestingly, running the same experiment on the slave yielded different behaviour.Steps (on a slave node):  Run TTL for a key, and monitor the countdown to expiry          Starting from 300 (seconds), it counted down to 0      After expiry, it returned -2 (expected behaviour)        Running GET on an expired key returned data!  Running GET for the key on master returned nothing  Subsequent GETs on the slave returned nothingThis finding, together with the fact that we don’t read from the master branch, explained how we ended up with Unicorn ghosts, but not why.To understand this better, we needed to RTFM. More precisely, we need two key pieces of information.How EXPIREs are Managed Between Master and Slave Nodes on Redis 2.xTo “maintain consistency”, slaves aren’t allowed to expire keys unless they receive a DEL from the master branch, even if they know the key is expired. The only exception is when a slave becomes master 4. So basically, if the master doesn’t send a DEL to the slave, the key (which might have been set with a TTL using the Redis API contract), is not guaranteed to respect the TTL it was set with. This is when you scale to have read slaves, which, apparently, is a shocking requirement in production systems.How EXPIREs are Managed for Keys that aren’t “Gotten from Master”Since every key needs to be deleted on master first, and some of our keys were expired correctly, there had to be a “passive” manner in which Redis was deleting expired keys that didn’t involve an explicit GET command from the client. The manual 5:  Redis keys are expired in two ways: a passive way, and an active way.  A key is passively expired simply when some client tries to access it, and the key is found to be timed out.  Of course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace.  Specifically this is what Redis does 10 times per second:      Test 20 random keys from the set of keys with an associated expire.    Delete all the keys found expired.    If more than 25% of keys were expired, start again from step 1.    This is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25%.  This means that at any given moment the maximum amount of keys already expired that are using memory is at max equal to max amount of write operations per second divided by 4.So that’s 200 keys tested for expiry each second on the master branch, and about 25% of your keys on the slaves guaranteed to be serving dead Unicorns, because they didn’t get the memo.While 200 keys might be enough to make it through a hackathon project blazingly fast, it certainly isn’t fast enough at our scale, to expire 25% of our 5594518 keys in time for Unicorn updates.Doing The MathNumber of expired keys (at iteration 0) = e0Total number of keys = sProbability of choosing an expired key (p) = e0 / sAssuming Binomial trials, the expected number of expired keys chosen in n trials:E = n * pNumber of expired keys for next iteration =e0 - E = e0 - n * (e0 / s) = e0 * (1 - n / s)Number of expired keys at the end of iteration k:ek = e0 * (1 - n / s)kSo to have fewer than 1 expired key,e0 * (1 - n / s)k &lt; 1=&gt; k &lt; ln(1 / e0) / ln(1 - n / s)Assuming we started with 25% keys expired, we plug in:e0 = 0.25 * 5594518, n = 20, s = 5594518We obtain a value of k around 3958395. Since this is repeated 10 times a second, it would take roughly 110 hours to achieve this (as ek is a decreasing function of k).The Bottom LineAt our scale, and assuming &gt;25% expired keys at the beginning of time, it would take at least 110 hours to guarantee no expired keys in our cache.What We Learnt  The Redis author pointed out and fixed this issue in a later version of Redis 6  Upgrade our Redis more often  Pay more attention to cache invalidation expectations and strategy during software designMany thanks to Althaf Hameez, Ryan Law, Nguyen Qui Hieu, Yu Zezhou and Ivan Poon who reviewed drafts and waited patiently for it to be published.Footnotes            ElastiCache is hardly elastic, considering your “scale up” is a deliberate process involving backup, replicate, deploy, and switch, during which time your server is serving peak hour teapots (as reads and writes may be disabled). http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Scaling.html &#8617;              Turns out that streaming solutions are rather good at this, when we applied them to some of our non-Unicorn offerings. (Writes are streamed, and readers listen and invalidate their cache as required.) &#8617;              This, as it turns out, is a bad idea. In case of failovers, AWS updates the master address to point to the new master, but this is not guaranteed for the slaves. So we could end up with an unused slave and a master with reads + writes in the worst case (unless we add some custom code to manage the failover). Best practice is to have read load distributed on master as well. &#8617;              https://redis.io/commands/expire#how-expires-are-handled-in-the-replication-link-and-aof-file &#8617;              https://redis.io/commands/expire#how-redis-expires-keys &#8617;              https://github.com/antirez/redis/issues/1768 (TL;DR: Slaves now use local clock to return null to clients when it thinks keys are expired. The trade-off is the possibility of early expires if a slave’s clock is faster than the master.) &#8617;      ",
        "url": "/a-key-expired-in-redis-you-wont-believe-what-happened-next"
      }
      ,
    
      "how-grab-hires-engineers-in-singapore": {
        "title": "How Grab Hires Engineers in Singapore",
        "author": "daniel-tay",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter.  When was the last time you met someone who was happy with his or her job?Yeah, me too. Complaining about work is probably one of the greatest Singaporean pastimes yet.A recent study conducted by JobStreet found that Singaporean workers were the most dissatisfied in the region. Out of the 7 Asian countries surveyed, Singaporean workers had the lowest average job satisfaction rating at 5.09 out of 10.That’s close to failing, something we don’t take kindly to. Here’s how we measure up:  Simply put, it’s not easy to find a job that you’ll be happy in. Each stage of the hiring process - from attending interviews to negotiating job offers - reveals a bit more information about your future position, but much of it is cloaked in hearsay and secrecy.We, however, are on your side. We want to make the hiring process as transparent as possible so that you, dear reader, will be able to make a more informed choice. After all, this is the job that you’ll spend a good bulk of your time at.For this reason, we’re embarking on a series of articles that will uncover the hiring processes of leading technology companies in Singapore. Let us know how we can improve on this - what other information you’d like to see, which companies you’d like to read about here, and so on.First up, a mobility services company that has raised US$1.4 billion in funding (that we know of) to date - Grab.Interview Process at GrabNot surprisingly, they experience a high volume of inbound candidates for some of their more popular roles, but few make it to the final stage. “On average, it could be as low as 3 to 5 per cent of candidates who start the interview process to reach to offer stage, as our bar for engineering talent is set really high – for good reason!” Rachel explains.From start to end, the number of interview rounds highly depends on the role in question, and how senior the position is. A 100offer user who recently joined Grab tells us that his journey took between three to four weeks, during which he went through the following interview rounds: one phone screen interview with a Human Resources representative, one online coding round, and two rounds of technical tests.The final technical round was conducted with three Grab software engineers in quick succession.In the first cut, Rachel takes a look at a variety of factors to assess if an engineering candidate is suitable or not.  “[First], we take a look at their demonstrated ability in previous projects as listed on GitHub. The complexity of the projects is of interest to us,” she says. “I will seek out their blogs, slideshow presentations, as well as review peer recommendations to ensure I am able to create a more holistic profile of the individual.”On the subject of qualifications, she deems them to be secondary, as “many qualified and suitable candidates for us would not have passed a typical CV screen otherwise.”Technical vs. Cultural Fit  Beyond technical proficiency and competency, however, they also take special care to evaluate if candidates fit Grab’s culture and values:  “To succeed and thrive in a growing company, we want adaptable people, equally balanced with soft and hard skills, who are driven and eager to make a difference to solving and improving transportation in Southeast Asia.”Sounds like a tall order? Bear in mind that only 3 to 5% of candidates actually get an offer.To be part of this select group, Rachel explains that there are some hard and soft skills that she tends to look out for:Hard Skills  Experience developing software that is highly scalable, distributed service geared for low latency read requests  Experience building complex distributed systems - helping our systems to be faster, more scalable, more reliable, better!  Mobile experience - Different than other engineering roles but share a lot of the same attributes, show some interest and knowledge in these areas: applications, data, and mobile UI/UXSoft Skills  Willingness to collaborate  Thoughtful communication style with clearly thought through, logical solutions  Entrepreneurial spirit and a track record of doing whatever it takes to succeedBetween cultural and technical fit, which weighs more heavily in Grab’s hiring process? To Rachel, both are equally important, though cultural fit is critical in sealing the deal.  “No matter how technically capable a candidate is, we will not proceed with a job offer if the team will not enjoy working with the person,” she says. “We are really focused on creating and maintaining a great working culture at Grab!”Rachel uses the example of one of Grab’s principles, “Your problem is my problem.”  “We want people who will take the initiative to offer help to their fellow colleagues.”Grab’s Interview QuestionsFor Rachel, she’s “laser focused on strategic recruitment for mid- to senior- level hires in engineering, and she “expects all our future Grabbers to come with a high level of technical ability.” The questions she asks candidates in the technical rounds follow accordingly:  “For senior leaders, we ask them about the last, or the best technical decisions they have made recently, that had impact on scalability and high availability performant systems; as well as their thought processes around design for solutions for backend microservices, if not, in areas of their pursuant domain.”In addition, our 100offer user recalls that he was fielded more algorithm questions than other interviews that he attended previously.Beyond that, Rachel and her colleagues tend to quiz candidates on their career ambitions, as well as find out whether they have “a good aptitude for learning and collaboration with colleagues from all around the world.” This is necessary as Grab currently has more than 30 nationalities in their ranks.For senior candidates, Rachel will “often ask them their views on their hiring philosophy - how they would hire a good engineer, as well as how they would build a strong, cohesive and high-performing team.”“It is critical that we understand a senior candidate’s management style,” she emphasises.For junior candidates, she would ask questions that help give a sense of their sense of responsibility and interest in being a team owner and manager, as well as their commitment to building a long and successful career with Grab.“Questions we ask are focused on assessing future aptitude for leadership roles, and their analytical skills and thought processes when it comes to solving problems.”Insider TipsAccording to Rachel, there are many opportunities to relocate and work at Grab’s Research &amp; Development Centres in Beijing, Seattle, and Singapore. When relocating candidates, though, she is careful to assess their ability to adapt to a new environment.“I recognise that their entire life can change!” she explains. “For those keen to explore an overseas work opportunity with Grab, do take time to consider and research about living in Singapore. Singapore is a great place for tech talent, as it comes with plenty of opportunities in the tech industry.”Indeed, she’s extremely optimistic about the prospects of those keen on moving to Singapore, where Grab chose to open its US$100 million R&amp;D centre - right in the heart of the Central Business District.  “The city-state hosts a mature tech ecosystem and the abundance of local, regional and global companies is beneficial to tech professionals. What’s more it has been consistently ranked as the top city in the world for technology readiness, transportation, infrastructure, tax and the ease of doing business by PwC’s Cities of Opportunity report.”Furthermore, she believes that working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter. This is due to the scale and speed at which they operate.“I personally wouldn’t trade this experience for anything else right now, and it makes it all the most critical to to have teammates who believe in the same - that we are all fighting a battle to bring lasting benefits and improvements to millions in Southeast Asia!”Grab is one of several leading technology companies hiring technical talent on 100offer’s marketplace. Sign up for 100offer to see what opportunities there are in the market right now.This article was first published on the 100offer blog.",
        "url": "/how-grab-hires-engineers-in-singapore"
      }
      ,
    
      "battling-with-tech-giants-for-the-worlds-best-talent": {
        "title": "Battling with Tech Giants for the World's Best Talent",
        "author": "grab-engineering",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Grab steadily attracts a diverse set of engineers from around the world in its three R&amp;D centres: Singapore, Seattle, and Beijing. Right now, half of Grab’s top leadership team is made up of women and we have attracted people from five continents to work together on solving the biggest challenges for Southeast Asia.30-year-old Grab engineer Brandon Gao was recently approached by a Seattle-based tech giant. Instead of jumping at the chance to relocate and work for this blue chip company, he turned them down immediately. Having worked in Grab for about two years, he understands what makes this company special and recognises the huge impact he could still make within this unicorn start-up.And a huge impact he has made – Brandon was our first engineer in the User Trust team, and his vision and efforts contributed to developing Grab’s risk and fraud detection system. This detection system has since gone on to win awards. It leverages big data and machine learning to now enable the largest mobile transaction volume on any Southeast Asian consumer platform.We spoke to Brandon about his decision to stay at Grab and the reasons behind it.Grab: How long have you worked here and why did you join?Brandon: I joined Grab because the problems we are solving are real world problems that I identify with. I was attracted to the sheer opportunity to learn and grow.I joined in May 2015 and it was a very interesting time to join because we [Grab] had just started the journey of migrating backend services from Node.js and Ruby to Golang. I contributed to some of these core libraries and I converted a few services from Node.js to Golang.One of my most memorable projects was for our data service that allows direct connections with all our drivers out in the field. It all started on a weekend – a brainwave where I was contemplating if I could just rewrite our code in Golang. I managed to build the prototype that very weekend and presented it to my team the following Monday. They absolutely loved it, helped complete the code and we launched it together as one team!This started as a small and simple weekend project, but it is now pivotal to helping us connect our servers directly with all 580,000 drivers across the region (as of January 2017).Grab: You were recently approached by a tech giant to join their team in the US.  Why did you choose to stay with Grab?Brandon: I know that the impact I have at Grab is much bigger than what I can do in the bigger global tech companies. We are still in our early rapid growth stage and there are so many opportunities to grow. Every week is an exciting time at Grab.More importantly, I really enjoy working with my team members!Grab: What are the three things that you love most about your role at Grab?Brandon: Grab has a unique position in Southeast Asia. As the region’s leading mobility services company, we have the opportunity to make life-changing positive experiences in how people commute, live, and pay. To me, this is really exciting and worth all our hard work.Secondly, the amazing talent I get to work with every day! Grab is willing to help our engineers grow and provides us with the resources that we need. Enough said!Ultimately, I really enjoy my role at Grab because I am constantly exposed to new challenges where I actively contribute to its solutions – I imagine this opportunity will be hard to come by at a large, structured and process-heavy company. I take joy in building programs and writing code from scratch. This keeps me motivated and I look forward to continue making a difference.    Brandon Gao (back row, third from left) with the Grab User Trust Team",
        "url": "/battling-with-tech-giants-for-the-worlds-best-talent"
      }
      ,
    
      "zero-downtime-migration": {
        "title": "This Rocket Ain't Stopping - Achieving Zero Downtime for Rails to Golang API Migration",
        "author": "lian-yuanlin",
        "tags": "[&quot;AWS&quot;, &quot;Golang&quot;, &quot;Ruby&quot;]",
        "category": "",
        "content": "Grab has been transitioning from a Rails + NodeJS stack to a full Golang Service Oriented Architecture. To contribute to a single common code base, we wanted to transfer engineers working on the Rails server powering our passenger app APIs to other Go teams.To do this, a newly formed API team was given the responsibility of carefully migrating the public passenger app APIs from the existing Rails app to a new Go server. Our goal was to have the public API hostname DNS point to the new Go server cluster.  Since the API endpoints are live and accepting requests, we developed some rules to maintain optimum stability for the service:      Endpoints have to pass a few tests before being deployed:    a. Rigorous unit tests    b. Load tests using predicted traffic based on data from production environment    c. Staging environment QA testing    d. Production environment shadow testing    Deploying migrated endpoints has to be done one by one  Deploying of each endpoint needs to be progressive  In the event of unforeseen bugs, all deployments must be instantly rolled backWe divided the migration work for each endpoint into the following phases:  Logic migration  Load testing  Shadow testing  Roll outLogic MigrationOur initial plan was to enforce a rapid takeover of the Rails server DNS, before porting the logic. To do that, we would clone the existing Rails repository and have the new Go server provide a thin layer proxy, which resembles this in practice:  Problems with Clone ProxyA key concern for us was the tripling of the HTTP request redirects for each endpoint. Entry into the Go server had to remain HTTP, as it needed to takeover the DNS. However, we recognise it was wasteful to have another HTTP entry at the Rails clone.gRPC was implemented between the Go server and Rails clone to optimise latency. As gRPC runs on HTTP/2 which our Elastic Load Balancer (ELB) did not support, we had to configure the ELB to carry out TCP balancing instead. TCP connections, being persistent, caused a load imbalance amongst our Rails clone instances whenever there was an Auto Scaling Group (ASG) scale event.  We identified 2 ways to solve this.The first was to implement service discovery into our gRPC setup, either by Zookeeper or etcd for client side load balancing. The second, which we adopted and deemed the easier way albeit more hackish, was to have a script slowly restart all the Go instances every time there was an ASG scaling event on the Rails Clone cluster to force a redistribution of connections. It may seem unwieldy, but it got the job done without distracting our team further.Grab API team then discovered that the gRPC RubyGem we were using in our Rails clone server had a memory leak issue. It required us to create a rake task to periodically restart the instances when memory usage reached a certain threshold. Our engineers went through the RubyGem’s C++ code and submitted a pull request to get it fixed.The memory leak problem was then followed by yet another. We noticed mysterious latency mismatches between the Rails clone processes, and the ones measured on the Go server.At this point, we realised no matter how focused and determined we were at identifying and solving all issues, it was a better use of engineering resources to start work on implementing the logic migration. We threw the month-long gRPC work out the window and started with porting over the Rails server logic.Interestingly, converting Ruby code to Go did not pose many issues, although we did have to implement several RubyGems in Go. We also took the opportunity to extract modules from the Rails server into separate services, which allowed for maintenance distribution for the various business logic components to separate engineering teams.Load TestingBefore receiving actual real world traffic, our team performed load testing by dumping all the day logs with the highest traffic in the past month. We proceeded to create a script that would parse the logs and send actual HTTP requests to our endpoint hosted on our staging servers. This ensured that our configurations were adequate for every anticipated traffic, and to verify that our new endpoints were maintaining the Service Level Agreement (SLA).Shadow TestingShadowing involves accepting real-time requests to our endpoints for actual load and logic testing. The Go server is as good as live, but does not return any response to the passenger app. The Rails server processes the requests and responses as usual, but it also sends a copy of the request and response to the Go server. The Go server then process the request and compare the resulting responses. This test was carried out on both staging and production environments.One of our engineers wrote a JSON tokenizer to carry out response comparison, which we used to track any mismatches. All mismatched data was sent to both our statsd server and Slack to alert us of potential migration logic issues.    Statsd error rate tracking on DataDog    Slack pikabot mismatch notificationIdempotency During ShadowIt was easy to shadow GET requests due to its idempotent nature in our system. However, we could not simply carry out the same process when we were shadowing PUT/POST/DELETE requests, as it would result in double data writes.We overcame this by wrapping our data access objects with a layer of mock code. Instead of writing to database, it generates the expected outcome of the database row before comparing with the actual row in the database.  As the shadowing process occurs only after the Rails server has processed the request, we knew database changes existed. Clearly, the booking states may have changed between the Rails processing time and the Go shadow juncture, resulting in a mismatch. For such situations, the occurrence rate was low enough that we could manually debug and verify.Progressive ShadowingThe shadowing process affects the number of outgoing connections the Rails server can make. We therefore had to ensure that we could control the gradual increase in shadow traffic. Code was implemented in the Rails server to check our configuration Redis for how much we would like to shadow, and then throttle the redirection accordingly. Percentage increments seemed intuitive to us at first, but we learnt our mistake the hard way when one of our ELBs started terminating requests due to spillovers.  As exemplified by the illustration above, one of our endpoints had such a huge number of requests that a mere single percent of its requests dwarfed the full load of 5 others combined. Percentages meant nothing without load context. We mitigated the issue when we switched to increments by requests per second (RPS).Prewarming ELBIn addition to switching to RPS increments, we notified AWS Support in advance to prewarm our ELBs. Although the operations of the ELB are within a black box, we can assume that it is built using proprietary scaling groups of Elastic Cloud Compute (EC2) instances. These instances are most likely configured with the following parameters:a. Connection count (network interface)b. Network throughput (memory and CPU)c. Scaling speed (more instances vs. larger instance hardware)This will provide more leeway in increasing RPS during shadow or roll out.Roll OutSimilar to shadowing endpoints, it was necessary to roll out discrete endpoints with traffic control. Simply changing DNS for roll out would require the migrated Go API server to be coded, tested and configured with perfect foresight to instantly take over 100% traffic of all passenger app requests across all 30 over endpoints. By adopting the same method used in shadowing, we could turn on a single endpoint at the RPS we want. The Go server will then be able to gradually take over the traffic before the final DNS switch.Final WordsWe hope this post will be useful for those planning to undertake migrations with similar scale and reliability requirements. If this type of challenges interest you, join our Engineering team!",
        "url": "/zero-downtime-migration"
      }
      ,
    
      "grab-vietnam-careers-week": {
        "title": "Grab Vietnam Careers Week",
        "author": "grab-engineering",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Grab is organising our first ever Grab Vietnam Careers Week in Ho Chi Minh City, Vietnam, from 22 to 26 October 2016. We are eager to have more engineers join our ranks to make a difference to improving transportation and reducing congestion in Southeast Asia. We are now on 23 million mobile devices supported by 460,000 drivers in the region, but we’re only started and have much more to achieve! To find out more about Grab, take a look at our corporate profile at the end of this post.We have a lot of Vietnamese talent delivering features that delight our users on our flagship mobile apps. Read the Q&amp;A with iOS engineer Hai Pham and Android engineer Son Nguyen from our Singapore R&amp;D centre for their perspectives on what it’s like working at Grab. There are even tips for our future Vietnamese Grabbers!    Grab Friends Forever -- our Vietnamese mobile engineers Son Nguyen and Hai Pham (L-R)Tell us what you do at Grab.Hai Pham: I am an iOS engineer with the passenger app team and I’ve been a Grabber for 1.5 years.Son Nguyen: I have more than 4 years experience in Android mobile development and it has been 1.5 years with Grab for me too.Why did you join Grab and what's your most meaningful Grab experience?Hai: I never thought of working for a large, social enterprise such as Grab, but my time here has been purposeful. It feels great knowing that we’re the leader in Southeast Asia for mobility services with the ability to improve livelihoods and make a positive difference to millions.Son: I like having the opportunity to work with awesome people from all over the world. From Mexico to Malaysia, India to Indonesia, Brazil to Belgium, you name it! We are a close-knit bunch and we have fun together. At Grab, you can be sure your ideas are always appreciated. We even have regular company hackathons which we call “Grabathons” where everyone comes together to improve our app and services! What’s more… we love having lunches with the boss and our happy hours every week! Free food and lots of drinks!Hai: For me, it is wonderful to be out in public and seeing people whip out their phones and launching the Grab app to book a ride. That pride I feel is indescribable and I find myself saying: “Yes, we made that.” All the hard work from the teams has led us to develop the best mobility services app in the world – it makes me happy and proud that we are doing great things here and helping millions of people in their daily lives.Both of you will be in Ho Chi Minh City conducting interviews for Grab Vietnam Careers Week from 22-26 October. Any tips for those looking to work with us -- what are we looking for?Hai: If you know how to build and maintain an app, care about quality, understand the importance of testing – we want you! Apart from that, your work attitude is important. We want those with determination, willing to help one another, open-minded with a learning mindset, and have an interest in our profession to keep up-to-date on the latest mobile developments.Son: Let’s not forget we want the best so we want to see your best. You have to pass the codility test first, and for mobile engineers, we want good experience developing for iOS or Android. We’re looking for people who are good with developing clean mobile architecture, testing and maintaining performance.Hai: Curiosity is important, programming is a lifelong learning process. Maintaining standard working hours hardly makes you great.Son: Just be yourself!What advice do you have for engineers looking to move to Singapore and starting a career with Grab?Son: If you get our offer, I suggest you start looking for an apartment. A good place to start is the VNCNUS Forum or Facebook groups.Hai: Fortunately, the Vietnamese community is supportive, and there’s no need to spend any money with a property agent. If you need help, I am sure Son and I can give you more advice.Singapore is very comfortable and convenient. I do joke when colleagues ask me how Singapore living is like: quiet, no surprises, and I was confused when the car stopped for me when I tried to cross the street on my first day.Son: Come to Singapore and you will have a great chance to improve yourself: your salary, your technical skills, your English skills and make friends from all over the world while spending time in a global city.What do you miss most about Vietnam? Any places or things you do in Singapore to provide that quick local fix?Hai: I crave for HCM street food! Although there are a few good Vietnamese restaurants here, you can’t compare with what you get back home. I will recommend Mrs Pho which I frequent every week.Son: Come join us, I’ll tell you lots of amazing things you can explore and try in Singapore to overcome your homesickness! Our Vietnamese friends at Grab are friendly and talented and we often have lunch together. Also, we have colleagues from all over the world to recommend other types of good food to try in Singapore. What’s more, HCM is just 2 hours away by plane!Find out more about Grab Vietnam Careers Week: https://grb.to/vn-careers          Grab Corporate Profile (Click for full image)",
        "url": "/grab-vietnam-careers-week"
      }
      ,
    
      "grabpay-wins-best-fraud-prevention-innovation-at-the-florin-awards": {
        "title": "GrabPay Wins Best Fraud Prevention Innovation at the Florin Awards",
        "author": "foo-wui-ngiap",
        "tags": "[&quot;User Trust&quot;]",
        "category": "",
        "content": "I am honoured to receive the Best Fraud Prevention Innovation (Community Votes) Award at the 2016 Florin Awards on behalf of Grab. For those of you who voted for Grab, we thank you for your support that made this award possible.  User Trust and Safety is paramount to Grab – we uphold the industry’s highest security standards for all cashless transactions that occur on our GrabPay platform. A large part of what underlies this protection is the risk and fraud detection system that we put in place at the launch of GrabPay early this year. It continues to evolve further today, using sophisticated machine learning algorithms that progressively builds on the knowledge we have of our drivers, passengers and their travel patterns to enable the largest mobile transaction volume on any Southeast Asian consumer platform in history.To top that, Grab is now one of the most frequently used mobile platforms in the region with up to 1.5 million bookings daily. The app has been downloaded more than 23 million times!With the strong growth story in mind and the drive to enable a seamless and ubiquitous transaction experience across the region, we at Grab have made it our mandate to provide full protection for all GrabPay transactions to our drivers and passengers – we cover any unauthorised fraudulent transactions on GrabPay. This is a testament and commitment from Grab that we place our users’ trust and security front and centre, and will do what it takes to ensure a seamless and safe transaction experience.We have big aspirations for GrabPay and will continue to provide our consumers greater accessibility, ubiquity and convenience in making safe and secure cashless payments in Southeast Asia. We’d like to thank all GrabPay users who have been on this journey with us, and for the exciting road ahead.",
        "url": "/grabpay-wins-best-fraud-prevention-innovation-at-the-florin-awards"
      }
      ,
    
      "round-robin-in-distributed-systems": {
        "title": "Round-robin in Distributed Systems",
        "author": "gao-chao",
        "tags": "[&quot;Back End&quot;, &quot;Data&quot;, &quot;Distributed Systems&quot;, &quot;ELB&quot;, &quot;Golang&quot;]",
        "category": "",
        "content": "While working on Grab’s Common Data Service (CDS), there was a need to implement client side load balancing between CDS clients and servers. However, I kept encountering persistent connection issues with AWS Elastic Load Balancers (ELB). Hence, I decided to focus my attention on using DNS discovery, as ELB’s performance is not optimal and the unpredictable scaling events could further affect the stability of our systems. At the same time, I didn’t want to have to manage the details of DNS TTL, different protocols, etc. Thus, the search for a reliable DNS library began.Eventually, I found this package after some research: https://github.com/benschw/srv-lb. It looked pretty neat and provides round-robin routing for IP addresses behind a DNS domain, which is exactly what I wanted.During my tests of the round-robin function, it turned out the round-robin didn’t work… We have 7 servers behind our etcd domain but when I tried with the package, it gave me the following sequence of IP addresses:  1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; 7 -&gt; 1 -&gt; 1…It turns out that there was a bug in that library, which I’ve fixed by submitting a pull request.We do implement some round-robin logic in our code too, which can be tricky to get right at times. Read on for a summary of our learnings from the different ways of doing round-robin.Round-robin with MutexThis is the simplest approach you can use to implement round-robin logic.Basically, all you need is an array and a counter in your programme and the use of a lock to protect usage. Here’s some example code in Golang to illustrate the idea:package mainimport \"sync\"// RoundRobin ...type RoundRobin struct {    sync.Mutex    current int    pool    []int}// NewRoundRobin ...func NewRoundRobin() *RoundRobin {    return &amp;RoundRobin{        current: 0,        pool:    []int{1, 2, 3, 4, 5},    }}// Get ...func (r *RoundRobin) Get() int {    r.Lock()    defer r.Unlock()    if r.current &gt;= len(r.pool) {        r.current = r.current % len(r.pool)    }    result := r.pool[r.current]    r.current++    return result}Looks pretty simple? That’s because only one action was defined for this struct – there is nothing complicated to worry about. However, if you want to add Set / Update methods to this struct, be sure to pay more attention to the usage of locks.Round-robin with Your Favourite ChannelAnother approach of implementing a round-robin pool is to use goroutines and channels. The programme is a little bit more complex:package mainimport \"time\"const timeout = 100 * time.Millisecond// RoundRobin ...type RoundRobin struct {    current int    pool    []int    requestQ chan chan int}// NewRoundRobin ...func NewRoundRobin() *RoundRobin {    r := &amp;RoundRobin{        current:  0,        pool:     []int{1, 2, 3, 4, 5},        requestQ: make(chan chan int),    }    go r.balancer()    return r}// Get ...func (r *RoundRobin) Get() int {    output := make(chan int, 1)    select {    case r.requestQ &lt;- output:        return &lt;-output    case &lt;-time.After(timeout):        // Timeout        return -1    }}// balancer ...func (r *RoundRobin) balancer() {    for {        select {        case output := &lt;-r.requestQ:            if r.current &gt;= len(r.pool) {                r.current = 0            }            output &lt;- r.pool[r.current]            r.current++        // other cases can be added here        // e.g. case change := &lt;-r.watch:        }    }}The benefits of this approach:  More granular control over operation timeouts. In the mutex approach, there isn’t a way for you to cancel an operation if it takes too long to complete.  balancer is the one centralised place that controls all your actions. If you add more operations to this struct, just add more cases there and you do not need to worry about the granularity of your locks.The drawbacks of this approach:  Code is more complicated.  Each op takes more time to complete, in the order of nanoseconds, because there are channel creations with each time.SummaryBased on your requirements, pick the preferred method of implementing a simple logic like round-robin.I would pick the mutex implementation for resource fetching and goroutine implementation for work load balancing. Leave a comment if you wish to discuss. I would love to hear your views!References:  https://talks.golang.org/2010/io/balance.go  https://github.com/mindreframer/golang-stuff/blob/master/github.com/youtube/vitess/go/pools/roundrobin.go",
        "url": "/round-robin-in-distributed-systems"
      }
      ,
    
      "why-test-the-design-with-only-5-users": {
        "title": "Why Test the Design with Only 5 Users",
        "author": "avinash-papatla",
        "tags": "[&quot;User Research&quot;, &quot;UX&quot;]",
        "category": "",
        "content": "The reasoning behind small sample sizes in qualitative usability research.The sufficiency and thereby reliability of findings derived from testing a feature with just 5 users is a common concern that various stakeholders of the design process share. Before I can delve into justifying the sample size for feature testing, there are few facets of research I would like share.The problem you are trying to solve defines the research method. The method defines the sample size.Depending on the problem (stage of the design process), user researchers suggest an appropriate methodology that is best suited to uncover insights. For example, if we want to understand what soap people are using, a survey (quantitative) is a recommended approach to reach a large audience in a short span of time. To understand why they use the soap, running a survey alone will not be enough. Beyond the common factors of price, branding, flavour of the soap, there may be others that require deeper understanding (For example, the way the soaps are stored in the shelves of the market). In a contextual inquiry (qualitative), participants are observed in their natural environment over a large period of time from pre purchase to post purchase. As this takes a longer time, we would have a smaller sample size. What the exact number is for a sample size depends on a lot of factors, which is beyond the scope of this paper.    Some common research methods that come to mindAs with everything else in life — budget, time and resources affect the ability to conduct more research and thereby with more users.If you work in a setup that is truly agile, all functions need to iterate on small changes and iterate continuously. This is no different for research. The minimum viable product (MVP) for research is to get as many insights as possible (return), with the least amount of time, resources and participants that are required to deliver reliable and valid results (quality) that are actionable. By stretching any of the above mentioned levers, the research project as a whole would be affected. Identifying the right fit without compromising on the integrity of the research is a challenge researchers face every day as do designers, product managers and engineers in their respective domains.  In an ideal world, we could test with many participants over many days and find as many insights as possible. However, this would slow down and possibly stagnate the design process.    MVP for research is something researchers think aboutThe Magical (or not) Number 5In the year 1993, renowned usability expert, Jakob Nielsen, published a scientific paper to describe his results from analysing multiple usability studies. He established that the probability of identifying more usability issues in the design reduces when you add more participants. With a sample of 5 users, you can uncover 75–80 % of the usability issues in the design. In an agile setup, the aim is to repeat the tests of 5 with iterations of design. The more users you add, the less value you will get in return.    Nielsen's hypothesis on return value of sample sizeWhen can We Test with 5 Users?  The research methodology is usability testing only that is qualitative in nature.  Testing flows, interactions and visuals of the same set of features. (not 2 different apps)When can We Not Test with Just 5 Users?  You are trying to understand opinions and attitudes which would be better addressed via a survey, requiring larger sample sizes.  You are trying to predict future behaviour via data modelling, A/B testing, which cannot be run with 5 users.  You have distinct user segments such as buyers and sellers on an e-commerce app. You will need to consider these two segments as separate.  With 5 users in usability testing, we are not testing the success of the design rather identify possible failures before going to market. It does not uncover “all” issues that people may face, just the more probable. The approach is a qualitative one, different from quant methods.Multiple Approaches to Solving the Same ProblemVarious stakeholders have different approaches. While engineers may think in terms of technical infrastructure, performance and risk, marketing leaders need to think of outreach and consumer acquisition among other things. Business and product leaders need to build the case for the need in the market for a new feature and designers/researchers translate the business vision into tangible outcomes while data driven people measure the success of everyone’s efforts. Although, a simplified version of the roles of stakeholders, the essence is that everyone is working towards solving the same problem. Understanding each other’s approaches and the value they bring will help us be collaborative and give a deeper meaning to the shared vision and success of a product.This post was first published on Medium.",
        "url": "/why-test-the-design-with-only-5-users"
      }
      ,
    
      "programmers-beware-ux-is-not-just-for-designers": {
        "title": "Programmers Beware - UX is Not Just for Designers",
        "author": "corey-scott",
        "tags": "[&quot;API&quot;, &quot;UX&quot;]",
        "category": "",
        "content": "Perhaps one of the biggest missed opportunities in Tech in recent history is UX.Somehow, UX became the domain of Product Designers and User Interface Designers.While they definitely are the right people to be thinking about web pages, mobile app screens and so on, we’ve missed a huge part of what we engineers work on every day: SDKs and APIs.We live in a time where “the API economy” exists and has tangible monetary and strategic value and yet, UX is seldom considered.Additionally, consider how many functions a programmer interacts with every day and yet how little (read: almost none) time is spent on the UX of these functions.What is UX?First let me give you my perspective on UX. UX stands for “User Experience” or to put it another way, “usability”.UX is not black art; you don’t even need to study it. I believe it can be uncovered through logic, persistence and experience.I believe good UX can be discovered using the following “UX Discovery Survey”.Ask yourself (or your team) these 5 quick questions and you will be well on your way to create better UX.  Who/What is the user? - Yes, users can be other systems and not just people.  What do they want to achieve? - Often the answer to this is a list of things, this is fine. However, it’s generally possible to apply the 80/20 rule; meaning users will want to do 1 thing 80% of the time and the rest about 20%. We should always over-optimise for the 80%; even if it means making the 20% a lot more complicated or inconvenient.  What are they capable of? - What skills do they have? What domain knowledge do they have? What kind of experience? When designing systems for others there is often a huge difference between these factors for the user and the creator. This factor shows up a lot more when the answer to “Who/What is the user” is a human and not a system.      What can I do to make their life easier? – This is really the driving force behind UX, focus on the user and how to please them.Is there anything similar out there that the user already knows how to use? – The best interfaces are often ubiquitous or intuitive.  The focus here is on modelling the interface to do what the user expects it to do, without prior training or experience with it. If you ever have access to the end user, try asking them these questions:          “What do you think it should do?”      “What did you expect to happen when you did X?”      Let me show you what I mean with some examples of Engineering UX:A REST API Called from a Mobile ApplicationWhen the app in question starts, it must make a call to the server to login and use the returned credentials to make another to download the latest news.What's wrong with this?This makes 2 round trips to the server, which results in:  2 potential points of failure  Double the network latency  Additional code complexity of handling the additional points of failure  Additional code complexity of handling the “session” between callsFinding a Better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - The user here is not the programmer using the API but the mobile application.  What do they want to achieve? - They want to load the data from the server in the fastest possible manner using the least amount of battery and data as possible.  What are they capable of? - It’s app. It’s capable of whatever the app programmer is capable of.  What can I do to make their life easier? - One call is always going to be easier to code than two.  One point of failure is always easier to handle than two.  Is there anything similar out there that the user already knows how to use? - Not applicable here.Merge the requests together and have the app send either the login credentials or the session as part of the request for news.While the call to the server is slightly more complicated, this is completely overshadowed by the complexity of coordinating 2 calls and failure points that it removes.SolutionYes, this adds some complexity to the server side but the server is significantly easier to test, maintain and update than the mobile app.A REST API Called from a Mobile Application (Redux)Some time passes from the above example and the app is updated and now it needs to download the weather and the news when it starts. In common REST ideology, we consider the news and weather to be separate entities and therefore, the request is to add a separate endpoint in order to be RESTful.What's Wrong with This?We are back to making 2 round trips to the server. But this time they are concurrent, which results in:  2 potential points of failure (again)  Additional code complexity of handling the additional points of failure and partial failures (again)  Paying battery and data charges for 2 calls (again)Finding a Better UXLet’s run through the “UX Discovery Survey”:Unsurprisingly, the answers will be similar to the previous section.However, let’s now also consider the user of the app (in addition to the app as the user of the API)  Who/What is the user? - This time, let’s consider the problem from the app user’s perspective.  What do they want to achieve? - The answer to this question becomes the key to understanding how the app should behave.  Does the user need both pieces of info in an “all or nothing” way?  Would partial info be better than none?  Does the user need all of that info when the app starts or could they wait for retries?  Bigger more complicated calls are bound to take a little longer.  Users these days are fairly used to content that “fills itself in” eventually but they doesn’t mean they like it.  Beyond that, not all information is of equal value to the user. If we are making a news app, the weather may be a “nice to have” for most users.  What are they capable of? - As before.  What can I do to make their life easier? - As before, this is the key. Whatever the user most wants/needs wins.  Is there anything similar out there that the user already knows how to use? - Not applicable here.SolutionSadly, my answer here is “it depends”. I would look to make as few round trips as possible and sacrifice RESTful correctness for performance or a better UX. The focus should always be on the end user and their needs. Both explicit (seeing the data/using the app) and implicit (costing less battery and data).There is often a temptation to follow whatever is easiest or quickest to implement. This is a valid optimisation when you need to get to market as fast as possible but it is also a debt, akin to technical debt, that will need to be paid sooner or later.An RPC APIThis time an internal (behind the firewall) service publishes an RPC API that allows a user to download an eBook. However, this book should only be accessible to certain users.As this service is not publicly accessible, we could ignore the validation and assume that calls to the API are only made in cases where the permission have already been verified.What's Wrong with This?  If the calling system is not aware of whether the user is permitted to perform this action, they will need to load this permission (perhaps from another system) before making the request.  If a second system also needs to make this API call, then the logic to validate the user can perform the action would need to be duplicated into this new system.  Any attempt to cache this permission in the calling systems would likely be inefficient and prone to duplication.Finding a better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - The other systems / API consumers.  What do they want to achieve? - They want to download the book on behalf of their user, if the user is permitted to do so.  What are they capable of? - Anything.  What can I do to make their life easier? - We could take complete ownership of the problem and allow our users to make blind / dumb calls to our API and we take care of everything else.  Is there anything similar out there that the user already knows how to use? - This question needs to asked within the problem space / company you are in. If all of your APIs are trusted then it might be better to follow that style rather than force your users to learn / handle your different way of doing things. Word of caution though: APIs should very often be stateless and require no more knowledge than how to call it; if all of your APIs are trusted then I suggest you raise that issue with your team.SolutionYou could introduce a gateway service between the callers and the destination; however this is likely adding complexity, latency and another service to build, manage and maintain. A generally more effective option is to push the validation logic into the RPC server.This will:  Eliminate any duplication between multiple clients.  Likely improve the overall performance as the storage / caching of the permissions can be optimised for this use-case.  Improve the UX to the users by allowing them to blindly make the request.Code APIsThe general problem here is the fact that code inherently makes more sense to the person writing it, when they are writing it, than it does the others and even to the writer in the future. Seldom do we think about other users when we are writing our functions.Consider the following code:AddBalance(5, false)What does the false indicate?Finding a Better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - Your future self. Your current and future team members.  What do they want to achieve? - They want to use your code so they don’t have to write their own.  What are they capable of? - There are many answers to this question, some nice and some not so nice. Generally, it’s better to assume the skill level is low and so is the domain knowledge.  What can I do to make their life easier? - Personally, I am lazy. This laziness forces me to come from a place of “what interface would allow my future self to use this without thinking or learning?”  Is there anything similar out there that the user already knows how to use? - Consistency in programming style, naming and many other things is programming will go a long way to a better UX. Often people will make the argument that a certain piece of code is “X style” where X is the current programming language or framework. I used to see this as a weak argument but as the teams I worked in got larger, consistency of style (preferably the team’s agreed and published style) has proven extremely valuable in terms of allowing folks to change teams, share code and tips and most importantly learn from each other.SolutionWhat happens to the usability if we replace the boolean parameter with 2 functions?AddBalanceCreateIfMissing(5)AddBalanceFailOnMissing(5)In actual fact, the result will often be 3 functions. These 2 above public / exported functions and the original function / common code as private.Boolean arguments are an easy target but there are many other easy and quick wins, consider this function:var day = toDay(\"Monday\")What happens if we call it like this?var day = toDay(\"MONDAY\")var day = toDay(\"monday\")var day = toDay(\"mon\")These are great examples of “What can I do to make their life easier?”.A good UX would consider all reasonable ways a user might use or misuse the interface and in many cases support them instead of forcing the user to learn and then remember the exact format required.TL;DR  UX is not just about Visual User Interfaces.  APIs and SDKs are also user interfaces.  Programmers are also users.  Other systems are also users.  UX is about designing the interface or interaction from the user’s perspective.  It’s about considering the user’s desires, tendencies and capabilities.  It’s about making the system feel like “it just works”.Finally, I would mention that the best UX are the result of iterative and interactive efforts.The best way to answer the questions of “What do they want to achieve?”, “What are they capable of?” and “What can I do to make their life easier?” is to give the interface to a real user, watch what they do it with it and how. Then respond by making the interface work they way they thought it would instead of teaching them otherwise.It is always better (and easier) to change the UX to match the user than the other way around.",
        "url": "/programmers-beware-ux-is-not-just-for-designers"
      }
      ,
    
      "grab-you-some-post-mortem-reports": {
        "title": "Grab You Some Post-Mortem Reports",
        "author": "lian-yuanlin",
        "tags": "[&quot;Post Mortem&quot;]",
        "category": "",
        "content": "Grab adopts a Service-Oriented Architecture (SOA) to rapidly develop and deploy new feature services. One of the drawbacks of such a design is that team members find it hard to help with debugging production issues that inevitably arise in services belonging to other stakeholders.This can generally be credited to unfamiliarity with code and architecture from other teams. On top of regular alignment meetings, post-mortem reports end up becoming the glue that adheres the different engineering teams together in understanding problems that arise in the monolithic architecture we have.Given the importance of such reports, it was surprising to find numerous incidents recorded as shown:  [2015-02-02][11pm] XXX Service Went Down  At 23:00 hrs, we experienced downtime in XXX service. We looked through the logs and found a bug in DB connections leading to memory leaks.  XXX Service team has pushed a fix and the problem is resolved.Let’s highlight some of the problems with the example given above:  It provides zero context. We know nothing of how the service is designed.  There is no explanation of what the bug was and how the code was fixed to prevent engineers from committing the same mistake again.  We have zero information on the downtime and impact on production.  The lack of chronological records undermine the efforts to improve our response procedures and timing.  Most importantly, there is nothing detailing the investigation process. An engineer from another team reading it, is just as clueless as before; they have learnt nothing about diagnosing problems on said service.We have henceforth distilled the benchmark for Grab Engineering post-mortem reports down to 4 requirements: Chronology, Context, Empowerment, Solutions.ChronologyA timeline detailing each event is required to track the response time and downtime impact. It becomes incredibly handy in ironing out bottlenecks in our pager processes while highlighting any design flaws in the metric alerts.ContextAdequate information about the inner workings of the service should be provided. Instead of “found a bug in DB connections”, a better sentence would be:  “XXX service connects to a master DB through the use of a pool of recycled connections. Code added in commit abc1234 [link to git commit] introduced a bug where used connections were not being recycled…”Readers would then be able to read the code with a clearer understanding of how the bug was causing the production issues. We leave the amount of details to the writer’s own fuzzy discretion.EmpowermentThe report should make any engineer reading it feel empowered in helping out with future issues. We break down the approach into several components:Blameless - Reports are supposed to be beneficial to the overall ops efficiency. Nothing demoralises an engineer as quickly as having his name tagged to an issue for eternity.Educational - Reports should act as a tutorial guide to solving production problems. Most people know how to grep logs, but only those with experience know exactly what to grep. A step by step display of how problems are diagnosed and the conclusions they lead to, should be recorded.SolutionsAfter the above information has all been fleshed out, problems and bottlenecks should be listed out with possible solutions to them. We divide the problems into 3 separate sections.People - This is generally a list of communication inhibitions amongst teams. Any practice that is currently leading to potential miscommunications should be removed or improved upon.Product - Are the services not designed to be sufficiently robust? Is the amount of metric alerts and error triggers currently set up sufficient, or can we do better?Process - More than often, process problems arise when there is a flaw in how various teams approach an issue. Some examples:a. Engineer A discovers root of problem but has to await Engineer B to approve of the hotfix. However, B is unavailable, leading to unnecessary extended downtime.b. Heavy reliance on a single party to execute certain operations. Said party experiences network issues and no one else is able to help.tl;dr Here is what we believe an example template report should look like:  Post Mortem Report - 20160201  Initial Symptoms  XXX metric alert was triggered at XX:XX hours. Notifications were sent to all on-call personnel.  Timeline  10:00 - CPU Utilization hit 95%  10:01 - XXX metric alert triggered  10:02 - First on-call response acknowledges alert. Begins investigation.  .  .  .  10:05 - Issue resolved  Investigation                    Logs were grepped from example-service-2015-02-02.log with filter “error                 timeout”              2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  This indicates that the code at this part of the service is throwing a timeout error.  // code snippet goes here    Further investigation of the endpoint shows that it was refusing connections.  .  etc.  .  Solution  The issue was temporarily resolved by a rollback to version 1.2.3 at 10:05. The bug was later fixed in commit abc1234 [link to git commit]  Improvements  People  Team A realised the problem at 10:03 but felt they had not enough authority to permit a rollback of the service to version X. We should strive to improve on …  Product  The code was added to optimise processes for feature Y, but this caused a side effect where …  Process  Code was reviewed, and deployments were checked on staging servers, but due to the requirement to carry out step J, we had missed out on step K. We attribute this to …Finished reports should be peer reviewed by engineers from another team for further input and improvements before it can be considered finalised. This is to ensure the service context is adequately provided without any personal bias.By following the rules and guidelines above, we are confident that any organisation new to writing post-mortem reports should be able to write actually useful documentation, instead of producing an unwanted article of little value out of reluctant obligation.",
        "url": "/grab-you-some-post-mortem-reports"
      }
      ,
    
      "curious-case-of-the-phantom-instance": {
        "title": "The Curious Case of the Phantom Instance",
        "author": "lian-yuanlin",
        "tags": "[&quot;AWS&quot;]",
        "category": "",
        "content": "Note: Timestamps used in this article are in UTC+8 Singapore time, unless stated otherwise.Here at the Grab Engineering team, we have built our entire backend stack on top of Amazon Web Services (AWS). Over time, it was inevitable that some habits have started to form when perceiving our backend monitoring statistics.Take a look at the following Datadog (DD) dashboard we have, which monitors the number of Elastic Load Balancer (ELB) health check requests sent to our grab_attention cluster:grab_attention is the tongue in cheek name for the in-app messaging API hostname.Upon first look, the usual reflex conclusion for the step waveforms would be an Auto Scaling Group (ASG) scaling up event. After all, a new instance equates to a proportionate increase in health check requests from the ELB.According to the graph shown, the values have jumped between 48 and 72 counts/min (the dashboard above collates in 10-minute intervals). The grab_attention ASG usually consists of 2 instances. 72 / 48 = 1.5, therefore there should have been an ASG scale up event at roughly 22 Dec 2015, 1610 hours.Now, here’s the weird part. Our ASG activity history, interestingly, did not match up with the observed data:The only ASG scaling events on 22 Dec were, a scale up at 1805 hours and a scale down at 2306 hours, which explains the middle step up/down waveform.So… where are the increased step ups in health checks on the two sides (22 Dec 16:10 - 17:45 &amp; 23 Dec 05:15 - 06:50) coming from?Further probing around in CloudWatch revealed that the ElastiCache (EC) NewConnections metric for the underlying Redis cluster mirrored the health check data:Metrics in UTCThe number of new connections made to the Redis cluster jumped between 96 and 144 at the identical moments of the ELB health check jumps; this is a similar 1.5 X increase in data. This seemed to clearly indicate a third instance, but no third IP address was found in the server logs.We have on our hands a phantom instance that has been sending out ELB health check data to our DD, and creating new Redis connections that is nowhere to be found.Fortunately, the engineering team had included the instance hostnames as one of the DD tags. Applying it gave the following dashboard:Surprise! While the middle step form was clearly contributed by the third instance spun up during an ASG scaling event, it would seem that the 2 similar step forms on each side were contributed only by the 2 existing instances. ELB health check ping counts to each of the instances jumped between 24 to 36 counts/min, a 1.5X increase.AWS Support replied with the following response (UTC timestamps):  I have taken a look at your ELB and found that there was a ELB’s scaling event during the time like below. ELB Scaling up: 2015-12-22T08:07 ELB Scaling down: 2015-12-22T21:15 Basically, ELB nodes can be replaced(scaling up/down/out/in) anytime depends on your traffic, ELB nodes resource utilisation or their health. Also, to offer continuous service without an outage the procedure will be like below.  1) new ELB nodes are deployed,  2) put in a service,  3) old ELB node detached from ELB(once after the new ELB nodes are working fine)  4) old ELB node terminated  So, during the ELB scaling event, your backends could get more health checks from ELB nodes than usual and it is expected. You don’t have to worry for the increased number of health checks but when you get less number of health checks than your ELB configured value, it would be a problem. Hope this helps your concern and please let us know if you need further assistance.The 2 mentioned scaling event timestamps coincide with the 2 step graphs on both sides, one for a scale up, and one for a scale down. Each step up lasted roughly 90 minutes. It was previously presumed that an increased number of nodes in ELB scaling would explain the increase in health checks. But that would not explain the increase in health checks for a scale down event. This seemed to indicate that the number of health checks would increase regardless of whether ELB scaling up or down.Moreover, the health check interval isn’t something set to each node, but to the entire ELB itself. To top it off, why a 1.5X increase? Why not 2X or any other whole number?A brief check of our configured health check interval revealed it to be 5 seconds. Which should yield:1 min * 60 sec / 5s interval * 2 instances = 24 counts/minIf there was a 1.5X increase in counts during ELB scaling, it should have increased to a total of 36 counts/min. This did not match up to the DD dashboard metric of 48 counts/min to 72 counts/min.Another round of scrolling through the list of ELBs gave the answer. 2 ELBs are actually being used for the grab_attention ASG cluster, one for public facing endpoints, and another for internal endpoints.Embarrassingly, this had been totally forgotten about. The internal ELB was indeed configured to have a 5-second interval health check too.Therefore, the calculation should be:1 min * 60 sec / 5s interval * 2 ELBs * 2 instances = 48 counts/minA scaling event occurring on the public facing ELB had in fact, doubled the number of health check counts for periods of ~90 minutes. Due to the internal ELB health check skewing the statistics sent to DD, it seemed like a third instance was spun up.So… why did a similar graph shape appear in the EC New Connections CloudWatch metrics?This code snippet was in the health check code:if err := gredis.HealthCheck(config.GrabAttention.Redis); err != nil {  logging.Error(logTag, \"health check Redis error %v\", err)  return err}It turned out that, because each health check request opens a new ping pong connection to the Redis cluster (which didn’t use the existing Redis connection pool), the increase in the ELB health checks also led to a proportionate increase in new Redis connections.A second response from AWS Support verifies the findings:  When the ELB scales up/down, it will replace nodes with bigger/smaller nodes. The older nodes, are not removed immediately. The ELB will remove the old nodes IP addresses from the ELB DNS endpoint, to avoid clients hitting the old nodes, however we still keep the nodes running for a while in case some clients are caching the IP addresses of the old nodes. While these nodes are running they are also performing health checks, that’s why the number of sample count doubles between 21:20 to 22:40. The ELB scaled down at 21:15, adding new smaller nodes. For a period of approximately 90 minutes new and old nodes were coexisting until they were removed.In essence, contrary to a typical ASG scaling event where instances are launched or terminated, an ELB scaling event is possibly a node cluster replacement! Both clusters exist for 90 minutes before the old one gets terminated, or so it seemed. Then AWS Support replied one last time:  Scaling events do not necessarily imply node replacement. At some point the ELB could have more nodes, hence, you will have more health checks performed against your backends.ConclusionA series of coincidental ELB scaling events strictly involving node replacements had occurred, leading us to believe that a phantom instance had been spun up.What We can Learn from ThisIt might be wise to separate dependency health checks within the instances from the ELB health checks, since it actually doubles the number of requests.Don’t always assume the same, predictable graph changes are always the result of the same causes.There is always something new about ELB to be learnt.",
        "url": "/curious-case-of-the-phantom-instance"
      }
      
    
  };
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>

    </div>
    <div class="progress-wrap">
    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98" />
    </svg>
    <i class="fa fa-chevron-up btt-btn"></i>
</div>
    <footer class="site-footer">
  <div class="wrapper">
    <div class="row">
      <div class="col-sm-6 col-xs-12">
        <h2 class="footer-heading">Grab Tech</h2>
        <ul class="social-media-list">
  
    <li>
      <a href="https://github.com/grab" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-github fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://www.linkedin.com/company/grabapp" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-linkedin fa-lg"></i>
      </a>
    </li>
  
  <li>
    <a href="https://engineering.grab.com/feed.xml" target="_blank">
      <i class="fa fa-rss fa-lg"></i>
    </a>
  </li>
</ul>

        <div>
          <script src="//platform.linkedin.com/in.js" type="text/javascript"> lang: en_US</script>
          <script type="IN/FollowCompany" data-id="5382086" data-counter="right"></script>
        </div>        
        <br>
      </div>
      <div class="col-sm-6 col-xs-12 hiring-section">
        <h2 class="footer-heading">Join Us</h2>
        <p class="text">
          Want to join us in our mission to revolutionize transportation?
        </p>
        <a class="btn" href="https://grab.careers" target="_blank">View open positions</a>

      </div>
    </div>
    
  <!-- Google Tag Manager -->
  <script>
    (function (w, d, s, l, i) {
      w[l] = w[l] || [];
      w[l].push({
        'gtm.start': new Date().getTime(),
        event: 'gtm.js'
      });
      var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s),
        dl = l != 'dataLayer' ? '&l=' + l : '';
      j.async = true;
      j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
      f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-T3CT72T');
  </script>
  <!-- End Google Tag Manager -->

  <!-- Old script 
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'GTM-T3CT72T', 'auto');
    ga('send', 'pageview');
  </script> -->
<!-- End of olf script -->


  </body>
</html>
