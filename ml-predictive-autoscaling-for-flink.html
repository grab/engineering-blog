<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Machine-learning predictive autoscaling for Flink</title>
    <meta name="description" content="Explore how Grab uses machine learning to perform predictive scaling on our data processing workloads.">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Open Graph -->
    <meta property="og:url" content="https://engineering.grab.com/ml-predictive-autoscaling-for-flink">
    <meta property="og:title" content="Machine-learning predictive autoscaling for Flink">
    <meta property="og:description" content="Explore how Grab uses machine learning to perform predictive scaling on our data processing workloads.">
    <meta property="og:site_name" content="Grab Tech">
    <meta property="og:type" content="article">
    <meta property="og:image" content="https://engineering.grab.com/img/ml-predictive-autoscaling-for-flink/banner.jpg">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">

    <!-- Favicons -->
    <link rel="icon" href="/favicon.ico">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">

    <!-- CSS -->
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,400i,700,700i" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap-theme.min.css">
    <script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="https://engineering.grab.com/ml-predictive-autoscaling-for-flink">

    <!-- RSS -->
    <link rel="alternate" type="application/rss+xml" title="RSS for Official Grab Tech Blog" href="/feed.xml">
    <!-- OneTrust Cookies Consent Notice start for grab.com -->
    <script type="text/javascript" src="https://cdn-apac.onetrust.com/consent/a3be3527-7455-48e0-ace6-557ddbd506d5/OtAutoBlock.js" ></script>
    <script src="https://cdn-apac.onetrust.com/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="a3be3527-7455-48e0-ace6-557ddbd506d5" ></script>
    <script type="text/javascript">
    function OptanonWrapper() { }
    </script>
    <!-- OneTrust Cookies Consent Notice end for grab.com -->
  </head>

  <body>
    <header class="site-header">
  <div class="wrapper-navbar">
    <div class="site-title-wrapper">
      <div class="row site-title-wrapper-inner">
        <div class="col-xs-1 visible-xs hamburger-nav" id="mobile-menu-btn">
          <div class="menu-btn"></div>
        </div>
        <div class="col-sm-3 col-xs-3">
          <div class="site-title-container">
            <a class="site-title" href="/"></a>
            <span class="site-subtitle">&nbsp;Tech Blog</span>
          </div>
        </div>
        <div class="col-sm-9 col-xs-8 text-right">
          <ul class="nav-category hidden-xs">
            
              
              <li>
                <a href="/categories/engineering/">Engineering</a>
              </li>
            
              
              <li>
                <a href="/categories/data-science/">Data Science</a>
              </li>
            
              
              <li>
                <a href="/categories/design/">Design</a>
              </li>
            
              
              <li>
                <a href="/categories/product/">Product</a>
              </li>
            
              
              <li>
                <a href="/categories/security/">Security</a>
              </li>
            
          </ul>
          <div class="site-search text-right">
            <form action="/search.html" role="search" class="search-form">
  <div class="search-icon"> </div>
  <input type="search" name="q" class="search-text" placeholder="Search...">
  <button class="search-submit"><i class="fa fa-chevron-right"></i></button>
</form>
    

          </div>
        </div>
      </div>
      <!--  Only visible on mobile view -->
      <div class="mobile-menu-container">
        <ul class="mobile-menu">
          
            
            <li>
              <a href="/categories/engineering/">Engineering</a>
            </li>
          
            
            <li>
              <a href="/categories/data-science/">Data Science</a>
            </li>
          
            
            <li>
              <a href="/categories/design/">Design</a>
            </li>
          
            
            <li>
              <a href="/categories/product/">Product</a>
            </li>
          
            
            <li>
              <a href="/categories/security/">Security</a>
            </li>
          
        </ul>
      </div>
    </div>
  </div>
</header>
<script src="/js/main.js"></script>

    <div class="page-content">
      
<div class="wrapper">
  <div class="post">
    <header class="post-header">
      <img src="/img/ml-predictive-autoscaling-for-flink/banner.jpg" class="post-cover-photo" alt="Machine-learning predictive autoscaling for Flink cover photo">
      
        
          <a class="post-category" href="/categories/engineering/">Engineering </a>
      
        
          &middot;
        
          <a class="post-category" href="/categories/data/">Data </a>
      

      <h1 class="post-title">Machine-learning predictive autoscaling for Flink</h1>
      
      <div class="post-meta">
        <div class="row">
          <div class="col-xs-12">
            <div class="post-author-thumbnail-container">
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/nhat-nguyen.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/shikai-ng.png">
                
              
                
                
                  <img class="post-author-thumbnail-large img-circle" src="/img/authors/calvin-tran.png">
                
              
            </div>

            <div class="post-meta-text-container">
              <span class="post-author-large">
                
                  
                  
                    
                    <a href="/authors#nhat-nguyen">Minh Nhat Nguyen</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#shikai-ng">Shi Kai Ng</a>
                  
                
                  
                  
                    
                      &middot;
                    
                    <a href="/authors#calvin-tran">Calvin Tran</a>
                  
                
              </span>
              <span class="post-date-large">30 Oct 2025 | 13 min read</span>
            </div>
          </div>
        </div>
      </div>

    </header>
    <div class="wrapper-content">
      <article class="post-content">
        <h2 id="introduction">Introduction</h2>

<p>As Grab transitions to derive more valuable insights from our wealth of operational data, we are witnessing a steep increase in stream-processing applications. Over the past year, the number of Flink applications grew 2.5 times, driven by interest in real-time stream processing and the improved accessibility of developing such applications with Flink SQL. At this scale, it has become crucial for the internal Flink platform team to provide a <strong>cost-effective</strong> and <strong>self-service</strong> offering that supports users of diverse backgrounds.</p>

<h2 id="background-flink-at-grab">Background: Flink at Grab</h2>

<p>Flink at Grab is deployed in application mode, each pipeline has its own isolated resources for JobManager and TaskManager. Flink pipeline creators control both application logic and deployment configuration that affect throughput and performance, including OSS configurations:</p>
<ul>
  <li>Number of TaskManagers and task slots per TaskManager</li>
  <li>CPU cores per TaskManager</li>
  <li>Memory per TaskManager</li>
</ul>

<p>As pipeline creation has become more accessible, users of different backgrounds (analyst, data scientist, engineers, etc.) often struggle to choose a set of configurations that work for their applications. Many go through a long process of trial and error and still end up over-provisioning their applications, leading to huge resource waste. Moreover, pipeline behavior changes over time due to changes in application logic or data pattern, invalidating previous efforts in tuning and causing users to repeat the exercise.</p>

<p>In this article, we focus on addressing the challenge of efficient CPU provisioning for TaskManagers, as CPU constraints are a common bottleneck in our clusters. Our solution specifically targets Flink applications sourcing data from our message bus system (eg. Kafka, Change Data Capture Streams, DynamoDB Streams) , which represents the majority of our use cases. These workloads offer significant opportunities for cost savings due to their clear seasonal patterns, making them an ideal starting point for optimising autoscaling strategies.</p>

<h2 id="limits-of-reactive-autoscaling">Limits of reactive autoscaling</h2>

<h3 id="our-initial-reactive-setup">Our initial reactive setup</h3>

<p>Our first automated solution relied on Flink’s Adaptive Scheduler in Reactive Mode. In this mode, each Flink application is deployed as its own individual Flink cluster running a dedicated job. The cluster greedily uses all available TaskManagers and scales its job parallelism accordingly. Running on Kubernetes, the cluster relies on Horizon Pod Autoscaler (HPA) to scale the number of TaskManager pods based on metrics such as CPU usage or custom metrics such as the pipeline’s consumer latency. While this solution was helpful initially, we quickly observed multiple issues with it.
It is important to note that while the below issues can be solved by fine-tuning, it is a tedious trial and error effort that only works for specific applications, requiring users to repeat the process for every pipeline they own.</p>

<h3 id="restart-spike-root-cause-of-many-issues">Restart spike: root cause of many issues</h3>

<p>When autoscaling a Flink pipeline, the job restarts from the last checkpoint. This triggers an immediate spike in load, as the pipeline must reprocess records from the period between the last checkpoint and job restart, along with any new records that were backlogged at the source during the downtime. As a result, CPU usage and P99 consumer latency  typically spikes after scaling events, for example, at 00:05 and 00:55, as shown in Figure 1. These spikes occur even though there is no change in source topic throughput. In this case, CPU usage surges from 0.5 cores to near provision limit of 2.5 cores,  while consumer latency temporarily spiked from sub-second levels to as high as three minutes.</p>

<div class="post-image-section"><figure>
  <img src="/img/ml-predictive-autoscaling-for-flink/cpu-usage-con-latency-after-restart.png" alt="" style="width:70%" /><figcaption align="middle">Figure 1: CPU usage and consumer latency spike after a pipeline restart.</figcaption>
  </figure>
</div>

<h3 id="reactive-spiral-and-fluctuation">Reactive spiral and fluctuation</h3>

<p>Typically, HPA scales on metrics such as CPU usage, consumer latency, or backpressure crossing a defined threshold. The challenge arises if these thresholds are misconfigured. The HPA’s reactive nature, when combined with restart spikes, can become detrimental to your Flink application. It piles additional load onto a system that’s already degrading, further amplifying the problem.</p>

<div class="post-image-section"><figure>
  <img src="/img/ml-predictive-autoscaling-for-flink/reactive-scaling-fluctuation.png" alt="" style="width:70%" /><figcaption align="middle">Figure 2: A reactive scaling incident that demonstrates scaling fluctuations and restarts.</figcaption>
  </figure>
</div>

<p>Figure 2 provides us a case study of reactive spiral and fluctuation, assuming we are having a pipeline that consumes a Kafka topic of 300 partitions:</p>
<ul>
  <li>07:00: As the source topic throughput increases, the P99 consumer latency rises due to insufficient processing power.</li>
  <li>07:15: Reactive scaling is triggered, resulting in a scale out event. This is reflected in the increased TaskManager and task slot count. The pipeline continues to operate, as there is no increase in restart count.</li>
  <li>07:30: As the P99 consumer latency remains high, reactive scaling continues to scale out incrementally. The records in rate by task rises rapidly as the pipeline reprocesses data from the checkpoint. During this period, the pipeline repeatedly restarts CPU usage drops significantly, and P99 consumer latency spikes to nearly one hour. This marks the onset of a spiral failure.</li>
  <li>08:00: Reactive scaling reaches its upper limit of 300 slots, corresponding to the number of partitions in the source topic. This halts the spiral effect as it cannot scale out any further. Without disruption from autoscaling restart, the pipeline begins to process the backlog since the last successful checkpoint, as observed by the significant increase in records in rate by task. As the pipeline catches up, it eventually stabilizes, and the P99 consumer latency returns to normal levels.</li>
  <li>08:30 - 10:15: The P99 consumer latency returns to normal levels, below the threshold. Reactive scaling triggers scale-in events despite the source topic throughput continuing to trend upward. During these scale-in events, P99 latency fluctuates, occasionally spiking up to 15 minutes. However, these fluctuations are not severe enough to prevent the repeated scale in process.</li>
  <li>10:15: The P99 consumer latency rises again, triggering a scale-out event back to the upper limit of 300 slots.</li>
  <li>11:15-11:45: Despite the source topic throughput maintaining an upward trend, the pipeline undergoes multiple scale-in events in quick succession, encounters latency issues due to reprocessing data from checkpoints, and scales out again shortly after. This is an example of fluctuation after scaling in, resulting in 6 restarts within a 30 minutes window.</li>
</ul>

<h3 id="limited-parallelism-constraints">Limited parallelism constraints</h3>

<p>Even with HPA, we frequently encounter a bottleneck when trying to scale our applications’ throughput. This is primarily because some of our connectors, most notably the Kafka connector, don’t inherently support dynamic parallelism changes.
Kafka topics, by design, have a fixed number of partitions. This directly limits the number of parallel consumers we can run. Consequently, once we reach this maximum parallelism for our consumers, we often have to scale up resources, for example, increase memory/CPU per instance instead of scaling out (adding more instances).</p>

<h2 id="predictive-resource-advisor">Predictive Resource Advisor</h2>

<h3 id="assumptions-and-hypothesis">Assumptions and hypothesis</h3>

<p>To tackle the issue of reactive spirals and fluctuations, the new solution should have the following characteristics:</p>

<ul>
  <li>Vertical scaling: To tackle the issue of limited parallelism with our dependencies, we should be looking at vertical instead of horizontal scaling.</li>
  <li>Predictive: Adjust CPU to scale up or down before demand spikes or dips occur, ensuring the system is prepared for changes in workload. This prevents artificial workload increases caused by processing backlogs on top of actual workload increase, further straining the system.</li>
  <li>Deterministic: The CPU configuration must be precisely calculated based on the workload demand, ensuring predictable and consistent resource allocation. For a given workload, the calculated CPU value should remain the same every time, eliminating variability and uncertainty in scaling decisions.</li>
  <li>Accurate: Determine the optimal CPU configuration required to handle workload demand in a single, precise calculation, avoiding the inefficiencies of multi-step, trial-and-error tuning.</li>
</ul>

<h3 id="key-observations">Key observations</h3>

<p>Our solution is conceptualized based on key observations of our Flink applications:</p>
<ol>
  <li>The CPU usage of Flink applications is primarily driven by the input load.</li>
  <li>The input load of our Flink applications can be accurately forecasted using time-series forecasting techniques.</li>
  <li>Time-based autoscaling that relies solely on historical CPU usage is not robust enough to adapt to evolving workloads. This approach also carries the risk of a negative self-amplifying feedback loop: each autoscaling restart causes a CPU usage spike (as illustrated in Figure 1), which, if anomalies are not properly handled, inflates subsequent CPU calculations.</li>
</ol>

<h3 id="model-formulation">Model formulation</h3>

<p>We then formulate the relationship between CPU usage and input load using a regression model to provide a mathematical framework for predicting CPU requirements based on workload patterns, expressed as:</p>

<p style="text-align:center; font-weight:bold;">C<sub>t</sub> = f(x<sub>t</sub>)</p>

<p>In this equation:</p>

<ul>
  <li><strong>C<sub>t</sub></strong> represents the CPU required at a specific point in time.</li>
  <li><strong>x<sub>t</sub></strong> represents the input workload at the corresponding point in time.</li>
  <li><strong>f()</strong> represents the regression function that maps the input load to the required CPU capacity.</li>
</ul>

<p>Input load, represented by Kafka source topic throughput in our case, is chosen as the independent variable <strong>x<sub>t</sub></strong> because it reflects true business demand and is entirely independent of Flink consumers. This metric is influenced solely by the business logic of upstream producers and remains unaffected by any changes or behaviors in the Flink consumer pipeline.</p>

<h3 id="proposed-solution">Proposed solution</h3>

<p>Our predictive autoscaler operates through four key stages as shown in Figure 3.</p>

<div class="post-image-section"><figure>
  <img src="/img/ml-predictive-autoscaling-for-flink/predictive-autosclaing-flow.png" alt="" style="width:70%" /><figcaption align="middle">Figure 3: The predictive autoscaling system operates through four key stages.</figcaption>
  </figure>
</div>

<p><strong>Stage 1: Workload forecast model</strong></p>

<p>The workload forecast model is a time-series forecasting model trained on actual workload data, specifically source topic throughput from our Kafka cluster (1). This approach is particularly effective as our workload exhibits seasonal patterns. While historical data could be directly used as input for CPU prediction, time-series forecasting offers a more robust solution by enabling the model to account for organic traffic growth over time. Through periodic retraining, the model adapts to evolving workload trends, ensuring more accurate and reliable predictions for resource provisioning.</p>

<p><strong>Stage 2: Resource prediction model</strong></p>

<p>This follows the regression-based model <strong>C<sub>t</sub> = f(x<sub>t</sub>)</strong> defined earlier. We use the same source topic throughput from our Kafka cluster (2a) as  input feature <strong>x<sub>t</sub></strong>, and  the Flink application’s Kubernetes CPU usage metric (2b) as output label <strong>C<sub>t</sub></strong> for model training. To ensure clean and representative data for model training, we collect CPU usage metrics under conditions that simulate infinite resource availability. We include data  exclusively from periods of continuous and stable operation, as determined by latency, uptime, and restart metrics (2b), eliminating biases caused by hardware limitations or disruptions.</p>

<p><strong>Stage 3: Workload forecasting</strong></p>

<p>To prepare for autoscaling, we forecast the workload for the future t-hour window (3) using our trained time-series forecast model.</p>

<p><strong>Stage 4: Predict CPU usage</strong></p>

<p>The forecasted workload (3) is fed into the resource prediction model to estimate the CPU usage required to handle that workload. The predicted value is then refined using custom safety feature adjustments to account for variability and ensure stability. This adjusted prediction is passed to the custom autoscaler controller, which evaluates the current CPU configuration of the TaskManager deployment. If the adjusted predicted value differs from the existing CPU configuration, the controller initiates vertical scaling to update the TaskManager deployment accordingly.</p>

<h2 id="proof-of-concept-and-results">Proof of concept and results</h2>

<h3 id="experiment-setup">Experiment setup</h3>

<p>To validate our hypothesis, we present a deep dive into one of our experiments. This pipeline features complex business logic, aggregates from multiple Kafka sources, with a checkpoint interval of one minute and a maximum consumer latency of five minutes.</p>

<p>We set up an experimental pipeline with configurations identical to the production pipeline (the control). Both applications sourced data from the same Kafka topics but sank data to alternative topics to maintain isolation. The Predictive Resource Advisor was enabled on the experimental pipeline, while the control pipeline operated with fixed CPU provisioning.</p>

<h3 id="results">Results</h3>

<p>Figure 4 demonstrates a strong correlation between CPU usage (yellow, green) and the total Kafka topics throughput. The variable CPU provisioning (blue) for the experimental pipeline is calculated by our autoscaler models, which were trained exclusively on data collected from the experiment pipeline. The CPU usage trend of the experimental pipeline closely mirrors that of the control pipeline and remains aligned with the Kafka throughput trend. However, the experimental pipeline’s CPU provisioning is dynamically adjusted to more closely match its actual CPU usage, whereas the control pipeline maintains a static CPU allocation (purple). This illustrates the model’s effectiveness in dynamically adjusting CPU allocation to meet variable workload demands.</p>

<div class="post-image-section"><figure>
  <img src="/img/ml-predictive-autoscaling-for-flink/cpu-usage-source-throughput.png" alt="" style="width:70%" /><figcaption align="middle">Figure 4: CPU usage closely correlates with source throughput for both the experimental and control pipelines.</figcaption>
  </figure>
</div>

<p>Without autoscaler enabled, the control pipeline experienced no disruptions and maintained latency (blue) consistently below one second, which is not visible in Figure 5. On the other hand, the experiment pipeline latency (red) experienced a highest recorded peak latency of just over four minutes during a single disruption window. Other latency spikes observed were comparable to or lower than the three minutes peak latency previously identified as part of the restart spike issue analysis. The varied durations and amplitudes of these spikes showed some correlation with the heavy Kafka topic throughput during those periods. Importantly, there were only nine autoscaling events throughout the day, resulting in nine restarts for the experiment pipeline.</p>

<div class="post-image-section"><figure>
  <img src="/img/ml-predictive-autoscaling-for-flink/impact-autoscaling-sla.png" alt="" style="width:70%" /><figcaption align="middle">Figure 5: Autoscaling impacts service-level agreement requirements through latency spikes during scaling events.</figcaption>
  </figure>
</div>

<h3 id="outcome">Outcome</h3>

<p>The Predictive Resource Advisor solution has been successfully deployed across more than 50% of applicable production applications, specifically those consuming from Kafka topics and exhibiting seasonal workload patterns with some tolerance for disruptions. This implementation has delivered significant results across three key areas, stability, efficiency, and user experience.</p>

<h4 id="stability">Stability</h4>

<p>With autoscaling becoming more predictable and controllable, our Flink applications experience fewer disruptions caused by autoscaling fluctuations. The machine learning and predictive capabilities of the solution also ensure that applications remain operational during periods of increased workload by automatically learning and adapting to organic growth trends and workload surges.</p>

<h4 id="efficiency">Efficiency</h4>

<p>Applications powered by the Predictive Resource Advisor demonstrated significant improvements in CPU provisioning, aligning CPU configuration more closely with actual requirements, particularly during low traffic periods. As a result of this optimization, on average, these applications made approximately &gt;35% savings in cloud infrastructure cost.</p>

<h4 id="user-experience">User experience</h4>

<p>The solution has simplified the deployment process for users, allowing them to simply deploy Flink applications with default configurations. The Predictive Resource Advisor automatically collects data, trains autoscaling models, and applies configuration changes, thus eliminating the need for manual fine-tuning. This significantly enhances the user experience by streamlining pipeline maintenance and enabling self-service capabilities, such as effortless onboarding. It empowers users to explore and derive value from real-time features with minimal effort.</p>

<h2 id="whats-next">What’s next?</h2>

<p>Our journey doesn’t stop here. We’re continuously working to enhance our predictive autoscaler, with the following key areas of focus:</p>

<ul>
  <li><strong>Tackling memory configuration (Predictive Resource Advisor’s next frontier)</strong> <br />
Memory is critical yet often misconfigured that can lead to unrecoverable failures for example, OOMKilled. Our next major goal for the Predictive Resource Advisor is to take on memory tuning, completely removing the burden of complex memory configuration from our users and further empowering them.</li>
  <li><strong>Enhancing model accuracy</strong> <br />
To further improve the robustness of our predictions, we are actively exploring advanced techniques in input feature engineering and anomaly detection, especially for workloads exhibiting frequent bursting patterns. By refining these aspects, we aim to extend the applicability of our solution to a broader range of Flink applications, including those connected to diverse sources such as change data capture systems or batch-like, spiky workloads, such as the Flink applications powering our real-time data lake.</li>
  <li><strong>Streamlining model training</strong> <br />
We’re developing a more efficient model training workflow. A particularly exciting avenue we’re investigating is the use of pretrained time-series forecasting models based on large language model architectures.</li>
</ul>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/resource-providers/standalone/kubernetes/#application-mode">Flink deployed in Application Mode</a></li>
  <li><a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/elastic_scaling/#reactive-mode">Flink Elastic Scaling in Reactive Mode</a></li>
  <li><a href="https://www.usenix.org/system/files/osdi18-kalavri.pdf">Three steps is all you need: fast, accurate, automatic scaling decisions for distributed streaming dataflows</a></li>
</ul>

<h2 id="join-us">Join us</h2>

<p>Grab is a leading superapp in Southeast Asia, operating across the deliveries, mobility and digital financial services sectors. Serving over 800 cities in eight Southeast Asian countries, Grab enables millions of people everyday to order food or groceries, send packages, hail a ride or taxi, pay for online purchases or access services such as lending and insurance, all through a single app. Grab was founded in 2012 with the mission to drive Southeast Asia forward by creating economic empowerment for everyone. Grab strives to serve a triple bottom line – we aim to simultaneously deliver financial performance for our shareholders and have a positive social impact, which includes economic empowerment for millions of people in the region, while mitigating our environmental footprint.</p>

<p>Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, <a href="https://grb.to/gebmlflink">join our team</a> today!</p>

      </article>
      <div>
        
          <div class="post-tags">
  
  
    <a href="/tags#data-science" class="label tags-label">data-science</a>
  
    <a href="/tags#engineering" class="label tags-label">engineering</a>
  
    <a href="/tags#performance" class="label tags-label">performance</a>
  
</div>

        
        <br>
      </div>
      <div class="sharing-links text-right">
  Share on &nbsp;
  <a href="https://twitter.com/intent/tweet?text=Machine-learning predictive autoscaling for Flink&url=https://engineering.grab.com/ml-predictive-autoscaling-for-flink&via=grabengineering&related=grabengineering" class="btn btn-sm btn-share btn-share-twitter" rel="nofollow" target="_new" title="Share on Twitter" onclick="onShareButtonClick(this); return false;"><i class="fa fa-lg fa-twitter"></i>&nbsp; Twitter</a>
  <a href="https://facebook.com/sharer.php?u=https://engineering.grab.com/ml-predictive-autoscaling-for-flink" class="btn btn-sm btn-share btn-share-facebook" rel="nofollow" target="_new" title="Share on Facebook" onclick="onShareButtonClick(this); return false;"><i class="fa fa-lg fa-facebook"></i>&nbsp; Facebook</a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://engineering.grab.com/ml-predictive-autoscaling-for-flink&title=Machine-learning predictive autoscaling for Flink
&summary=Explore how Grab uses machine learning to perform predictive scaling on our data processing workloads.&source=Grab Tech" class="btn btn-sm btn-share btn-share-linkedin" rel="nofollow" target="_new" title="Share on LinkedIn" onclick="onShareButtonClick(this); return false;"><i class="fa fa-lg fa-linkedin"></i>&nbsp; LinkedIn</a>
</div>
<script>
  function onShareButtonClick(button) {
    var width = 600;
    var height = 600;
    var left = (window.screen.width / 2) - (width / 2);
    var top = (window.screen.height / 2) - (height / 2);
    window.open(button.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=' + height + ',width=' + width + ',top=' + top + ',left=' + left);
    return false;
  }
</script>

      <hr class="section-divider">

      <br/>
      <!-- 
        <div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    this.page.url = 'https://engineering.grab.com/ml-predictive-autoscaling-for-flink';
    this.page.identifier = '/ml-predictive-autoscaling-for-flink';
  };
  (function() {
    var d = document, s = d.createElement('script');
    s.src = '//grabengineering.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

       -->

    </div>
  </div>
</div>

    </div>
    <div class="progress-wrap">
    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98" />
    </svg>
    <i class="fa fa-chevron-up btt-btn"></i>
</div>
    <footer class="site-footer">
  <div class="wrapper">
    <div class="row">
      <div class="col-sm-6 col-xs-12">
        <h2 class="footer-heading">Grab Tech</h2>
        <ul class="social-media-list">
  
    <li>
      <a href="https://github.com/grab" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-github fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://www.linkedin.com/company/grabapp" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-linkedin fa-lg"></i>
      </a>
    </li>
  
  <li>
    <a href="https://engineering.grab.com/feed.xml" target="_blank">
      <i class="fa fa-rss fa-lg"></i>
    </a>
  </li>
</ul>

        <div>
          <script src="//platform.linkedin.com/in.js" type="text/javascript"> lang: en_US</script>
          <script type="IN/FollowCompany" data-id="5382086" data-counter="right"></script>
        </div>        
        <br>
      </div>
      <div class="col-sm-6 col-xs-12 hiring-section">
        <h2 class="footer-heading">Join Us</h2>
        <p class="text">
          Want to join us in our mission to revolutionize transportation?
        </p>
        <a class="btn" href="https://grab.careers" target="_blank">View open positions</a>

      </div>
    </div>
    
  <!-- Google Tag Manager -->
  <script>
    (function (w, d, s, l, i) {
      w[l] = w[l] || [];
      w[l].push({
        'gtm.start': new Date().getTime(),
        event: 'gtm.js'
      });
      var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s),
        dl = l != 'dataLayer' ? '&l=' + l : '';
      j.async = true;
      j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
      f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-T3CT72T');
  </script>
  <!-- End Google Tag Manager -->

  <!-- Old script 
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'GTM-T3CT72T', 'auto');
    ga('send', 'pageview');
  </script> -->
<!-- End of olf script -->


  </body>
</html>
